<div id=toc></div>

# Table of Contents

- [physics.ao-ph](#physics.ao-ph) [Total: 3]
- [cs.NE](#cs.NE) [Total: 8]
- [cs.CV](#cs.CV) [Total: 156]
- [cs.AI](#cs.AI) [Total: 55]


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [1] [Exploring Multimodal AI Reasoning for Meteorological Forecasting from Skew-T Diagrams](https://arxiv.org/abs/2508.12198)
*ChangJae Lee,Heecheol Yang,Jonghak Choi*

Main category: physics.ao-ph

TL;DR: 这篇论文提出了一种轻量级的多模态AI助手，通过细调小型语言模型和视觉-语言模型来解释天气音响图，达到了与操作数值预报模型相当的预报技能。


<details>
  <summary>Details</summary>
Motivation: 天气音响图预报是气象预报的基础任务，但现有的视觉-语言模型在气象图解释方面应用不够。需要开发计算效率高的解释性模型来支持天气预报任务。

Method: 使用课程学习框架，先训练模型识别天气特征，然后进行链式思绪推理估算降雨概率。输入包括文本摘要或生成的Skew-T图。

Result: 细调后的VLM达到了与操作NWP模型相当的技能，虽然仅依赖静态天气配置。视觉基础和推理监督对性能至关重要。

Conclusion: 该方法展示了细调多模态模型在天气预报中的潜力，提供了计算效率高的替代方案，并可扩展到更复杂的应用。

Abstract: Forecasting from atmospheric soundings is a fundamental task in operational
meteorology, often requiring structured visual reasoning over Skew-T log-P
diagrams by human forecasters. While recent advances in Vision-Language Models
(VLMs) have shown promise in other scientific domains, their application to
meteorological diagram interpretation remains largely unexplored. In this
study, we present a lightweight AI assistant that interprets Skew-T diagrams
using a small language model (LM) and a small VLM fine-tuned to emulate human
forecasters. Using a curriculum learning framework, we first train the models
to identify key atmospheric features from diagrams through visual question
answering, followed by chain-of-thought reasoning tasks that estimate
precipitation probability based on the derived visual groundings. Model inputs
include either textual summaries or generated Skew-T diagrams derived from
operational Numerical Weather Prediction (NWP) forecasts, paired with
three-hour precipitation observations from South Korea's Auto Weather Stations
network. Evaluation results demonstrate that the fine-tuned VLM achieves skill
comparable to an operational NWP model, despite relying solely on static
atmospheric profiles. Ablation studies reveal that visual grounding and
reasoning supervision are critical for performance, while attention map
analysis confirms that the model learns to focus on relevant meteorological
features. These findings highlight the potential of compact, interpretable
multimodal models to support weather forecasting tasks. The approach offers a
computationally efficient alternative to large-scale systems, and future work
could extend it to more complex applications.

</details>


### [2] [Dynamic Forcing Behind Rapid Intensification of Hurricane Lidia](https://arxiv.org/abs/2508.12481)
*Mauricio López-Reyes,M. L. Martín-Pérez,C. Calvo-Sancho,J. J. González-Alemán*

Main category: physics.ao-ph

TL;DR: 这篇论文研究了北太平洋台风Lidia的快速加强过程，发现上层港沟通过增强天气升函、高层敦度和激子通量聚集来促进台风快速加强，为数据稀缩地区提供了低成本的预报框架。


<details>
  <summary>Details</summary>
Motivation: 研究北太平洋东北部区域台风快速加强的动力学机制，特别是台风与上层港沟的相互作用，以洞子运行模型访问有限的地区提供预护工具。

Method: 使用IFS-ECMWF集合预报和ERA5再分析数据，分析大规模动力学机制，对比高强度集合与低强度集合的差异。

Result: 港沟在快速加强前产生更强的Trenberth迫势，触发潜热释放并改善上层潜温结构，减少垂直风切变。弱迫势集合则显示更高的风切变和缺乏持续通风。

Conclusion: 识别早期动力学触发因素对快速加强预护至关重要，集合基础诊断方法为数据稀缩地区提供了成本效益高的预报框架。

Abstract: This study examines Hurricane Lidia rapid intensification (RI) in the
understudied northeastern Pacific, focusing on its interaction with an
upper-level trough. Using IFS-ECMWF ensemble forecasts and ERA5 reanalysis, we
analyze the large-scale dynamical mechanisms driving Lidias intensification.
Results show that the trough played a crucial role in promoting RI by enhancing
synoptic-scale ascent, upper-level divergence, and eddy flux convergence. In
the higher-intensification ensemble group, stronger Trenberth forcing emerged
prior to RI onset, suggesting a causative role in preconditioning the storm
environment. This dynamical forcing likely triggered latent heat release, which
in turn modified the upper-level potential vorticity structure and contributed
to a subsequent reduction in vertical wind shear. In contrast, the
lower-intensification group exhibited weaker forcing, higher shear, and a lack
of sustained ventilation. These findings highlight the importance of diagnosing
early dynamical triggers for RI, particularly in regions where operational
access to high-resolution models is limited. This approach provides a
cost-effective framework for anticipating RI using ensemble-based diagnostics
and could serve as a valuable forecasting tool in data-sparse areas such as the
Pacific coast of Mexico. Future studies should combine this large-scale
methodology with high-resolution simulations to better capture storm-scale
processes and validate multi-scale interactions in RI events.

</details>


### [3] [Rare event sampling for moving targets: extremes of temperature and daily precipitation in a general circulation model](https://arxiv.org/abs/2508.13120)
*Justin Finkel,Paul A. O'Gorman*

Main category: physics.ao-ph

TL;DR: 提出了TEAMS算法，通过提前分裂时间参数优化罕见事件采样，在理想化大气环流模型中实现5-10倍加速，仅用20年模拟就能估算100-150年重现期的极端降水温度事件


<details>
  <summary>Details</summary>
Motivation: 极端天气事件模拟成本高昂，传统方法需要数百年才能采样到百年一遇事件。中纬度风暴带的突发性极端事件（如强降水和热浪）由于持续时间短，传统罕见事件采样方法难以有效处理

Method: 扩展标准罕见事件算法TEAMS（trying-early adaptive multilevel splitting），利用提前分裂时间参数，基于集合离散率诊断来优化采样过程，在理想化大气环流模型中进行测试

Result: 算法实现了5-10倍的加速效果，仅用20年模拟就能准确估算100-150年重现期的极端事件，显著降低了计算成本

Conclusion: TEAMS算法为在计算资源受限的复杂模型中加速各种物理灾害风险评估提供了有前景的解决方案

Abstract: Extreme weather events epitomize high cost: to society through their physical
impacts, and to computer servers that are used to simulate them to provide
information to mitigate those impacts. It costs hundreds of years to sample a
few once-per-century events with straightforward model integration, but that
cost can be much reduced with rare event sampling, which nudges ensembles of
simulations to convert moderate events to severe ones, e.g., by steering a
cyclone directly through a region of interest. With proper statistical
accounting, rare event algorithms can provide quantitative climate risk
assessment at reduced cost. But this can only work if ensemble members diverge
fast enough. Sudden, transient events characteristic of Earth's midlatitude
storm track regions, such as heavy precipitation and heat extremes, pose a
particular challenge because they come and go faster than an ensemble can
explore the possibilities. Here we extend standard rare event algorithms to
handle this challenging case in an idealized atmospheric general circulation
model, achieving 5-10 times sped-up estimation of long return periods, such as
100-150 years from only 20 years of simulation for extremes of daily
precipitation and surface temperature. The algorithm, called TEAMS
(``trying-early adaptive multilevel splitting''), was developed previously in
Finkel and O'Gorman (2024) using a toy chaotic system, and relies on a key
parameter -- the advance split time -- which may be estimated based on simple
diagnostics of ensemble dispersion rates. The results are promising for
accelerated risk assessment across a wide range of physical hazards using more
realistic and complex models with acute computational constraints.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [4] [Toward Practical Equilibrium Propagation: Brain-inspired Recurrent Neural Network with Feedback Regulation and Residual Connections](https://arxiv.org/abs/2508.11659)
*Zhuo Liu,Tao Chen*

Main category: cs.NE

TL;DR: 提出了一种生物启发的反馈调节残差循环神经网络(FRE-RNN)，解决了平衡传播(EP)算法的不稳定性和高计算成本问题，使EP在大规模网络中具有实用性


<details>
  <summary>Details</summary>
Motivation: 脑启发智能系统需要脑启发的学习方法。平衡传播(EP)是一种生物可信的学习框架，但现有实现存在不稳定性和过高计算成本的问题

Method: 设计生物启发的反馈调节残差循环神经网络(FRE-RNN)，通过反馈调节降低谱半径实现快速收敛，残差连接缓解深度RNN中的梯度消失问题

Result: 将EP的计算成本和训练时间降低了数量级，在基准任务中达到与反向传播(BP)相当的性能

Conclusion: 该方法显著增强了EP在大规模网络中的适用性和实用性，为物理神经网络中的原位学习实现提供了指导

Abstract: Brain-like intelligent systems need brain-like learning methods. Equilibrium
Propagation (EP) is a biologically plausible learning framework with strong
potential for brain-inspired computing hardware. However, existing
im-plementations of EP suffer from instability and prohibi-tively high
computational costs. Inspired by the structure and dynamics of the brain, we
propose a biologically plau-sible Feedback-regulated REsidual recurrent neural
network (FRE-RNN) and study its learning performance in EP framework. Feedback
regulation enables rapid convergence by reducing the spectral radius. The
improvement in con-vergence property reduces the computational cost and
train-ing time of EP by orders of magnitude, delivering perfor-mance on par
with backpropagation (BP) in benchmark tasks. Meanwhile, residual connections
with brain-inspired topologies help alleviate the vanishing gradient problem
that arises when feedback pathways are weak in deep RNNs. Our approach
substantially enhances the applicabil-ity and practicality of EP in large-scale
networks that un-derpin artificial intelligence. The techniques developed here
also offer guidance to implementing in-situ learning in physical neural
networks.

</details>


### [5] [Learning Internal Biological Neuron Parameters and Complexity-Based Encoding for Improved Spiking Neural Networks Performance](https://arxiv.org/abs/2508.11674)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.NE

TL;DR: 提出用概率元神经元替代传统感知器神经元，结合Lempel-Ziv复杂度构建新型SNN分类框架，训练效率最高提升11%


<details>
  <summary>Details</summary>
Motivation: 传统SNN神经元模型参数学习有限，需要更生物启发的神经元模型和分类框架来提升时空神经数据分类性能

Method: 使用概率元神经元替代LIF神经元，结合LZC复杂度构建分类框架，采用BP、STDP、Tempotron等多种学习算法，用泊松过程模拟神经元发放

Result: 根据不同训练方法，分类器效率最高可提升11.00%，证明学习额外神经元参数的优势

Conclusion: 概率元神经元模型和LZC-SNN结合框架能有效提升SNN分类性能，为时空神经数据分析提供新途径

Abstract: This study introduces a novel approach by replacing the traditional
perceptron neuron model with a biologically inspired probabilistic meta neuron,
where the internal neuron parameters are jointly learned, leading to improved
classification accuracy of spiking neural networks (SNNs). To validate this
innovation, we implement and compare two SNN architectures: one based on
standard leaky integrate-and-fire (LIF) neurons and another utilizing the
proposed probabilistic meta neuron model. As a second key contribution, we
present a new biologically inspired classification framework that uniquely
integrates SNNs with Lempel-Ziv complexity (LZC) a measure closely related to
entropy rate. By combining the temporal precision and biological plausibility
of SNNs with the capacity of LZC to capture structural regularity, the proposed
approach enables efficient and interpretable classification of spatiotemporal
neural data, an aspect not addressed in existing works. We consider learning
algorithms such as backpropagation, spike-timing-dependent plasticity (STDP),
and the Tempotron learning rule. To explore neural dynamics, we use Poisson
processes to model neuronal spike trains, a well-established method for
simulating the stochastic firing behavior of biological neurons. Our results
reveal that depending on the training method, the classifier's efficiency can
improve by up to 11.00%, highlighting the advantage of learning additional
neuron parameters beyond the traditional focus on weighted inputs alone.

</details>


### [6] [Adaptive Spiking with Plasticity for Energy Aware Neuromorphic Systems](https://arxiv.org/abs/2508.11689)
*Eduardo Calle-Ortiz,Hui Guan,Deepak Ganesan,Phuc Nguyen*

Main category: cs.NE

TL;DR: ASPEN是一种新型的神经形态系统能量感知技术，通过随机扰动神经元阈值来减少脉冲活动，从而显著降低能量消耗，同时保持与最先进方法相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 探索神经形态计算在可穿戴设备中的可行性，开发适用于资源受限设备的自适应脉冲技术，解决始终开启应用中能量受限的问题。

Method: 利用训练期间对神经元阈值的随机扰动，增强网络鲁棒性，作为正则化器减少脉冲活动，无需复杂重新训练或剪枝即可实现能量控制。

Result: 在神经形态仿真器和硬件上的评估显示，ASPEN显著减少了脉冲计数和能量消耗，同时保持与最先进方法相当的准确性。

Conclusion: ASPEN提供了一种轻量级且可扩展的动态能量控制技术，无需重新配置整个模型，为资源受限的始终开启应用带来了突破性解决方案。

Abstract: This paper presents ASPEN, a novel energy-aware technique for neuromorphic
systems that could unleash the future of intelligent, always-on,
ultra-low-power, and low-burden wearables. Our main research objectives are to
explore the feasibility of neuromorphic computing for wearables, identify open
research directions, and demonstrate the feasibility of developing an adaptive
spiking technique for energy-aware computation, which can be game-changing for
resource-constrained devices in always-on applications. As neuromorphic
computing systems operate based on spike events, their energy consumption is
closely related to spiking activity, i.e., each spike incurs computational and
power costs; consequently, minimizing the number of spikes is a critical
strategy for operating under constrained energy budgets. To support this goal,
ASPEN utilizes stochastic perturbations to the neuronal threshold during
training to not only enhance the network's robustness across varying
thresholds, which can be controlled at inference time, but also act as a
regularizer that improves generalization, reduces spiking activity, and enables
energy control without the need for complex retraining or pruning. More
specifically, ASPEN adaptively adjusts intrinsic neuronal parameters as a
lightweight and scalable technique for dynamic energy control without
reconfiguring the entire model. Our evaluation on neuromorphic emulator and
hardware shows that ASPEN significantly reduces spike counts and energy
consumption while maintaining accuracy comparable to state-of-the-art methods.

</details>


### [7] [Data-Driven Discovery of Interpretable Kalman Filter Variants through Large Language Models and Genetic Programming](https://arxiv.org/abs/2508.11703)
*Vasileios Saketos,Sebastian Kaltenbach,Sergey Litvinov,Petros Koumoutsakos*

Main category: cs.NE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Algorithmic discovery has traditionally relied on human ingenuity and
extensive experimentation. Here we investigate whether a prominent scientific
computing algorithm, the Kalman Filter, can be discovered through an automated,
data-driven, evolutionary process that relies on Cartesian Genetic Programming
(CGP) and Large Language Models (LLM). We evaluate the contributions of both
modalities (CGP and LLM) in discovering the Kalman filter under varying
conditions. Our results demonstrate that our framework of CGP and LLM-assisted
evolution converges to near-optimal solutions when Kalman optimality
assumptions hold. When these assumptions are violated, our framework evolves
interpretable alternatives that outperform the Kalman filter. These results
demonstrate that combining evolutionary algorithms and generative models for
interpretable, data-driven synthesis of simple computational modules is a
potent approach for algorithmic discovery in scientific computing.

</details>


### [8] [LLM4CMO: Large Language Model-aided Algorithm Design for Constrained Multiobjective Optimization](https://arxiv.org/abs/2508.11871)
*Zhen-Song Chen,Hong-Wei Ding,Xian-Jia Wang,Witold Pedrycz*

Main category: cs.NE

TL;DR: LLM4CMO是一个基于双种群两阶段框架的约束多目标进化算法，利用大语言模型辅助设计核心模块，在基准测试和实际问题上优于11种先进基线算法。


<details>
  <summary>Details</summary>
Motivation: 现有双种群两阶段算法在利用不可行解提升解质量方面有潜力，但算法组件设计复杂；同时大语言模型在算法设计辅助方面尚未充分探索，需要填补这一空白。

Method: 采用双种群两阶段框架：第一阶段识别约束和非约束帕累托前沿；第二阶段使用混合算子、epsilon约束处理、分类策略和动态资源分配机制进行针对性优化，核心模块通过提示模板工程和LLM-人类交互设计。

Result: 在6个基准测试套件和10个实际约束多目标问题上，LLM4CMO优于11种最先进的基线算法，消融研究验证了LLM辅助模块化设计的有效性。

Conclusion: 大语言模型可以作为复杂进化优化算法开发的高效协同设计者，为算法设计提供了新的可能性。

Abstract: Constrained multi-objective optimization problems (CMOPs) frequently arise in
real-world applications where multiple conflicting objectives must be optimized
under complex constraints. Existing dual-population two-stage algorithms have
shown promise by leveraging infeasible solutions to improve solution quality.
However, designing high-performing constrained multi-objective evolutionary
algorithms (CMOEAs) remains a challenging task due to the intricacy of
algorithmic components. Meanwhile, large language models (LLMs) offer new
opportunities for assisting with algorithm design; however, their effective
integration into such tasks remains underexplored. To address this gap, we
propose LLM4CMO, a novel CMOEA based on a dual-population, two-stage framework.
In Stage 1, the algorithm identifies both the constrained Pareto front (CPF)
and the unconstrained Pareto front (UPF). In Stage 2, it performs targeted
optimization using a combination of hybrid operators (HOps), an epsilon-based
constraint-handling method, and a classification-based UPF-CPF relationship
strategy, along with a dynamic resource allocation (DRA) mechanism. To reduce
design complexity, the core modules, including HOps, epsilon decay function,
and DRA, are decoupled and designed through prompt template engineering and
LLM-human interaction. Experimental results on six benchmark test suites and
ten real-world CMOPs demonstrate that LLM4CMO outperforms eleven
state-of-the-art baseline algorithms. Ablation studies further validate the
effectiveness of the LLM-aided modular design. These findings offer preliminary
evidence that LLMs can serve as efficient co-designers in the development of
complex evolutionary optimization algorithms. The code associated with this
article is available at https://anonymous.4open.science/r/LLM4CMO971.

</details>


### [9] [Improving MSA Estimation through Adaptive Weight Vectors in MOEA/D](https://arxiv.org/abs/2508.12133)
*Saem Hasan,Muhammad Ali Nayeem,M. Sohel Rahman*

Main category: cs.NE

TL;DR: 提出了MOEA/D-ADF算法改进探索-利用平衡，结合PMAO形成PMAO++方法，通过生成多样化的比对-树对集合来提升系统发育推断的准确性。


<details>
  <summary>Details</summary>
Motivation: 生物序列的系统发育推断严重依赖于多重序列比对的质量，但最优比对对于许多序列来说是计算不可行的，并且对评分选择敏感。

Method: 开发了MOEA/D-ADF算法（基于适应度方差自适应调整权重向量的MOEA/D变体），与PMAO结合形成PMAO++，使用30个权重向量进化种群生成多样化的比对-树对集合。

Result: PMAO++在大多数基准测试中优于原始PMAO，在17个BAliBASE数据集中12个获得更好的假阴性率，产生多个零假阴性率的最佳树。

Conclusion: PMAO++产生的丰富比对-树对集合对于下游汇总方法特别有价值，通过整合多个可信比对和树的信号来实现更稳健的系统发育推断。

Abstract: Accurate phylogenetic inference from biological sequences depends critically
on the quality of multiple sequence alignments, yet optimal alignment for many
sequences is computationally intractable and sensitive to scoring choices. In
this work we introduce MOEA/D-ADF, a novel variant of MOEA/D that adaptively
adjusts subproblem weight vectors based on fitness variance to improve the
exploration-exploitation trade-off. We combine MOEA/D-ADF with PMAO (PASTA with
many application-aware optimization criteria) to form PMAO++, where
PMAO-generated solutions are used to seed MOEA/D-ADF, which then evolves a
population using 30 weight vectors to produce a diverse ensemble of
alignment-tree pairs. PMAO++ outperforms the original PMAO on a majority of
benchmark cases, achieving better false-negative (FN) rates on 12 of 17
BAliBASE-derived datasets and producing superior best-case trees, including
several instances with zero FN rate. Beyond improving single best alignments,
the rich set of alignment-tree pairs produced by PMAO++ is especially valuable
for downstream summary methods (for example, consensus and summary-tree
approaches), allowing more robust phylogenetic inference by integrating signal
across multiple plausible alignments and trees. Certain dataset features, such
as large terminal N/C extensions found in the RV40 group, remain challenging,
but overall PMAO++ demonstrates clear advantages for sequence-based
phylogenetic analysis. Future work will explore parameter tuning, larger
benchmark suites, and tighter integration with summary-tree pipelines to
further enhance applicability for biological sequence studies.

</details>


### [10] [A Self-Ensemble Inspired Approach for Effective Training of Binary-Weight Spiking Neural Networks](https://arxiv.org/abs/2508.12609)
*Qingyan Meng,Mingqing Xiao,Zhengyu Ma,Huihui Zhou,Yonghong Tian,Zhouchen Lin*

Main category: cs.NE

TL;DR: 这篇论文提出了一种新的视角，将脉冲神经网络(SNNs)训练视为带有噪声注入的二进制激活神经网络的自我集成训练，并提出SEI-BWSNN方法来训练二进制权重SNNs，在ImageNet上达到82.52%的准确率。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络(SNNs)和二进制神经网络(BNNs)都面临非可微的激活函数问题，但两者之间的深层关联和训练技术的相互奖励缺乏系统研究。特别是二进制权重SNNs的训练更加困难。

Method: 通过对反向传播过程的分析，将SNN训练视为带有噪声注入的二进制激活神经网络的自我集成训练。提出SEI-BWSNN方法，利用多重短接结构和知识蓄粉基础的训练技术来改善(binary-weight) SNNs的训练。

Result: 在Transformer架构中二进制化FFN层后，仅用2个时间步长就在ImageNet上达到82.52%的准确率，显示了方法的有效性和二进制权重SNNs的潜力。

Conclusion: 该研究为SNNs和BNNs之间的关联提供了新的理解视角，并通过SEI-BWSNN方法成功地实现了高性能的二进制权重SNNs训练，为低功耗神经纳米硬件应用开启了新的可能性。

Abstract: Spiking Neural Networks (SNNs) are a promising approach to low-power
applications on neuromorphic hardware due to their energy efficiency. However,
training SNNs is challenging because of the non-differentiable spike generation
function. To address this issue, the commonly used approach is to adopt the
backpropagation through time framework, while assigning the gradient of the
non-differentiable function with some surrogates. Similarly, Binary Neural
Networks (BNNs) also face the non-differentiability problem and rely on
approximating gradients. However, the deep relationship between these two
fields and how their training techniques can benefit each other has not been
systematically researched. Furthermore, training binary-weight SNNs is even
more difficult. In this work, we present a novel perspective on the dynamics of
SNNs and their close connection to BNNs through an analysis of the
backpropagation process. We demonstrate that training a feedforward SNN can be
viewed as training a self-ensemble of a binary-activation neural network with
noise injection. Drawing from this new understanding of SNN dynamics, we
introduce the Self-Ensemble Inspired training method for (Binary-Weight) SNNs
(SEI-BWSNN), which achieves high-performance results with low latency even for
the case of the 1-bit weights. Specifically, we leverage a structure of
multiple shortcuts and a knowledge distillation-based training technique to
improve the training of (binary-weight) SNNs. Notably, by binarizing FFN layers
in a Transformer architecture, our approach achieves 82.52% accuracy on
ImageNet with only 2 time steps, indicating the effectiveness of our
methodology and the potential of binary-weight SNNs.

</details>


### [11] [IzhiRISC-V -- a RISC-V-based Processor with Custom ISA Extension for Spiking Neuron Networks Processing with Izhikevich Neurons](https://arxiv.org/abs/2508.12846)
*Wiktor J. Szczerek,Artur Podobas*

Main category: cs.NE

TL;DR: 基于RISC-V处理器的脏死神经网络处理效率低下，提出通过定制ISA扩展和硬件扩展来提高效率的方案。


<details>
  <summary>Details</summary>
Motivation: 通用硬件上运行脏死神经网络导致代码效率低下，影响了脏死事件稀疏性带来的能源效率优势。

Method: 为RISC-V处理器引入定制的神经模式ISA扩展指令，并在现有ALU上实现专门的硬件扩展来支持这些指令。

Result: 开发了称为IzhiRISC-V的RISC-V兼容处理器，支持定制神经模式ISA扩展，为大规模系统实现奠定了第一步基础。

Conclusion: 通过定制硬件扩展和ISA指令集可以有效解决通用硬件上脏死神经网络处理效率低的问题，为高效能脏死神经网络处理提供了技术路径。

Abstract: Spiking Neural Network processing promises to provide high energy efficiency
due to the sparsity of the spiking events. However, when realized on
general-purpose hardware -- such as a RISC-V processor -- this promise can be
undermined and overshadowed by the inefficient code, stemming from repeated
usage of basic instructions for updating all the neurons in the network. One of
the possible solutions to this issue is the introduction of a custom ISA
extension with neuromorphic instructions for spiking neuron updating, and
realizing those instructions in bespoke hardware expansion to the existing ALU.
In this paper, we present the first step towards realizing a large-scale system
based on the RISC-V-compliant processor called IzhiRISC-V, supporting the
custom neuromorphic ISA extension.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [12] [A Deep Learning-Based CCTV System for Automatic Smoking Detection in Fire Exit Zones](https://arxiv.org/abs/2508.11696)
*Sami Sadat,Mohammad Irtiza Hossain,Junaid Ahmed Sifat,Suhail Haque Rafi,Md. Waseq Alauddin Alvi,Md. Khalilur Rhaman*

Main category: cs.CV

TL;DR: 基于YOLOv8的自定义模型在CCTV监控中实时检测火灾途径吸烟行为，达到78.90%回率和83.70% mAP性能，在Jetson Xavier NX边缘设备上处理速度52-97ms/次，适用于实时安全监控。


<details>
  <summary>Details</summary>
Motivation: 火灾途径的吸烟行为存在重大安全风险，需要开发能够在复杂监控环境下实时检测的系统，确保公共安全和自动遵规执行。

Method: 使用8,124张图片和2,708个明暗环境样本构建数据集，评估YOLOv8、YOLOv11、YOLOv12三种目标检测模型，并在YOLOv8基础上增加特殊结构构建自定义模型以应对监控挑战。

Result: 自定义模型表现最优，达到78.90%回率和83.70% mAP。在Jetson Xavier NX边缘设备上通过多线程运算，每次推理耗时52-97毫秒，适合实时操作。

Conclusion: 该系统提供了一个稳健且适应性强的平台，能够在多样化环境下有效监控公共安全，并支持自动遵规执行。

Abstract: A deep learning real-time smoking detection system for CCTV surveillance of
fire exit areas is proposed due to critical safety requirements. The dataset
contains 8,124 images from 20 different scenarios along with 2,708 raw samples
demonstrating low-light areas. We evaluated three advanced object detection
models: YOLOv8, YOLOv11, and YOLOv12, followed by development of a custom model
derived from YOLOv8 with added structures for challenging surveillance
contexts. The proposed model outperformed the others, achieving a recall of
78.90 percent and mAP at 50 of 83.70 percent, delivering optimal object
detection across varied environments. Performance evaluation on multiple edge
devices using multithreaded operations showed the Jetson Xavier NX processed
data at 52 to 97 milliseconds per inference, establishing its suitability for
time-sensitive operations. This system offers a robust and adaptable platform
for monitoring public safety and enabling automatic regulatory compliance.

</details>


### [13] [Separating Knowledge and Perception with Procedural Data](https://arxiv.org/abs/2508.11697)
*Adrián Rodríguez-Muñoz,Manel Baradad,Phillip Isola,Antonio Torralba*

Main category: cs.CV

TL;DR: 这篇论文提出了一种仅使用程序化数据训练表征模型的方法，通过视觉记忆机制在视觉相似性、分类和语义分割任务上实现无需额外训练的强性能表现。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要大量真实图像数据训练的问题，探索完全使用程序化数据构建视觉表征模型的可能性，同时实现与真实世界图像的完全隔离。

Method: 使用程序化数据训练表征模型，并应用视觉记忆机制——一个显式的参考图像嵌入数据库，在多个视觉任务上进行零样本推理。

Result: 在NIGHTS视觉相似性任务上仅差1%，在CUB200和Flowers102细粒度分类上分别超越8%和15%，在ImageNet-1K分类上差距10%，在COCO零样本分割任务上R²差距10%。

Conclusion: 程序化数据模型能够达到接近真实数据模型的性能，但同一物体的不同部分在程序化模型中的表征存在差异，导致内存搜索错误，这是剩余性能差距的原因。

Abstract: We train representation models with procedural data only, and apply them on
visual similarity, classification, and semantic segmentation tasks without
further training by using visual memory -- an explicit database of reference
image embeddings. Unlike prior work on visual memory, our approach achieves
full compartmentalization with respect to all real-world images while retaining
strong performance. Compared to a model trained on Places, our procedural model
performs within $1\%$ on NIGHTS visual similarity, outperforms by $8\%$ and
$15\%$ on CUB200 and Flowers102 fine-grained classification, and is within
$10\%$ on ImageNet-1K classification. It also demonstrates strong zero-shot
segmentation, achieving an $R^2$ on COCO within $10\%$ of the models trained on
real data. Finally, we analyze procedural versus real data models, showing that
parts of the same object have dissimilar representations in procedural models,
resulting in incorrect searches in memory and explaining the remaining
performance gap.

</details>


### [14] [FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis](https://arxiv.org/abs/2508.11721)
*Ke Zou,Jocelyn Hui Lin Goh,Yukun Zhou,Tian Lin,Samantha Min Er Yew,Sahana Srinivasan,Meng Wang,Rui Santos,Gabor M. Somfai,Huazhu Fu,Haoyu Chen,Pearse A. Keane,Ching-Yu Cheng,Yih Chung Tham*

Main category: cs.CV

TL;DR: 本研究系统性评估了四种眼科基础模型，并提出FusionFM融合框架。DINORET和RetiZero表现最佳，RetiZero具有更强的外部数据集法化能力。融合策略在某些疾病预测中可提升性能，但系统性疾病预测仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 眼科领域出现了多个基础模型，但缺乏系统性评估来回答哪个模型表现最佳、是否在不同任务上都表现良好以及融合所有模型的效果如何等基本问题。

Method: 提出FusionFM评估框架，包括两种融合方法来整合不同眼科基础模型。评估范围涵盖眼科疾病检测（青光眼、糖尿病视网膜病变、鹿鹿眼）和系统性疾病预测（糖尿病和高血压）。对四种先进模型（RETFound、VisionFM、RetiZero、DINORET）进行标准化测试，使用AUC和F1指标评估性能。

Result: DINORET和RetiZero在眼科和系统性疾病任务中都表现最佳，其中RetiZero在外部数据集上显示出更强的法化能力。门控融合策略在预测青光眼、AMD和高血压时能提供轻微的性能提升。然而，预测系统性疾病（特别是外部群体中的高血压）仍然具有挑战性。

Conclusion: 研究提供了基于证据的眼科基础模型评估，强调了模型融合的优势，并指出了提高临床应用性的策略方向。

Abstract: Foundation models (FMs) have shown great promise in medical image analysis by
improving generalization across diverse downstream tasks. In ophthalmology,
several FMs have recently emerged, but there is still no clear answer to
fundamental questions: Which FM performs the best? Are they equally good across
different tasks? What if we combine all FMs together? To our knowledge, this is
the first study to systematically evaluate both single and fused ophthalmic
FMs. To address these questions, we propose FusionFM, a comprehensive
evaluation suite, along with two fusion approaches to integrate different
ophthalmic FMs. Our framework covers both ophthalmic disease detection
(glaucoma, diabetic retinopathy, and age-related macular degeneration) and
systemic disease prediction (diabetes and hypertension) based on retinal
imaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM,
RetiZero, and DINORET) using standardized datasets from multiple countries and
evaluated their performance using AUC and F1 metrics. Our results show that
DINORET and RetiZero achieve superior performance in both ophthalmic and
systemic disease tasks, with RetiZero exhibiting stronger generalization on
external datasets. Regarding fusion strategies, the Gating-based approach
provides modest improvements in predicting glaucoma, AMD, and hypertension.
Despite these advances, predicting systemic diseases, especially hypertension
in external cohort remains challenging. These findings provide an
evidence-based evaluation of ophthalmic FMs, highlight the benefits of model
fusion, and point to strategies for enhancing their clinical applicability.

</details>


### [15] [UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction](https://arxiv.org/abs/2508.11728)
*Chunxia Ren,Ning Zhu,Yue Lai,Gui Chen,Ruijie Wang,Yangyi Hu,Suyao Liu,Shuwen Mao,Hong Su,Yu Zhang,Li Xiao*

Main category: cs.CV

TL;DR: UniDCF是一个统一的多模态深度学习框架，能够通过点云和多视角图像的融合编码，重建多种牙颌面硬组织，解决了现有单模态方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 牙颌面硬组织缺损严重影响患者生理功能、面部美观和心理健康，当前深度学习模型仅限于单组织和特定模态输入，导致泛化性差，需要在解剖保真度、计算效率和跨组织适应性之间权衡。

Method: 提出UniDCF框架，通过点云和多视角图像的多模态融合编码，利用各模态互补优势，并引入基于分数的去噪模块来优化表面平滑度。构建了最大的多模态数据集，包含6,609名患者的口内扫描、CBCT和CT数据，共54,555个标注实例。

Result: 评估显示UniDCF在几何精度、结构完整性和空间准确性方面优于现有最先进方法。临床模拟表明，UniDCF将重建设计时间减少99%，临床医生接受度超过94%。

Conclusion: UniDCF实现了快速、自动化和高保真度的重建，支持个性化和精确的修复治疗，简化临床工作流程，改善患者治疗效果。

Abstract: Dentocraniofacial hard tissue defects profoundly affect patients'
physiological functions, facial aesthetics, and psychological well-being,
posing significant challenges for precise reconstruction. Current deep learning
models are limited to single-tissue scenarios and modality-specific imaging
inputs, resulting in poor generalizability and trade-offs between anatomical
fidelity, computational efficiency, and cross-tissue adaptability. Here we
introduce UniDCF, a unified framework capable of reconstructing multiple
dentocraniofacial hard tissues through multimodal fusion encoding of point
clouds and multi-view images. By leveraging the complementary strengths of each
modality and incorporating a score-based denoising module to refine surface
smoothness, UniDCF overcomes the limitations of prior single-modality
approaches. We curated the largest multimodal dataset, comprising intraoral
scans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated
instances. Evaluations demonstrate that UniDCF outperforms existing
state-of-the-art methods in terms of geometric precision, structural
completeness, and spatial accuracy. Clinical simulations indicate UniDCF
reduces reconstruction design time by 99% and achieves clinician-rated
acceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and
high-fidelity reconstruction, supporting personalized and precise restorative
treatments, streamlining clinical workflows, and enhancing patient outcomes.

</details>


### [16] [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737)
*Shiyin Lu,Yang Li,Yu Xia,Yuwei Hu,Shanshan Zhao,Yanqing Ma,Zhichao Wei,Yinglun Li,Lunhao Duan,Jianshan Zhao,Yuxuan Han,Haijun Li,Wanying Chen,Junke Tang,Chengkun Hou,Zhixing Du,Tianli Zhou,Wenjie Zhang,Huping Ding,Jiahe Li,Wen Li,Gui Hu,Yiliang Gu,Siran Yang,Jiamang Wang,Hailong Sun,Yibo Wang,Hui Sun,Jinlong Huang,Yuping He,Shengze Shi,Weihong Zhang,Guodong Zheng,Junpeng Jiang,Sensen Gao,Yi-Feng Wu,Sijia Chen,Yuhui Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: Ovis2.5是一个先进的多模态大语言模型，通过原生分辨率视觉处理和反射推理能力，在OpenCompass多模态排行榜上取得了78.3分的优异成绩，在40B参数以下的开源模型中达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 为了解决固定分辨率图像处理导致的细节丢失问题，以及提升模型在复杂视觉内容（如复杂图表）上的推理能力，需要开发能够处理原生分辨率图像并具备自我检查和修正能力的多模态模型。

Method: 采用原生分辨率视觉Transformer处理可变分辨率图像，引入反射推理（包括自我检查和修订）作为可选的"思考模式"，通过五阶段课程学习（包括视觉预训练、大规模指令调优、DPO和GRPO对齐）进行训练，使用多模态数据打包和混合并行技术提升效率。

Result: Ovis2.5-9B在OpenCompass排行榜平均得分78.3，相比前代Ovis2-8B有显著提升；Ovis2.5-2B得分73.9，在同等规模模型中达到SOTA。在STEM基准测试、接地任务、视频任务和复杂图表分析方面都取得了领先结果。

Conclusion: Ovis2.5通过原生分辨率视觉处理和反射推理能力，在多模态理解任务上实现了显著的性能提升，特别是在复杂视觉内容分析方面表现出色，为资源受限的设备提供了高性能的小模型解决方案。

Abstract: We present Ovis2.5, a successor to Ovis2 designed for native-resolution
visual perception and strong multimodal reasoning. Ovis2.5 integrates a
native-resolution vision transformer that processes images at their native,
variable resolutions, avoiding the degradation from fixed-resolution tiling and
preserving both fine detail and global layout -- crucial for visually dense
content like complex charts. To strengthen reasoning, we train the model to
move beyond linear chain-of-thought and perform reflection -- including
self-checking and revision. This advanced capability is exposed as an optional
"thinking mode" at inference time, allowing users to trade latency for enhanced
accuracy on difficult inputs. The model is trained via a comprehensive
five-phase curriculum that progressively builds its skills. The process begins
with foundational visual and multimodal pretraining, advances through
large-scale instruction tuning, and culminates in alignment and reasoning
enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ
multimodal data packing and hybrid parallelism, yielding a significant
end-to-end speedup. We release two open-source models: Ovis2.5-9B and
Ovis2.5-2B. The latter continues the "small model, big performance" philosophy
of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the
OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a
substantial improvement over its predecessor, Ovis2-8B, and achieving
state-of-the-art results among open-source MLLMs in the sub-40B parameter
range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate
scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong
capabilities on grounding and video tasks, and achieves open-source SOTA at its
scale for complex chart analysis.

</details>


### [17] [VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models](https://arxiv.org/abs/2508.11801)
*Ming Cheng,Tong Wu,Jiazhen Hu,Jiaying Gong,Hoda Eldardiry*

Main category: cs.CV

TL;DR: VideoAVE是首个公开的视频到文本电商属性值提取数据集，涵盖14个领域和172个属性，包含224k训练数据和25k评估数据，并建立了全面的基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决现有AVE数据集缺乏视频支持、属性覆盖不足和公开可用性的问题，填补视频电商属性提取的研究空白。

Method: 提出基于CLIP的混合专家过滤系统(CLIP-MoE)来移除不匹配的视频-产品对，确保数据质量；建立包含属性条件值预测和开放属性值对提取任务的基准测试。

Result: 视频到文本AVE仍然是一个具有挑战性的问题，特别是在开放设置中，现有视频视觉语言模型在利用有效时序信息方面还有改进空间。

Conclusion: VideoAVE数据集填补了视频电商属性提取的空白，为开发更先进的视觉语言模型提供了重要资源，展示了该领域的发展潜力。

Abstract: Attribute Value Extraction (AVE) is important for structuring product
information in e-commerce. However, existing AVE datasets are primarily limited
to text-to-text or image-to-text settings, lacking support for product videos,
diverse attribute coverage, and public availability. To address these gaps, we
introduce VideoAVE, the first publicly available video-to-text e-commerce AVE
dataset across 14 different domains and covering 172 unique attributes. To
ensure data quality, we propose a post-hoc CLIP-based Mixture of Experts
filtering system (CLIP-MoE) to remove the mismatched video-product pairs,
resulting in a refined dataset of 224k training data and 25k evaluation data.
In order to evaluate the usability of the dataset, we further establish a
comprehensive benchmark by evaluating several state-of-the-art video vision
language models (VLMs) under both attribute-conditioned value prediction and
open attribute-value pair extraction tasks. Our results analysis reveals that
video-to-text AVE remains a challenging problem, particularly in open settings,
and there is still room for developing more advanced VLMs capable of leveraging
effective temporal information. The dataset and benchmark code for VideoAVE are
available at: https://github.com/gjiaying/VideoAVE

</details>


### [18] [An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation](https://arxiv.org/abs/2508.11803)
*Azam Nouri*

Main category: cs.CV

TL;DR: 研究证明仅使用二阶几何特征（平面曲率、曲率符号和梯度方向）就能让MLP在手写字符识别上达到高准确率，为CNN提供了可解释的替代方案


<details>
  <summary>Details</summary>
Motivation: 探索是否仅凭二阶几何特征就能实现高效的手写字符识别，为深度学习提供可解释性更强的替代方案

Method: 使用手工设计的三个特征图（曲率大小、曲率符号、梯度方向）作为输入，构建多层感知机分类器

Result: 在MNIST数字上达到97%准确率，在EMNIST字母上达到89%准确率

Conclusion: 曲率特征在手写字符识别中具有强大判别能力，深度学习优势可以通过可解释的手工特征实现

Abstract: This study investigates whether second-order geometric cues - planar
curvature magnitude, curvature sign, and gradient orientation - are sufficient
on their own to drive a multilayer perceptron (MLP) classifier for handwritten
character recognition (HCR), offering an alternative to convolutional neural
networks (CNNs). Using these three handcrafted feature maps as inputs, our
curvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89
percent on EMNIST letters. These results underscore the discriminative power of
curvature-based representations for handwritten character images and
demonstrate that the advantages of deep learning can be realized even with
interpretable, hand-engineered features.

</details>


### [19] [Labels or Input? Rethinking Augmentation in Multimodal Hate Detection](https://arxiv.org/abs/2508.11808)
*Sahajpreet Singh,Rongxin Ouyang,Subhayan Mukerjee,Kokil Jaidka*

Main category: cs.CV

TL;DR: 本文提出双重方法改进多模态仇恨检测：提示优化框架和多模态数据增强管道，通过结构化提示和反事实数据生成提升模型鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现代网络充斥着多模态内容，仇恨表情包检测面临挑战，因为有害意图往往通过文本和图像的微妙互动以幽默或讽刺形式呈现。现有视觉语言模型缺乏细粒度监督支持，容易受到隐含仇恨言论的影响。

Method: 1) 提出提示优化框架，系统变化提示结构、监督粒度和训练模态；2) 引入多模态数据增强管道，通过多智能体LLM-VLM设置生成2479个反事实中性表情包，隔离并重写仇恨模态。

Result: 结构化提示即使在小型模型中也能提高鲁棒性，InternVL2在二元和分级设置中均获得最佳F1分数。数据增强管道成功减少虚假相关性并提高分类器泛化能力。

Conclusion: 提示结构和数据组成与模型规模同等重要，针对性增强可以支持更可信和上下文敏感的仇恨检测，为构建合成数据训练鲁棒公平的视觉语言模型提供了新方向。

Abstract: The modern web is saturated with multimodal content, intensifying the
challenge of detecting hateful memes, where harmful intent is often conveyed
through subtle interactions between text and image under the guise of humor or
satire. While recent advances in Vision-Language Models (VLMs) show promise,
these models lack support for fine-grained supervision and remain susceptible
to implicit hate speech. In this paper, we present a dual-pronged approach to
improve multimodal hate detection. First, we propose a prompt optimization
framework that systematically varies prompt structure, supervision granularity,
and training modality. We show that prompt design and label scaling both
influence performance, with structured prompts improving robustness even in
small models, and InternVL2 achieving the best F1-scores across binary and
scaled settings. Second, we introduce a multimodal data augmentation pipeline
that generates 2,479 counterfactually neutral memes by isolating and rewriting
the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,
successfully reduces spurious correlations and improves classifier
generalization. Our approaches inspire new directions for building synthetic
data to train robust and fair vision-language models. Our findings demonstrate
that prompt structure and data composition are as critical as model size, and
that targeted augmentation can support more trustworthy and context-sensitive
hate detection.

</details>


### [20] [Towards Understanding 3D Vision: the Role of Gaussian Curvature](https://arxiv.org/abs/2508.11825)
*Sherlon Almeida da Silva,Davi Geiger,Luiz Velho,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: 这篇论文探讨了高斯曲率在3D表面建模中的作用，指出当前深度学习方法缺乏显式的几何模型，高斯曲率作为一种不变量可以提供稀疏缩略的3D表面描述，并作为几何先验知识改善重建效果。


<details>
  <summary>Details</summary>
Motivation: 当前主流的深度学习方法虽然在立体匹配和单目深度重建中取得了显著成功，但缺乏可以直接分析、跨模态转移或系统性修改的显式3D几何模型。研究人员想要探索高斯曲率在表面建模中的作用。

Method: 研究人员使用Middlebury立体数据集来研究高斯曲率的特性。高斯曲率是一种在观察者变换或坐标系变换下保持不变的量，研究人员通过实验展示了它的几项关键特性。

Result: 研究发现：(i)高斯曲率能提供稀疏缩略的3D表面描述；(ii)当前最先进的单目和立体方法似乎都隐式考虑了高斯曲率，但无法提取显式模块；(iii)高斯曲率可作为几何先验知识来提升3D表面重建的效果；(iv)它还可能作为立体方法的无监督评量指标。

Conclusion: 高斯曲率在3D表面建模中具有重要价值，它不仅是一种稀疏缩略的表征方式，还可以作为几何先验来改善重建质量，并为立体方法提供无监督评估。这为开发更可解释和可控的几何模型提供了新的思路。

Abstract: Recent advances in computer vision have predominantly relied on data-driven
approaches that leverage deep learning and large-scale datasets. Deep neural
networks have achieved remarkable success in tasks such as stereo matching and
monocular depth reconstruction. However, these methods lack explicit models of
3D geometry that can be directly analyzed, transferred across modalities, or
systematically modified for controlled experimentation. We investigate the role
of Gaussian curvature in 3D surface modeling. Besides Gaussian curvature being
an invariant quantity under change of observers or coordinate systems, we
demonstrate using the Middlebury stereo dataset that it offers: (i) a sparse
and compact description of 3D surfaces, (ii) state-of-the-art monocular and
stereo methods seem to implicitly consider it, but no explicit module of such
use can be extracted, (iii) a form of geometric prior that can inform and
improve 3D surface reconstruction, and (iv) a possible use as an unsupervised
metric for stereo methods.

</details>


### [21] [From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images](https://arxiv.org/abs/2508.11826)
*Dehn Xu,Tim Katzke,Emmanuel Müller*

Main category: cs.CV

TL;DR: 这篇论文系统性评估了多种图像到图象转换方法在图神经网络图级异常检测任务中的效果，发现颜色描述符最重要，结合形状和纹理特征能进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 虽然GNN已应用于图像演算的图表示，但还没有研究系统比较不同图像到图象转换方法在图级异常检测任务中的效果。

Method: 系统评估多种分割方案、边缘构建策略和节点特征集（包括颜色、纹理、形状描述符），在皮肤镜图像上进行广泛实验，测试了无监督、弱监督和全监督三种模式。

Result: 颜色描述符单独性能最好，结合形状和纹理特征能一致提升检测效果。最佳无监督配置达到AUC-ROC 0.805，加入稀疏标签后提升到0.872，全监督时达到0.914。

Conclusion: 图像到图象转换方法的选择对GNN图级异常检测性能有重要影响，颜色特征是最重要的单独因素，多特征融合能显著提升性能。

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful approach for
graph-based machine learning tasks. Previous work applied GNNs to image-derived
graph representations for various downstream tasks such as classification or
anomaly detection. These transformations include segmenting images, extracting
features from segments, mapping them to nodes, and connecting them. However, to
the best of our knowledge, no study has rigorously compared the effectiveness
of the numerous potential image-to-graph transformation approaches for
GNN-based graph-level anomaly detection (GLAD). In this study, we
systematically evaluate the efficacy of multiple segmentation schemes, edge
construction strategies, and node feature sets based on color, texture, and
shape descriptors to produce suitable image-derived graph representations to
perform graph-level anomaly detection. We conduct extensive experiments on
dermoscopic images using state-of-the-art GLAD models, examining performance
and efficiency in purely unsupervised, weakly supervised, and fully supervised
regimes. Our findings reveal, for example, that color descriptors contribute
the best standalone performance, while incorporating shape and texture features
consistently enhances detection efficacy. In particular, our best unsupervised
configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805
without relying on pretrained backbones like comparable image-based approaches.
With the inclusion of sparse labels, the performance increases substantially to
0.872 and with full supervision to 0.914 AUC-ROC.

</details>


### [22] [Recent Advances in Transformer and Large Language Models for UAV Applications](https://arxiv.org/abs/2508.11834)
*Hamza Kheddar,Yassine Habchi,Mohamed Chahine Ghanem,Mustapha Hemis,Dusit Niyato*

Main category: cs.CV

TL;DR: 这篇综述论文系统性地分类和评估了Transformer架构在无人机系统中的最新应用进展，包括注意力机制、CNN-Transformer混合模型、强化学习Transformer和大语言模型，提供了统一的分类体系和性能基准分析。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型的快速发展，其在无人机感知、决策和自主性方面的应用日益广泛，但缺乏系统性的综述来统一分类和评估这些进展，需要为研究者和从业者提供全面的技术指导。

Method: 采用系统性文献综述方法，对Transformer在无人机领域的应用进行分类整理，构建统一的分类体系，通过结构化表格和性能基准进行对比分析，并回顾关键数据集、模拟器和评估指标。

Result: 建立了Transformer在无人机应用的统一分类框架，识别了包括精准农业和自主导航等新兴应用领域，提供了详细的性能比较和分析，揭示了当前研究中的技术空白。

Conclusion: 论文为Transformer驱动的无人机技术提供了全面的技术路线图，指出了计算效率和实时部署等关键挑战，并为未来研究方向提供了指导，有助于推动该领域的进一步发展。

Abstract: The rapid advancement of Transformer-based models has reshaped the landscape
of uncrewed aerial vehicle (UAV) systems by enhancing perception,
decision-making, and autonomy. This review paper systematically categorizes and
evaluates recent developments in Transformer architectures applied to UAVs,
including attention mechanisms, CNN-Transformer hybrids, reinforcement learning
Transformers, and large language models (LLMs). Unlike previous surveys, this
work presents a unified taxonomy of Transformer-based UAV models, highlights
emerging applications such as precision agriculture and autonomous navigation,
and provides comparative analyses through structured tables and performance
benchmarks. The paper also reviews key datasets, simulators, and evaluation
metrics used in the field. Furthermore, it identifies existing gaps in the
literature, outlines critical challenges in computational efficiency and
real-time deployment, and offers future research directions. This comprehensive
synthesis aims to guide researchers and practitioners in understanding and
advancing Transformer-driven UAV technologies.

</details>


### [23] [ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages](https://arxiv.org/abs/2508.11854)
*Matthew Hull,Haoyang Yang,Pratham Mehta,Mansi Phute,Aeree Cho,Haorang Wang,Matthew Lau,Wenke Lee,Wilian Lunardi,Martin Andreoni,Polo Chau*

Main category: cs.CV

TL;DR: 首个黑盒攻击方法ComplicitSplat，利用3D高斯拟合的轮廓渲染技术在特定视角下嵌入阿尔法内容，攻击各种目标检测器而无需模型权重。


<details>
  <summary>Details</summary>
Motivation: 随着3D高斯拟合技术在安全关键任务中快速应用，需要研究恶意攻击者如何篡改图像造成害，曝露新的安全风险。

Method: 利用标准3DGS渲染方法创建视角特异性谜色等效果，在场景对象中嵌入仅在特定视角可见的阿尔法内容，无需模型架构或权重访问。

Result: 实验表明ComplicitSplat成功攻击多种流行检测器（单阶段、多阶段、Transformer基础），在真实物理对象和合成场景中都有效。

Conclusion: 该攻击曝露了3DGS在自主导航等任务关键系统中的新安全风险，是首个基于3DGS的黑盒下游目标检测器攻击。

Abstract: As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks
for efficient novel-view synthesis from static images, how might an adversary
tamper images to cause harm? We introduce ComplicitSplat, the first attack that
exploits standard 3DGS shading methods to create viewpoint-specific camouflage
- colors and textures that change with viewing angle - to embed adversarial
content in scene objects that are visible only from specific viewpoints and
without requiring access to model architecture or weights. Our extensive
experiments show that ComplicitSplat generalizes to successfully attack a
variety of popular detector - both single-stage, multi-stage, and
transformer-based models on both real-world capture of physical objects and
synthetic scenes. To our knowledge, this is the first black-box attack on
downstream object detectors using 3DGS, exposing a novel safety risk for
applications like autonomous navigation and other mission-critical robotic
systems.

</details>


### [24] [Impact of Clinical Image Quality on Efficient Foundation Model Finetuning](https://arxiv.org/abs/2508.11864)
*Yucheng Tang,Pawel Rajwa,Alexander Ng,Yipei Wang,Wen Yan,Natasha Thorley,Aqua Asif,Clare Allen,Louise Dickinson,Francesco Giganti,Shonit Punwani,Daniel C. Alexander,Veeru Kasivisvanathan,Yipeng Hu*

Main category: cs.CV

TL;DR: 基础模型在医学形态学中显示了标签效率优势，但图像质量分布和学习-测试集的质量匹配很重要。高质量图像的充足性强刻影响模型性能，质量分布不一致会导致性能差异，而且不同下游任务对质量匹配的需求不同。


<details>
  <summary>Details</summary>
Motivation: 评估基础模型在医学形态学中的标签效率，并研究图像质量变化如何影响精调模型的普适性。重点关注高/低质量图像比例在学习和测试集中的匹配问题。

Method: 使用ProFound基础模型（领域特定视觉基础模型，在大规模前列腰部MRI数据集上预训练）进行实验。系统性变化学习集和评估集中高/低质量图像比例，测量精调模型的普适性。

Result: 图像质量分布和学习-测试集质量不匹配显著影响模型性能。学习集中充足的高质量图像对维持强劲性能至关重要。不同下游任务（如自动化形态学报告和前列腰癌检测）对质量匹配的需求不同。质量比例一致时，精调需要的标签数据比从头训练少得多，但标签效率依赖于图像质量分布。

Conclusion: 质量分布的评估和对齐对于完全实现基础模型的数据和计算效率优势至关重要。如果精调数据中没有充足的高质量图像，预训练模型可能无法超过从头训练的模型。需要为特定下游任务制定精调数据的质量标准。

Abstract: Foundation models in medical imaging have shown promising label efficiency,
achieving high downstream performance with only a fraction of annotated data.
Here, we evaluate this in prostate multiparametric MRI using ProFound, a
domain-specific vision foundation model pretrained on large-scale prostate MRI
datasets. We investigate how variable image quality affects label-efficient
finetuning by measuring the generalisability of finetuned models. Experiments
systematically vary high-/low-quality image ratios in finetuning and evaluation
sets. Our findings indicate that image quality distribution and its
finetune-and-test mismatch significantly affect model performance. In
particular: a) Varying the ratio of high- to low-quality images between
finetuning and test sets leads to notable differences in downstream
performance; and b) The presence of sufficient high-quality images in the
finetuning set is critical for maintaining strong performance, whilst the
importance of matched finetuning and testing distribution varies between
different downstream tasks, such as automated radiology reporting and prostate
cancer detection.When quality ratios are consistent, finetuning needs far less
labeled data than training from scratch, but label efficiency depends on image
quality distribution. Without enough high-quality finetuning data, pretrained
models may fail to outperform those trained without pretraining. This
highlights the importance of assessing and aligning quality distributions
between finetuning and deployment, and the need for quality standards in
finetuning data for specific downstream tasks. Using ProFound, we show the
value of quantifying image quality in both finetuning and deployment to fully
realise the data and compute efficiency benefits of foundation models.

</details>


### [25] [AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition](https://arxiv.org/abs/2508.11870)
*Ying Huang,Yuanbin Man,Wenqi Jia,Zhengzhong Tu,Junzhou Huang,Miao Yin*

Main category: cs.CV

TL;DR: 基于张量环分解的跨层适配器框架AdaRing，通过利用张量低秩性和多样化适配器协作，在减少90%参数的同时达到最佳性能


<details>
  <summary>Details</summary>
Motivation: 解决现有适配器方法的两大问题：1)跨层冗余导致压缩率有限；2)同质化适配器表征能力不足

Method: 采用张量环分解(TRD)把适配器构造为层共享的张量核和层特异切片，并在泛化感知指导下让不同秩的适配器协同工作

Result: 在各种任务上达到state-of-the-art性能，平均训练参数减少90%

Conclusion: AdaRing框架通过利用跨层冗余和多样化适配器协作，实现了超轻量的视觉-语言模型高效调整

Abstract: Adapter-based fine-tuning has gained remarkable attention in adapting large
pre-trained vision language models (VLMs) for a wide range of downstream tasks
efficiently. In this paradigm, only the inserted adapters are fine-tuned,
without the need for training the original VLM backbone. Existing works scale
adapters by integrating them into every layer of VLMs to increase the capacity
of adapters. However, these methods face two primary limitations: 1) limited
compression rate due to ignoring cross-layer redundancy, and 2) limited
representational capacity across homogeneous adapters. In this paper, we
propose a novel vision-language fine-tuning framework based on cross-layer
tensor ring decomposition (TRD) with the integration and collaboration of
diverse adapters, called AdaRing, achieving ultra-light parameter-efficient
adaptation of VLMs on various tasks. To remove the high redundancy that exists
among adapters across layers, we exploit the tensor-level low-rankness to
formulate adapters as layer-shared tensor cores and layer-specific slices.
Moreover, guided by generalization-aware fine-tuning, diverse rank-driven
adapters cooperate to handle tasks that require different representations. Our
experiments show that the proposed AdaRing achieves the state-of-the-art
performance while reducing average training parameters by 90%.

</details>


### [26] [EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models](https://arxiv.org/abs/2508.11886)
*Wenhui Zhu,Xiwen Chen,Zhipeng Wang,Shao Tang,Sayan Ghosh,Xuanzhao Dong,Rajat Koner,Yalin Wang*

Main category: cs.CV

TL;DR: 通过空间信息优化的视觉token剪枝方法EVTP-IV，在指令式视觉分割任务中实现5倍速度提升，保持粗糕精度


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视频分割任务中的推理成本过高，需要通过token剪枝来加速推理

Method: 提出EVTP-IV方法，基于k-center算法集成空间信息，选择空间代表性强的简约token子集

Result: 在标准IVS测试集上实现了视频任务5倍加速、图片任务3.5倍加速，仅使用20%token却保持相当的精度

Conclusion: 该方法通过空间代表性token选择，有效降低计算成本，在各种剪枝比下都超过现有最佳方法

Abstract: Instructed Visual Segmentation (IVS) tasks require segmenting objects in
images or videos based on natural language instructions. While recent
multimodal large language models (MLLMs) have achieved strong performance on
IVS, their inference cost remains a major bottleneck, particularly in video. We
empirically analyze visual token sampling in MLLMs and observe a strong
correlation between subset token coverage and segmentation performance. This
motivates our design of a simple and effective token pruning method that
selects a compact yet spatially representative subset of tokens to accelerate
inference. In this paper, we introduce a novel visual token pruning method for
IVS, called EVTP-IV, which builds upon the k-center by integrating spatial
information to ensure better coverage. We further provide an
information-theoretic analysis to support our design. Experiments on standard
IVS benchmarks show that our method achieves up to 5X speed-up on video tasks
and 3.5X on image tasks, while maintaining comparable accuracy using only 20%
of the tokens. Our method also consistently outperforms state-of-the-art
pruning baselines under varying pruning ratios.

</details>


### [27] [Large Kernel Modulation Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.11893)
*Quanwei Hu,Yinggan Tang,Xuguang Zhang*

Main category: cs.CV

TL;DR: 一种纯CNN的轻量级图像超分辨模型LKMN，通过大内核调制结构在保持高效性的同时提升了质量，在Manga109数据集上比SOTA模型提升0.23dB PSNR且速度提升4.8倍


<details>
  <summary>Details</summary>
Motivation: 解决轻量级图像超分辨中CNN模型缺乏非局部特征抓取能力而Transformer模型速度慢的问题，尝试在CNN框架下实现高效的非局部建模

Method: 提出LKMN模型，包含两个核心组件：EPLKB模块通过频道混淆、频道注意力和大内核卷积提取非局部特征，CGFN模块通过交叉门控策动态融合局部和非局部特征

Result: 在多个数据集上超越现有SOTA轻量级SR模型，特别是在Manga109数据集上比DAT-light提升0.23dB PSNR，速度提升4.8倍

Conclusion: LKMN模型成功在纯CNN框架下实现了高效的非局部特征抓取，在质量和效率之间取得优异平衡，为资源受限场景下的图像超分辨提供了一种有效解决方案

Abstract: Image super-resolution (SR) in resource-constrained scenarios demands
lightweight models balancing performance and latency. Convolutional neural
networks (CNNs) offer low latency but lack non-local feature capture, while
Transformers excel at non-local modeling yet suffer slow inference. To address
this trade-off, we propose the Large Kernel Modulation Network (LKMN), a pure
CNN-based model. LKMN has two core components: Enhanced Partial Large Kernel
Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN). The EPLKB utilizes
channel shuffle to boost inter-channel interaction, incorporates channel
attention to focus on key information, and applies large kernel strip
convolutions on partial channels for non-local feature extraction with reduced
complexity. The CGFN dynamically adjusts discrepancies between input, local,
and non-local features via a learnable scaling factor, then employs a
cross-gate strategy to modulate and fuse these features, enhancing their
complementarity. Extensive experiments demonstrate that our method outperforms
existing state-of-the-art (SOTA) lightweight SR models while balancing quality
and efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over
DAT-light on the Manga109 dataset at $\times$4 upscale, with nearly $\times$4.8
times faster. Codes are in the supplementary materials. The code is available
at https://github.com/Supereeeee/LKMN.

</details>


### [28] [A Sobel-Gradient MLP Baseline for Handwritten Character Recognition](https://arxiv.org/abs/2508.11902)
*Azam Nouri*

Main category: cs.CV

TL;DR: 使用仅包含水平和垂直Sobel导数作为输入的MLP网络，在手写字符识别任务上达到了接近CNN的性能，证明一阶梯度已包含足够的分类信息


<details>
  <summary>Details</summary>
Motivation: 重新审视经典Sobel算子，探索是否仅使用一阶边缘图就能驱动MLP网络进行手写字符识别，作为CNN的替代方案

Method: 仅使用水平和垂直Sobel导数作为输入，训练全连接多层感知机(MLP)在MNIST和EMNIST Letters数据集上进行手写字符识别

Result: 在MNIST数字上达到98%准确率，在EMNIST字母上达到92%准确率，接近CNN性能但具有更小的内存占用和透明特征

Conclusion: 手写字符图像中的大部分类别区分信息已经包含在一阶梯度中，边缘感知的MLP是HCR的一个有吸引力的选择

Abstract: We revisit the classical Sobel operator to ask a simple question: Are
first-order edge maps sufficient to drive an all-dense multilayer perceptron
(MLP) for handwritten character recognition (HCR), as an alternative to
convolutional neural networks (CNNs)? Using only horizontal and vertical Sobel
derivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its
extreme simplicity, the resulting network reaches 98% accuracy on MNIST digits
and 92% on EMNIST letters -- approaching CNNs while offering a smaller memory
footprint and transparent features. Our findings highlight that much of the
class-discriminative information in handwritten character images is already
captured by first-order gradients, making edge-aware MLPs a compelling option
for HCR.

</details>


### [29] [OVG-HQ: Online Video Grounding with Hybrid-modal Queries](https://arxiv.org/abs/2508.11903)
*Runhao Zeng,Jiaqi Mao,Minghao Lai,Minh Hieu Phan,Yanjie Dong,Wei Wang,Qi Chen,Xiping Hu*

Main category: cs.CV

TL;DR: 提出了一种新的在线视频基准任务OVG-HQ，支持文本、图像、视频等混合模态查询，解决了传统视频基准在流式视频和视觉符号查询方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统视频基准任务在流式视频和使用视觉符号的查询场景中遇到困难，需要一种能够在线处理混合模态查询的方案。

Method: 提出OVG-HQ-Unify统一框架，包含参数化记忆块(PMB)来保留学习知识，以及跨模态荟蓉策略来平衡不同模态的学习效果。构建了QVHighlights-Unify数据集和新的在线评估指标。

Result: 实验结果显示OVG-HQ-Unify在准确性和效率方面都超过现有模型，为在线混合模态视频基准提供了健壮的解决方案。

Conclusion: 该研究成功地解决了在线视频基准中的模态不平衡和上下文限制问题，为处理混合模态查询的实时视频分析开启了新方向。

Abstract: Video grounding (VG) task focuses on locating specific moments in a video
based on a query, usually in text form. However, traditional VG struggles with
some scenarios like streaming video or queries using visual cues. To fill this
gap, we present a new task named Online Video Grounding with Hybrid-modal
Queries (OVG-HQ), which enables online segment localization using text, images,
video segments, and their combinations. This task poses two new challenges:
limited context in online settings and modality imbalance during training,
where dominant modalities overshadow weaker ones. To address these, we propose
OVG-HQ-Unify, a unified framework featuring a Parametric Memory Block (PMB)
that retain previously learned knowledge to enhance current decision and a
cross-modal distillation strategy that guides the learning of non-dominant
modalities. This design enables a single model to effectively handle
hybrid-modal queries. Due to the lack of suitable datasets, we construct
QVHighlights-Unify, an expanded dataset with multi-modal queries. Besides,
since offline metrics overlook prediction timeliness, we adapt them to the
online setting, introducing oR@n, IoU=m, and online mean Average Precision
(omAP) to evaluate both accuracy and efficiency. Experiments show that our
OVG-HQ-Unify outperforms existing models, offering a robust solution for
online, hybrid-modal video grounding. Source code and datasets are available at
https://github.com/maojiaqi2324/OVG-HQ.

</details>


### [30] [SafeCtrl: Region-Based Safety Control for Text-to-Image Diffusion via Detect-Then-Suppress](https://arxiv.org/abs/2508.11904)
*Lingyun Zhang,Yu Xie,Yanwei Fu,Ping Chen*

Main category: cs.CV

TL;DR: SafeCtrl是一个轻量级非侵入式插件，通过检测-抑制范式来提升文本到图像模型的安全性，在保持高保真度的同时有效抑制有害内容生成。


<details>
  <summary>Details</summary>
Motivation: 现有安全方法（如提示重写或模型微调）在安全性和保真度之间存在权衡，基于定位的方法依赖显式概念替换可能导致语义不连贯。

Method: 使用Direct Preference Optimization训练策略，利用图像级偏好数据训练模块，实现精确定位有害内容并进行语义抑制而非硬替换。

Result: 在安全有效性和保真度保持方面显著优于最先进方法，实验证明其有效性。

Conclusion: 解耦的基于抑制的控制是构建更负责任生成模型的高效且可扩展方向。

Abstract: The widespread deployment of text-to-image models is challenged by their
potential to generate harmful content. While existing safety methods, such as
prompt rewriting or model fine-tuning, provide valuable interventions, they
often introduce a trade-off between safety and fidelity. Recent
localization-based approaches have shown promise, yet their reliance on
explicit ``concept replacement" can sometimes lead to semantic incongruity. To
address these limitations, we explore a more flexible detect-then-suppress
paradigm. We introduce SafeCtrl, a lightweight, non-intrusive plugin that first
precisely localizes unsafe content. Instead of performing a hard A-to-B
substitution, SafeCtrl then suppresses the harmful semantics, allowing the
generative process to naturally and coherently resolve into a safe,
context-aware alternative. A key aspect of our work is a novel training
strategy using Direct Preference Optimization (DPO). We leverage readily
available, image-level preference data to train our module, enabling it to
learn nuanced suppression behaviors and perform region-guided interventions at
inference without requiring costly, pixel-level annotations. Extensive
experiments show that SafeCtrl significantly outperforms state-of-the-art
methods in both safety efficacy and fidelity preservation. Our findings suggest
that decoupled, suppression-based control is a highly effective and scalable
direction for building more responsible generative models.

</details>


### [31] [TimeSenCLIP: A Vision-Language Model for Remote Sensing Using Single-Pixel Time Series](https://arxiv.org/abs/2508.11919)
*Pallavi Jain,Diego Marcos,Dino Ienco,Roberto Interdonato,Tristan Berchoux*

Main category: cs.CV

TL;DR: TimeSenCLIP是一个轻量级框架，通过利用单个像素的时序和光谱信息进行土地利用分类，减少了对大空间瓦片和文本监督的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在遥感应用中面临两个关键挑战：依赖大空间瓦片增加计算成本，以及依赖文本监督但文本数据往往不易获得。

Method: 利用Sentinel-2影像的光谱和时序信息，通过与地理标记的地面照片进行跨视角学习，最小化基于标题的训练需求，同时保持卫星和地面视角之间的语义对齐。

Result: 实验证明，单个像素输入结合时序和光谱线索足以进行专题制图，为大规模遥感应用提供了可扩展和高效的替代方案。

Conclusion: TimeSenCLIP展示了在土地利用分类中，时序和光谱信息可以替代空间上下文，提供了一种更轻量级和高效的遥感分析方法。

Abstract: Vision-language models have shown significant promise in remote sensing
applications, particularly for land-use and land-cover (LULC) via zero-shot
classification and retrieval. However, current approaches face two key
challenges: reliance on large spatial tiles that increase computational cost,
and dependence on text-based supervision, which is often not readily available.
In this work, we present TimeSenCLIP, a lightweight framework that reevaluate
the role of spatial context by evaluating the effectiveness of a single pixel
by leveraging its temporal and spectral dimensions, for classifying LULC and
ecosystem types. By leveraging spectral and temporal information from
Sentinel-2 imagery and cross-view learning with geo-tagged ground-level photos,
we minimises the need for caption-based training while preserving semantic
alignment between overhead (satellite) and ground perspectives. Our approach is
grounded in the LUCAS and Sen4Map datasets, and evaluated on classification
tasks including LULC, crop type, and ecosystem type. We demonstrate that single
pixel inputs, when combined with temporal and spectral cues, are sufficient for
thematic mapping, offering a scalable and efficient alternative for large-scale
remote sensing applications. Code is available at
https://github.com/pallavijain-pj/TimeSenCLIP

</details>


### [32] [Assessment of Using Synthetic Data in Brain Tumor Segmentation](https://arxiv.org/abs/2508.11922)
*Aditi Jahagirdar,Sameer Joshi*

Main category: cs.CV

TL;DR: 使用GAN生成的合成MRI数据来补充真实数据训练脑部脱病分割模型，在整体胯病边界分割方面有所改善，但对胯病核心区域的分割效果仍有限。


<details>
  <summary>Details</summary>
Motivation: 解决脑部脱病分割中的数据稀缺、类别不平衡和胯病异质性问题，通过合成数据来提升数据集多样性。

Method: 使用预训练GAN模型生成合成MRI数据，将真实数据（BraTS 2020）与合成数据按不同比例混合，训练U-Net分割网络进行对比实验。

Result: 数量指标（Dice、IoU等）在真实数据和混合数据训练下相似，但在40%真实+60%合成的混合数据下，整体胯病边界分割质量有所提升，胯病核心和增强区域的分割精度仍较低。

Conclusion: 合成数据可作为脑部脱病分割的有效数据扩充策略，但需要进一步解决类别不平衡问题并进行更大规模的验证。

Abstract: Manual brain tumor segmentation from MRI scans is challenging due to tumor
heterogeneity, scarcity of annotated data, and class imbalance in medical
imaging datasets. Synthetic data generated by generative models has the
potential to mitigate these issues by improving dataset diversity. This study
investigates, as a proof of concept, the impact of incorporating synthetic MRI
data, generated using a pre-trained GAN model, into training a U-Net
segmentation network. Experiments were conducted using real data from the BraTS
2020 dataset, synthetic data generated with the medigan library, and hybrid
datasets combining real and synthetic samples in varying proportions. While
overall quantitative performance (Dice coefficient, IoU, precision, recall,
accuracy) was comparable between real-only and hybrid-trained models,
qualitative inspection suggested that hybrid datasets, particularly with 40%
real and 60% synthetic data, improved whole tumor boundary delineation.
However, region-wise accuracy for the tumor core and the enhancing tumor
remained lower, indicating a persistent class imbalance. The findings support
the feasibility of synthetic data as an augmentation strategy for brain tumor
segmentation, while highlighting the need for larger-scale experiments,
volumetric data consistency, and mitigating class imbalance in future work.

</details>


### [33] [Deep Learning For Point Cloud Denoising: A Survey](https://arxiv.org/abs/2508.11932)
*Chengwei Zhang,Xueyi Zhang,Mingrui Lao,Tao Jiang,Xinhao Xu,Wenjie Li,Fubo Zhang,Longyong Chen*

Main category: cs.CV

TL;DR: 这篇论文是一个深度学习基于点云去噪的综述性研究，系统总结了该领域的发展状况、提出了分类体系，并分析了现有方法的优缺点以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 实际环境中的点云数据存在多种模态和强度的噪声，去噪作为预处理步骤对下游任务至关重要。深度学习方法虽然性能超越传统方法，但缺乏系统的综述性研究来总结该领域的发展。

Method: 将点云去噪模型化为两个步骤：离群点移除和表面噪声恢复。通过这种分类方式来包含大部分PCD场景和需求，并对现有方法进行详细的比较分析。

Result: 论文提出了一个专门为去噪任务设计的分类体系，识别了DL基于PCD的关键挑战，系统总结了现有方法的主要贡献。

Conclusion: 论文填补了DL基于点云去噪领域缺乏综述性研究的空白，为该领域提供了系统的分析框架和研究指南，并提出了未来研究方向和挑战。

Abstract: Real-world environment-derived point clouds invariably exhibit noise across
varying modalities and intensities. Hence, point cloud denoising (PCD) is
essential as a preprocessing step to improve downstream task performance. Deep
learning (DL)-based PCD models, known for their strong representation
capabilities and flexible architectures, have surpassed traditional methods in
denoising performance. To our best knowledge, despite recent advances in
performance, no comprehensive survey systematically summarizes the developments
of DL-based PCD. To fill the gap, this paper seeks to identify key challenges
in DL-based PCD, summarizes the main contributions of existing methods, and
proposes a taxonomy tailored to denoising tasks. To achieve this goal, we
formulate PCD as a two-step process: outlier removal and surface noise
restoration, encompassing most scenarios and requirements of PCD. Additionally,
we compare methods in terms of similarities, differences, and respective
advantages. Finally, we discuss research limitations and future directions,
offering insights for further advancements in PCD.

</details>


### [34] [DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects](https://arxiv.org/abs/2508.11950)
*Tingbang Liang,Yixin Zeng,Jiatong Xie,Boyu Zhou*

Main category: cs.CV

TL;DR: DynamicPose是一个无需重新训练的6D位姿跟踪框架，专门针对快速移动的相机和物体场景，通过视觉惯性里程计、深度信息2D跟踪器和VIO引导的卡尔曼滤波器三个组件协同工作，实现鲁棒的实时6D位姿跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要适用于静态或准静态场景，当相机和物体都快速移动时性能显著下降，需要解决快速运动场景下的6D位姿跟踪鲁棒性问题。

Method: 提出三个协同组件：1）视觉惯性里程计补偿相机运动引起的ROI偏移；2）深度信息2D跟踪器校正大物体平移引起的ROI偏差；3）VIO引导的卡尔曼滤波器预测物体旋转并生成候选位姿，通过分层细化获得最终位姿。形成闭环系统确保精确的位姿初始化和跟踪。

Result: 仿真和真实世界实验证明了方法的有效性，能够实现快速移动相机和物体的实时鲁棒6D位姿跟踪。

Conclusion: DynamicPose框架成功解决了快速运动场景下的6D位姿跟踪挑战，通过三个组件的协同工作和闭环系统设计，实现了优于现有方法的鲁棒性和实时性能。

Abstract: We present DynamicPose, a retraining-free 6D pose tracking framework that
improves tracking robustness in fast-moving camera and object scenarios.
Previous work is mainly applicable to static or quasi-static scenes, and its
performance significantly deteriorates when both the object and the camera move
rapidly. To overcome these challenges, we propose three synergistic components:
(1) A visual-inertial odometry compensates for the shift in the Region of
Interest (ROI) caused by camera motion; (2) A depth-informed 2D tracker
corrects ROI deviations caused by large object translation; (3) A VIO-guided
Kalman filter predicts object rotation, generates multiple candidate poses, and
then obtains the final pose by hierarchical refinement. The 6D pose tracking
results guide subsequent 2D tracking and Kalman filter updates, forming a
closed-loop system that ensures accurate pose initialization and precise pose
tracking. Simulation and real-world experiments demonstrate the effectiveness
of our method, achieving real-time and robust 6D pose tracking for fast-moving
cameras and objects.

</details>


### [35] [Transferable Class Statistics and Multi-scale Feature Approximation for 3D Object Detection](https://arxiv.org/abs/2508.11951)
*Hao Peng,Hong Sang,Yajing Ma,Ping Qiu,Chao Ji*

Main category: cs.CV

TL;DR: 这篇论文通过知识蓄约和可转移特征嵌入机制，在单一邻域内近似多尺度特征，实现了轻量化的点云物体检测方案。


<details>
  <summary>Details</summary>
Motivation: 多尺度特征对点云物体检测至关重要，但传统方法需要多次邻域搜索和尺度感知层，计算成本高且不利于轻量化模型的发展。

Method: 基于知识蓄约在单一邻域近似多尺度特征，设计可转移特征嵌入机制补偿构造性多样性损失，使用类别统计特征作为可转移特征，并提出中心加权IoU优化定位准确性。

Result: 在公开数据集上进行了广泛实验，证明了所提方法的有效性，同时节省了计算成本。

Conclusion: 该方法通过知识蓄约和可转移特征机制，在保持检测性能的同时实现了轻量化设计，为计算资源有限的环境下的点云物体检测提供了有效解决方案。

Abstract: This paper investigates multi-scale feature approximation and transferable
features for object detection from point clouds. Multi-scale features are
critical for object detection from point clouds. However, multi-scale feature
learning usually involves multiple neighborhood searches and scale-aware
layers, which can hinder efforts to achieve lightweight models and may not be
conducive to research constrained by limited computational resources. This
paper approximates point-based multi-scale features from a single neighborhood
based on knowledge distillation. To compensate for the loss of constructive
diversity in a single neighborhood, this paper designs a transferable feature
embedding mechanism. Specifically, class-aware statistics are employed as
transferable features given the small computational cost. In addition, this
paper introduces the central weighted intersection over union for localization
to alleviate the misalignment brought by the center offset in optimization.
Note that the method presented in this paper saves computational costs.
Extensive experiments on public datasets demonstrate the effectiveness of the
proposed method.

</details>


### [36] [UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding](https://arxiv.org/abs/2508.11952)
*Yueming Xu,Jiahui Zhang,Ze Huang,Yurui Chen,Yanpeng Zhou,Zhenyu Chen,Yu-Jie Yuan,Pengxiang Xia,Guowei Huang,Xinyue Cai,Zhongang Qi,Xingyue Quan,Jianye Hao,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: UniUGG是首个统一理解和生成3D模态的框架，使用LLM理解和解码文本与3D表示，通过潜在扩散模型生成高质量3D内容，支持空间视觉问答和3D场景生成。


<details>
  <summary>Details</summary>
Motivation: 尽管统一架构在图像理解和生成方面取得显著进展，但3D任务的整合仍然具有挑战性且未被充分探索，需要开发能够同时处理3D理解和生成的统一框架。

Method: 使用大型语言模型(LLM)来理解和解码句子与3D表示，核心采用空间解码器结合潜在扩散模型生成高质量3D表示，并提出几何-语义学习策略预训练视觉编码器。

Result: 大量实验结果表明该方法在视觉表示、空间理解和3D生成方面具有优越性能。

Conclusion: UniUGG成功实现了3D模态的统一理解和生成，为3D视觉任务提供了有效的解决方案，源代码将在论文接受后发布。

Abstract: Despite the impressive progress on understanding and generating images shown
by the recent unified architectures, the integration of 3D tasks remains
challenging and largely unexplored. In this paper, we introduce UniUGG, the
first unified understanding and generation framework for 3D modalities. Our
unified framework employs an LLM to comprehend and decode sentences and 3D
representations. At its core, we propose a spatial decoder leveraging a latent
diffusion model to generate high-quality 3D representations. This allows for
the generation and imagination of 3D scenes based on a reference image and an
arbitrary view transformation, while remaining supports for spatial visual
question answering (VQA) tasks. Additionally, we propose a geometric-semantic
learning strategy to pretrain the vision encoder. This design jointly captures
the input's semantic and geometric cues, enhancing both spatial understanding
and generation. Extensive experimental results demonstrate the superiority of
our method in visual representation, spatial understanding, and 3D generation.
The source code will be released upon paper acceptance.

</details>


### [37] [SAMDWICH: Moment-aware Video-text Alignment for Referring Video Object Segmentation](https://arxiv.org/abs/2508.11955)
*Seunghun Lee,Jiwan Seo,Jeonghoon Kim,Siwon Kim,Haeun Yun,Hyogyeong Jeon,Wonhyeok Choi,Jaehoon Jeong,Zane Durante,Sang Hyun Park,Sunghoon Im*

Main category: cs.CV

TL;DR: SAMDWICH是一个基于时刻感知的Referring Video Object Segmentation框架，通过新标注的MeViS-M数据集和时刻引导的双路径传播策略，解决了现有方法中的语义错位问题，在复杂场景下实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Referring Video Object Segmentation方法存在语义错位问题，主要原因是训练时对所有可见对象进行无差别帧采样和监督，而不考虑它们与文本查询的实际相关性。

Method: 提出了SAMDWICH框架，包含：1）新标注的MeViS-M数据集，手动标注了每个对象被表达式引用的时间时刻；2）时刻引导的双路径传播策略（MDP），通过时刻中心记忆机制在相关和不相关帧上进行训练；3）对象级选择性监督（OSS），只监督与表达式时间对齐的对象。

Result: 在具有挑战性的MeViS基准测试中实现了最先进的性能，特别是在涉及多样化表达式的复杂场景中表现出色。

Conclusion: 通过时刻感知的监督和选择性训练策略，SAMDWICH显著增强了视频-文本对齐和参考理解能力，为解决RVOS中的语义错位问题提供了有效解决方案。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment and track objects
in videos based on natural language expressions, requiring precise alignment
between visual content and textual queries. However, existing methods often
suffer from semantic misalignment, largely due to indiscriminate frame sampling
and supervision of all visible objects during training -- regardless of their
actual relevance to the expression. To address this, we introduce a
moment-aware RVOS framework named SAMDWICH, along with a newly annotated
dataset, MeViS-M, built upon the challenging MeViS benchmark. We manually
annotate temporal moments indicating when each object is referred to by the
expression, enabling semantically grounded supervision that strengthens
video-text alignment. SAMDWICH leverages these aligned text-to-clip pairs to
guide training, significantly enhancing referential understanding. Building
upon this framework, we propose Moment-guided Dual-path Propagation (MDP), a
moment-aware propagation strategy that improves both object grounding and
tracking by training on both relevant and irrelevant frames through a
moment-centric memory mechanism. In addition, we introduce Object-level
Selective Supervision (OSS), an object-level filtering strategy that supervises
only the objects temporally aligned with the expression in each training clip.
This selective supervision reduces semantic noise and reinforces
language-conditioned learning. Extensive experiments show that SAMDWICH
achieves state-of-the-art performance on challenging MeViS benchmark,
particularly excelling in complex scenarios involving diverse expressions.

</details>


### [38] [PEdger++: Practical Edge Detection via Assembling Cross Information](https://arxiv.org/abs/2508.11961)
*Yuanbin Fu,Liang Li,Xiaojie Guo*

Main category: cs.CV

TL;DR: PEdger++是一个协作学习框架，通过利用异构架构、不同训练时刻和参数采样的跨信息来提升边缘检测性能，在保持高精度的同时显著降低计算成本和模型大小。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习边缘检测方法计算成本高、模型复杂度大的问题，使其能够在资源受限设备上部署，实现精度与效率的平衡。

Method: 提出协作学习框架PEdger++，利用异构架构、多样化训练时刻和多重参数采样的跨信息来增强学习效果，从集成角度提升性能。

Result: 在BSDS500、NYUD和Multicue数据集上的实验表明，该方法在定量和定性评估中都优于现有方法，同时提供多个不同计算需求的模型版本。

Conclusion: PEdger++成功实现了边缘检测精度与计算效率的良好平衡，展示了在不同资源约束下的强适应性，为实际部署提供了可行解决方案。

Abstract: Edge detection serves as a critical foundation for numerous computer vision
applications, including object detection, semantic segmentation, and image
editing, by extracting essential structural cues that define object boundaries
and salient edges. To be viable for broad deployment across devices with
varying computational capacities, edge detectors shall balance high accuracy
with low computational complexity. While deep learning has evidently improved
accuracy, they often suffer from high computational costs, limiting their
applicability on resource-constrained devices. This paper addresses the
challenge of achieving that balance: \textit{i.e.}, {how to efficiently capture
discriminative features without relying on large-size and sophisticated
models}. We propose PEdger++, a collaborative learning framework designed to
reduce computational costs and model sizes while improving edge detection
accuracy. The core principle of our PEdger++ is that cross-information derived
from heterogeneous architectures, diverse training moments, and multiple
parameter samplings, is beneficial to enhance learning from an ensemble
perspective. Extensive experimental results on the BSDS500, NYUD and Multicue
datasets demonstrate the effectiveness of our approach, both quantitatively and
qualitatively, showing clear improvements over existing methods. We also
provide multiple versions of the model with varying computational requirements,
highlighting PEdger++'s adaptability with respect to different resource
constraints. Codes are accessible at
https://github.com/ForawardStar/EdgeDetectionviaPEdgerPlus/.

</details>


### [39] [Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis](https://arxiv.org/abs/2508.11988)
*Nicolas Mastropasqua,Ignacio Bugueno-Cordova,Rodrigo Verschae,Daniel Acevedo,Pablo Negri,Maria E. Buemi*

Main category: cs.CV

TL;DR: 事件相机在微表情分析中显示优势，准确率比RGB相机提升一倍以上，并能完整重建帧界面


<details>
  <summary>Details</summary>
Motivation: 微表情分析在人机交互和驾驶监控中有重要应用，但传统RGB相机因时间分辨率和运动模糊限制而难以捐捕细微面部运动

Method: 提出了一个新的多分辨率多模态微表情数据集，使用同步RGB和事件相机在变化光照条件下进行记录，并使用疯射神经网络进行动作单元分类和条件变分自编码器进行帧重建

Result: 事件相机在动作单元分类中达到51.23%准确率（RGB仅23.12%），在帧重建中达到SSIM=0.8513和PSNR=26.89dB的高质量结果

Conclusion: 事件基于数据在微表情识别和帧重建任务中表现出艰望的潜力，为微科表情分析提供了更有效的解决方案

Abstract: Micro-expression analysis has applications in domains such as Human-Robot
Interaction and Driver Monitoring Systems. Accurately capturing subtle and fast
facial movements remains difficult when relying solely on RGB cameras, due to
limitations in temporal resolution and sensitivity to motion blur. Event
cameras offer an alternative, with microsecond-level precision, high dynamic
range, and low latency. However, public datasets featuring event-based
recordings of Action Units are still scarce. In this work, we introduce a
novel, preliminary multi-resolution and multi-modal micro-expression dataset
recorded with synchronized RGB and event cameras under variable lighting
conditions. Two baseline tasks are evaluated to explore the spatial-temporal
dynamics of micro-expressions: Action Unit classification using Spiking Neural
Networks (51.23\% accuracy with events vs. 23.12\% with RGB), and frame
reconstruction using Conditional Variational Autoencoders, achieving SSIM =
0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising
results show that event-based data can be used for micro-expression recognition
and frame reconstruction.

</details>


### [40] [MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2508.11999)
*Daoze Zhang,Zhanheng Nie,Jianyu Liu,Chenghan Fu,Wanxian Guan,Yuan Gao,Jun Song,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: MOON是首个基于生成式多模态大语言模型的产品表示学习方法，通过引导式专家混合模块、核心语义区域检测和专门负采样策略，解决了产品图像文本多对一对齐、背景噪声干扰等挑战，并在多个下游任务中展现出强大的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有判别式双流架构难以建模产品多图像与文本之间的多对一对应关系，而生成式多模态大语言模型在改进产品表示学习方面具有巨大潜力，但面临缺乏多模态建模模块、产品图像背景噪声干扰以及缺乏标准评估基准等挑战。

Method: 1) 采用引导式专家混合(MoE)模块进行多模态和特定方面的目标建模；2) 有效检测产品图像中的核心语义区域以减少背景噪声干扰；3) 引入专门的负采样策略增加负样本的难度和多样性；4) 发布大规模多模态基准MBE。

Result: 模型在新建基准和公共数据集上均表现出竞争力的零样本性能，在跨模态检索、产品分类和属性预测等各种下游任务中展现出强大的泛化能力。案例研究和可视化证明了MOON在产品理解方面的有效性。

Conclusion: MOON成功解决了生成式MLLM在产品表示学习中的关键挑战，通过创新的多模态建模方法和噪声处理技术，为产品理解任务提供了有效的解决方案，并建立了标准评估基准推动该领域发展。

Abstract: With the rapid advancement of e-commerce, exploring general representations
rather than task-specific ones has attracted increasing research attention. For
product understanding, although existing discriminative dual-flow architectures
drive progress in this field, they inherently struggle to model the many-to-one
alignment between multiple images and texts of products. Therefore, we argue
that generative Multimodal Large Language Models (MLLMs) hold significant
potential for improving product representation learning. Nevertheless,
achieving this goal still remains non-trivial due to several key challenges:
the lack of multimodal and aspect-aware modeling modules in typical LLMs; the
common presence of background noise in product images; and the absence of a
standard benchmark for evaluation. To address these issues, we propose the
first generative MLLM-based model named MOON for product representation
learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for
targeted modeling of multimodal and aspect-specific product content; (2)
effectively detects core semantic regions in product images to mitigate the
distraction and interference caused by background noise; and (3) introduces the
specialized negative sampling strategy to increase the difficulty and diversity
of negative samples. In addition, we release a large-scale multimodal benchmark
MBE for various product understanding tasks. Experimentally, our model
demonstrates competitive zero-shot performance on both our benchmark and the
public dataset, showcasing strong generalization across various downstream
tasks, including cross-modal retrieval, product classification, and attribute
prediction. Furthermore, the case study and visualization illustrate the
effectiveness of MOON for product understanding.

</details>


### [41] [InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2508.12015)
*Hongyuan Liu,Haochen Yu,Jianfei Jiang,Qiankun Liu,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: InstDrive是一个面向动态驾驶场景的实例感知3D高斯泼溅框架，通过SAM生成的掩码作为伪真值指导2D特征学习，在3D层面引入正则化隐式编码实例身份，实现了无需数据预处理的动态开放世界驾驶场景3D实例分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法将所有背景元素统一为单一表示，阻碍了实例级理解和灵活场景编辑；现有室内场景方法不适用于室外驾驶场景；需要解决2D分割到3D空间的映射问题。

Method: 使用SAM生成的掩码作为伪真值，通过对比损失和伪监督目标指导2D特征学习；在3D层面引入正则化隐式编码实例身份，通过体素损失强制一致性；使用轻量级静态代码桥接连续特征和离散身份。

Result: 定量和定性实验证明了InstDrive的有效性，是首个在动态开放世界驾驶场景中实现3D实例分割的框架。

Conclusion: InstDrive成功解决了动态驾驶场景的实例感知重建问题，无需数据预处理或复杂优化，为自动驾驶和场景理解提供了有效的解决方案。

Abstract: Reconstructing dynamic driving scenes from dashcam videos has attracted
increasing attention due to its significance in autonomous driving and scene
understanding. While recent advances have made impressive progress, most
methods still unify all background elements into a single representation,
hindering both instance-level understanding and flexible scene editing. Some
approaches attempt to lift 2D segmentation into 3D space, but often rely on
pre-processed instance IDs or complex pipelines to map continuous features to
discrete identities. Moreover, these methods are typically designed for indoor
scenes with rich viewpoints, making them less applicable to outdoor driving
scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian
Splatting framework tailored for the interactive reconstruction of dynamic
driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D
feature learning via contrastive loss and pseudo-supervised objectives. At the
3D level, we introduce regularization to implicitly encode instance identities
and enforce consistency through a voxel-based loss. A lightweight static
codebook further bridges continuous features and discrete identities without
requiring data pre-processing or complex optimization. Quantitative and
qualitative experiments demonstrate the effectiveness of InstDrive, and to the
best of our knowledge, it is the first framework to achieve 3D instance
segmentation in dynamic, open-world driving scenes.More visualizations are
available at our project page.

</details>


### [42] [WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements](https://arxiv.org/abs/2508.12023)
*Durgesh Kumar Singh,Qing Cao,Sarina Thomas,Ahcène Boubekki,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 基于强化学习的全自动化左室线性测量框架，通过结合B模式图像结构信息和AMM模式运动信息来提高测量精度和稳健性


<details>
  <summary>Details</summary>
Motivation: 解决现有自动化方法在B模式图像中直接预测标记点时，小的位移导致测量错误过大的问题，影响临床可靠性

Method: 提出了WiseLVAM框架：1）使用弱监督B模式标记点检测器估计左室轮廓 2）推断左室长轴和基底水平来定位扫描线 3）在AMM模式中自动进行线性测量

Result: 方法结合了B模式图像的结构感知能力和AMM模式的运动感知能力，提高了测量的稳健性和准确性

Conclusion: WiseLVAM为左室线性测量提供了一种全自动化但又可手动调整的实用解决方案，有潜力在常规临床应用中推广

Abstract: Clinical guidelines recommend performing left ventricular (LV) linear
measurements in B-mode echocardiographic images at the basal level -- typically
at the mitral valve leaflet tips -- and aligned perpendicular to the LV long
axis along a virtual scanline (SL). However, most automated methods estimate
landmarks directly from B-mode images for the measurement task, where even
small shifts in predicted points along the LV walls can lead to significant
measurement errors, reducing their clinical reliability. A recent
semi-automatic method, EnLVAM, addresses this limitation by constraining
landmark prediction to a clinician-defined SL and training on generated
Anatomical Motion Mode (AMM) images to predict LV landmarks along the same. To
enable full automation, a contour-aware SL placement approach is proposed in
this work, in which the LV contour is estimated using a weakly supervised
B-mode landmark detector. SL placement is then performed by inferring the LV
long axis and the basal level-mimicking clinical guidelines. Building on this
foundation, we introduce \textit{WiseLVAM} -- a novel, fully automated yet
manually adaptable framework for automatically placing the SL and then
automatically performing the LV linear measurements in the AMM mode.
\textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the
motion-awareness from AMM mode to enhance robustness and accuracy with the
potential to provide a practical solution for the routine clinical application.

</details>


### [43] [Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering](https://arxiv.org/abs/2508.12036)
*Rakesh Thakur,Yusra Tariq*

Main category: cs.CV

TL;DR: 基于频域表示和量子检索的医疗视觉问答模型Q-FSRU，通过FFT频域转换和量子反馈增强技术，在VQA-RAD数据集上得到了更优的表现


<details>
  <summary>Details</summary>
Motivation: 解决需要同时理解图像和文本的复杂临床问题，提高医疗AI的准确性和可解释性

Method: 结合频谱表示融合(FSRU)和量子检索增强生成(Quantum RAG)技术，通过FFT将图像文本特征转换到频域，使用量子相似性技术从外部源获取医学知识

Result: 在VQA-RAD数据集上超过之前模型，尤其在需要图像-文本推理的复杂案例中表现更优

Conclusion: 频域信息与量子信息的结合提高了性能和可解释性，为建立智能、清晰、有用的医生AI工具提供了有前景的方法

Abstract: Solving tough clinical questions that require both image and text
understanding is still a major challenge in healthcare AI. In this work, we
propose Q-FSRU, a new model that combines Frequency Spectrum Representation and
Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation
(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in
features from medical images and related text, then shifts them into the
frequency domain using Fast Fourier Transform (FFT). This helps it focus on
more meaningful data and filter out noise or less useful information. To
improve accuracy and ensure that answers are based on real knowledge, we add a
quantum-inspired retrieval system. It fetches useful medical facts from
external sources using quantum-based similarity techniques. These details are
then merged with the frequency-based features for stronger reasoning. We
evaluated our model using the VQA-RAD dataset, which includes real radiology
images and questions. The results showed that Q-FSRU outperforms earlier
models, especially on complex cases needing image-text reasoning. The mix of
frequency and quantum information improves both performance and explainability.
Overall, this approach offers a promising way to build smart, clear, and
helpful AI tools for doctors.

</details>


### [44] [VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models](https://arxiv.org/abs/2508.12081)
*Haidong Xu,Guangwei Xu,Zhedong Zheng,Xiatian Zhu,Wei Ji,Xiangtai Li,Ruijie Guo,Meishan Zhang,Min zhang,Hao Fei*

Main category: cs.CV

TL;DR: VimoRAG是一个基于视频检索增强的运动生成框架，通过从大规模视频数据库中检索相关2D人体运动信号来解决运动大语言模型的数据稀缺问题，显著提升了仅文本输入条件下的3D运动生成性能。


<details>
  <summary>Details</summary>
Motivation: 运动大语言模型由于标注数据有限而面临严重的域外/词汇外问题，需要利用大规模野外视频数据库来增强3D运动生成能力。

Method: 设计了Gemini Motion Video Retriever机制进行有效的运动中心视频检索，以及Motion-centric Dual-alignment DPO Trainer来缓解检索结果不佳导致的错误传播问题。

Result: 实验结果表明，VimoRAG显著提升了仅文本输入条件下运动大语言模型的性能。

Conclusion: VimoRAG框架成功解决了视频运动检索增强的关键瓶颈问题，为运动生成提供了有效的检索增强解决方案。

Abstract: This paper introduces VimoRAG, a novel video-based retrieval-augmented motion
generation framework for motion large language models (LLMs). As motion LLMs
face severe out-of-domain/out-of-vocabulary issues due to limited annotated
data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D
motion generation by retrieving relevant 2D human motion signals. While
video-based motion RAG is nontrivial, we address two key bottlenecks: (1)
developing an effective motion-centered video retrieval model that
distinguishes human poses and actions, and (2) mitigating the issue of error
propagation caused by suboptimal retrieval results. We design the Gemini Motion
Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,
enabling effective retrieval and generation processes. Experimental results
show that VimoRAG significantly boosts the performance of motion LLMs
constrained to text-only input.

</details>


### [45] [Automated Model Evaluation for Object Detection via Prediction Consistency and Reliablity](https://arxiv.org/abs/2508.12082)
*Seungju Yoo,Hyuk Kwon,Joong-Won Hwang,Kibok Lee*

Main category: cs.CV

TL;DR: 自动化对检测器评估的新方法PCR，通过分析NMS前后检测框的空间一致性和可靠性来估计性能，无需手动标注


<details>
  <summary>Details</summary>
Motivation: 解决对象检测器在实际应用中性能评估依赖成本高昂的手动标注问题

Method: 提出预测一致性和可靠性(PCR)方法，聚合分析NMS前后检测框的空间一致性和重叠框的信心度可靠性

Result: PCR在构建的元数据集上表现超过现有自动评估方法，能够提供更准确的性能估计

Conclusion: 该研究为对象检测器的自动化性能评估提供了有效解决方案，元数据集覆盖了更广泛的检测性能范围

Abstract: Recent advances in computer vision have made training object detectors more
efficient and effective; however, assessing their performance in real-world
applications still relies on costly manual annotation. To address this
limitation, we develop an automated model evaluation (AutoEval) framework for
object detection. We propose Prediction Consistency and Reliability (PCR),
which leverages the multiple candidate bounding boxes that conventional
detectors generate before non-maximum suppression (NMS). PCR estimates
detection performance without ground-truth labels by jointly measuring 1) the
spatial consistency between boxes before and after NMS, and 2) the reliability
of the retained boxes via the confidence scores of overlapping boxes. For a
more realistic and scalable evaluation, we construct a meta-dataset by applying
image corruptions of varying severity. Experimental results demonstrate that
PCR yields more accurate performance estimates than existing AutoEval methods,
and the proposed meta-dataset covers a wider range of detection performance.
The code is available at https://github.com/YonseiML/autoeval-det.

</details>


### [46] [Generic Event Boundary Detection via Denoising Diffusion](https://arxiv.org/abs/2508.12084)
*Jaejun Hwang,Dayoung Gong,Manjin Kim,Minsu Cho*

Main category: cs.CV

TL;DR: DiffGEBD是一个基于扩散模型的通用事件边界检测方法，通过生成式方法处理事件边界的主观多样性问题，在Kinetics-GEBD和TAPOS基准上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 传统的事件边界检测方法只做确定性预测，忽视了事件边界的主观性和多样性，需要一种能够生成多种合理边界的方法

Method: 提出基于扩散模型的DiffGEBD，通过时间自相似性编码相邻帧变化，然后迭代地将随机噪声解码为合理的事件边界，使用分类器自由引导控制多样性

Result: 在Kinetics-GEBD和TAPOS两个标准基准上取得了强劲性能，能够生成多样且合理的事件边界

Conclusion: 扩散模型为通用事件边界检测提供了有效的生成式解决方案，能够处理边界标注的主观多样性问题

Abstract: Generic event boundary detection (GEBD) aims to identify natural boundaries
in a video, segmenting it into distinct and meaningful chunks. Despite the
inherent subjectivity of event boundaries, previous methods have focused on
deterministic predictions, overlooking the diversity of plausible solutions. In
this paper, we introduce a novel diffusion-based boundary detection model,
dubbed DiffGEBD, that tackles the problem of GEBD from a generative
perspective. The proposed model encodes relevant changes across adjacent frames
via temporal self-similarity and then iteratively decodes random noise into
plausible event boundaries being conditioned on the encoded features.
Classifier-free guidance allows the degree of diversity to be controlled in
denoising diffusion. In addition, we introduce a new evaluation metric to
assess the quality of predictions considering both diversity and fidelity.
Experiments show that our method achieves strong performance on two standard
benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event
boundaries.

</details>


### [47] [Enhancing 3D point accuracy of laser scanner through multi-stage convolutional neural network for applications in construction](https://arxiv.org/abs/2508.12089)
*Qinyuan Fan,Clemens Gühmann*

Main category: cs.CV

TL;DR: 通过多段卷积神经网络和高低端扫描仪配对，实现了粗糕室内环境中低端光谱扫描仪测量精度的显著提升，MSE降低70%以上


<details>
  <summary>Details</summary>
Motivation: 解决不同级别光谱扫描仪因设备限制和环境因素导致的位置误差问题，提供更准确的空间测量以支撑高精度几何模型创建

Method: 采用高精度扫描仪作为参考，与低端设备在同一环境中配对测量，通过统计关系建立测量偏差与空间分布的关联，结合传统几何处理和神经网络精细化进行系统误差等量化和缩正

Result: 在粗糕室内数据集中实验显示测量精度显著提升，均方误差降低70%以上，峰值信噪比提升约6分贝

Conclusion: 该方法使低端设备在不改变硬件的情况下能够达到接近高端设备的测量不确定性水平，同时保持了关键几何特征

Abstract: We propose a multi-stage convolutional neural network (MSCNN) based
integrated method for reducing uncertainty of 3D point accuracy of lasar
scanner (LS) in rough indoor rooms, providing more accurate spatial
measurements for high-precision geometric model creation and renovation. Due to
different equipment limitations and environmental factors, high-end and low-end
LS have positional errors. Our approach pairs high-accuracy scanners (HAS) as
references with corresponding low-accuracy scanners (LAS) of measurements in
identical environments to quantify specific error patterns. By establishing a
statistical relationship between measurement discrepancies and their spatial
distribution, we develop a correction framework that combines traditional
geometric processing with targeted neural network refinement. This method
transforms the quantification of systematic errors into a supervised learning
problem, allowing precise correction while preserving critical geometric
features. Experimental results in our rough indoor rooms dataset show
significant improvements in measurement accuracy, with mean square error (MSE)
reductions exceeding 70% and peak signal-to-noise ratio (PSNR) improvements of
approximately 6 decibels. This approach enables low-end devices to achieve
measurement uncertainty levels approaching those of high-end devices without
hardware modifications.

</details>


### [48] [Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion](https://arxiv.org/abs/2508.12094)
*Songwei Liu,Hong Liu,Fangmin Chen,Xurui Peng,Chenqian Yan,Lean Fu,Xing Mei*

Main category: cs.CV

TL;DR: 这篇论文提出了一种时间步感知的累积错误补偿方案，用于减少漫散模型在量化后的迭代过程中的错误传播，提高低精度模型的输出质量。


<details>
  <summary>Details</summary>
Motivation: 漫散模型虽然在图像合成方面表现突出，但迭代反污哄过程计算开销很大，后训练量化(PTQ)可以加速金取样，但迭代性质导致量化错误在生成过程中累积，影响输出质量。

Method: 建立理论框架数学地形容错误传播，求解每步量化错误传播方程，得到累积错误的闭式解，基于此提出时间步感知的累积错误补偿方案。

Result: 在多个图像数据集上的实验表明，该补偿策略有效减少错误传播，显著提升现有PTQ方法的性能，在低精度漫散模型上达到最先进水平。

Conclusion: 通过理论分析错误传播机制并提出相应的补偿方案，可以有效解决漫散模型量化后的性能下降问题，为大规模部署提供了有效的解决方案。

Abstract: Diffusion models have transformed image synthesis by establishing
unprecedented quality and creativity benchmarks. Nevertheless, their
large-scale deployment faces challenges due to computationally intensive
iterative denoising processes. Although post-training quantization(PTQ)
provides an effective pathway for accelerating sampling, the iterative nature
of diffusion models causes stepwise quantization errors to accumulate
progressively during generation, inevitably compromising output fidelity. To
address this challenge, we develop a theoretical framework that mathematically
formulates error propagation in Diffusion Models (DMs), deriving per-step
quantization error propagation equations and establishing the first closed-form
solution for cumulative error. Building on this theoretical foundation, we
propose a timestep-aware cumulative error compensation scheme. Extensive
experiments across multiple image datasets demonstrate that our compensation
strategy effectively mitigates error propagation, significantly enhancing
existing PTQ methods to achieve state-of-the-art(SOTA) performance on
low-precision diffusion models.

</details>


### [49] [VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine](https://arxiv.org/abs/2508.12108)
*Ziyang Zhang,Yang Yu,Xulei Yang,Si Yong Yeo*

Main category: cs.CV

TL;DR: VELVET-Med是一个专门针对有限3D医学数据（如CT扫描和放射报告）的视觉语言预训练框架，通过创新的预训练目标和架构设计，在小规模数据上实现优异的下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 医学领域中3D体积数据（如CT扫描）与文本的配对数据收集困难且耗时，限制了视觉语言模型在医学下游任务中的性能表现。

Method: 提出VELVET-Med框架：1）将单模态自监督学习融入VLP框架；2）设计TriBERT语言编码器学习多层次文本语义；3）开发分层对比学习捕获多层次视觉语言对应关系。仅使用38,875个扫描-报告对。

Result: 学习到的编码器展现出强大的迁移能力，在3D分割、跨模态检索、视觉问答和报告生成等多种下游任务中达到最先进性能。

Conclusion: 该研究表明，通过精心设计的预训练目标和架构，即使在小规模医学数据上也能有效学习丰富的空间和语义关系，显著提升医学视觉语言模型的泛化能力。

Abstract: Vision-and-language models (VLMs) have been increasingly explored in the
medical domain, particularly following the success of CLIP in general domain.
However, unlike the relatively straightforward pairing of 2D images and text,
curating large-scale paired data in the medical field for volumetric modalities
such as CT scans remains a challenging and time-intensive process. This
difficulty often limits the performance on downstream tasks. To address these
challenges, we propose a novel vision-language pre-training (VLP) framework,
termed as \textbf{VELVET-Med}, specifically designed for limited volumetric
data such as 3D CT and associated radiology reports. Instead of relying on
large-scale data collection, our method focuses on the development of effective
pre-training objectives and model architectures. The key contributions are: 1)
We incorporate uni-modal self-supervised learning into VLP framework, which are
often underexplored in the existing literature. 2) We propose a novel language
encoder, termed as \textbf{TriBERT}, for learning multi-level textual
semantics. 3) We devise the hierarchical contrastive learning to capture
multi-level vision-language correspondence. Using only 38,875 scan-report
pairs, our approach seeks to uncover rich spatial and semantic relationships
embedded in volumetric medical images and corresponding clinical narratives,
thereby enhancing the generalization ability of the learned encoders. The
resulting encoders exhibit strong transferability, achieving state-of-the-art
performance across a wide range of downstream tasks, including 3D segmentation,
cross-modal retrieval, visual question answering, and report generation.

</details>


### [50] [Simple o3: Towards Interleaved Vision-Language Reasoning](https://arxiv.org/abs/2508.12109)
*Ye Wang,Qianglong Chen,Zejun Li,Siyuan Wang,Shijie Guo,Zhirui Zhang,Zhongyu Wei*

Main category: cs.CV

TL;DR: Simple o3是一个端到端的多模态思维链框架，通过动态视觉工具交互和"观察-推理-行动"循环，显著提升多模态大语言模型的视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在长链多模态思维推理方面能力不足，需要探索类似人类"图像思维"的迭代视觉转换和语言推理方法。

Method: 提出监督微调框架，集成裁剪、缩放、重用等动态工具交互，构建TWI-Tools-146K数据集，采用"观察-推理-行动"循环生成高质量推理链。

Result: 在多个基准测试中表现优异，重用和放大原始图像显著改善视觉推理，基于精确视觉定位的图像裁剪有效聚焦关键区域。

Conclusion: Simple o3建立了计算高效的多模态推理范式，首次深入分析了不同交错推理策略对模型性能的影响，为多模态推理发展提供了有力工具。

Abstract: Multimodal Large Language Models (MLLMs) have shown impressive performance on
vision-language tasks, but their long Chain-of-Thought (CoT) capabilities in
multimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which
emulates human-like ''thinking with image'' through iterative visual
transformations and linguistic reasoning, we propose Simple o3, an end-to-end
framework that integrates dynamic tool interactions (e.g., cropping, zooming,
and reusing) into interleaved vision-language reasoning via supervised
fine-tuning (SFT). Our approach features a scalable data synthesis pipeline
that generates high-quality interleaved vision-language reasoning chains via an
''observe-reason-act'' cycle, complete with executable visual operations and
rigorous verification, yielding the open-source TWI-Tools-146K dataset.
Experimental results demonstrate Simple o3's superior performance on diverse
benchmarks, outperforming existing approaches. By combining enhanced reasoning
capabilities, Simple o3 establishes a powerful yet computationally affordable
paradigm for advancing multimodal reasoning. Remarkably, we provide the first
in-depth analysis of different interleaved reasoning strategies, offering
insights into their impact on model performance. We found that by introducing
additional visual tokens for interleaved vision-language reasoning, reusing and
magnifying the original image significantly improves the model's visual
reasoning and fine-grained perception, while image cropping based on precise
visual grounding allows the model to effectively focus on key entities or
regions, further enhancing its capabilities.

</details>


### [51] [DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis](https://arxiv.org/abs/2508.12131)
*Minh Tran,Johnmark Clements,Annie Prasanna,Tri Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: DualFit是一个两阶段的虚拟试穿混合管道，通过流场变形和保真度合成模块，在保持服装细节的同时实现视觉无缝的试穿效果


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的免变形方法虽然提升了感知质量，但无法保持服装的细粒度细节（如logo和印刷文字），这些细节对品牌完整性和客户信任至关重要

Method: 采用两阶段方法：第一阶段使用学习到的流场将目标服装变形以对齐人物图像；第二阶段通过保真度试穿模块将变形后的服装与保留的人体区域进行融合，使用保留区域输入和修复掩码指导过程

Result: 广泛的定性结果表明DualFit实现了视觉无缝的试穿结果，同时忠实地保持了高频服装细节，在重建准确性和感知真实性之间取得了有效平衡

Conclusion: DualFit通过混合方法成功解决了虚拟试穿中细节保持的问题，为在线时尚零售提供了更好的解决方案

Abstract: Virtual Try-On technology has garnered significant attention for its
potential to transform the online fashion retail experience by allowing users
to visualize how garments would look on them without physical trials. While
recent advances in diffusion-based warping-free methods have improved
perceptual quality, they often fail to preserve fine-grained garment details
such as logos and printed text elements that are critical for brand integrity
and customer trust. In this work, we propose DualFit, a hybrid VTON pipeline
that addresses this limitation by two-stage approach. In the first stage,
DualFit warps the target garment to align with the person image using a learned
flow field, ensuring high-fidelity preservation. In the second stage, a
fidelity-preserving try-on module synthesizes the final output by blending the
warped garment with preserved human regions. Particularly, to guide this
process, we introduce a preserved-region input and an inpainting mask, enabling
the model to retain key areas and regenerate only where necessary, particularly
around garment seams. Extensive qualitative results show that DualFit achieves
visually seamless try-on results while faithfully maintaining high-frequency
garment details, striking an effective balance between reconstruction accuracy
and perceptual realism.

</details>


### [52] [TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks](https://arxiv.org/abs/2508.12132)
*Amira Guesmi,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: TriQDef是一个三级量化感知防御框架，通过特征不对齐惩罚和梯度感知失谐惩罚来破坏跨位宽的补丁对抗攻击的可转移性，在保持高清洁精度的同时将攻击成功率降低40%以上。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络(QNNs)在边缘设备中部署日益增多，虽然对像素级攻击有一定鲁棒性，但对跨位宽的补丁对抗攻击存在泛化漏洞，现有防御方法要么过拟合固定量化设置，要么无法解决这种跨位宽泛化脆弱性。

Method: TriQDef包含三个核心组件：(1)特征不对齐惩罚(FDP)通过惩罚中间表示的感知相似性来强制语义不一致；(2)梯度感知失谐惩罚(GPDP)通过边缘IoU和HOG余弦度量最小化结构性和方向性一致性来显式错位输入梯度；(3)联合量化感知训练协议，在多个量化级别的共享权重训练方案中统一这些惩罚。

Result: 在CIFAR-10和ImageNet上的大量实验表明，TriQDef在未见过的补丁和量化组合上将攻击成功率(ASR)降低了40%以上，同时保持了高清洁精度。

Conclusion: 研究强调了破坏语义和感知梯度对齐对于减轻QNNs中补丁可转移性的重要性，TriQDef框架有效解决了跨位宽补丁攻击的泛化漏洞问题。

Abstract: Quantized Neural Networks (QNNs) are increasingly deployed in edge and
resource-constrained environments due to their efficiency in computation and
memory usage. While shown to distort the gradient landscape and weaken
conventional pixel-level attacks, it provides limited robustness against
patch-based adversarial attacks-localized, high-saliency perturbations that
remain surprisingly transferable across bit-widths. Existing defenses either
overfit to fixed quantization settings or fail to address this cross-bit
generalization vulnerability. We introduce \textbf{TriQDef}, a tri-level
quantization-aware defense framework designed to disrupt the transferability of
patch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature
Disalignment Penalty (FDP) that enforces semantic inconsistency by penalizing
perceptual similarity in intermediate representations; (2) a Gradient
Perceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients
across bit-widths by minimizing structural and directional agreement via Edge
IoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training
Protocol that unifies these penalties within a shared-weight training scheme
across multiple quantization levels. Extensive experiments on CIFAR-10 and
ImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over
40\% on unseen patch and quantization combinations, while preserving high clean
accuracy. Our findings underscore the importance of disrupting both semantic
and perceptual gradient alignment to mitigate patch transferability in QNNs.

</details>


### [53] [Infusing fine-grained visual knowledge to Vision-Language Models](https://arxiv.org/abs/2508.12137)
*Nikolaos-Antonios Ypsilantis,Kaifeng Chen,André Araujo,Ondřej Chum*

Main category: cs.CV

TL;DR: 这篇论文提出了一种细调方法，在保持预训练视觉-语言模型多模态知识的同时，优化细粒度领域适配性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模对比预训练VLMs在细粒度开放集视觉检索中表现次优的问题，避免传统细调方法导致的灾难性遗忘问题。

Method: 受持续学习文献的启发，系统分析并组合标准正则化技术，同时重视验证集设计和超参数调整的关键细节。

Result: 在细粒度和粗粒度图像-图像、图像-文本棆索测试中均获得了突出结果，无需使用文本数据或原始文本编码器即能保持视觉-文本对齐能力。

Conclusion: 该方法能够有效平衡预训练多模态知识保留与领域特定细调的需求，为视觉-语言模型的领域适配提供了可靠解决方案。

Abstract: Large-scale contrastive pre-training produces powerful Vision-and-Language
Models (VLMs) capable of generating representations (embeddings) effective for
a wide variety of visual and multimodal tasks. However, these pretrained
embeddings remain suboptimal for fine-grained open-set visual retrieval, where
state-of-the-art results require fine-tuning the vision encoder using annotated
domain-specific samples. Naively performing such fine-tuning typically leads to
catastrophic forgetting, severely diminishing the model's general-purpose
visual and cross-modal capabilities.
  In this work, we propose a fine-tuning method explicitly designed to achieve
optimal balance between fine-grained domain adaptation and retention of the
pretrained VLM's broad multimodal knowledge. Drawing inspiration from continual
learning literature, we systematically analyze standard regularization
techniques aimed at knowledge retention and propose an efficient and effective
combination strategy. Additionally, we address the commonly overlooked yet
critical aspects of validation set design and hyperparameter tuning to ensure
reproducibility and robust generalization across datasets and pretrained
models. We extensively evaluate our method on both fine-grained and
coarse-grained image-image and image-text retrieval benchmarks. Our approach
consistently achieves strong results, notably retaining the visual-text
alignment without utilizing any text data or the original text encoder during
fine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .

</details>


### [54] [KP-INR: A Dual-Branch Implicit Neural Representation Model for Cardiac Cine MRI Reconstruction](https://arxiv.org/abs/2508.12147)
*Donghang Lyu,Marius Staring,Mariya Doneva,Hildo J. Lamb,Nicola Pezzotti*

Main category: cs.CV

TL;DR: KP-INR是一种用于心脏电影MRI重建的双分支隐式神经表示方法，通过在k空间坐标位置嵌入和局部多尺度特征表示之间进行交叉交互，实现了比基线模型更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的INR方法主要关注基于坐标的位置嵌入来学习映射，但忽略了目标点及其邻域上下文的特征表示，这限制了心脏电影MRI重建的质量。

Method: 提出KP-INR双分支方法：一个分支处理k空间坐标的位置嵌入，另一个分支学习该坐标处的局部多尺度k空间特征表示，并通过跨分支交互来近似目标k空间值。

Result: 在CMRxRecon2024数据集上的实验证实，KP-INR相比基线模型具有改进的性能，在具有挑战性的笛卡尔k空间数据上表现出色。

Conclusion: KP-INR通过结合位置嵌入和局部特征表示的双分支方法，为心脏电影MRI重建提供了有效的解决方案，在该领域显示出巨大潜力。

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a non-invasive method for
assessing cardiac structure, function, and blood flow. Cine MRI extends this by
capturing heart motion, providing detailed insights into cardiac mechanics. To
reduce scan time and breath-hold discomfort, fast acquisition techniques have
been utilized at the cost of lowering image quality. Recently, Implicit Neural
Representation (INR) methods have shown promise in unsupervised reconstruction
by learning coordinate-to-value mappings from undersampled data, enabling
high-quality image recovery. However, current existing INR methods primarily
focus on using coordinate-based positional embeddings to learn the mapping,
while overlooking the feature representations of the target point and its
neighboring context. In this work, we propose KP-INR, a dual-branch INR method
operating in k-space for cardiac cine MRI reconstruction: one branch processes
the positional embedding of k-space coordinates, while the other learns from
local multi-scale k-space feature representations at those coordinates. By
enabling cross-branch interaction and approximating the target k-space values
from both branches, KP-INR can achieve strong performance on challenging
Cartesian k-space data. Experiments on the CMRxRecon2024 dataset confirms its
improved performance over baseline models and highlights its potential in this
field.

</details>


### [55] [Demystifying Foreground-Background Memorization in Diffusion Models](https://arxiv.org/abs/2508.12148)
*Jimmy Z. Di,Yiwei Lu,Yaoliang Yu,Gautam Kamath,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

TL;DR: 提出了FB-Mem方法，通过分割技术量化扩散模型中的记忆化现象，发现记忆化比之前认为的更普遍，现有缓解方法效果有限。


<details>
  <summary>Details</summary>
Motivation: 当前检测方法只能识别完全相同的记忆化，无法量化小图像区域的局部记忆化，也无法捕捉超越特定提示-图像对的记忆化模式。

Method: 提出基于分割的FB-Mem度量方法，对生成图像中的记忆化区域进行分类和量化，并使用聚类方法进行更强的缓解。

Result: 发现记忆化现象比之前理解的更普遍：(1)单个提示的生成可能与多个相似训练图像相关；(2)现有缓解方法无法消除局部记忆化，特别是在前景区域。

Conclusion: 建立了有效的扩散模型记忆化测量框架，证明了当前缓解方法的不足，并提出了基于聚类的更强缓解方法。

Abstract: Diffusion models (DMs) memorize training images and can reproduce
near-duplicates during generation. Current detection methods identify verbatim
memorization but fail to capture two critical aspects: quantifying partial
memorization occurring in small image regions, and memorization patterns beyond
specific prompt-image pairs. To address these limitations, we propose
Foreground Background Memorization (FB-Mem), a novel segmentation-based metric
that classifies and quantifies memorized regions within generated images. Our
method reveals that memorization is more pervasive than previously understood:
(1) individual generations from single prompts may be linked to clusters of
similar training images, revealing complex memorization patterns that extend
beyond one-to-one correspondences; and (2) existing model-level mitigation
methods, such as neuron deactivation and pruning, fail to eliminate local
memorization, which persists particularly in foreground regions. Our work
establishes an effective framework for measuring memorization in diffusion
models, demonstrates the inadequacy of current mitigation approaches, and
proposes a stronger mitigation method using a clustering approach.

</details>


### [56] [RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis](https://arxiv.org/abs/2508.12163)
*Wenqing Wang,Yun Fu*

Main category: cs.CV

TL;DR: RealTalk是一个新颖的情感说话头合成框架，通过VAE生成3D面部关键点，结合情感标签嵌入和NeRF技术，实现了高情感准确性、增强的情感可控性和鲁棒的身份保持。


<details>
  <summary>Details</summary>
Motivation: 当前方法在唇部同步和图像质量方面表现出色，但在生成准确可控的情感表情同时保持主体身份方面存在不足，这限制了人工智能社交智能的发展。

Method: 使用变分自编码器(VAE)从驱动音频生成3D面部关键点，通过ResNet-based关键点变形模型(LDM)将情感标签嵌入与关键点连接，生成情感关键点。这些关键点和面部混合形状系数共同条件化新型三平面注意力NeRF来合成情感说话头。

Result: 大量实验表明，RealTalk在情感准确性、可控性和身份保持方面优于现有方法。

Conclusion: RealTalk框架显著提升了情感说话头合成的性能，推动了社交智能AI系统的发展。

Abstract: Emotion is a critical component of artificial social intelligence. However,
while current methods excel in lip synchronization and image quality, they
often fail to generate accurate and controllable emotional expressions while
preserving the subject's identity. To address this challenge, we introduce
RealTalk, a novel framework for synthesizing emotional talking heads with high
emotion accuracy, enhanced emotion controllability, and robust identity
preservation. RealTalk employs a variational autoencoder (VAE) to generate 3D
facial landmarks from driving audio, which are concatenated with emotion-label
embeddings using a ResNet-based landmark deformation model (LDM) to produce
emotional landmarks. These landmarks and facial blendshape coefficients jointly
condition a novel tri-plane attention Neural Radiance Field (NeRF) to
synthesize highly realistic emotional talking heads. Extensive experiments
demonstrate that RealTalk outperforms existing methods in emotion accuracy,
controllability, and identity preservation, advancing the development of
socially intelligent AI systems.

</details>


### [57] [Scalable RF Simulation in Generative 4D Worlds](https://arxiv.org/abs/2508.12176)
*Zhiwei Zheng,Dongyin Hu,Mingmin Zhao*

Main category: cs.CV

TL;DR: WaveVerse是一个基于提示的RF信号仿真框架，能够从生成的室内场景和人体运动中模拟真实的射频信号，解决了RF数据收集的挑战。


<details>
  <summary>Details</summary>
Motivation: 射频传感作为视觉方法的隐私保护替代方案，但在动态多样的室内环境中收集高质量RF数据仍然困难。

Method: 引入语言引导的4D世界生成器，包括状态感知因果变换器用于条件化人体运动生成，以及相位相干射线追踪模拟器用于准确RF信号模拟。

Result: 实验证明了在条件化人体运动生成方面的有效性，展示了相位相干性在波束成形和呼吸监测中的应用，并在RF成像和人体活动识别中实现了性能提升。

Conclusion: WaveVerse首次实现了RF成像数据生成，在数据有限和数据充足场景下都能获得一致的性能增益，为RF感知提供了可扩展的解决方案。

Abstract: Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving
alternative to vision-based methods for indoor perception tasks. However,
collecting high-quality RF data in dynamic and diverse indoor environments
remains a major challenge. To address this, we introduce WaveVerse, a
prompt-based, scalable framework that simulates realistic RF signals from
generated indoor scenes with human motions. WaveVerse introduces a
language-guided 4D world generator, which includes a state-aware causal
transformer for human motion generation conditioned on spatial constraints and
texts, and a phase-coherent ray tracing simulator that enables the simulation
of accurate and coherent RF signals. Experiments demonstrate the effectiveness
of our approach in conditioned human motion generation and highlight how phase
coherence is applied to beamforming and respiration monitoring. We further
present two case studies in ML-based high-resolution imaging and human activity
recognition, demonstrating that WaveVerse not only enables data generation for
RF imaging for the first time, but also consistently achieves performance gain
in both data-limited and data-adequate scenarios.

</details>


### [58] [Splat Feature Solver](https://arxiv.org/abs/2508.12216)
*Butian Xiong,Rong Liu,Kenneth Xu,Meida Chen,Andrew Feng*

Main category: cs.CV

TL;DR: 提出了一种统一的特征提升方法，将多视角图像特征（如DINO、CLIP）附加到基于splat的3D表示上，通过稀疏线性逆问题求解，在开放词汇3D分割任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D场景理解中特征提升的核心挑战：如何最优地将丰富的通用图像特征分配给3D基元，同时处理多视角图像的不一致性问题。

Method: 将特征提升问题表述为稀疏线性逆问题，可闭式求解；引入Tikhonov正则化确保数值稳定性，后提升聚合通过特征聚类过滤噪声输入。

Result: 在开放词汇3D分割基准测试中达到最先进性能，优于基于训练、分组和启发式的前沿基线方法，且能在几分钟内生成提升特征。

Conclusion: 提出的统一特征提升框架在理论和实践上都表现出色，为3D场景理解提供了高质量的特征表示解决方案。

Abstract: Feature lifting has emerged as a crucial component in 3D scene understanding,
enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP)
onto splat-based 3D representations. The core challenge lies in optimally
assigning rich general attributes to 3D primitives while addressing the
inconsistency issues from multi-view images. We present a unified, kernel- and
feature-agnostic formulation of the feature lifting problem as a sparse linear
inverse problem, which can be solved efficiently in closed form. Our approach
admits a provable upper bound on the global optimal error under convex losses
for delivering high quality lifted features. To address inconsistencies and
noise in multi-view observations, we introduce two complementary regularization
strategies to stabilize the solution and enhance semantic fidelity. Tikhonov
Guidance enforces numerical stability through soft diagonal dominance, while
Post-Lifting Aggregation filters noisy inputs via feature clustering. Extensive
experiments demonstrate that our approach achieves state-of-the-art performance
on open-vocabulary 3D segmentation benchmarks, outperforming training-based,
grouping-based, and heuristic-forward baselines while producing the lifted
features in minutes. Code is available at
\href{https://github.com/saliteta/splat-distiller.git}{\textbf{github}}. We
also have a \href{https://splat-distiller.pages.dev/}

</details>


### [59] [C2PSA-Enhanced YOLOv11 Architecture: A Novel Approach for Small Target Detection in Cotton Disease Diagnosis](https://arxiv.org/abs/2508.12219)
*Kaiyuan Wang,Jixing Liu,Xiaobo Cai*

Main category: cs.CV

TL;DR: 基于YOLOv11的深度学习优化方案，通过C2PSA模块、动态类别权重和改进数据增帽技术，显著提升棉芬病害检测精度和速度，实现了高效的种植业实时监测系统。


<details>
  <summary>Details</summary>
Motivation: 解决棉芬病害检测中的三大挑战：早期病斑检测精度低（小于5mm²病斑漏检率35%），田间环境下性能下降（准确率下降25%），以及多病害场景错误率高（34.7%）。

Method: 提出C2PSA模块优化小目标特征提取，采用动态类别权重处理样本不平衡问题，通过Mosaic-MixUp缩放技术改进数据增帽。

Result: 在4078张图片数据集上验证：mAP50达到0.820（提升8.0%），mAP50-95为0.705（提升10.5%），推理速度158FPS。系统已部署到移动设备实现实时监测。

Conclusion: 该优化方案有效解决了棉芬病害检测的关键问题，显著提升了检测精度和速度，为农业智能监测和精准治疗提供了可靠技术支撑。

Abstract: This study presents a deep learning-based optimization of YOLOv11 for cotton
disease detection, developing an intelligent monitoring system. Three key
challenges are addressed: (1) low precision in early spot detection (35%
leakage rate for sub-5mm2 spots), (2) performance degradation in field
conditions (25% accuracy drop), and (3) high error rates (34.7%) in
multi-disease scenarios. The proposed solutions include: C2PSA module for
enhanced small-target feature extraction; Dynamic category weighting to handle
sample imbalance; Improved data augmentation via Mosaic-MixUp scaling.
Experimental results on a 4,078-image dataset show: mAP50: 0.820 (+8.0%
improvement); mAP50-95: 0.705 (+10.5% improvement); Inference speed: 158 FPS.
The mobile-deployed system enables real-time disease monitoring and precision
treatment in agricultural applications.

</details>


### [60] [In vivo 3D ultrasound computed tomography of musculoskeletal tissues with generative neural physics](https://arxiv.org/abs/2508.12226)
*Zhijun Zeng,Youjia Zheng,Chang Su,Qianhang Wu,Hao Hu,Zeyuan Dong,Shan Gao,Yang Lv,Rui Tang,Ligang Cui,Zhiyong Hou,Weijun Lin,Zuoqiang Shi,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: 提出了一种结合生成网络和物理信息神经模拟的生成式神经物理框架，用于快速、高保真的3D超声计算机断层扫描，解决了传统射线重建方法在强散射环境下的局限性。


<details>
  <summary>Details</summary>
Motivation: 超声计算机断层扫描（USCT）是一种无辐射、高分辨率的成像方式，但在肌肉骨骼成像中受到限制，因为传统的基于射线的重建方法忽略了强散射效应。

Method: 开发了一个生成式神经物理框架，通过从仅几十张跨模态图像中学习超声波传播的紧凑代理模型，将波动建模的准确性与深度学习的效率和稳定性相结合。

Result: 在合成和体内数据（乳房、手臂、腿部）上，在十分钟内重建了组织参数的3D图谱，对肌肉和骨骼的生物力学特性具有敏感性，分辨率与MRI相当。

Conclusion: 通过克服强散射状态下的计算瓶颈，该方法将USCT推向肌肉骨骼疾病常规临床评估的应用。

Abstract: Ultrasound computed tomography (USCT) is a radiation-free, high-resolution
modality but remains limited for musculoskeletal imaging due to conventional
ray-based reconstructions that neglect strong scattering. We propose a
generative neural physics framework that couples generative networks with
physics-informed neural simulation for fast, high-fidelity 3D USCT. By learning
a compact surrogate of ultrasonic wave propagation from only dozens of
cross-modality images, our method merges the accuracy of wave modeling with the
efficiency and stability of deep learning. This enables accurate quantitative
imaging of in vivo musculoskeletal tissues, producing spatial maps of acoustic
properties beyond reflection-mode images. On synthetic and in vivo data
(breast, arm, leg), we reconstruct 3D maps of tissue parameters in under ten
minutes, with sensitivity to biomechanical properties in muscle and bone and
resolution comparable to MRI. By overcoming computational bottlenecks in
strongly scattering regimes, this approach advances USCT toward routine
clinical assessment of musculoskeletal disease.

</details>


### [61] [WXSOD: A Benchmark for Robust Salient Object Detection in Adverse Weather Conditions](https://arxiv.org/abs/2508.12250)
*Quan Chen,Xiong Yang,Rongfeng Lu,Qianyu Zhang,Yu Liu,Xiaofei Zhou,Bolun Zheng*

Main category: cs.CV

TL;DR: 这篇论文提出了一个新的天气噪声扩展显著物体检测数据集WXSOD和一种天气感知特征聚合网络WFANet，以解决复杂天气条件下显著物体检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的显著物体检测方法在有天气噪声的环境中性能会严重降低，但缺乏有相关数据集来进行研究。这篇论文的动机是填补这一空白，提供一个包含多种天气噪声的标注数据集并提出有效的解决方案。

Method: 提出了天气感知特征聚合网络(WFANet)，采用双分支结构。一个分支负责预测天气条件并挖掘天气相关特征，另一个分支则将主干网络提取的语义特征与天气特征融合进行显著物体检测。

Result: 在新提出的WXSOD数据集上，与17种现有显著物体检测方法进行了综合比较，结果显示WFANet能够获得更优异的性能。

Conclusion: 这项研究为复杂天气条件下的显著物体检测提供了一个重要的数据集基准和有效的基线方法，显示了考虑天气因素对于提高检测精度的重要性。

Abstract: Salient object detection (SOD) in complex environments remains a challenging
research topic. Most existing methods perform well in natural scenes with
negligible noise, and tend to leverage multi-modal information (e.g., depth and
infrared) to enhance accuracy. However, few studies are concerned with the
damage of weather noise on SOD performance due to the lack of dataset with
pixel-wise annotations. To bridge this gap, this paper introduces a novel
Weather-eXtended Salient Object Detection (WXSOD) dataset. It consists of
14,945 RGB images with diverse weather noise, along with the corresponding
ground truth annotations and weather labels. To verify algorithm
generalization, WXSOD contains two test sets, i.e., a synthesized test set and
a real test set. The former is generated by adding weather noise to clean
images, while the latter contains real-world weather noise. Based on WXSOD, we
propose an efficient baseline, termed Weather-aware Feature Aggregation Network
(WFANet), which adopts a fully supervised two-branch architecture.
Specifically, the weather prediction branch mines weather-related deep
features, while the saliency detection branch fuses semantic features extracted
from the backbone with weather features for SOD. Comprehensive comparisons
against 17 SOD methods shows that our WFANet achieves superior performance on
WXSOD. The code and benchmark results will be made publicly available at
https://github.com/C-water/WXSOD

</details>


### [62] [Superpixel-informed Continuous Low-Rank Tensor Representation for Multi-Dimensional Data Recovery](https://arxiv.org/abs/2508.12261)
*Zhizhou Wang,Ruijing Zheng,Zhenyu Wu,Jianli Wang*

Main category: cs.CV

TL;DR: 提出SCTR框架，通过超像素分割和不对称低秩张量分解，解决了传统低秩张量表示方法在空间变化处理和离散网格数据限制方面的问题，在多个数据集上实现了3-5 dB的PSNR提升。


<details>
  <summary>Details</summary>
Motivation: 传统低秩张量表示方法存在两个关键局限：1）假设整体数据是低秩的，这在具有显著空间变化的真实场景中往往不成立；2）仅限于离散网格数据，限制了灵活性和适用性。

Method: 提出超像素感知的连续低秩张量表示（SCTR）框架：1）使用超像素作为基本建模单元，利用语义一致区域具有更强低秩特性的观察；2）提出不对称低秩张量分解（ALTF），通过共享神经网络参数化超像素特定因子矩阵，分离全局模式学习和局部适应。

Result: 在多个基准数据集上的广泛实验表明，SCTR在多光谱图像、视频和彩色图像上相比现有LRTR方法实现了3-5 dB的PSNR改进。

Conclusion: SCTR框架能够连续灵活地建模多维数据，超越了传统基于网格的约束，在表达能力和模型效率之间取得了良好平衡。

Abstract: Low-rank tensor representation (LRTR) has emerged as a powerful tool for
multi-dimensional data processing. However, classical LRTR-based methods face
two critical limitations: (1) they typically assume that the holistic data is
low-rank, this assumption is often violated in real-world scenarios with
significant spatial variations; and (2) they are constrained to discrete
meshgrid data, limiting their flexibility and applicability. To overcome these
limitations, we propose a Superpixel-informed Continuous low-rank Tensor
Representation (SCTR) framework, which enables continuous and flexible modeling
of multi-dimensional data beyond traditional grid-based constraints. Our
approach introduces two main innovations: First, motivated by the observation
that semantically coherent regions exhibit stronger low-rank characteristics
than holistic data, we employ superpixels as the basic modeling units. This
design not only encodes rich semantic information, but also enhances
adaptability to diverse forms of data streams. Second, we propose a novel
asymmetric low-rank tensor factorization (ALTF) where superpixel-specific
factor matrices are parameterized by a shared neural network with specialized
heads. By strategically separating global pattern learning from local
adaptation, this framework efficiently captures both cross-superpixel
commonalities and within-superpixel variations. This yields a representation
that is both highly expressive and compact, balancing model efficiency with
adaptability. Extensive experiments on several benchmark datasets demonstrate
that SCTR achieves 3-5 dB PSNR improvements over existing LRTR-based methods
across multispectral images, videos, and color images.

</details>


### [63] [Region-Level Context-Aware Multimodal Understanding](https://arxiv.org/abs/2508.12263)
*Hongliang Wei,Xianqi Zhang,Xingtao Wang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: 本文提出了区域级上下文感知多模态理解(RCMU)能力，通过新的指令微调方法RCVIT和数据集RCMU dataset，使MLLM模型能够结合图像内容和对象文本信息进行更深入的多模态理解。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型主要关注通用视觉理解，忽视了结合对象相关文本上下文进行更深入理解的能力，即区域级上下文感知多模态理解(RCMU)能力。

Method: 提出Region-level Context-aware Visual Instruction Tuning (RCVIT)方法，将对象信息整合到模型输入中，利用边界框坐标关联对象的视觉内容和文本信息；构建大规模RCMU数据集和RC&P-Bench评测标准。

Result: 通过在Qwen2-VL模型上进行RCVIT训练，得到了RC-Qwen2-VL模型，在多个RCMU任务上表现出艰出，并成功应用于多模态RAG和个性化对话。

Conclusion: 本文为MLLM模型提供了区域级上下文感知多模态理解的有效解决方案，开拓了多模态理解的新方向，为更深入的视觉-语言交互研究奠定了基础。

Abstract: Despite significant progress, existing research on Multimodal Large Language
Models (MLLMs) mainly focuses on general visual understanding, overlooking the
ability to integrate textual context associated with objects for a more
context-aware multimodal understanding -- an ability we refer to as
Region-level Context-aware Multimodal Understanding (RCMU). To address this
limitation, we first formulate the RCMU task, which requires models to respond
to user instructions by integrating both image content and textual information
of regions or objects. To equip MLLMs with RCMU capabilities, we propose
Region-level Context-aware Visual Instruction Tuning (RCVIT), which
incorporates object information into the model input and enables the model to
utilize bounding box coordinates to effectively associate objects' visual
content with their textual information. To address the lack of datasets, we
introduce the RCMU dataset, a large-scale visual instruction tuning dataset
that covers multiple RCMU tasks. We also propose RC\&P-Bench, a comprehensive
benchmark that can evaluate the performance of MLLMs in RCMU and multimodal
personalized understanding tasks. Additionally, we propose a reference-free
evaluation metric to perform a comprehensive and fine-grained evaluation of the
region-level context-aware image descriptions. By performing RCVIT on Qwen2-VL
models with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental
results indicate that RC-Qwen2-VL models not only achieve outstanding
performance on multiple RCMU tasks but also demonstrate successful applications
in multimodal RAG and personalized conversation. Our data, model and benchmark
are available at https://github.com/hongliang-wei/RC-MLLM

</details>


### [64] [SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration](https://arxiv.org/abs/2508.12271)
*Ronghua Xu,Jin Xie,Jing Nie,Jiale Cao,Yanwei Pang*

Main category: cs.CV

TL;DR: 提出SNNSIR，一种完全脉冲驱动的脉冲神经网络，用于立体图像恢复，在保持竞争性恢复性能的同时显著降低计算开销


<details>
  <summary>Details</summary>
Motivation: 现有的混合SNN-ANN模型仍依赖浮点矩阵除法或指数运算，与SNN的二进制和事件驱动特性不兼容，需要开发完全脉冲驱动的架构来实现低功耗和硬件友好计算

Method: 采用轻量级脉冲残差基本块(SRBB)增强信息流，脉冲立体卷积调制(SSCM)模块通过逐元素乘法引入简化非线性，脉冲立体交叉注意力(SSCA)模块在脉冲兼容框架内实现跨视图双向特征交互

Result: 在多种立体图像恢复任务（雨纹去除、雨滴去除、低光增强和超分辨率）上的广泛实验表明，模型实现了竞争性的恢复性能，同时显著减少计算开销

Conclusion: 该工作展示了实时、低功耗立体视觉应用的潜力，完全脉冲驱动的架构为硬件友好的高效计算提供了可行方案

Abstract: Spiking Neural Networks (SNNs), characterized by discrete binary activations,
offer high computational efficiency and low energy consumption, making them
well-suited for computation-intensive tasks such as stereo image restoration.
In this work, we propose SNNSIR, a simple yet effective Spiking Neural Network
for Stereo Image Restoration, specifically designed under the spike-driven
paradigm where neurons transmit information through sparse, event-based binary
spikes. In contrast to existing hybrid SNN-ANN models that still rely on
operations such as floating-point matrix division or exponentiation, which are
incompatible with the binary and event-driven nature of SNNs, our proposed
SNNSIR adopts a fully spike-driven architecture to achieve low-power and
hardware-friendly computation. To address the expressiveness limitations of
binary spiking neurons, we first introduce a lightweight Spike Residual Basic
Block (SRBB) to enhance information flow via spike-compatible residual
learning. Building on this, the Spike Stereo Convolutional Modulation (SSCM)
module introduces simplified nonlinearity through element-wise multiplication
and highlights noise-sensitive regions via cross-view-aware modulation.
Complementing this, the Spike Stereo Cross-Attention (SSCA) module further
improves stereo correspondence by enabling efficient bidirectional feature
interaction across views within a spike-compatible framework. Extensive
experiments on diverse stereo image restoration tasks, including rain streak
removal, raindrop removal, low-light enhancement, and super-resolution
demonstrate that our model achieves competitive restoration performance while
significantly reducing computational overhead. These results highlight the
potential for real-time, low-power stereo vision applications. The code will be
available after the article is accepted.

</details>


### [65] [TSLA: A Task-Specific Learning Adaptation for Semantic Segmentation on Autonomous Vehicles Platform](https://arxiv.org/abs/2508.12279)
*Jun Liu,Zhenglun Kong,Pu Zhao,Weihao Zeng,Hao Tang,Xuan Shen,Changdi Yang,Wenbin Zhang,Geng Yuan,Wei Niu,Xue Lin,Yanzhi Wang*

Main category: cs.CV

TL;DR: 提出了一种针对自动驾驶硬件平台的三层动态适应机制，通过宽度乘数、分类器深度和分类器核大小来控制语义分割网络，结合贝叶斯优化实现高效超参数搜索，以适应不同计算资源和精度需求。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶平台面临多样化的驾驶场景，每个场景都有不同的硬件资源和精度要求。由于嵌入式设备的计算限制，在目标平台（如NVIDIA DRIVE PX 2）上部署时需要考虑计算成本。

Method: 采用三层控制机制实现动态适应性：1）宽度乘数控制模型整体规模；2）分类器深度控制最终层的细化；3）分类器核大小进行场景特定的优化。结合贝叶斯优化和代理建模在有限计算预算下高效探索超参数空间。

Result: 该方法能够根据自动驾驶硬件平台的计算能力和特定场景需求定制语义分割网络，实现任务特定的学习适应（TSLA），生成针对不同自动驾驶任务的替代配置。

Conclusion: 提出的动态适应机制和贝叶斯优化方法能够有效优化硬件利用率，在满足计算约束的同时最大化模型精度，为自动驾驶平台提供了灵活且高效的网络定制解决方案。

Abstract: Autonomous driving platforms encounter diverse driving scenarios, each with
varying hardware resources and precision requirements. Given the computational
limitations of embedded devices, it is crucial to consider computing costs when
deploying on target platforms like the NVIDIA\textsuperscript{\textregistered}
DRIVE PX 2. Our objective is to customize the semantic segmentation network
according to the computing power and specific scenarios of autonomous driving
hardware. We implement dynamic adaptability through a three-tier control
mechanism -- width multiplier, classifier depth, and classifier kernel --
allowing fine-grained control over model components based on hardware
constraints and task requirements. This adaptability facilitates broad model
scaling, targeted refinement of the final layers, and scenario-specific
optimization of kernel sizes, leading to improved resource allocation and
performance.
  Additionally, we leverage Bayesian Optimization with surrogate modeling to
efficiently explore hyperparameter spaces under tight computational budgets.
Our approach addresses scenario-specific and task-specific requirements through
automatic parameter search, accommodating the unique computational complexity
and accuracy needs of autonomous driving. It scales its Multiply-Accumulate
Operations (MACs) for Task-Specific Learning Adaptation (TSLA), resulting in
alternative configurations tailored to diverse self-driving tasks. These TSLA
customizations maximize computational capacity and model accuracy, optimizing
hardware utilization.

</details>


### [66] [CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval](https://arxiv.org/abs/2508.12290)
*Chor Boon Tan,Conghui Hu,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出CLAIR方法，通过CLIP生成的噪声伪标签进行弱监督零样本跨域图像检索，利用置信度分数、对比学习和跨域映射函数提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型能够为大量未标注数据生成伪标签，使得无监督零样本跨域图像检索变得不那么相关，因此转向研究使用CLIP等大型基础模型生成噪声伪标签的弱监督方法。

Method: 1) 使用CLIP文本和图像特征的相似度计算置信度分数来细化噪声伪标签；2) 设计实例间和簇间对比损失来编码类感知潜在空间；3) 设计域间对比损失缓解域差异；4) 学习闭式跨域映射函数；5) 引入可学习提示增强零样本泛化能力。

Result: 在TUBerlin、Sketchy、Quickdraw和DomainNet等零样本数据集上的广泛实验表明，CLAIR方法相比现有最先进方法 consistently 显示出优越性能。

Conclusion: CLAIR方法通过有效处理CLIP生成的噪声伪标签，结合多种对比学习策略和跨域映射技术，成功提升了弱监督零样本跨域图像检索的性能。

Abstract: The recent growth of large foundation models that can easily generate
pseudo-labels for huge quantity of unlabeled data makes unsupervised Zero-Shot
Cross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, we
therefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) with
noisy pseudo labels generated by large foundation models such as CLIP. To this
end, we propose CLAIR to refine the noisy pseudo-labels with a confidence score
from the similarity between the CLIP text and image features. Furthermore, we
design inter-instance and inter-cluster contrastive losses to encode images
into a class-aware latent space, and an inter-domain contrastive loss to
alleviate domain discrepancies. We also learn a novel cross-domain mapping
function in closed-form, using only CLIP text embeddings to project image
features from one domain to another, thereby further aligning the image
features for retrieval. Finally, we enhance the zero-shot generalization
ability of our CLAIR to handle novel categories by introducing an extra set of
learnable prompts. Extensive experiments are carried out using TUBerlin,
Sketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIR
consistently shows superior performance compared to existing state-of-the-art
methods.

</details>


### [67] [Improving Densification in 3D Gaussian Splatting for High-Fidelity Rendering](https://arxiv.org/abs/2508.12313)
*Xiaobin Deng,Changyu Diao,Min Li,Ruohan Yu,Duanqing Xu*

Main category: cs.CV

TL;DR: 通过边缘感知分数、长轴分割策略和抗过拟合技术，全面改善了3D高斯散点的密度化策略，在不增加计算开销的情况下提升了渲染质量和减少高斯元数量


<details>
  <summary>Details</summary>
Motivation: 3D高斯散点的密度化策略导致重建质量不佳，需要从何时密度化、如何密度化和如何减轻过拟合三个角度进行全面改进

Method: 提出边缘感知分数选择分割候选高斯元，长轴分割策略减少几何失真，以及恢复感知剪枝、多步更新和增长控制等抗过拟合技术

Result: 方法在不增加训练或推理开销的情况下提升了渲染保真度，以更少的高斯元数量达到了最先进的性能

Conclusion: 该研究通过系统性的密度化流程改进，有效解决了3D高斯散点的重建质量问题，为实时渲染领域提供了更优化的解决方案

Abstract: Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in
real-time rendering, its densification strategy often results in suboptimal
reconstruction quality. In this work, we present a comprehensive improvement to
the densification pipeline of 3DGS from three perspectives: when to densify,
how to densify, and how to mitigate overfitting. Specifically, we propose an
Edge-Aware Score to effectively select candidate Gaussians for splitting. We
further introduce a Long-Axis Split strategy that reduces geometric distortions
introduced by clone and split operations. To address overfitting, we design a
set of techniques, including Recovery-Aware Pruning, Multi-step Update, and
Growth Control. Our method enhances rendering fidelity without introducing
additional training or inference overhead, achieving state-of-the-art
performance with fewer Gaussians.

</details>


### [68] [Neural Cellular Automata for Weakly Supervised Segmentation of White Blood Cells](https://arxiv.org/abs/2508.12322)
*Michael Deutges,Chen Yang,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: 基于神经细胞自动机(NCA)的弱监督分割方法，无需重新训练即可从分类特征图中提取分割掩码，在白细胞分割任务上显著超过现有方法


<details>
  <summary>Details</summary>
Motivation: 医疗诊断中白细胞检测和分割需要大量标签数据，而标注工作耗时耗费，需要更高效的弱监督方法

Method: 提出NCA-WSS方法，利用神经细胞自动机在分类过程中生成的特征图，直接提取分割掩码而无需分割标签进行重新训练

Result: 在三个白细胞显微镜数据集上评估，NCA-WSS方法在弱监督分割任务上显著超过现有方法

Conclusion: 该方法展示了NCA在弱监督框架下同时做好分类和分割的潜力，为医学图像分析提供了可扩展和高效的解决方案

Abstract: The detection and segmentation of white blood cells in blood smear images is
a key step in medical diagnostics, supporting various downstream tasks such as
automated blood cell counting, morphological analysis, cell classification, and
disease diagnosis and monitoring. Training robust and accurate models requires
large amounts of labeled data, which is both time-consuming and expensive to
acquire. In this work, we propose a novel approach for weakly supervised
segmentation using neural cellular automata (NCA-WSS). By leveraging the
feature maps generated by NCA during classification, we can extract
segmentation masks without the need for retraining with segmentation labels. We
evaluate our method on three white blood cell microscopy datasets and
demonstrate that NCA-WSS significantly outperforms existing weakly supervised
approaches. Our work illustrates the potential of NCA for both classification
and segmentation in a weakly supervised framework, providing a scalable and
efficient solution for medical image analysis.

</details>


### [69] [Attention Pooling Enhances NCA-based Classification of Microscopy Images](https://arxiv.org/abs/2508.12324)
*Chen Yang,Michael Deutges,Jingsong Liu,Han Li,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: 将注意力池化机制与神经细胞自动机（NCA）结合，显著提升显微镜图像分类性能，在保持参数效率和可解释性的同时超越现有NCA方法和传统轻量级架构


<details>
  <summary>Details</summary>
Motivation: 神经细胞自动机在图像分类中具有鲁棒性和可解释性优势，但性能与大型复杂架构存在差距，需要提升特征提取能力和分类精度

Method: 集成注意力池化机制到NCA中，通过关注信息最丰富的区域来优化特征提取

Result: 在八个不同显微镜图像数据集上评估，显著超越现有NCA方法，与传统轻量级CNN和ViT架构相比性能更好且参数数量显著更低

Conclusion: 基于NCA的模型具有作为可解释图像分类替代方案的潜力，注意力池化的集成有效缩小了性能差距

Abstract: Neural Cellular Automata (NCA) offer a robust and interpretable approach to
image classification, making them a promising choice for microscopy image
analysis. However, a performance gap remains between NCA and larger, more
complex architectures. We address this challenge by integrating attention
pooling with NCA to enhance feature extraction and improve classification
accuracy. The attention pooling mechanism refines the focus on the most
informative regions, leading to more accurate predictions. We evaluate our
method on eight diverse microscopy image datasets and demonstrate that our
approach significantly outperforms existing NCA methods while remaining
parameter-efficient and explainable. Furthermore, we compare our method with
traditional lightweight convolutional neural network and vision transformer
architectures, showing improved performance while maintaining a significantly
lower parameter count. Our results highlight the potential of NCA-based models
an alternative for explainable image classification.

</details>


### [70] [DoppDrive: Doppler-Driven Temporal Aggregation for Improved Radar Object Detection](https://arxiv.org/abs/2508.12330)
*Yuval Haitman,Oded Bialer*

Main category: cs.CV

TL;DR: DoppDrive是一种基于多普勒效应的雷达点云时域聚合方法，通过径向动态补偿和个性化聚合时长来增强点云密度并减少散射，提升目标检测性能


<details>
  <summary>Details</summary>
Motivation: 雷达在自动驾驶中具有长距离检测优势，但远距离点云稀疏性问题严重。现有时域聚合方法会因动态物体产生散射，降低检测性能

Method: 提出DoppDrive方法：1）根据多普勒动态分量径向移动历史帧点云消除径向散射；2）基于多普勒和角度为每个点分配个性化聚合时长来减少切向散射

Result: DoppDrive作为检测前的点云密度增强步骤，与各种检测器兼容，在多个数据集上显著提升了目标检测性能

Conclusion: 基于多普勒驱动的时域聚合方法能有效解决雷达点云稀疏性问题，同时避免动态物体带来的散射问题，为雷达目标检测提供了有效的预处理方案

Abstract: Radar-based object detection is essential for autonomous driving due to
radar's long detection range. However, the sparsity of radar point clouds,
especially at long range, poses challenges for accurate detection. Existing
methods increase point density through temporal aggregation with ego-motion
compensation, but this approach introduces scatter from dynamic objects,
degrading detection performance. We propose DoppDrive, a novel Doppler-Driven
temporal aggregation method that enhances radar point cloud density while
minimizing scatter. Points from previous frames are shifted radially according
to their dynamic Doppler component to eliminate radial scatter, with each point
assigned a unique aggregation duration based on its Doppler and angle to
minimize tangential scatter. DoppDrive is a point cloud density enhancement
step applied before detection, compatible with any detector, and we demonstrate
that it significantly improves object detection performance across various
detectors and datasets.

</details>


### [71] [Geometry-Aware Video Inpainting for Joint Headset Occlusion Removal and Face Reconstruction in Social XR](https://arxiv.org/abs/2508.12336)
*Fatemeh Ghorbani Lohesara,Karen Eguiazarian,Sebastian Knorr*

Main category: cs.CV

TL;DR: 这项研究提出了一种基于深度学习的几何感知框架，能够从单视点RGB视频中同时移除头显设备遮挡并重建完整的3D面部几何，为社交XR应用提供涉入式面部表情恢复。


<details>
  <summary>Details</summary>
Motivation: 头显设备(HMDs)遮挡用户面部上部分，影响外部视频录制和社交XR应用的涉入式体验，特别是在需要面部表情和眼神交流的远程会议场景中。

Method: 集成GAN基础的视频修复网络，通过密集面部关键点和单张无遮挡参考帧指导，恢复缺失面部区域保持身份识别。然后使用SynergyNet基础模块从修复后的帧中回归3D形态模型(3DMM)参数，实现准确的3D面部重建。整个流程中融入密集关键点优化来提升修复质量和几何保真度。

Result: 实验结果表明该框架能成功从RGB面部视频中移除HMD遮挡，同时保持面部身份识别和真实感，产生超实际的3D面部几何输出。分离实验还显示框架在不同关键点密度下仍保持稳健性，在稀疏关键点配置下仅有轻微质量下降。

Conclusion: 该研究提供了一种有效的解决方案，能够在单视点RGB视频中同时实现HMD遮挡移除和3D面部重建，为社交XR应用的面部表情恢复提供了重要技术支撑。

Abstract: Head-mounted displays (HMDs) are essential for experiencing extended reality
(XR) environments and observing virtual content. However, they obscure the
upper part of the user's face, complicating external video recording and
significantly impacting social XR applications such as teleconferencing, where
facial expressions and eye gaze details are crucial for creating an immersive
experience. This study introduces a geometry-aware learning-based framework to
jointly remove HMD occlusions and reconstruct complete 3D facial geometry from
RGB frames captured from a single viewpoint. The method integrates a GAN-based
video inpainting network, guided by dense facial landmarks and a single
occlusion-free reference frame, to restore missing facial regions while
preserving identity. Subsequently, a SynergyNet-based module regresses 3D
Morphable Model (3DMM) parameters from the inpainted frames, enabling accurate
3D face reconstruction. Dense landmark optimization is incorporated throughout
the pipeline to improve both the inpainting quality and the fidelity of the
recovered geometry. Experimental results demonstrate that the proposed
framework can successfully remove HMDs from RGB facial videos while maintaining
facial identity and realism, producing photorealistic 3D face geometry outputs.
Ablation studies further show that the framework remains robust across
different landmark densities, with only minor quality degradation under sparse
landmark configurations.

</details>


### [72] [Semantic Discrepancy-aware Detector for Image Forgery Identification](https://arxiv.org/abs/2508.12341)
*Ziye Wang,Minghang Yu,Chunyan Xu,Zhen Cui*

Main category: cs.CV

TL;DR: 提出SDD检测器，通过重建学习在细粒度视觉层面对齐伪造痕迹和语义概念空间，利用预训练视觉语言模型的概念知识来提升伪造图像检测性能


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的快速发展，需要强大的伪造检测来确保数字媒体的可信度。现有方法中伪造空间与语义概念空间的不对齐限制了检测性能

Method: 设计语义标记采样模块缓解空间偏移，构建概念级伪造差异学习模块加强视觉语义概念与伪造痕迹的交互，通过低级伪造特征增强整合学习到的概念级差异

Result: 在两个标准图像伪造数据集上的实验表明，SDD相比现有方法取得了更优越的结果

Conclusion: 所提出的SDD方法通过空间对齐和概念引导的差异学习，有效提升了伪造图像检测的性能

Abstract: With the rapid advancement of image generation techniques, robust forgery
detection has become increasingly imperative to ensure the trustworthiness of
digital media. Recent research indicates that the learned semantic concepts of
pre-trained models are critical for identifying fake images. However, the
misalignment between the forgery and semantic concept spaces hinders the
model's forgery detection performance. To address this problem, we propose a
novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction
learning to align the two spaces at a fine-grained visual level. By exploiting
the conceptual knowledge embedded in the pre-trained vision language model, we
specifically design a semantic token sampling module to mitigate the space
shifts caused by features irrelevant to both forgery traces and semantic
concepts. A concept-level forgery discrepancy learning module, built upon a
visual reconstruction paradigm, is proposed to strengthen the interaction
between visual semantic concepts and forgery traces, effectively capturing
discrepancies under the concepts' guidance. Finally, the low-level forgery
feature enhancemer integrates the learned concept level forgery discrepancies
to minimize redundant forgery information. Experiments conducted on two
standard image forgery datasets demonstrate the efficacy of the proposed SDD,
which achieves superior results compared to existing methods. The code is
available at https://github.com/wzy1111111/SSD.

</details>


### [73] [AquaFeat: A Features-Based Image Enhancement Model for Underwater Object Detection](https://arxiv.org/abs/2508.12343)
*Emanuel C. Silva,Tatiana T. Schein,Stephanie L. Brião,Guilherme L. M. Costa,Felipe G. Oliveira,Gustavo P. Almeida,Eduardo L. Silva,Sam S. Devincenzi,Karina S. Machado,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: AquaFeat是一个即插即用的任务驱动特征增强模块，专门针对水下目标检测任务，通过多尺度特征增强网络和端到端训练，显著提升检测精度同时保持实时处理速度。


<details>
  <summary>Details</summary>
Motivation: 水下环境的严重图像退化会影响目标检测模型的性能，传统图像增强方法通常没有针对下游检测任务进行优化。

Method: 提出AquaFeat模块，集成多尺度特征增强网络，与检测器损失函数进行端到端训练，确保增强过程明确指导优化与检测任务最相关的特征。

Result: 在YOLOv8m上集成AquaFeat，在挑战性水下数据集上达到最先进的精度(0.877)和召回率(0.624)，以及竞争力的mAP分数(mAP@0.5为0.677，mAP@[0.5:0.95]为0.421)，处理速度为46.5 FPS。

Conclusion: AquaFeat提供了有效且计算高效的解决方案，适用于海洋生态系统监测和基础设施检查等实际应用。

Abstract: The severe image degradation in underwater environments impairs object
detection models, as traditional image enhancement methods are often not
optimized for such downstream tasks. To address this, we propose AquaFeat, a
novel, plug-and-play module that performs task-driven feature enhancement. Our
approach integrates a multi-scale feature enhancement network trained
end-to-end with the detector's loss function, ensuring the enhancement process
is explicitly guided to refine features most relevant to the detection task.
When integrated with YOLOv8m on challenging underwater datasets, AquaFeat
achieves state-of-the-art Precision (0.877) and Recall (0.624), along with
competitive mAP scores (mAP@0.5 of 0.677 and mAP@[0.5:0.95] of 0.421). By
delivering these accuracy gains while maintaining a practical processing speed
of 46.5 FPS, our model provides an effective and computationally efficient
solution for real-world applications, such as marine ecosystem monitoring and
infrastructure inspection.

</details>


### [74] [MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring](https://arxiv.org/abs/2508.12346)
*Hu Gao,Depeng Dang*

Main category: cs.CV

TL;DR: 基于Mamba的图像去模糊网络MBMamba，通过内存缓冲机制和Ising受灵感的正则化损失，解决了原生Mamba在图像去模糊中的局部像素遗忘和频道冗余问题，在保持计算效率的同时提升了性能。


<details>
  <summary>Details</summary>
Motivation: Mamba架构在图像去模糊中展现潜力，但其平铺扫描策略导致局部像素遗忘和频道冗余，限制了二维空间信息聚合能力。现有方法通过修改扫描策略或添加局部特征模块来缓解这些问题，但增加了计算复杂度并影响实时性能。

Method: 提出MBMamba网络，在不改变原生Mamba架构的前提下：1）设计内存缓冲机制保留历史信息以便后期融合，实现相邻特征间的可靠关联建模；2）引入Ising受灵感的正则化损失，模拟物理系统中像素间"相互吸引"的能量最小化，帮助维持图像结构和一致性。

Result: 实验结果显示，该方法在广泛使用的测试集上超越了当前最先进的方法。

Conclusion: MBMamba通过创新的内存缓冲机制和物理受灵感的正则化损失，有效解决了Mamba在图像去模糊任务中的限制，在保持计算效率的同时实现了更优的性能表现，为图像处理领域提供了一种新的解决方案。

Abstract: The Mamba architecture has emerged as a promising alternative to CNNs and
Transformers for image deblurring. However, its flatten-and-scan strategy often
results in local pixel forgetting and channel redundancy, limiting its ability
to effectively aggregate 2D spatial information. Although existing methods
mitigate this by modifying the scan strategy or incorporating local feature
modules, it increase computational complexity and hinder real-time performance.
In this paper, we propose a structure-aware image deblurring network without
changing the original Mamba architecture. Specifically, we design a memory
buffer mechanism to preserve historical information for later fusion, enabling
reliable modeling of relevance between adjacent features. Additionally, we
introduce an Ising-inspired regularization loss that simulates the energy
minimization of the physical system's "mutual attraction" between pixels,
helping to maintain image structure and coherence. Building on this, we develop
MBMamba. Experimental results show that our method outperforms state-of-the-art
approaches on widely used benchmarks.

</details>


### [75] [EgoLoc: A Generalizable Solution for Temporal Interaction Localization in Egocentric Videos](https://arxiv.org/abs/2508.12349)
*Junyi Ma,Erhang Zhang,Yin-Dong Zheng,Yuchen Xie,Yixuan Zhou,Hesheng Wang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的零样本方法EgoLoc，用于在主视角视频中准确定位手部与物体的接触和分离时刻，无需物体掩码或动作类别标注。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注如何交互，而更具挑战的手物接触/分离时刻定位问题被忽视，这对混合现实体验和机器人运动规划至关重要。

Method: 提出EgoLoc方法，采用手部动力学导向的采样生成高质量视觉提示，利用视觉-语言模型识别接触/分离属性并定位具体时间戳，通过闭环反馈进行精炼。

Result: 在公开数据集和新标准上的完整实验表明，EgoLoc能够在主视角视频中实现准确的时间交互定位，并有效支持多丫下游应用。

Conclusion: EgoLoc提供了一种无需标注训练的零样本方案，能够准确捕捉手物交互的关键时刻，为VR/AR应用和机器人控制提供了重要技术支持。

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR
applications and human-robot policy transfer. Existing research has mostly
focused on modeling the behavior paradigm of interactive actions (i.e., ``how
to interact''). However, the more challenging and fine-grained problem of
capturing the critical moments of contact and separation between the hand and
the target object (i.e., ``when to interact'') is still underexplored, which is
crucial for immersive interactive experiences in mixed reality and robotic
motion planning. Therefore, we formulate this problem as temporal interaction
localization (TIL). Some recent works extract semantic masks as TIL references,
but suffer from inaccurate object grounding and cluttered scenarios. Although
current temporal action localization (TAL) methods perform well in detecting
verb-noun action segments, they rely on category annotations during training
and exhibit limited precision in localizing hand-object contact/separation
moments. To address these issues, we propose a novel zero-shot approach dubbed
EgoLoc to localize hand-object contact and separation timestamps in egocentric
videos. EgoLoc introduces hand-dynamics-guided sampling to generate
high-quality visual prompts. It exploits the vision-language model to identify
contact/separation attributes, localize specific timestamps, and provide
closed-loop feedback for further refinement. EgoLoc eliminates the need for
object masks and verb-noun taxonomies, leading to generalizable zero-shot
implementation. Comprehensive experiments on the public dataset and our novel
benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric
videos. It is also validated to effectively facilitate multiple downstream
applications in egocentric vision and robotic manipulation tasks. Code and
relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [76] [Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data](https://arxiv.org/abs/2508.12356)
*Ahmet H. Güzel,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.CV

TL;DR: 通过生成合成训练数据来改喂视觉基于离线强化学习的演策通用性，包括数据扩充和潜空间激光模型生成新数据


<details>
  <summary>Details</summary>
Motivation: 离线RL策略在视觉数据上容易过拟合，缺乏多样性导致通用性差，需要解决视觉数据的噪声、干扰和偏相关系问题

Method: 两步法：首先对离线数据进行扩充增加多样性，然后使用激光模型在潜空间生成额外的合成训练数据

Result: 在Visual D4RL和Procgen数据集上验证，显著提升了通用性能力，减小了测试时的通用性差距，同时保持计算效率

Conclusion: 该方法不需改变现有离线RL算法就能提升通用性，为使用合成数据训练更具通用性的模型提供了有前景的方向

Abstract: Offline reinforcement learning (RL) offers a promising framework for training
agents using pre-collected datasets without the need for further environment
interaction. However, policies trained on offline data often struggle to
generalise due to limited exposure to diverse states. The complexity of visual
data introduces additional challenges such as noise, distractions, and spurious
correlations, which can misguide the policy and increase the risk of
overfitting if the training data is not sufficiently diverse. Indeed, this
makes it challenging to leverage vision-based offline data in training robust
agents that can generalize to unseen environments. To solve this problem, we
propose a simple approach generating additional synthetic training data. We
propose a two-step process, first augmenting the originally collected offline
data to improve zero-shot generalization by introducing diversity, then using a
diffusion model to generate additional data in latent space. We test our method
across both continuous action spaces (Visual D4RL) and discrete action spaces
(Procgen), demonstrating that it significantly improves generalization without
requiring any algorithmic changes to existing model-free offline RL methods. We
show that our method not only increases the diversity of the training data but
also significantly reduces the generalization gap at test time while
maintaining computational efficiency. We believe this approach could fuel
additional progress in generating synthetic data to train more general agents
in the future.

</details>


### [77] [IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis](https://arxiv.org/abs/2508.12381)
*Guo Tang,Songhan Jiang,Jinpeng Lu,Linghan Cai,Yongbing Zhang*

Main category: cs.CV

TL;DR: IPGPhormer是一种用于癌症生存分析的可解释病理图-Transformer框架，能够同时捕获肿瘤微环境特征和空间依赖关系，在预测准确性和可解释性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生存分析方法难以平衡长距离空间关系建模与局部上下文依赖，且缺乏内在可解释性，限制了临床实用性。

Method: 提出Interpretable Pathology Graph-Transformer (IPGPhormer)框架，通过图-Transformer结构捕获肿瘤微环境特征和空间依赖关系，无需后处理手动标注即可提供组织和细胞级别的可解释性。

Result: 在四个公共基准数据集上的综合评估表明，IPGPhormer在预测准确性和可解释性方面均优于最先进的方法。

Conclusion: IPGPhormer为癌症预后评估提供了一个有前景的工具，为病理学中更可靠和可解释的决策支持系统铺平了道路。

Abstract: Pathological images play an essential role in cancer prognosis, while
survival analysis, which integrates computational techniques, can predict
critical clinical events such as patient mortality or disease recurrence from
whole-slide images (WSIs). Recent advancements in multiple instance learning
have significantly improved the efficiency of survival analysis. However,
existing methods often struggle to balance the modeling of long-range spatial
relationships with local contextual dependencies and typically lack inherent
interpretability, limiting their clinical utility. To address these challenges,
we propose the Interpretable Pathology Graph-Transformer (IPGPhormer), a novel
framework that captures the characteristics of the tumor microenvironment and
models their spatial dependencies across the tissue. IPGPhormer uniquely
provides interpretability at both tissue and cellular levels without requiring
post-hoc manual annotations, enabling detailed analyses of individual WSIs and
cross-cohort assessments. Comprehensive evaluations on four public benchmark
datasets demonstrate that IPGPhormer outperforms state-of-the-art methods in
both predictive accuracy and interpretability. In summary, our method,
IPGPhormer, offers a promising tool for cancer prognosis assessment, paving the
way for more reliable and interpretable decision-support systems in pathology.
The code is publicly available at
https://anonymous.4open.science/r/IPGPhormer-6EEB.

</details>


### [78] [ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers](https://arxiv.org/abs/2508.12384)
*Hanwen Cao,Haobo Lu,Xiaosen Wang,Kun He*

Main category: cs.CV

TL;DR: 通过对替代ViT模型进行对抗增帼（多头投弃、注意力给分缩放、MLP特征混合），组合自动重新加权和步长扩大模块，提升了ViT集成攻击的可转移性。


<details>
  <summary>Details</summary>
Motivation: 现有集成攻击主要关注权重精炼或集成路径优化，忽视了通过增强替代模型来提升攻击可转移性的潜力。同时ViT集成攻击得到的关注较少。

Method: 提出ViT-EnsembleAttack：1）使用三种对抗增帼策略（多头投弃、注意力分数缩放、MLP特征混合）生成增强的替代模型；2）通过贝叶斯优化调整参数；3）集成这些增强模型生成对抗样本；4）添加自动重新加权和步长扩大模块来提升可转移性。

Result: 庞大实验表明ViT-EnsembleAttack显著提升了ViT集成攻击的对抗可转移性，远超现有方法。

Conclusion: 通过对替代ViT模型进行对抗增帼可以有效提升集成攻击的可转移性，减少对抗过拟合风险，为ViT安全研究提供了新方向。

Abstract: Ensemble-based attacks have been proven to be effective in enhancing
adversarial transferability by aggregating the outputs of models with various
architectures. However, existing research primarily focuses on refining
ensemble weights or optimizing the ensemble path, overlooking the exploration
of ensemble models to enhance the transferability of adversarial attacks. To
address this gap, we propose applying adversarial augmentation to the surrogate
models, aiming to boost overall generalization of ensemble models and reduce
the risk of adversarial overfitting. Meanwhile, observing that ensemble Vision
Transformers (ViTs) gain less attention, we propose ViT-EnsembleAttack based on
the idea of model adversarial augmentation, the first ensemble-based attack
method tailored for ViTs to the best of our knowledge. Our approach generates
augmented models for each surrogate ViT using three strategies: Multi-head
dropping, Attention score scaling, and MLP feature mixing, with the associated
parameters optimized by Bayesian optimization. These adversarially augmented
models are ensembled to generate adversarial examples. Furthermore, we
introduce Automatic Reweighting and Step Size Enlargement modules to boost
transferability. Extensive experiments demonstrate that ViT-EnsembleAttack
significantly enhances the adversarial transferability of ensemble-based
attacks on ViTs, outperforming existing methods by a substantial margin. Code
is available at https://github.com/Trustworthy-AI-Group/TransferAttack.

</details>


### [79] [DeCoT: Decomposing Complex Instructions for Enhanced Text-to-Image Generation with Large Language Models](https://arxiv.org/abs/2508.12396)
*Xiaochuan Lin,Xiangyong Chen,Xuan Li,Yichen Su*

Main category: cs.CV

TL;DR: DeCoT是一个通过LLM分解复杂文本指令为结构化语义单元，再整合为分层提示来提升T2I模型性能的框架，在LongBench-T2I基准上显著改善了文本和构图等挑战性任务的生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型在处理复杂长文本指令时存在明显不足，经常无法准确渲染细节、空间关系和特定约束，LongBench-T2I等基准测试揭示了在组合性、文本准确性和精细纹理方面的缺陷。

Method: DeCoT框架包含两个核心阶段：1）复杂指令分解与语义增强 - 使用LLM将原始指令分解为结构化、可操作的语义单元并澄清歧义；2）多阶段提示整合与自适应生成 - 将这些单元转换为分层或优化的单一提示，适配现有T2I模型。

Result: 在LongBench-T2I数据集上的实验表明，DeCoT显著提升了领先T2I模型在所有评估维度上的性能，特别是在"文本"和"构图"等挑战性方面。与Infinity-8B集成时平均得分从3.44提升到3.52，人类评估也证实了感知质量和指令保真度的优越性。

Conclusion: DeCoT有效弥合了高级用户意图与T2I模型需求之间的差距，实现了更忠实和准确的图像生成，消融研究证实了各组件的关键贡献和复杂LLM提示的重要性。

Abstract: Despite remarkable advancements, current Text-to-Image (T2I) models struggle
with complex, long-form textual instructions, frequently failing to accurately
render intricate details, spatial relationships, or specific constraints. This
limitation is highlighted by benchmarks such as LongBench-T2I, which reveal
deficiencies in handling composition, specific text, and fine textures. To
address this, we propose DeCoT (Decomposition-CoT), a novel framework that
leverages Large Language Models (LLMs) to significantly enhance T2I models'
understanding and execution of complex instructions. DeCoT operates in two core
stages: first, Complex Instruction Decomposition and Semantic Enhancement,
where an LLM breaks down raw instructions into structured, actionable semantic
units and clarifies ambiguities; second, Multi-Stage Prompt Integration and
Adaptive Generation, which transforms these units into a hierarchical or
optimized single prompt tailored for existing T2I models. Extensive experiments
on the LongBench-T2I dataset demonstrate that DeCoT consistently and
substantially improves the performance of leading T2I models across all
evaluated dimensions, particularly in challenging aspects like "Text" and
"Composition". Quantitative results, validated by multiple MLLM evaluators
(Gemini-2.0-Flash and InternVL3-78B), show that DeCoT, when integrated with
Infinity-8B, achieves an average score of 3.52, outperforming the baseline
Infinity-8B (3.44). Ablation studies confirm the critical contribution of each
DeCoT component and the importance of sophisticated LLM prompting. Furthermore,
human evaluations corroborate these findings, indicating superior perceptual
quality and instruction fidelity. DeCoT effectively bridges the gap between
high-level user intent and T2I model requirements, leading to more faithful and
accurate image generation.

</details>


### [80] [Federated Cross-Modal Style-Aware Prompt Generation](https://arxiv.org/abs/2508.12399)
*Suraj Prasad,Navyansh Mahla,Sunny Gupta,Amit Sethi*

Main category: cs.CV

TL;DR: FedCSAP是一个联邦学习框架，利用CLIP模型的多尺度视觉特征和客户端特定风格指标来生成鲁棒的跨模态提示，在保护数据隐私的同时提升分类准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法仅使用最终层特征，无法充分利用多尺度视觉线索和客户端数据的领域特定风格变化，限制了模型在非IID数据分布下的性能。

Method: 从CLIP视觉编码器提取低、中、高三个层次的特征，结合客户端批处理统计信息生成风格指标，将精细视觉细节与文本上下文融合来生成独特且非冗余的提示token。

Result: 在多个图像分类数据集上的实验表明，FedCSAP在准确性和泛化能力方面均优于现有的联邦提示学习方法。

Conclusion: FedCSAP通过有效整合多尺度视觉特征和客户端风格信息，在联邦学习框架下实现了更好的跨域泛化性能，同时确保了数据隐私保护。

Abstract: Prompt learning has propelled vision-language models like CLIP to excel in
diverse tasks, making them ideal for federated learning due to computational
efficiency. However, conventional approaches that rely solely on final-layer
features miss out on rich multi-scale visual cues and domain-specific style
variations in decentralized client data. To bridge this gap, we introduce
FedCSAP (Federated Cross-Modal Style-Aware Prompt Generation). Our framework
harnesses low, mid, and high-level features from CLIP's vision encoder
alongside client-specific style indicators derived from batch-level statistics.
By merging intricate visual details with textual context, FedCSAP produces
robust, context-aware prompt tokens that are both distinct and non-redundant,
thereby boosting generalization across seen and unseen classes. Operating
within a federated learning paradigm, our approach ensures data privacy through
local training and global aggregation, adeptly handling non-IID class
distributions and diverse domain-specific styles. Comprehensive experiments on
multiple image classification datasets confirm that FedCSAP outperforms
existing federated prompt learning methods in both accuracy and overall
generalization.

</details>


### [81] [MPCAR: Multi-Perspective Contextual Augmentation for Enhanced Visual Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2508.12400)
*Amirul Rahman,Qiang Xu,Xueying Huang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种无需微调的推理时策略MPCAR，通过多角度生成补充描述来增强大视觉-语言模型的复杂视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大视觉-语言模型在复杂视觉理解任务中面临挑战，单次图像编码和提示方式限制了对细腻视觉信息的完整抓取能力。

Method: MPCAR算法包含三个阶段：1）从多个角度生成N个异质补充描述；2）将这些描述与原问题智能整合构建丰富提示；3）使用增强提示进行深度理解和答案生成。

Result: 在GQA、VQA-CP v2和ScienceQA等具有挑战性的VQA数据集上，MPCAR持续超过基线方法，在需要深度上下文理解的任务上显著提升准确率，人工评估也证实了答案的连贯性和完整性提升。

Conclusion: 这项工作证明了利用大视觉-语言模型的生成能力来丰富输入上下文，可以释放其潜在的理解能力，为复杂多模态任务提供了一种无需微调的有效解决方案。

Abstract: Despite significant advancements, Large Vision-Language Models (LVLMs)
continue to face challenges in complex visual reasoning tasks that demand deep
contextual understanding, multi-angle analysis, or meticulous detail
recognition. Existing approaches often rely on single-shot image encoding and
prompts, limiting their ability to fully capture nuanced visual information.
Inspired by the notion that strategically generated "additional" information
can serve as beneficial contextual augmentation, we propose Multi-Perspective
Contextual Augmentation for Reasoning (MPCAR), a novel inference-time strategy
designed to enhance LVLM performance. MPCAR operates in three stages: first, an
LVLM generates N diverse and complementary descriptions or preliminary
reasoning paths from various angles; second, these descriptions are
intelligently integrated with the original question to construct a
comprehensive context-augmented prompt; and finally, this enriched prompt
guides the ultimate LVLM for deep reasoning and final answer generation.
Crucially, MPCAR achieves these enhancements without requiring any fine-tuning
of the underlying LVLM's parameters. Extensive experiments on challenging
Visual Question Answering (VQA) datasets, including GQA, VQA-CP v2, and
ScienceQA (Image-VQA), demonstrate that MPCAR consistently outperforms
established baseline methods. Our quantitative results show significant
accuracy gains, particularly on tasks requiring robust contextual
understanding, while human evaluations confirm improved coherence and
completeness of the generated answers. Ablation studies further highlight the
importance of diverse prompt templates and the number of generated
perspectives. This work underscores the efficacy of leveraging LVLMs' inherent
generative capabilities to enrich input contexts, thereby unlocking their
latent reasoning potential for complex multimodal tasks.

</details>


### [82] [LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving](https://arxiv.org/abs/2508.12404)
*Nan Song,Bozhou Zhang,Xiatian Zhu,Jiankang Deng,Li Zhang*

Main category: cs.CV

TL;DR: LMAD是一个专为自动驾驶设计的视觉语言框架，通过引入初步场景交互和专家适配器，显著提升了现有VLM在驾驶推理任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有方法主要在多视角图像和场景推理文本上微调VLM，但缺乏对自动驾驶所需的整体细致场景识别和强大空间感知能力，特别是在复杂情况下

Method: 提出LMAD框架，模拟现代端到端驾驶范式，包含全面场景理解和任务专用结构，引入初步场景交互和专家适配器，与现有VLM完全兼容并可与规划导向驾驶系统无缝集成

Result: 在DriveLM和nuScenes-QA数据集上的大量实验表明，LMAD显著提升了现有VLM在驾驶推理任务上的性能

Conclusion: LMAD为可解释自动驾驶设立了新标准，通过更好的VLM与自动驾驶场景对齐，解决了现有方法的局限性

Abstract: Large vision-language models (VLMs) have shown promising capabilities in
scene understanding, enhancing the explainability of driving behaviors and
interactivity with users. Existing methods primarily fine-tune VLMs on on-board
multi-view images and scene reasoning text, but this approach often lacks the
holistic and nuanced scene recognition and powerful spatial awareness required
for autonomous driving, especially in complex situations. To address this gap,
we propose a novel vision-language framework tailored for autonomous driving,
called LMAD. Our framework emulates modern end-to-end driving paradigms by
incorporating comprehensive scene understanding and a task-specialized
structure with VLMs. In particular, we introduce preliminary scene interaction
and specialized expert adapters within the same driving task structure, which
better align VLMs with autonomous driving scenarios. Furthermore, our approach
is designed to be fully compatible with existing VLMs while seamlessly
integrating with planning-oriented driving systems. Extensive experiments on
the DriveLM and nuScenes-QA datasets demonstrate that LMAD significantly boosts
the performance of existing VLMs on driving reasoning tasks,setting a new
standard in explainable autonomous driving.

</details>


### [83] [S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing](https://arxiv.org/abs/2508.12409)
*Liang Lv,Di Wang,Jing Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: 这篇论文提出了S5框架，通过解锁大规模未标注地球观测数据的潜力，实现了远感半监督语义分割的可扩展性方案，在多个远感测试集上创造了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督语义分割方法多基于小规模数据集和模型，限制了实际应用能力。需要解决大规模未标注地球观测数据利用不充分的问题。

Method: 构建RS4P-1M大规模数据集，统一预训练不同规模的远感基础模型，并在微调阶段采用专家混合策略进行多数据集微调。

Result: 在土地覆盖分割和目标检测任务上获得显著性能提升，在所有远感测试集上达到最佳性能。

Conclusion: 证明了通过扩展半监督学习来利用大规模未标注远感数据的可行性，为远感分析领域提供了可扩展的解决方案。

Abstract: Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS)
analysis by leveraging unlabeled data through pseudo-labeling and consistency
learning. However, existing S4 studies often rely on small-scale datasets and
models, limiting their practical applicability. To address this, we propose S5,
the first scalable framework for semi-supervised semantic segmentation in RS,
which unlocks the potential of vast unlabeled Earth observation data typically
underutilized due to costly pixel-level annotations. Built upon existing
large-scale RS datasets, S5 introduces a data selection strategy that
integrates entropy-based filtering and diversity expansion, resulting in the
RS4P-1M dataset. Using this dataset, we systematically scales S4 methods by
pre-training RS foundation models (RSFMs) of varying sizes on this extensive
corpus, significantly boosting their performance on land cover segmentation and
object detection tasks. Furthermore, during fine-tuning, we incorporate a
Mixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which
enables efficient adaptation to multiple RS benchmarks with fewer parameters.
This approach improves the generalization and versatility of RSFMs across
diverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance
across all benchmarks, underscoring the viability of scaling semi-supervised
learning for RS applications. All datasets, code, and models will be released
at https://github.com/MiliLab/S5

</details>


### [84] [SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes](https://arxiv.org/abs/2508.12410)
*Jun Zeng,Yannan Huang,Elif Keles,Halil Ertugrul Aktas,Gorkem Durak,Nikhil Kumar Tomar,Quoc-Huy Trinh,Deepak Ranjan Nayak,Ulas Bagci,Debesh Jha*

Main category: cs.CV

TL;DR: 提出SRMA-Mamba网络，通过空间解剖感知的Mamba模块和空间反向注意力模块，实现肝脏MRI体积数据的精确病理分割，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 肝硬化早期检测对降低死亡率至关重要，但现有方法未能充分利用MRI体积数据中的空间解剖细节，限制了临床效果和可解释性。

Method: 集成SABMamba模块在肝硬化组织内进行选择性Mamba扫描，结合三平面解剖信息构建全局空间上下文；引入SRMA模块利用粗分割图和分层编码特征逐步细化分割细节。

Result: 大量实验表明SRMA-Mamba在3D病理肝脏分割方面超越了最先进方法，表现出卓越性能。

Conclusion: SRMA-Mamba通过有效建模MRI体积中的空间解剖关系，为肝硬化病变的精确检测和表征提供了有效的解决方案。

Abstract: Liver Cirrhosis plays a critical role in the prognosis of chronic liver
disease. Early detection and timely intervention are critical in significantly
reducing mortality rates. However, the intricate anatomical architecture and
diverse pathological changes of liver tissue complicate the accurate detection
and characterization of lesions in clinical settings. Existing methods
underutilize the spatial anatomical details in volumetric MRI data, thereby
hindering their clinical effectiveness and explainability. To address this
challenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to
model the spatial relationships within the complex anatomical structures of MRI
volumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba),
SRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and
combines anatomical information from the sagittal, coronal, and axial planes to
construct a global spatial context representation, enabling efficient
volumetric segmentation of pathological liver structures. Furthermore, we
introduce the Spatial Reverse Attention module (SRMA), designed to
progressively refine cirrhotic details in the segmentation map, utilizing both
the coarse segmentation map and hierarchical encoding features. Extensive
experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods,
delivering exceptional performance in 3D pathological liver segmentation. Our
code is available for public:
{\color{blue}{https://github.com/JunZengz/SRMA-Mamba}}.

</details>


### [85] [TiP4GEN: Text to Immersive Panorama 4D Scene Generation](https://arxiv.org/abs/2508.12415)
*Ke Xing,Hanwen Liang,Dejia Xu,Yuyang Yin,Konstantinos N. Plataniotis,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: TiP4GEN是一个文本到动态全景场景生成框架，通过双分支生成模型和几何对齐重建模型，实现了360度沉浸式动态场景的生成，解决了现有方法在视角广度和几何一致性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着VR/AR技术的快速发展，对高质量沉浸式动态场景的需求日益增长，但现有生成方法主要关注静态场景或窄视角动态场景，无法提供真正的360度沉浸体验。

Method: 提出双分支生成模型（全景分支和透视分支）进行视频生成，通过双向交叉注意力机制实现信息交换；基于3D高斯泼溅的几何对齐重建模型，利用度量深度图对齐时空点云，确保几何一致性和时间连贯性。

Result: 大量实验证明了所提设计的有效性，TiP4GEN在生成视觉吸引人且运动连贯的动态全景场景方面表现出优越性。

Conclusion: TiP4GEN框架成功实现了从文本生成高质量360度沉浸式动态场景，为VR/AR应用提供了有效的解决方案。

Abstract: With the rapid advancement and widespread adoption of VR/AR technologies,
there is a growing demand for the creation of high-quality, immersive dynamic
scenes. However, existing generation works predominantly concentrate on the
creation of static scenes or narrow perspective-view dynamic scenes, falling
short of delivering a truly 360-degree immersive experience from any viewpoint.
In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamic
panorama scene generation framework that enables fine-grained content control
and synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN
integrates panorama video generation and dynamic scene reconstruction to create
360-degree immersive virtual environments. For video generation, we introduce a
\textbf{Dual-branch Generation Model} consisting of a panorama branch and a
perspective branch, responsible for global and local view generation,
respectively. A bidirectional cross-attention mechanism facilitates
comprehensive information exchange between the branches. For scene
reconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model}
based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using
metric depth maps and initializing scene cameras with estimated poses, our
method ensures geometric consistency and temporal coherence for the
reconstructed scenes. Extensive experiments demonstrate the effectiveness of
our proposed designs and the superiority of TiP4GEN in generating visually
compelling and motion-coherent dynamic panoramic scenes. Our project page is at
https://ke-xing.github.io/TiP4GEN/.

</details>


### [86] [Illusions in Humans and AI: How Visual Perception Aligns and Diverges](https://arxiv.org/abs/2508.12422)
*Jianyi Yang,Junyi Ye,Ankan Dash,Guiling Wang*

Main category: cs.CV

TL;DR: 通过对比生物和人工智能视觉系统在视觉幻觉上的差异，揭示了AI视觉的特有弱点和对齐问题，为开发更稳健可解释的AI视觉系统提供了见解。


<details>
  <summary>Details</summary>
Motivation: 理解人类和AI视觉系统在构建视觉现实方面的根本差异，以发展更稳健、可解释性更高且与人类对齐的人工智能视觉系统。

Method: 通过系统性对比人类和AI对经典视觉幻觉（颜色、大小、形状、运动）的反应，分析AI模型中出现的幻觉效应和独特幻觉。

Result: 发现AI会出现某些类似人类的幻觉效应，同时也有独特的AI幻觉（如像素级敏感性、幻觉）。揭示了AI特有的感知弱点和与人类的对齐间隙。

Conclusion: 这些发现为开发保持人类有益感知偏见、避免破坏信任和安全的抑刻幻觉的视觉系统提供了重要见解。

Abstract: By comparing biological and artificial perception through the lens of
illusions, we highlight critical differences in how each system constructs
visual reality. Understanding these divergences can inform the development of
more robust, interpretable, and human-aligned artificial intelligence (AI)
vision systems. In particular, visual illusions expose how human perception is
based on contextual assumptions rather than raw sensory data. As artificial
vision systems increasingly perform human-like tasks, it is important to ask:
does AI experience illusions, too? Does it have unique illusions? This article
explores how AI responds to classic visual illusions that involve color, size,
shape, and motion. We find that some illusion-like effects can emerge in these
models, either through targeted training or as by-products of pattern
recognition. In contrast, we also identify illusions unique to AI, such as
pixel-level sensitivity and hallucinations, that lack human counterparts. By
systematically comparing human and AI responses to visual illusions, we uncover
alignment gaps and AI-specific perceptual vulnerabilities invisible to human
perception. These findings provide insights for future research on vision
systems that preserve human-beneficial perceptual biases while avoiding
distortions that undermine trust and safety.

</details>


### [87] [Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations](https://arxiv.org/abs/2508.12430)
*Yahsin Yeh,Yilun Wu,Bokai Ruan,Honghan Shuai*

Main category: cs.CV

TL;DR: 这篇论文揭示了视觉问答中自然语言解释系统的漏洞，通过攻击和防御方法提高模型的稳健性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的VQA-NLE系统存在解释不一致和理解不深入的问题，需要揭示这些漏洞并提出改善方案。

Method: 使用对抗性问题扰动和新的图像最小修改策略来导致矛盾输出，并采用外部知识来缓解不一致性。

Result: 在两个标准测试集和两个广泛使用的VQA-NLE模型上，攻击方法显示出高效性，知识基础防御方案显示了提高模型稳健性的潜力。

Conclusion: 当前VQA-NLE系统存在严重的安全性和可靠性问题，知识基础的防御方法为提升模型稳健性提供了有效途径。

Abstract: Natural language explanations in visual question answering (VQA-NLE) aim to
make black-box models more transparent by elucidating their decision-making
processes. However, we find that existing VQA-NLE systems can produce
inconsistent explanations and reach conclusions without genuinely understanding
the underlying context, exposing weaknesses in either their inference pipeline
or explanation-generation mechanism. To highlight these vulnerabilities, we not
only leverage an existing adversarial strategy to perturb questions but also
propose a novel strategy that minimally alters images to induce contradictory
or spurious outputs. We further introduce a mitigation method that leverages
external knowledge to alleviate these inconsistencies, thereby bolstering model
robustness. Extensive evaluations on two standard benchmarks and two widely
used VQA-NLE models underscore the effectiveness of our attacks and the
potential of knowledge-based defenses, ultimately revealing pressing security
and reliability concerns in current VQA-NLE systems.

</details>


### [88] [X-Ray-CoT: Interpretable Chest X-ray Diagnosis with Vision-Language Models via Chain-of-Thought Reasoning](https://arxiv.org/abs/2508.12455)
*Chee Ng,Liliang Sun,Shaoqing Tang*

Main category: cs.CV

TL;DR: X-Ray-CoT是一个基于视觉语言大模型的新型框架，通过模拟放射科医生的思维链过程，实现胸部X光片的智能诊断和可解释报告生成，在保持竞争力的诊断准确率的同时提供高质量的可解释性。


<details>
  <summary>Details</summary>
Motivation: 胸部X光影像诊断需要丰富的临床经验且存在观察者间差异，虽然深度学习模型诊断准确率高，但其黑盒特性阻碍了在高风险医疗环境中的临床应用。

Method: 提出X-Ray-CoT框架，首先提取多模态特征和视觉概念，然后使用基于LLM的组件配合结构化思维链提示策略进行推理，生成详细的自然语言诊断报告。

Result: 在CORDA数据集上评估，疾病诊断的平衡准确率达到80.52%，F1分数为78.65%，略优于现有黑盒模型，并能生成高质量的可解释报告。

Conclusion: 消融研究证实了多模态融合和思维链推理对于构建稳健透明医疗AI系统的必要性，这项工作代表了医学影像领域向可信赖和临床可操作AI系统迈出的重要一步。

Abstract: Chest X-ray imaging is crucial for diagnosing pulmonary and cardiac diseases,
yet its interpretation demands extensive clinical experience and suffers from
inter-observer variability. While deep learning models offer high diagnostic
accuracy, their black-box nature hinders clinical adoption in high-stakes
medical settings. To address this, we propose X-Ray-CoT (Chest X-Ray
Chain-of-Thought), a novel framework leveraging Vision-Language Large Models
(LVLMs) for intelligent chest X-ray diagnosis and interpretable report
generation. X-Ray-CoT simulates human radiologists' "chain-of-thought" by first
extracting multi-modal features and visual concepts, then employing an
LLM-based component with a structured Chain-of-Thought prompting strategy to
reason and produce detailed natural language diagnostic reports. Evaluated on
the CORDA dataset, X-Ray-CoT achieves competitive quantitative performance,
with a Balanced Accuracy of 80.52% and F1 score of 78.65% for disease
diagnosis, slightly surpassing existing black-box models. Crucially, it
uniquely generates high-quality, explainable reports, as validated by
preliminary human evaluations. Our ablation studies confirm the integral role
of each proposed component, highlighting the necessity of multi-modal fusion
and CoT reasoning for robust and transparent medical AI. This work represents a
significant step towards trustworthy and clinically actionable AI systems in
medical imaging.

</details>


### [89] [Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping](https://arxiv.org/abs/2508.12466)
*Xuhui Zhan,Tyler Derr*

Main category: cs.CV

TL;DR: Inverse-LLaVA是一种新的多模态学习方法，无需对齐预训练，将文本嵌入映射到视觉表示空间而非传统相反方向，在推理任务上表现优异但感知任务有所下降。


<details>
  <summary>Details</summary>
Motivation: 挑战传统多模态学习需要昂贵对齐预训练的假设，探索更高效的多模态融合方法，减少计算需求并保持模态特性。

Method: 将文本嵌入映射到连续视觉表示空间，在transformer中间层进行融合，通过注意力机制中的选择性加法组件实现动态集成。

Result: 在9个多模态基准测试中显示性能权衡：推理任务显著提升（MM-VET +0.2%, VizWiz +1.8%, ScienceQA +0.2%, 认知推理 +27.2%），但感知任务下降（名人识别 -49.5%, OCR -21.3%），计算需求减少45%。

Conclusion: 首次证明对齐预训练对有效多模态学习并非必要，特别是复杂推理任务；建立新范式可行性，挑战传统模态融合观念，为高效多模态架构开辟新方向。

Abstract: Traditional multimodal learning approaches require expensive alignment
pre-training to bridge vision and language modalities, typically projecting
visual features into discrete text token spaces. We challenge both fundamental
assumptions underlying this paradigm by proposing Inverse-LLaVA, a novel
approach that eliminates alignment pre-training entirely while inverting the
conventional mapping direction. Rather than projecting visual features to text
space, our method maps text embeddings into continuous visual representation
space and performs fusion within transformer intermediate layers. Through
selective additive components in attention mechanisms, we enable dynamic
integration of visual and textual representations without requiring massive
image-text alignment datasets. Comprehensive experiments across nine multimodal
benchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves
notable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,
VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing
expected decreases in perception tasks requiring memorized visual-text
associations (celebrity recognition: -49.5%, OCR: -21.3%). These results
provide the first empirical evidence that alignment pre-training is not
necessary for effective multimodal learning, particularly for complex reasoning
tasks. Our work establishes the feasibility of a new paradigm that reduces
computational requirements by 45%, challenges conventional wisdom about
modality fusion, and opens new research directions for efficient multimodal
architectures that preserve modality-specific characteristics. Our project
website with code and additional resources is available at
https://inverse-llava.github.io.

</details>


### [90] [Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System](https://arxiv.org/abs/2508.12473)
*Eranga Bandara,Ross Gore,Sachin Shetty,Ravi Mukkamala,Christopher Rhea,Atmaram Yarlagadda,Shaifali Kaushik,L. H. M. P. De Silva,Andriy Maznychenko,Inna Sokolowska,Amin Hass,Kasun De Zoysa*

Main category: cs.CV

TL;DR: 使用细调视觉-语言模型联盟和推理大语言模型构建自动化H-反射波形诊断系统，提高神经肌肉诊断的准确性和标准化


<details>
  <summary>Details</summary>
Motivation: 传统H-反射EMG波形分析存在主观偏差和变异性，影响诊断可靠性和标准化

Method: 多个细调VLM模型分析波形图像，通过共识机制聚合结果，再由专门推理LLM精炼诊断结论，形成可解释的决策支持

Result: 混合系统实现了高准确、一致性和可解释的H-反射评估，大大推进神经肌肉诊断的自动化和标准化

Conclusion: 这是首次将细调VLM联盟与推理LLM结合用于图像基H-反射分析，为下一代AI辅助神经肌肉评估平台奠定基础

Abstract: Accurate assessment of neuromuscular reflexes, such as the H-reflex, plays a
critical role in sports science, rehabilitation, and clinical neurology.
Traditional analysis of H-reflex EMG waveforms is subject to variability and
interpretation bias among clinicians and researchers, limiting reliability and
standardization. To address these challenges, we propose a Fine-Tuned
Vision-Language Model (VLM) Consortium and a reasoning Large-Language Model
(LLM)-enabled Decision Support System for automated H-reflex waveform
interpretation and diagnosis. Our approach leverages multiple VLMs, each
fine-tuned on curated datasets of H-reflex EMG waveform images annotated with
clinical observations, recovery timelines, and athlete metadata. These models
are capable of extracting key electrophysiological features and predicting
neuromuscular states, including fatigue, injury, and recovery, directly from
EMG images and contextual metadata. Diagnostic outputs from the VLM consortium
are aggregated using a consensus-based method and refined by a specialized
reasoning LLM, which ensures robust, transparent, and explainable decision
support for clinicians and sports scientists. The end-to-end platform
orchestrates seamless communication between the VLM ensemble and the reasoning
LLM, integrating prompt engineering strategies and automated reasoning
workflows using LLM Agents. Experimental results demonstrate that this hybrid
system delivers highly accurate, consistent, and interpretable H-reflex
assessments, significantly advancing the automation and standardization of
neuromuscular diagnostics. To our knowledge, this work represents the first
integration of a fine-tuned VLM consortium with a reasoning LLM for image-based
H-reflex analysis, laying the foundation for next-generation AI-assisted
neuromuscular assessment and athlete monitoring platforms.

</details>


### [91] [Skin Cancer Classification: Hybrid CNN-Transformer Models with KAN-Based Fusion](https://arxiv.org/abs/2508.12484)
*Shubhi Agarwal,Amulya Kumar Mahto*

Main category: cs.CV

TL;DR: 提出结合CNN-Transformer混合架构和卷积Kolmogorov-Arnold网络(CKAN)的皮肤癌分类方法，在多个数据集上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 皮肤癌分类在医学图像分析中至关重要，需要精确区分恶性和非恶性病变以实现早期诊断和治疗。现有方法需要更好地整合局部空间特征和全局依赖关系。

Method: 采用顺序和并行混合CNN-Transformer模型，结合卷积Kolmogorov-Arnold网络(CKAN)进行非线性特征融合。使用迁移学习和广泛数据增强，CNN提取局部空间特征，Transformer建模全局依赖，CKAN通过可学习激活函数增强特征融合。

Result: 在HAM10000数据集上达到92.81%准确率和92.47% F1分数，PAD-UFES数据集上97.83%准确率和F1分数，BCN20000数据集上91.17%准确率和91.79% F1分数，展现了优异的分类性能和泛化能力。

Conclusion: 混合CNN-Transformer架构能有效捕获空间和上下文特征，CKAN集成通过可学习激活函数增强特征融合，产生更具判别性的表示。研究强调了特征表示和模型设计在推进稳健准确医学图像分类中的重要性。

Abstract: Skin cancer classification is a crucial task in medical image analysis, where
precise differentiation between malignant and non-malignant lesions is
essential for early diagnosis and treatment. In this study, we explore
Sequential and Parallel Hybrid CNN-Transformer models with Convolutional
Kolmogorov-Arnold Network (CKAN). Our approach integrates transfer learning and
extensive data augmentation, where CNNs extract local spatial features,
Transformers model global dependencies, and CKAN facilitates nonlinear feature
fusion for improved representation learning. To assess generalization, we
evaluate our models on multiple benchmark datasets (HAM10000,BCN20000 and
PAD-UFES) under varying data distributions and class imbalances. Experimental
results demonstrate that hybrid CNN-Transformer architectures effectively
capture both spatial and contextual features, leading to improved
classification performance. Additionally, the integration of CKAN enhances
feature fusion through learnable activation functions, yielding more
discriminative representations. Our proposed approach achieves competitive
performance in skin cancer classification, demonstrating 92.81% accuracy and
92.47% F1-score on the HAM10000 dataset, 97.83% accuracy and 97.83% F1-score on
the PAD-UFES dataset, and 91.17% accuracy with 91.79% F1- score on the BCN20000
dataset highlighting the effectiveness and generalizability of our model across
diverse datasets. This study highlights the significance of feature
representation and model design in advancing robust and accurate medical image
classification.

</details>


### [92] [Design and Validation of a Responsible Artificial Intelligence-based System for the Referral of Diabetic Retinopathy Patients](https://arxiv.org/abs/2508.12506)
*E. Ulises Moya-Sánchez,Abraham Sánchez-Perez,Raúl Nanclares Da Veiga,Alejandro Zarate-Macías,Edgar Villareal,Alejandro Sánchez-Montes,Edtna Jauregui-Ulloa,Héctor Moreno,Ulises Cortés*

Main category: cs.CV

TL;DR: 这篇论文提出了RAIS-DR系统，一种负责任的人工智能系统，用于糖尿病视网膜病筛查，在准确性和公平性方面都显著超越了FDA批准的EyeArt系统。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病是工作年龄人变益的主要原因，早期发现可大幅降低失明风险。但眼科医生短缺和质量偏差的数据影响了AI系统在临床中的应用。

Method: 研究人员开发了RAIS-DR系统，这是一个在AI生命周期中融入伦理原则的负责任AI系统。系统集成了高效卷积模型用于预处理、质量评估和三个专门的DR分类模型。

Result: 在1,046名患者的本地数据集上，RAIS-DR与FDA批准的EyeArt系统相比，F1分数提高5-12%，准确性提高6-19%，特异性提高10-20%。同时在公平性指标上也表现更佳。

Conclusion: RAIS-DR作为一个健壮且符合伦理要求的解决方案，有力推动糖尿病视网膜病筛查在临床环境中的应用，并且所有代码和模型都开源可用。

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss in working-age
individuals. Early detection of DR can reduce the risk of vision loss by up to
95%, but a shortage of retinologists and challenges in timely examination
complicate detection. Artificial Intelligence (AI) models using retinal fundus
photographs (RFPs) offer a promising solution. However, adoption in clinical
settings is hindered by low-quality data and biases that may lead AI systems to
learn unintended features. To address these challenges, we developed RAIS-DR, a
Responsible AI System for DR screening that incorporates ethical principles
across the AI lifecycle. RAIS-DR integrates efficient convolutional models for
preprocessing, quality assessment, and three specialized DR classification
models. We evaluated RAIS-DR against the FDA-approved EyeArt system on a local
dataset of 1,046 patients, unseen by both systems. RAIS-DR demonstrated
significant improvements, with F1 scores increasing by 5-12%, accuracy by
6-19%, and specificity by 10-20%. Additionally, fairness metrics such as
Disparate Impact and Equal Opportunity Difference indicated equitable
performance across demographic subgroups, underscoring RAIS-DR's potential to
reduce healthcare disparities. These results highlight RAIS-DR as a robust and
ethically aligned solution for DR screening in clinical settings. The code,
weights of RAIS-DR are available at
https://gitlab.com/inteligencia-gubernamental-jalisco/jalisco-retinopathy with
RAIL.

</details>


### [93] [LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models](https://arxiv.org/abs/2508.12512)
*Krishna Teja Chitty-Venkata,Murali Emani,Venkatram Vishwanath*

Main category: cs.CV

TL;DR: 本文提出了LangVision-LoRA-NAS框架，通过神经架构搜索为视觉语言模型动态优化LoRA秩配置，在提升性能的同时降低微调成本。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA方法使用固定秩进行微调，限制了在不同多模态任务中的灵活性和效率，需要一种能够根据具体任务动态调整秩配置的方法。

Method: 将神经架构搜索(NAS)与LoRA相结合，动态搜索最优的LoRA秩配置，针对特定多模态任务平衡性能和计算效率。

Result: 在LLaMA-3.2-11B模型上的大量实验表明，该方法显著提升了模型性能，同时降低了微调成本。

Conclusion: LangVision-LoRA-NAS框架为视觉语言模型提供了一种高效的自适应微调方法，通过动态秩优化实现了性能与效率的良好平衡。

Abstract: Vision Language Models (VLMs) integrate visual and text modalities to enable
multimodal understanding and generation. These models typically combine a
Vision Transformer (ViT) as an image encoder and a Large Language Model (LLM)
for text generation. LoRA (Low-Rank Adaptation) is an efficient fine-tuning
method to adapt pre-trained models to new tasks by introducing low-rank updates
to their weights. While LoRA has emerged as a powerful technique for
fine-tuning large models by introducing low-rank updates, current
implementations assume a fixed rank, potentially limiting flexibility and
efficiency across diverse tasks. This paper introduces
\textit{LangVision-LoRA-NAS}, a novel framework that integrates Neural
Architecture Search (NAS) with LoRA to optimize VLMs for variable-rank
adaptation. Our approach leverages NAS to dynamically search for the optimal
LoRA rank configuration tailored to specific multimodal tasks, balancing
performance and computational efficiency. Through extensive experiments using
the LLaMA-3.2-11B model on several datasets, LangVision-LoRA-NAS demonstrates
notable improvement in model performance while reducing fine-tuning costs. Our
Base and searched fine-tuned models on LLaMA-3.2-11B-Vision-Instruct can be
found
\href{https://huggingface.co/collections/krishnateja95/llama-32-11b-vision-instruct-langvision-lora-nas-6786cac480357a6a6fcc59ee}{\textcolor{blue}{here}}
and the code for LangVision-LoRA-NAS can be found
\href{https://github.com/krishnateja95/LangVision-NAS}{\textcolor{blue}{here}}.

</details>


### [94] [An Initial Study of Bird's-Eye View Generation for Autonomous Vehicles using Cross-View Transformers](https://arxiv.org/abs/2508.12520)
*Felipe Carlos dos Santos,Eric Aislan Antonelo,Gustavo Claudio Karl Couto*

Main category: cs.CV

TL;DR: 使用交叉视图变换器(CVT)将相机图像映射到鸟瞰图(BEV)的三个通道：道路、车道标记和规划轨迹，通过模拟器验证了泛化能力和不同相机配置的效果。


<details>
  <summary>Details</summary>
Motivation: 鸟瞰图(BEV)为自动驾驶感知提供了结构化的俯视抽象表示，但如何从相机图像准确映射到BEV地图是一个重要挑战。

Method: 采用交叉视图变换器(CVT)架构，使用城市驾驶模拟器生成数据，研究不同相机布局和损失函数（焦点损失和L1损失）的效果。

Result: 在仅使用一个城镇训练数据的情况下，采用四相机配置和L1损失的CVT模型在新城镇测试中表现出最鲁棒的性能。

Conclusion: 交叉视图变换器在将相机输入映射到相对准确的BEV地图方面展现出良好潜力，L1损失和四相机配置是最佳组合。

Abstract: Bird's-Eye View (BEV) maps provide a structured, top-down abstraction that is
crucial for autonomous-driving perception. In this work, we employ Cross-View
Transformers (CVT) for learning to map camera images to three BEV's channels -
road, lane markings, and planned trajectory - using a realistic simulator for
urban driving. Our study examines generalization to unseen towns, the effect of
different camera layouts, and two loss formulations (focal and L1). Using
training data from only a town, a four-camera CVT trained with the L1 loss
delivers the most robust test performance, evaluated in a new town. Overall,
our results underscore CVT's promise for mapping camera inputs to reasonably
accurate BEV maps.

</details>


### [95] [MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training](https://arxiv.org/abs/2508.12522)
*Muhammad Osama Zeeshan,Natacha Gillet,Alessandro Lameiras Koerich,Marco Pedersoli,Francois Bremond,Eric Granger*

Main category: cs.CV

TL;DR: MuSACo是一个基于协同训练的多模态个性化表情识别方法，通过选择相关源主体并利用多模态互补信息进行主体特异性适应，在BioVid和StressID数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多源域适应方法往往忽视多模态信息或将多个源混合为单一域，限制了主体多样性，无法明确捕捉主体特异性特征。个性化表情识别需要适应主体间差异，特别是在数字健康应用中主体级别的细微差别至关重要。

Method: MuSACo基于协同训练框架，选择与目标相关的源主体，使用主导模态生成伪标签进行类感知学习，结合类无关损失从置信度较低的目标样本中学习。同时对齐每个模态的源特征，仅组合置信的目标特征。

Result: 在BioVid和StressID两个具有挑战性的多模态表情识别数据集上，MuSACo超越了无监督域适应（混合）和最先进的多源域适应方法。

Conclusion: MuSACo通过有效利用多模态和多源域信息，成功实现了主体特异性的表情识别适应，特别适用于数字健康应用中需要捕捉主体细微差异的场景。

Abstract: Personalized expression recognition (ER) involves adapting a machine learning
model to subject-specific data for improved recognition of expressions with
considerable interpersonal variability. Subject-specific ER can benefit
significantly from multi-source domain adaptation (MSDA) methods, where each
domain corresponds to a specific subject, to improve model accuracy and
robustness. Despite promising results, state-of-the-art MSDA approaches often
overlook multimodal information or blend sources into a single domain, limiting
subject diversity and failing to explicitly capture unique subject-specific
characteristics. To address these limitations, we introduce MuSACo, a
multi-modal subject-specific selection and adaptation method for ER based on
co-training. It leverages complementary information across multiple modalities
and multiple source domains for subject-specific adaptation. This makes MuSACo
particularly relevant for affective computing applications in digital health,
such as patient-specific assessment for stress or pain, where subject-level
nuances are crucial. MuSACo selects source subjects relevant to the target and
generates pseudo-labels using the dominant modality for class-aware learning,
in conjunction with a class-agnostic loss to learn from less confident target
samples. Finally, source features from each modality are aligned, while only
confident target features are combined. Our experimental results on challenging
multimodal ER datasets: BioVid and StressID, show that MuSACo can outperform
UDA (blending) and state-of-the-art MSDA methods.

</details>


### [96] [REVEAL -- Reasoning and Evaluation of Visual Evidence through Aligned Language](https://arxiv.org/abs/2508.12543)
*Ipsita Praharaj,Yukta Butala,Yash Butala*

Main category: cs.CV

TL;DR: REVEAL框架利用视觉语言模型进行图像伪造检测，通过整体场景评估和区域异常检测两种方法，在多个领域数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 生成模型的快速发展使得视觉伪造检测和解释变得更加困难，需要既能检测伪造又能提供推理和定位的鲁棒框架，而现有方法在跨领域泛化方面存在挑战。

Method: 将伪造检测构建为提示驱动的视觉推理任务，利用大型视觉语言模型的语义对齐能力。提出两种方法：(1)整体场景级评估（基于物理、语义、透视和真实性）(2)区域级异常检测（将图像分割成多个区域进行分析）。

Result: 在多个领域数据集（Photoshop、DeepFake和AIGC编辑）上进行实验，与竞争基线比较并分析模型提供的推理能力。

Conclusion: REVEAL框架通过视觉语言模型的语义对齐能力，在跨领域图像伪造检测方面表现出良好的泛化性能和推理能力。

Abstract: The rapid advancement of generative models has intensified the challenge of
detecting and interpreting visual forgeries, necessitating robust frameworks
for image forgery detection while providing reasoning as well as localization.
While existing works approach this problem using supervised training for
specific manipulation or anomaly detection in the embedding space,
generalization across domains remains a challenge. We frame this problem of
forgery detection as a prompt-driven visual reasoning task, leveraging the
semantic alignment capabilities of large vision-language models. We propose a
framework, `REVEAL` (Reasoning and Evaluation of Visual Evidence through
Aligned Language), that incorporates generalized guidelines. We propose two
tangential approaches - (1) Holistic Scene-level Evaluation that relies on the
physics, semantics, perspective, and realism of the image as a whole and (2)
Region-wise anomaly detection that splits the image into multiple regions and
analyzes each of them. We conduct experiments over datasets from different
domains (Photoshop, DeepFake and AIGC editing). We compare the Vision Language
Models against competitive baselines and analyze the reasoning provided by
them.

</details>


### [97] [Structure-preserving Feature Alignment for Old Photo Colorization](https://arxiv.org/abs/2508.12570)
*Yingxue Pang,Xin Jin,Jun Fu,Zhibo Chen*

Main category: cs.CV

TL;DR: 一种仅需两张图片训练的深度学习方法SFAC，用于老照片着色，免除了大规模数据集和域间差问题


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在老照片着色任务中遇到困难，包括缺乏真实标签和自然灰度图与老照片之间的域间差

Method: 提出SFAC算法，通过特征分布对齐损失确保语义相关对象有相似颜色，并使用结构保持机制（特征层感知约束和像素层冲冻-更新金字塔）来减少结构扭曲

Result: 实验结果显示方法在老照片着色任务上有效，定性和定量指标都证明了其优势

Conclusion: SFAC方法能够有效解决老照片着色的挑战，免除了对大规模数据集的依赖，通过语义对应和结构保持机制实现了高质量的颜色转移

Abstract: Deep learning techniques have made significant advancements in
reference-based colorization by training on large-scale datasets. However,
directly applying these methods to the task of colorizing old photos is
challenging due to the lack of ground truth and the notorious domain gap
between natural gray images and old photos. To address this issue, we propose a
novel CNN-based algorithm called SFAC, i.e., Structure-preserving Feature
Alignment Colorizer. SFAC is trained on only two images for old photo
colorization, eliminating the reliance on big data and allowing direct
processing of the old photo itself to overcome the domain gap problem. Our
primary objective is to establish semantic correspondence between the two
images, ensuring that semantically related objects have similar colors. We
achieve this through a feature distribution alignment loss that remains robust
to different metric choices. However, utilizing robust semantic correspondence
to transfer color from the reference to the old photo can result in inevitable
structure distortions. To mitigate this, we introduce a structure-preserving
mechanism that incorporates a perceptual constraint at the feature level and a
frozen-updated pyramid at the pixel level. Extensive experiments demonstrate
the effectiveness of our method for old photo colorization, as confirmed by
qualitative and quantitative metrics.

</details>


### [98] [Foundation Model for Skeleton-Based Human Action Understanding](https://arxiv.org/abs/2508.12586)
*Hongsong Wang,Wanjiang Weng,Junbo Wang,Fang Zhao,Guo-Sen Xie,Xin Geng,Liang Wang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种统一的骨架基础模型USDRL，通过Transformer编码器、多粒度特征解相关和多视角一致性训练，在25个标准数据集上显著超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏具有良好扩展性和通用性的骨架基础模型，无法处理多样化的行为理解任务。需要一个可适配广泛行为理解任务的统一框架。

Method: 设计了USDRL框架，包括：1）Transformer基础的密集时空编码器（DSTE）用于学习时间动态和空间结构特征；2）多粒度特征解相关（MG-FD）模块减少维度冗余；3）多视角一致性训练（MPCT）模块通过多视角和多模态自监督提升特征学习。

Result: 在9种不同的骨架基于行为理解任务上进行了测试，涵盖粗粒度预测、密集预测和转移预测。在25个标准数据集上显著超越了当前最优方法。

Conclusion: 该工作为骨架基础行为理解领域提供了一个基础模型，拓宽了研究范围，并鼓励更多关注密集预测任务。

Abstract: Human action understanding serves as a foundational pillar in the field of
intelligent motion perception. Skeletons serve as a modality- and
device-agnostic representation for human modeling, and skeleton-based action
understanding has potential applications in humanoid robot control and
interaction. \RED{However, existing works often lack the scalability and
generalization required to handle diverse action understanding tasks. There is
no skeleton foundation model that can be adapted to a wide range of action
understanding tasks}. This paper presents a Unified Skeleton-based Dense
Representation Learning (USDRL) framework, which serves as a foundational model
for skeleton-based human action understanding. USDRL consists of a
Transformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained Feature
Decorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). The
DSTE module adopts two parallel streams to learn temporal dynamic and spatial
structure features. The MG-FD module collaboratively performs feature
decorrelation across temporal, spatial, and instance domains to reduce
dimensional redundancy and enhance information extraction. The MPCT module
employs both multi-view and multi-modal self-supervised consistency training.
The former enhances the learning of high-level semantics and mitigates the
impact of low-level discrepancies, while the latter effectively facilitates the
learning of informative multimodal features. We perform extensive experiments
on 25 benchmarks across across 9 skeleton-based action understanding tasks,
covering coarse prediction, dense prediction, and transferred prediction. Our
approach significantly outperforms the current state-of-the-art methods. We
hope that this work would broaden the scope of research in skeleton-based
action understanding and encourage more attention to dense prediction tasks.

</details>


### [99] [OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion](https://arxiv.org/abs/2508.12610)
*Chen Qian,Danyang Li,Xinran Yu,Zheng Yang,Qiang Ma*

Main category: cs.CV

TL;DR: 通过提出CMU-Occlu数据集和OpenMoCap模型，解决了光学动作抓取中大规模标记点遮挡问题，提高了在复杂场景下的动作重建精度和稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有光学动作抓取系统在实际应用中遇到大规模标记点遮挡时性能严重下降，主要因为缺乏真实遮挡模式的训练数据和无法抓取标记点间的长程依赖关系。

Method: 使用光线追踪技术模拟真实遮挡模式构建CMU-Occlu数据集，设计OpenMoCap模型通过标记点-关节链推理机制实现同时优化和构建深度约束。

Result: 实验结果显示OpenMoCap在多种场景下都一致超过了竞争方法，CMU-Occlu数据集为稳健动作解决研究打开了新大门。

Conclusion: 该研究通过创新的数据集和模型设计，有效解决了光学动作抓取中遮挡问题的核心挑战，提高了系统在实际应用中的稳健性和可靠性。

Abstract: Optical motion capture is a foundational technology driving advancements in
cutting-edge fields such as virtual reality and film production. However,
system performance suffers severely under large-scale marker occlusions common
in real-world applications. An in-depth analysis identifies two primary
limitations of current models: (i) the lack of training datasets accurately
reflecting realistic marker occlusion patterns, and (ii) the absence of
training strategies designed to capture long-range dependencies among markers.
To tackle these challenges, we introduce the CMU-Occlu dataset, which
incorporates ray tracing techniques to realistically simulate practical marker
occlusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving
model designed specifically for robust motion capture in environments with
significant occlusions. Leveraging a marker-joint chain inference mechanism,
OpenMoCap enables simultaneous optimization and construction of deep
constraints between markers and joints. Extensive comparative experiments
demonstrate that OpenMoCap consistently outperforms competing methods across
diverse scenarios, while the CMU-Occlu dataset opens the door for future
studies in robust motion solving. The proposed OpenMoCap is integrated into the
MoSen MoCap system for practical deployment. The code is released at:
https://github.com/qianchen214/OpenMoCap.

</details>


### [100] [Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models](https://arxiv.org/abs/2508.12587)
*Tan-Hanh Pham,Chris Ngo*

Main category: cs.CV

TL;DR: 多模态连续思维链(MCOUT)方法，通过在联合潜在空间中进行连续向量推理，充分利用视觉和文本特征的对齐，在多模态理由任务上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统的语言模型推理方法(如思维链提示)在多模态环境中存在限制，难以动态对齐音频、视觉和文本信息，需要一种更有效的多模态推理方案。

Method: 提出MCOUT方法，将推理状态表示为连续隐藏向量，通过迭代精炼并与视觉、文本嵌入形式对齐。包括两个变体：MCOUT-Base(重用语言模型隐藏状态)和MCOUT-Multi(集成多模态潜在注意力机制)。

Result: 在MMMU、ScienceQA、MMStar等框架上进行实验，MCOUT在多模态理由任务上实现了一致性提升，准确率最高提升8.23%，BLEU分数最高提升8.27%。

Conclusion: 潜在连续推理是提升大型多模态模型性能的有前景方向，MCOUT提供了一种可扩展的框架，能够进行类似人类反思类型的多模态推理。

Abstract: Many reasoning techniques for large multimodal models adapt language model
approaches, such as Chain-of-Thought (CoT) prompting, which express reasoning
as word sequences. While effective for text, these methods are suboptimal for
multimodal contexts, struggling to align audio, visual, and textual information
dynamically. To explore an alternative paradigm, we propose the Multimodal
Chain of Continuous Thought (MCOUT), which enables reasoning directly in a
joint latent space rather than in natural language. In MCOUT, the reasoning
state is represented as a continuous hidden vector, iteratively refined and
aligned with visual and textual embeddings, inspired by human reflective
cognition. We develop two variants: MCOUT-Base, which reuses the language
model`s last hidden state as the continuous thought for iterative reasoning,
and MCOUT-Multi, which integrates multimodal latent attention to strengthen
cross-modal alignment between visual and textual features. Experiments on
benchmarks including MMMU, ScienceQA, and MMStar show that MCOUT consistently
improves multimodal reasoning, yielding up to 8.23% accuracy gains over strong
baselines and improving BLEU scores up to 8.27% across multiple-choice and
open-ended tasks. These findings highlight latent continuous reasoning as a
promising direction for advancing LMMs beyond language-bound CoT, offering a
scalable framework for human-like reflective multimodal inference. Code is
available at https://github.com/Hanhpt23/OmniMod.

</details>


### [101] [SpotVLM: Cloud-edge Collaborative Real-time VLM based on Context Transfer](https://arxiv.org/abs/2508.12638)
*Chen Qian,Xinran Yu,Zewen Huang,Danyang Li,Qiang Ma,Fan Dang,Xuan Ding,Guangyong Shang,Zheng Yang*

Main category: cs.CV

TL;DR: 基于云端协同的Vision-Language模型框架SpotVLM，通过将大型模型的延迟输出作为历史上下文来指小型模型的实时推理


<details>
  <summary>Details</summary>
Motivation: 现有云端协同方案无法处理云端延迟波动，且没有充分利用大型模型延迟但准确的响应

Method: 提出Context Transfer框架，将大型LVLM的延迟输出作为历史上下文指小型SVLM的实时推理，设计了上下文替换和视觉聚焦模块

Result: 在四个数据集的三个实时视觉任务中验证了框架的有效性

Conclusion: 该新框架为未来VLM系统的效率和延迟感知协同策略奠定了基础

Abstract: Vision-Language Models (VLMs) are increasingly deployed in real-time
applications such as autonomous driving and human-computer interaction, which
demand fast and reliable responses based on accurate perception. To meet these
requirements, existing systems commonly employ cloud-edge collaborative
architectures, such as partitioned Large Vision-Language Models (LVLMs) or task
offloading strategies between Large and Small Vision-Language Models (SVLMs).
However, these methods fail to accommodate cloud latency fluctuations and
overlook the full potential of delayed but accurate LVLM responses. In this
work, we propose a novel cloud-edge collaborative paradigm for VLMs, termed
Context Transfer, which treats the delayed outputs of LVLMs as historical
context to provide real-time guidance for SVLMs inference. Based on this
paradigm, we design SpotVLM, which incorporates both context replacement and
visual focus modules to refine historical textual input and enhance visual
grounding consistency. Extensive experiments on three real-time vision tasks
across four datasets demonstrate the effectiveness of the proposed framework.
The new paradigm lays the groundwork for more effective and latency-aware
collaboration strategies in future VLM systems.

</details>


### [102] [ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.12603)
*Can Cui,Yupeng Zhou,Juntong Peng,Sung-Yeon Park,Zichong Yang,Prashanth Sankaranarayanan,Jiaru Zhang,Ruqi Zhang,Ziran Wang*

Main category: cs.CV

TL;DR: ViLaD是一种基于掩码次散模型的新型视觉语言模型框架，通过并行生成驾驶决策序列，解决了自回归VLM模型的高推理延迟问题，在nuScenes数据集上实现了更高的规划准确性和更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决自回归视觉语言模型在自主驾驶中的实时性问题，包括高推理延迟、缺乏双向推理能力，以及在动态安全关键环境中的适用性问题。

Method: 提出ViLaD框架，利用掩码次散模型并行生成整个驾驶决策序列，支持双向推理和渐进式生成，通过考虑过去和未来来迭代改善决策质量。

Result: 在nuScenes数据集上，ViLaD在规划准确性和推理速度方面超越了最先进的自回归VLM基线模型，并实现了近零失败率。实际部署在自主驾驶车进行交互停车任务时验证了其实际有效性。

Conclusion: ViLaD代表了自主驾驶系统的范式转移，通过并行生成和双向推理能力，有效解决了自回归模型在实时应用中的局限性，为安全关键环境中的实际部署提供了可行解决方案。

Abstract: End-to-end autonomous driving systems built on Vision Language Models (VLMs)
have shown significant promise, yet their reliance on autoregressive
architectures introduces some limitations for real-world applications. The
sequential, token-by-token generation process of these models results in high
inference latency and cannot perform bidirectional reasoning, making them
unsuitable for dynamic, safety-critical environments. To overcome these
challenges, we introduce ViLaD, a novel Large Vision Language Diffusion (LVLD)
framework for end-to-end autonomous driving that represents a paradigm shift.
ViLaD leverages a masked diffusion model that enables parallel generation of
entire driving decision sequences, significantly reducing computational
latency. Moreover, its architecture supports bidirectional reasoning, allowing
the model to consider both past and future simultaneously, and supports
progressive easy-first generation to iteratively improve decision quality. We
conduct comprehensive experiments on the nuScenes dataset, where ViLaD
outperforms state-of-the-art autoregressive VLM baselines in both planning
accuracy and inference speed, while achieving a near-zero failure rate.
Furthermore, we demonstrate the framework's practical viability through a
real-world deployment on an autonomous vehicle for an interactive parking task,
confirming its effectiveness and soundness for practical applications.

</details>


### [103] [TTA-DAME: Test-Time Adaptation with Domain Augmentation and Model Ensemble for Dynamic Driving Conditions](https://arxiv.org/abs/2508.12690)
*Dongjae Jeon,Taeheon Kim,Seongwon Cho,Minhyuk Seo,Jonghyun Choi*

Main category: cs.CV

TL;DR: TTA-DAME通过源域数据增强、域判别器和专用域检测器处理驾驶场景中的天气域偏移，特别是在白天到夜间的剧烈变化，通过多检测器集成和NMS提升性能，在SHIFT基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决测试时适应(TTA)在真实驾驶场景中频繁出现的天气域偏移问题，特别是应对从白天到夜间等剧烈域变化带来的挑战。

Method: 利用源域数据增强到目标域，引入域判别器和专用域检测器来缓解剧烈域偏移，训练多个检测器并通过非极大值抑制(NMS)整合预测结果。

Result: 在SHIFT基准测试中显示出显著性能提升，验证了方法的有效性。

Conclusion: TTA-DAME方法能够有效处理动态域变化，特别是在驾驶场景中的天气和光照条件变化，为实时自适应提供了可行的解决方案。

Abstract: Test-time Adaptation (TTA) poses a challenge, requiring models to dynamically
adapt and perform optimally on shifting target domains. This task is
particularly emphasized in real-world driving scenes, where weather domain
shifts occur frequently. To address such dynamic changes, our proposed method,
TTA-DAME, leverages source domain data augmentation into target domains.
Additionally, we introduce a domain discriminator and a specialized domain
detector to mitigate drastic domain shifts, especially from daytime to
nighttime conditions. To further improve adaptability, we train multiple
detectors and consolidate their predictions through Non-Maximum Suppression
(NMS). Our empirical validation demonstrates the effectiveness of our method,
showing significant performance enhancements on the SHIFT Benchmark.

</details>


### [104] [ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images](https://arxiv.org/abs/2508.12605)
*Wenjie Liao,Jieyu Yuan,Yifang Xu,Chunle Guo,Zilong Zhang,Jihong Li,Jiachen Fu,Haotian Fan,Tao Li,Junhui Cui,Chongyi Li*

Main category: cs.CV

TL;DR: 这篇论文提出了首个大规模的用户生成内容图像视觉失真评估指令微调数据集ViDA-UGC，通过链式思绪框架按变形标准生成详细的质量描述，并构建了评测基准ViDA-UGC-Bench，显著提升了多模态大语言模型的图像质量分析能力。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释性图像质量评估方法存在两个主要问题：一是将用户生成内容(UGC)和AI生成内容(AIGC)图像混为谈，使用相同的失真标准进行评估；二是缺乏详细的质量分析能力，无法有效监控图像质量和指导图像恢复。

Method: 研究构建了首个大规模的ViDA-UGC数据集，包含11K张图像，具有细粒度质量基准、详细质量感知和推理质量描述。通过失真导向的流水线流程，结合人工标注和链式思绪(CoT)评估框架，指导GPT-4o识别和分析UGC失真生成质量描述。从中精选了476张图像和6,149个问答对，构建了ViDA-UGC-Bench评测基准。

Result: 实验结果表明，ViDA-UGC数据集和CoT框架能够一致地提升多个基础MLLM模型在ViDA-UGC-Bench和Q-Bench上的各种图像质量分析能力，甚至超越了GPT-4o的表现。

Conclusion: 该研究为UGC图像质量评估领域提供了一个重要的数据集和评测基准，通过链式思绪框架生成的详细质量描述能够抓取与失真模式内在相关的丰富低级视觉特征，显著提升了多模态大语言模型的图像质量分析性能。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have introduced a
paradigm shift for Image Quality Assessment (IQA) from unexplainable image
quality scoring to explainable IQA, demonstrating practical applications like
quality control and optimization guidance. However, current explainable IQA
methods not only inadequately use the same distortion criteria to evaluate both
User-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also
lack detailed quality analysis for monitoring image quality and guiding image
restoration. In this study, we establish the first large-scale Visual
Distortion Assessment Instruction Tuning Dataset for UGC images, termed
ViDA-UGC, which comprises 11K images with fine-grained quality grounding,
detailed quality perception, and reasoning quality description data. This
dataset is constructed through a distortion-oriented pipeline, which involves
human subject annotation and a Chain-of-Thought (CoT) assessment framework.
This framework guides GPT-4o to generate quality descriptions by identifying
and analyzing UGC distortions, which helps capturing rich low-level visual
features that inherently correlate with distortion patterns. Moreover, we
carefully select 476 images with corresponding 6,149 question answer pairs from
ViDA-UGC and invite a professional team to ensure the accuracy and quality of
GPT-generated information. The selected and revised data further contribute to
the first UGC distortion assessment benchmark, termed ViDA-UGC-Bench.
Experimental results demonstrate the effectiveness of the ViDA-UGC and CoT
framework for consistently enhancing various image quality analysis abilities
across multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing
GPT-4o.

</details>


### [105] [Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning](https://arxiv.org/abs/2508.12692)
*Taeheon Kim,San Kim,Minhyuk Seo,Dongjae Jeon,Wonje Jeong,Jonghyun Choi*

Main category: cs.CV

TL;DR: 这篇论文提出了两种方法来解决类别增量学习中的重复类问题：多级知识蓄粉和动态自监督损失，利用外部未标注数据提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统类别增量学习假设每个任务都包含新类别，而实际情况下旧类别可能重复出现。这种类别增量重复(CIR)场景更加现实，且可以利用网络等外部源的丰富未标注数据。

Method: 1. 多级知识蓄粉(MLKD)：从多个历史模型中蓄粉知识，包括特征和逻辑值等多个视角。
2. 动态自监督损失(SSL)：利用未标注数据加速新类别学习，通过动态权重保持主要任务的重点。

Result: 方法在CIR场景下显著提升了模型性能，获得了CVPR第5届CLVISION挑战赛的第2名。

Conclusion: 通过有效利用外部未标注数据和多次平沿的知识蓄粉技术，可以在类别增量重复场景中同时保持模型的稳定性和可塑性。

Abstract: Class-incremental with repetition (CIR), where previously trained classes
repeatedly introduced in future tasks, is a more realistic scenario than the
traditional class incremental setup, which assumes that each task contains
unseen classes. CIR assumes that we can easily access abundant unlabeled data
from external sources, such as the Internet. Therefore, we propose two
components that efficiently use the unlabeled data to ensure the high stability
and the plasticity of models trained in CIR setup. First, we introduce
multi-level knowledge distillation (MLKD) that distills knowledge from multiple
previous models across multiple perspectives, including features and logits, so
the model can maintain much various previous knowledge. Moreover, we implement
dynamic self-supervised loss (SSL) to utilize the unlabeled data that
accelerates the learning of new classes, while dynamic weighting of SSL keeps
the focus of training to the primary task. Both of our proposed components
significantly improve the performance in CIR setup, achieving 2nd place in the
CVPR 5th CLVISION Challenge.

</details>


### [106] [DCSCR: A Class-Specific Collaborative Representation based Network for Image Set Classification](https://arxiv.org/abs/2508.12745)
*Xizhan Gao,Wei Hu*

Main category: cs.CV

TL;DR: 本文提出了一种深度类别特异协同表征网络(DCSCR)，用于解决少样本图像集分类问题，同时学习框架级和概念级特征表征，并通过类别特异协同表征实现自适应性距离测量。


<details>
  <summary>Details</summary>
Motivation: 图像集分类中的两个关键挑战是如何学习有效特征表征和探索不同图像集之间的相似性。传统方法忽视特异学习，而现有深度方法在测量集距离时无法自适应调整特异，导致少样本性能有限。

Method: 结合传统ISC方法与深度模型，设计了DCSCR网络，包含三个模块：全卷积深度特异提取器、全局特异学习模块和基于类别特异协同表征的距离学习模块。前两个模块学习框架级特异，后者通过新的CSCR对比损失函数学习概念级特异表征和集距离。

Result: 在多个知名的少样本图像集分类数据集上进行了广泛实验，结果表明该方法与一些最先进的图像集分类算法相比具有显著的效果性。

Conclusion: DCSCR方法能够同时学习框架级和概念级特异表征，并通过类别特异协同表征实现自适应性的集距离测量，有效解决了少样本图像集分类的挑战。

Abstract: Image set classification (ISC), which can be viewed as a task of comparing
similarities between sets consisting of unordered heterogeneous images with
variable quantities and qualities, has attracted growing research attention in
recent years. How to learn effective feature representations and how to explore
the similarities between different image sets are two key yet challenging
issues in this field. However, existing traditional ISC methods classify image
sets based on raw pixel features, ignoring the importance of feature learning.
Existing deep ISC methods can learn deep features, but they fail to adaptively
adjust the features when measuring set distances, resulting in limited
performance in few-shot ISC. To address the above issues, this paper combines
traditional ISC methods with deep models and proposes a novel few-shot ISC
approach called Deep Class-specific Collaborative Representation (DCSCR)
network to simultaneously learn the frame- and concept-level feature
representations of each image set and the distance similarities between
different sets. Specifically, DCSCR consists of a fully convolutional deep
feature extractor module, a global feature learning module, and a
class-specific collaborative representation-based metric learning module. The
deep feature extractor and global feature learning modules are used to learn
(local and global) frame-level feature representations, while the
class-specific collaborative representation-based metric learning module is
exploit to adaptively learn the concept-level feature representation of each
image set and thus obtain the distance similarities between different sets by
developing a new CSCR-based contrastive loss function. Extensive experiments on
several well-known few-shot ISC datasets demonstrate the effectiveness of the
proposed method compared with some state-of-the-art image set classification
algorithms.

</details>


### [107] [WIPES: Wavelet-based Visual Primitives](https://arxiv.org/abs/2508.12615)
*Wenhao Zhang,Hao Zhu,Delong Wu,Di Kang,Linchao Bao,Zhan Ma,Xun Cao*

Main category: cs.CV

TL;DR: WIPES是一种基于小波的通用视觉基元表示方法，通过小波的空间-频率局部化优势有效捕捉高低频信息，并开发了小波可微分光栅化器实现快速渲染，在多种视觉任务中表现出优于现有方法的渲染质量和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉表示方法依赖频率引导或复杂神经网络解码，导致频谱损失或渲染速度慢，需要一种能够同时保持高质量渲染和快速推理的通用视觉表示方法。

Method: 提出基于小波的视觉基元表示WIPES，利用小波的空间-频率局部化特性捕捉多尺度特征，并开发了小波可微分光栅化器来实现快速视觉渲染。

Result: 在2D图像表示、5D静态和6D动态新视角合成等视觉任务中，WIPES相比基于INR的方法具有更高的渲染质量和更快的推理速度，在渲染质量上也优于基于高斯的方法。

Conclusion: WIPES作为一种通用视觉基元表示，成功解决了现有方法在频谱保持和渲染速度方面的局限性，为多维度视觉信号表示提供了有效的解决方案。

Abstract: Pursuing a continuous visual representation that offers flexible frequency
modulation and fast rendering speed has recently garnered increasing attention
in the fields of 3D vision and graphics. However, existing representations
often rely on frequency guidance or complex neural network decoding, leading to
spectrum loss or slow rendering. To address these limitations, we propose
WIPES, a universal Wavelet-based vIsual PrimitivES for representing
multi-dimensional visual signals. Building on the spatial-frequency
localization advantages of wavelets, WIPES effectively captures both the
low-frequency "forest" and the high-frequency "trees." Additionally, we develop
a wavelet-based differentiable rasterizer to achieve fast visual rendering.
Experimental results on various visual tasks, including 2D image
representation, 5D static and 6D dynamic novel view synthesis, demonstrate that
WIPES, as a visual primitive, offers higher rendering quality and faster
inference than INR-based methods, and outperforms Gaussian-based
representations in rendering quality.

</details>


### [108] [CLAIRE-DSA: Fluoroscopic Image Classification for Quality Assurance of Computer Vision Pipelines in Acute Ischemic Stroke](https://arxiv.org/abs/2508.12755)
*Cristo J. van den Berg,Frank G. te Nijenhuis,Mirre J. Blaauboer,Daan T. W. van Erp,Carlijn M. Keppels,Matthijs van der Sluijs,Bob Roozenbeek,Wim van Zwam,Sandra Cornelissen,Danny Ruijters,Ruisheng Su,Theo van Walsum*

Main category: cs.CV

TL;DR: CLAIRE-DSA是一个基于深度学习的框架，用于在急性缺血性卒中机械取栓术中分类关键图像属性，提高下游质量控制和工作流程优化。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉模型在机械取栓术中辅助应用时，图像质量差会降低性能表现，需要自动化的图像质量评估工具。

Method: 使用预训练的ResNet骨干网络进行微调，预测九个图像属性（如对比度存在、投影角度、运动伪影严重程度等），在包含1,758张荧光MinIPs的标注数据集上训练单独分类器。

Result: 模型在所有标签上表现优异，ROC-AUC范围为0.91-0.98，精确度范围为0.70-1.00。在分割任务中，过滤低质量图像后分割成功率从42%提升至69%（p<0.001）。

Conclusion: CLAIRE-DSA作为自动化工具在急性缺血性卒中患者DSA序列中准确分类图像属性方面显示出强大潜力，支持临床和研究应用中的图像标注和质量控制。

Abstract: Computer vision models can be used to assist during mechanical thrombectomy
(MT) for acute ischemic stroke (AIS), but poor image quality often degrades
performance. This work presents CLAIRE-DSA, a deep learning--based framework
designed to categorize key image properties in minimum intensity projections
(MinIPs) acquired during MT for AIS, supporting downstream quality control and
workflow optimization. CLAIRE-DSA uses pre-trained ResNet backbone models,
fine-tuned to predict nine image properties (e.g., presence of contrast,
projection angle, motion artefact severity). Separate classifiers were trained
on an annotated dataset containing $1,758$ fluoroscopic MinIPs. The model
achieved excellent performance on all labels, with ROC-AUC ranging from $0.91$
to $0.98$, and precision ranging from $0.70$ to $1.00$. The ability of
CLAIRE-DSA to identify suitable images was evaluated on a segmentation task by
filtering poor quality images and comparing segmentation performance on
filtered and unfiltered datasets. Segmentation success rate increased from
$42%$ to $69%$, $p < 0.001$. CLAIRE-DSA demonstrates strong potential as an
automated tool for accurately classifying image properties in DSA series of
acute ischemic stroke patients, supporting image annotation and quality control
in clinical and research applications. Source code is available at
https://gitlab.com/icai-stroke-lab/wp3_neurointerventional_ai/claire-dsa.

</details>


### [109] [Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning](https://arxiv.org/abs/2508.12628)
*Yukang Lin,Xiang Zhang,Shichang Jia,Bowen Wan,Chenghan Fu,Xudong Ren,Yueran Liu,Wanxian Guan,Pengji Wang,Jian Xu,Bo Zheng,Baolin Liu*

Main category: cs.CV

TL;DR: 本文提出了首个可解释性创意图片评估与选择方法Creative4U，利用多模态大语言模型，通过Reason-to-Select RFT训练方法实现了准确的创意图片选择。


<details>
  <summary>Details</summary>
Motivation: 当前AIGC技术能以低成本生产大量创意图片，但缺乏有效的评估方法来选择质量最佳的创意，现有方法主要关注排名而无法提供可解释的选择。

Method: 构建CreativePair数据集（8k带注释的图片对），提出Creative4U模型，采用Reason-to-Select RFT训练方法（包括CoT-SFT和GRPO强化学习），将创意评估转换为自然语言生成任务。

Result: 离线和在线实验都证明了方法的有效性，能够准确评估和选择创意图片。

Conclusion: 该研究为创意图片选择提供了可解释的新范式，通过MLLMs技术实现了考虑用户兴趣的智能选择，对互联网广告行业具有重要意义。

Abstract: Creative image in advertising is the heart and soul of e-commerce platform.
An eye-catching creative image can enhance the shopping experience for users,
boosting income for advertisers and advertising revenue for platforms. With the
advent of AIGC technology, advertisers can produce large quantities of creative
images at minimal cost. However, they struggle to assess the creative quality
to select. Existing methods primarily focus on creative ranking, which fails to
address the need for explainable creative selection.
  In this work, we propose the first paradigm for explainable creative
assessment and selection. Powered by multimodal large language models (MLLMs),
our approach integrates the assessment and selection of creative images into a
natural language generation task. To facilitate this research, we construct
CreativePair, the first comparative reasoning-induced creative dataset
featuring 8k annotated image pairs, with each sample including a label
indicating which image is superior. Additionally, we introduce Creative4U
(pronounced Creative for You), a MLLMs-based creative selector that takes into
account users' interests. Through Reason-to-Select RFT, which includes
supervised fine-tuning with Chain-of-Thought (CoT-SFT) and Group Relative
Policy Optimization (GRPO) based reinforcement learning, Creative4U is able to
evaluate and select creative images accurately. Both offline and online
experiments demonstrate the effectiveness of our approach. Our code and dataset
will be made public to advance research and industrial applications.

</details>


### [110] [Harnessing Group-Oriented Consistency Constraints for Semi-Supervised Semantic Segmentation in CdZnTe Semiconductors](https://arxiv.org/abs/2508.12766)
*Peihao Li,Yan Fang,Man Liu,Huihui Bai,Anhong Wang,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: 提出了ICAF框架解决CdZnTe半导体图像标注难题，通过组内一致性增强和多视图信息交互，在仅使用2%标注数据下达到70.6% mIoU


<details>
  <summary>Details</summary>
Motivation: CdZnTe半导体图像存在低对比度缺陷边界，需要多视图交叉参考，传统半监督分割方法受限于一对一关系，在低对比度区域容易产生误差累积

Method: 提出组内一致性增强框架ICAF，包含伪标签校正网络PCN，通过视图增强模块动态合成边界感知视图，视图校正模块进行信息交互以突出显著区域并减少噪声

Result: 使用DeepLabV3+和ResNet-101骨干网络，在仅2%组标注数据下，在CdZnTe数据集上达到70.6% mIoU

Conclusion: ICAF框架有效解决了CdZnTe材料的多视图标注挑战，通过组内一致性约束和视图合成校正，显著提升了低对比度区域的语义分割性能

Abstract: Labeling Cadmium Zinc Telluride (CdZnTe) semiconductor images is challenging
due to the low-contrast defect boundaries, necessitating annotators to
cross-reference multiple views. These views share a single ground truth (GT),
forming a unique ``many-to-one'' relationship. This characteristic renders
advanced semi-supervised semantic segmentation (SSS) methods suboptimal, as
they are generally limited by a ``one-to-one'' relationship, where each image
is independently associated with its GT. Such limitation may lead to error
accumulation in low-contrast regions, further exacerbating confirmation bias.
To address this issue, we revisit the SSS pipeline from a group-oriented
perspective and propose a human-inspired solution: the Intra-group Consistency
Augmentation Framework (ICAF). First, we experimentally validate the inherent
consistency constraints within CdZnTe groups, establishing a group-oriented
baseline using the Intra-group View Sampling (IVS). Building on this insight,
we introduce the Pseudo-label Correction Network (PCN) to enhance consistency
representation, which consists of two key modules. The View Augmentation Module
(VAM) improves boundary details by dynamically synthesizing a boundary-aware
view through the aggregation of multiple views. In the View Correction Module
(VCM), this synthesized view is paired with other views for information
interaction, effectively emphasizing salient regions while minimizing noise.
Extensive experiments demonstrate the effectiveness of our solution for CdZnTe
materials. Leveraging DeepLabV3+ with a ResNet-101 backbone as our segmentation
model, we achieve a 70.6\% mIoU on the CdZnTe dataset using only 2
group-annotated data (5\textperthousand). The code is available at
\href{https://github.com/pipixiapipi/ICAF}{https://github.com/pipixiapipi/ICAF}.

</details>


### [111] [Vehicle detection from GSV imagery: Predicting travel behaviour for cycling and motorcycling using Computer Vision](https://arxiv.org/abs/2508.12794)
*Kyriaki,Kokka,Rahul Goel,Ali Abbas,Kerry A. Nice,Luca Martial,SM Labib,Rihuan Ke,Carola Bibiane Schönlieb,James Woodcock*

Main category: cs.CV

TL;DR: 通过深度学习分析Google街景图像，发展了一种全球规模估计自行车和摩托车使用水平的新方法，相关性高且预测准确度良好


<details>
  <summary>Details</summary>
Motivation: 交通方式影响健康，但全球范围内自行车和摩托车行为数据缺乏，需要高效的数据收集方法

Method: 使用YOLOv4模型分析185个城市的Google街景图像，每个城市采样8000张图片，通过调整模型进行物体检测，使用beta回归建立全球预测模型

Result: 摩托车检测与实际使用水平相关系数高达0.78，自行车0.51，预测模型R²值分别为0.614和0.612，中位绝对误差仅为1.3%和1.4%

Conclusion: 街景图像结合计算机视觉技术可以高效捕获交通行为数据，为传统数据源提供补充，尤其在数据缺乏地区具有重要价值

Abstract: Transportation influence health by shaping exposure to physical activity, air
pollution and injury risk.Comparative data on cycling and motorcycling
behaviours is scarce, particularly at a global scale.Street view imagery, such
as Google Street View (GSV), combined with computer vision, is a valuable
resource for efficiently capturing travel behaviour data.This study
demonstrates a novel approach using deep learning on street view images to
estimate cycling and motorcycling levels across diverse cities worldwide.We
utilized data from 185 global cities.The data on mode shares of cycling and
motorcycling estimated using travel surveys or censuses.We used GSV images to
detect cycles and motorcycles in sampled locations, using 8000 images per
city.The YOLOv4 model, fine-tuned using images from six cities, achieved a mean
average precision of 89% for detecting cycles and motorcycles in GSV images.A
global prediction model was developed using beta regression with city-level
mode shares as outcome, with log transformed explanatory variables of counts of
GSV-detected images with cycles and motorcycles, while controlling for
population density.We found strong correlations between GSV motorcycle counts
and motorcycle mode share (0.78) and moderate correlations between GSV cycle
counts and cycling mode share (0.51).Beta regression models predicted mode
shares with $R^2$ values of 0.614 for cycling and 0.612 for motorcycling,
achieving median absolute errors (MDAE) of 1.3% and 1.4%,
respectively.Scatterplots demonstrated consistent prediction accuracy, though
cities like Utrecht and Cali were outliers.The model was applied to 60 cities
globally for which we didn't have recent mode share data.We provided estimates
for some cities in the Middle East, Latin America and East Asia.With computer
vision, GSV images capture travel modes and activity, providing insights
alongside traditional data sources.

</details>


### [112] [Synthesizing Accurate and Realistic T1-weighted Contrast-Enhanced MR Images using Posterior-Mean Rectified Flow](https://arxiv.org/abs/2508.12640)
*Bastian Brandstötter,Erich Kobler*

Main category: cs.CV

TL;DR: 提出一种两阶段PMRF管道，从非对比MRI合成对比增强脑MRI，避免使用钆造影剂


<details>
  <summary>Details</summary>
Motivation: 对比增强MRI需要钆造影剂，增加成本、扫描时间、环境问题，并对患者有潜在风险

Method: 两阶段方法：先用3D U-Net预测后验均值，再用时间条件3D整流流细化以保持结构保真度并增加真实纹理

Result: 在360个测试样本上，最佳输出达到轴向FID 12.46和KID 0.007（比后验均值低68.7%），体积MSE为0.057（比后验均值高27%）

Conclusion: 该方法能真实恢复病灶边缘和血管细节，有效平衡感知-失真权衡，适合临床部署

Abstract: Contrast-enhanced (CE) T1-weighted MRI is central to neuro-oncologic
diagnosis but requires gadolinium-based agents, which add cost and scan time,
raise environmental concerns, and may pose risks to patients. In this work, we
propose a two-stage Posterior-Mean Rectified Flow (PMRF) pipeline for
synthesizing volumetric CE brain MRI from non-contrast inputs. First, a
patch-based 3D U-Net predicts the voxel-wise posterior mean (minimizing MSE).
Then, this initial estimate is refined by a time-conditioned 3D rectified flow
to incorporate realistic textures without compromising structural fidelity. We
train this model on a multi-institutional collection of paired pre- and
post-contrast T1w volumes (BraTS 2023-2025). On a held-out test set of 360
diverse volumes, our best refined outputs achieve an axial FID of $12.46$ and
KID of $0.007$ ($\sim 68.7\%$ lower FID than the posterior mean) while
maintaining low volumetric MSE of $0.057$ ($\sim 27\%$ higher than the
posterior mean). Qualitative comparisons confirm that our method restores
lesion margins and vascular details realistically, effectively navigating the
perception-distortion trade-off for clinical deployment.

</details>


### [113] [Next Visual Granularity Generation](https://arxiv.org/abs/2508.12811)
*Yikai Wang,Zhouxia Wang,Zhonghua Wu,Qingyi Tao,Kang Liao,Chen Change Loy*

Main category: cs.CV

TL;DR: 新的NVG框架通过将图像分解为结构化的视觉粒度序列，从全局布局到细节逐步精细地生成图像，在ImageNet上较VAR系列获得更优的FID指标。


<details>
  <summary>Details</summary>
Motivation: 传统图像生成方法缺乏层次化的粒度控制，需要一种能够在不同粒度级别上精细控制生成过程的新方法。

Method: 提出Next Visual Granularity (NVG)生成框架，将图像解构为结构化序列，每个元素共享相同空间分辨率但使用不同数量的唯一token。从空白图像开始，逐步精细地生成从全局到细节的视觉粒度序列。

Result: 在ImageNet数据集上训练的NVG模型显示了清晰的缩放行为。与VAR系列相比，NVG在FID指标上持续优于对手（3.30->3.03, 2.57->2.44, 2.09->2.06）。

Conclusion: NVG框架通过层次化的视觉粒度序列生成，提供了多粒度级别的精细控制能力，在图像生成性能上显著提升，具有强大的潜力。

Abstract: We propose a novel approach to image generation by decomposing an image into
a structured sequence, where each element in the sequence shares the same
spatial resolution but differs in the number of unique tokens used, capturing
different level of visual granularity. Image generation is carried out through
our newly introduced Next Visual Granularity (NVG) generation framework, which
generates a visual granularity sequence beginning from an empty image and
progressively refines it, from global layout to fine details, in a structured
manner. This iterative process encodes a hierarchical, layered representation
that offers fine-grained control over the generation process across multiple
granularity levels. We train a series of NVG models for class-conditional image
generation on the ImageNet dataset and observe clear scaling behavior. Compared
to the VAR series, NVG consistently outperforms it in terms of FID scores (3.30
-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to
showcase the capability and potential of the NVG framework. Our code and models
will be released.

</details>


### [114] [Learn Faster and Remember More: Balancing Exploration and Exploitation for Continual Test-time Adaptation](https://arxiv.org/abs/2508.12643)
*Pinci Yang,Peisong Wen,Ke Ma,Qianqian Xu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的持续测试时适配方法BEE，通过多层次一致性正则化和补充锐回放机制，有效解决了在持续适配中平衡探索新域咋利用历史知识的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的持续测试时适配方法在平衡探索新域咋利用历史知识方面遇到两大挑战：1）域偏移影响浅层特征，但现有方法依赖深层输出调整，导致适配速度慢；2）单一模型在适配新域时容易遗忘历史知识。

Method: 提出BEE框架，采用均值教师模型结构。使用多层次一致性正则化(MCR)捕捉中间特征并加速适配新域，采用补充锐回放(CAR)机制重用历史检查点来恢复多样化的域知识。

Result: 在多个标准数据集上，BEE方法显著超过了现有的最先进方法，证明了其在持续测试时适配任务中的有效性。

Conclusion: BEE方法通过均值教师框架、多层次特征对齐咋补充历史知识重用，有效解决了持续适配中探索与利用的平衡问题，为CTTA领域提供了新的解决方案。

Abstract: Continual Test-Time Adaptation (CTTA) aims to adapt a source pre-trained
model to continually changing target domains during inference. As a fundamental
principle, an ideal CTTA method should rapidly adapt to new domains
(exploration) while retaining and exploiting knowledge from previously
encountered domains to handle similar domains in the future. Despite
significant advances, balancing exploration and exploitation in CTTA is still
challenging: 1) Existing methods focus on adjusting predictions based on
deep-layer outputs of neural networks. However, domain shifts typically affect
shallow features, which are inefficient to be adjusted from deep predictions,
leading to dilatory exploration; 2) A single model inevitably forgets knowledge
of previous domains during the exploration, making it incapable of exploiting
historical knowledge to handle similar future domains. To address these
challenges, this paper proposes a mean teacher framework that strikes an
appropriate Balance between Exploration and Exploitation (BEE) during the CTTA
process. For the former challenge, we introduce a Multi-level Consistency
Regularization (MCR) loss that aligns the intermediate features of the student
and teacher models, accelerating adaptation to the current domain. For the
latter challenge, we employ a Complementary Anchor Replay (CAR) mechanism to
reuse historical checkpoints (anchors), recovering complementary knowledge for
diverse domains. Experiments show that our method significantly outperforms
state-of-the-art methods on several benchmarks, demonstrating its effectiveness
for CTTA tasks.

</details>


### [115] [CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis](https://arxiv.org/abs/2508.12900)
*Jiayi Wang,Hadrien Reynaud,Franciskus Xaverius Erick,Bernhard Kainz*

Main category: cs.CV

TL;DR: CTFlow是一个基于临床报告生成3D CT体积的0.5B潜在流匹配变换器模型，在时间一致性、图像多样性和文本-图像对齐方面优于现有方法


<details>
  <summary>Details</summary>
Motivation: 通过基于临床报告生成完整CT体积，可以加速医学研究、实现隐私保护的数据合成，并减少对患者数据的监管限制，同时保留诊断信号

Method: 使用FLUX的A-VAE定义潜在空间，CT-Clip文本编码器编码临床报告。采用自定义自回归方法生成一致的全CT体积，先基于纯文本预测第一个切片序列，然后基于先前生成的切片序列和文本来预测后续序列

Result: 在FID、FVD、IS分数和CLIP分数等指标上，CTFlow在时间一致性、图像多样性和文本-图像对齐方面都优于最先进的CT生成模型

Conclusion: CTFlow成功实现了基于临床报告的高质量3D CT体积生成，为医学数据增强和隐私保护应用提供了有效解决方案

Abstract: Generative modelling of entire CT volumes conditioned on clinical reports has
the potential to accelerate research through data augmentation,
privacy-preserving synthesis and reducing regulator-constraints on patient data
while preserving diagnostic signals. With the recent release of CT-RATE, a
large-scale collection of 3D CT volumes paired with their respective clinical
reports, training large text-conditioned CT volume generation models has become
achievable. In this work, we introduce CTFlow, a 0.5B latent flow matching
transformer model, conditioned on clinical reports. We leverage the A-VAE from
FLUX to define our latent space, and rely on the CT-Clip text encoder to encode
the clinical reports. To generate consistent whole CT volumes while keeping the
memory constraints tractable, we rely on a custom autoregressive approach,
where the model predicts the first sequence of slices of the volume from
text-only, and then relies on the previously generated sequence of slices and
the text, to predict the following sequence. We evaluate our results against
state-of-the-art generative CT model, and demonstrate the superiority of our
approach in terms of temporal coherence, image diversity and text-image
alignment, with FID, FVD, IS scores and CLIP score.

</details>


### [116] [DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video](https://arxiv.org/abs/2508.12644)
*Hao Wen,Hongbo Kang,Jian Ma,Jing Huang,Yuanwang Yang,Haozhe Lin,Yu-Kun Lai,Kun Li*

Main category: cs.CV

TL;DR: DyCrowd是首个从大场景视频中重建数百人3D姿态、位置和形状的时空一致性框架，采用粗到细的群体引导运动优化策略，结合VAE运动先验和异步运动一致性损失，有效解决遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法从静态图像重建3D人群缺乏时间一致性，无法有效处理遮挡问题，需要开发能够从大场景视频中重建动态人群的时空一致性方法。

Method: 提出粗到细的群体引导运动优化策略，结合VAE运动先验和段级群体引导优化，利用异步运动一致性损失(AMC)让高质量无遮挡运动段指导遮挡段的恢复。

Result: 实验结果表明该方法在大场景动态人群重建任务中达到最先进性能，并贡献了VirtualCrowd虚拟基准数据集。

Conclusion: DyCrowd框架能够有效处理大场景视频中的动态人群重建，解决了时间不稳定性和严重遮挡问题，为城市监控和人群分析应用提供了有力工具。

Abstract: 3D reconstruction of dynamic crowds in large scenes has become increasingly
important for applications such as city surveillance and crowd analysis.
However, current works attempt to reconstruct 3D crowds from a static image,
causing a lack of temporal consistency and inability to alleviate the typical
impact caused by occlusions. In this paper, we propose DyCrowd, the first
framework for spatio-temporally consistent 3D reconstruction of hundreds of
individuals' poses, positions and shapes from a large-scene video. We design a
coarse-to-fine group-guided motion optimization strategy for occlusion-robust
crowd reconstruction in large scenes. To address temporal instability and
severe occlusions, we further incorporate a VAE (Variational Autoencoder)-based
human motion prior along with a segment-level group-guided optimization. The
core of our strategy leverages collective crowd behavior to address long-term
dynamic occlusions. By jointly optimizing the motion sequences of individuals
with similar motion segments and combining this with the proposed Asynchronous
Motion Consistency (AMC) loss, we enable high-quality unoccluded motion
segments to guide the motion recovery of occluded ones, ensuring robust and
plausible motion recovery even in the presence of temporal desynchronization
and rhythmic inconsistencies. Additionally, in order to fill the gap of no
existing well-annotated large-scene video dataset, we contribute a virtual
benchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction
from large-scene videos. Experimental results demonstrate that the proposed
method achieves state-of-the-art performance in the large-scene dynamic crowd
reconstruction task. The code and dataset will be available for research
purposes.

</details>


### [117] [SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for Class Incremental Learning with Small Memory](https://arxiv.org/abs/2508.12932)
*Hongyang Chen,Shaoling Pu,Lingyu Zheng,Zhongwu Sun*

Main category: cs.CV

TL;DR: SEDEG是一个两阶段训练框架，通过提升编码器和解码器的泛化能力来缓解增量学习中的灾难性遗忘问题，特别在小内存场景下表现优异


<details>
  <summary>Details</summary>
Motivation: 现有的增量学习方法通常只关注编码器或解码器中的一个组件，限制了缓解灾难性遗忘的效果，特别是在小内存场景下表现更差

Method: 采用两阶段训练：第一阶段通过特征增强训练集成编码器学习泛化表示，提升解码器泛化能力；第二阶段使用知识蒸馏策略压缩集成编码器，开发新的泛化编码器

Result: 在三个基准数据集上的广泛实验显示SEDEG具有优越性能，消融研究确认了各组件有效性

Conclusion: SEDEG通过顺序提升编码器和解码器的泛化能力，有效缓解了增量学习中的灾难性遗忘问题，特别在小内存场景下表现突出

Abstract: In incremental learning, enhancing the generality of knowledge is crucial for
adapting to dynamic data inputs. It can develop generalized representations or
more balanced decision boundaries, preventing the degradation of long-term
knowledge over time and thus mitigating catastrophic forgetting. Some emerging
incremental learning methods adopt an encoder-decoder architecture and have
achieved promising results. In the encoder-decoder achitecture, improving the
generalization capabilities of both the encoder and decoder is critical, as it
helps preserve previously learned knowledge while ensuring adaptability and
robustness to new, diverse data inputs. However, many existing continual
methods focus solely on enhancing one of the two components, which limits their
effectiveness in mitigating catastrophic forgetting. And these methods perform
even worse in small-memory scenarios, where only a limited number of historical
samples can be stored. To mitigate this limitation, we introduces SEDEG, a
two-stage training framework for vision transformers (ViT), focusing on
sequentially improving the generality of both Decoder and Encoder. Initially,
SEDEG trains an ensembled encoder through feature boosting to learn generalized
representations, which subsequently enhance the decoder's generality and
balance the classifier. The next stage involves using knowledge distillation
(KD) strategies to compress the ensembled encoder and develop a new, more
generalized encoder. This involves using a balanced KD approach and feature KD
for effective knowledge transfer. Extensive experiments on three benchmark
datasets show SEDEG's superior performance, and ablation studies confirm the
efficacy of its components. The code is available at
https://github.com/ShaolingPu/CIL.

</details>


### [118] [Stable Diffusion-Based Approach for Human De-Occlusion](https://arxiv.org/abs/2508.12663)
*Seung Young Noh,Ju Yong Chang*

Main category: cs.CV

TL;DR: 一种两阶段演进式人体去遮蔽方法，先通过次散模型完成遮蔽区域的掩码重建，再使用稳定次散模型进行RGB外观生成，并通过VQA模型提取人体特征提升效果


<details>
  <summary>Details</summary>
Motivation: 人类能够通过先验知识推断遮蔽物体的缺失部分，但现有深度学习模型在准确预测遮蔽区域方面仍面临挑战，特别是在人体去遮蔽任务中

Method: 1. 推理阶段分解：先掩码完成阶段，使用次散模型给出人体结构的完整表示，结合遮蔽关节热力图提供空间线索
2. RGB完成阶段：使用稳定次散模型，以重建的无模掩码作为条件输入
3. 提取人体特征：通过VQA模型提取人体特征并用CLIP编码器编码
4. 细调解码器：减少可见区域的像素级退化

Result: 方法能够有效重建严重遮蔽下的人体外观，在掩码和RGB完成任务中都持续超越现有方法，甚至能够提升下游任务如2D姿势估计和3D人体重建的性能

Conclusion: 该研究提出了一种高效的人体去遮蔽方法，通过两阶段演进式接近和人体特征提取，有效解决了遮蔽区域预测的挑战，为人员中心计算视觉任务提供了有力的工具

Abstract: Humans can infer the missing parts of an occluded object by leveraging prior
knowledge and visible cues. However, enabling deep learning models to
accurately predict such occluded regions remains a challenging task.
De-occlusion addresses this problem by reconstructing both the mask and RGB
appearance. In this work, we focus on human de-occlusion, specifically
targeting the recovery of occluded body structures and appearances. Our
approach decomposes the task into two stages: mask completion and RGB
completion. The first stage leverages a diffusion-based human body prior to
provide a comprehensive representation of body structure, combined with
occluded joint heatmaps that offer explicit spatial cues about missing regions.
The reconstructed amodal mask then serves as a conditioning input for the
second stage, guiding the model on which areas require RGB reconstruction. To
further enhance RGB generation, we incorporate human-specific textual features
derived using a visual question answering (VQA) model and encoded via a CLIP
encoder. RGB completion is performed using Stable Diffusion, with decoder
fine-tuning applied to mitigate pixel-level degradation in visible regions -- a
known limitation of prior diffusion-based de-occlusion methods caused by latent
space transformations. Our method effectively reconstructs human appearances
even under severe occlusions and consistently outperforms existing methods in
both mask and RGB completion. Moreover, the de-occluded images generated by our
approach can improve the performance of downstream human-centric tasks, such as
2D pose estimation and 3D human reconstruction. The code will be made publicly
available.

</details>


### [119] [Multi-Phase Automated Segmentation of Dental Structures in CBCT Using a Lightweight Auto3DSeg and SegResNet Implementation](https://arxiv.org/abs/2508.12962)
*Dominic LaBella,Keshav Jha,Jared Robbins,Esther Yu*

Main category: cs.CV

TL;DR: DLaBella29团队在MICCAI 2025 ToothFairy3挑战赛中提出基于3D SegResNet的深度学习管道，用于CBCT牙齿多类别分割，在验证集上达到平均Dice系数0.87


<details>
  <summary>Details</summary>
Motivation: CBCT在牙科诊断和治疗规划中至关重要，自动化牙齿结构分割可帮助识别病理（如牙髓或根尖周病变）并促进头颈癌患者的放射治疗规划

Method: 使用MONAI Auto3DSeg框架和3D SegResNet架构，采用5折交叉验证训练，关键预处理包括图像重采样和强度裁剪，采用两阶段分割策略：第一阶段使用Multi-Label STAPLE集成融合，第二阶段对下颌骨进行紧密裁剪以分割神经结构

Result: 在ToothFairy3挑战赛的样本外验证集上获得了0.87的平均Dice系数

Conclusion: 自动化牙齿分割方法在CBCT图像分析中表现出色，对改善放射肿瘤学患者护理具有重要临床意义

Abstract: Cone-beam computed tomography (CBCT) has become an invaluable imaging
modality in dentistry, enabling 3D visualization of teeth and surrounding
structures for diagnosis and treatment planning. Automated segmentation of
dental structures in CBCT can efficiently assist in identifying pathology
(e.g., pulpal or periapical lesions) and facilitate radiation therapy planning
in head and neck cancer patients. We describe the DLaBella29 team's approach
for the MICCAI 2025 ToothFairy3 Challenge, which involves a deep learning
pipeline for multi-class tooth segmentation. We utilized the MONAI Auto3DSeg
framework with a 3D SegResNet architecture, trained on a subset of the
ToothFairy3 dataset (63 CBCT scans) with 5-fold cross-validation. Key
preprocessing steps included image resampling to 0.6 mm isotropic resolution
and intensity clipping. We applied an ensemble fusion using Multi-Label STAPLE
on the 5-fold predictions to infer a Phase 1 segmentation and then conducted
tight cropping around the easily segmented Phase 1 mandible to perform Phase 2
segmentation on the smaller nerve structures. Our method achieved an average
Dice of 0.87 on the ToothFairy3 challenge out-of-sample validation set. This
paper details the clinical context, data preparation, model development,
results of our approach, and discusses the relevance of automated dental
segmentation for improving patient care in radiation oncology.

</details>


### [120] [WP-CLIP: Leveraging CLIP to Predict Wölfflin's Principles in Visual Art](https://arxiv.org/abs/2508.12668)
*Abhijay Ghildyal,Li-Yun Wang,Feng Liu*

Main category: cs.CV

TL;DR: 本文研究CLIP模型在艺术分析中的应用，通过微调CLIP来预测Wölfflin五大风格原则，并在GAN生成海报和Pandora-18K数据集上验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 虽然Wölfflin五大原则提供了结构化的风格分析方法，但现有计量方法无法有效预测所有原则。需要能够解释颜色、构图等关键元素的计量方法来计算分析绘画的视觉方面。

Method: 使用CLIP模型，在大规模数据上预训练后，通过在真实艺术图像注释数据集上进行微调，来预测每个Wölfflin原则的分数。开发了WP-CLIP模型。

Result: 研究发现CLIP模型本身无法抓取Wölfflin原则的细致风格元素。但通过微调后的WP-CLIP模型在GAN生成海报和Pandora-18K艺术数据集上都表现出良好的模性适应能力，能够模糊逆合多样的艺术风格。

Conclusion: 这项研究呈现了视觉-语言模型在自动化艺术分析中的潜力，通过有目标的微调可以让大型模型理解和预测细致的艺术风格原则。

Abstract: W\"olfflin's five principles offer a structured approach to analyzing
stylistic variations for formal analysis. However, no existing metric
effectively predicts all five principles in visual art. Computationally
evaluating the visual aspects of a painting requires a metric that can
interpret key elements such as color, composition, and thematic choices. Recent
advancements in vision-language models (VLMs) have demonstrated their ability
to evaluate abstract image attributes, making them promising candidates for
this task. In this work, we investigate whether CLIP, pre-trained on
large-scale data, can understand and predict W\"olfflin's principles. Our
findings indicate that it does not inherently capture such nuanced stylistic
elements. To address this, we fine-tune CLIP on annotated datasets of real art
images to predict a score for each principle. We evaluate our model, WP-CLIP,
on GAN-generated paintings and the Pandora-18K art dataset, demonstrating its
ability to generalize across diverse artistic styles. Our results highlight the
potential of VLMs for automated art analysis.

</details>


### [121] [Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation](https://arxiv.org/abs/2508.12680)
*Yuheng Zha,Kun Zhou,Yujia Wu,Yushu Wang,Jie Feng,Zhi Xu,Shibo Hao,Zhengzhong Liu,Eric P. Xing,Zhiting Hu*

Main category: cs.CV

TL;DR: 这篇论文提出了Vision-G1模型，通过多域RL训练来提升视觉理解能力，在多个视觉理解测试集上达到最佳性能


<details>
  <summary>Details</summary>
Motivation: 解决现有理解VLM模型在范围限制下训练、数据资源缺乏以及多域数据整合困难的问题

Method: 构建包含46个数据源、8个维度的理解数据集，使用影响函数数据选择和难度筛选策略，通过多轮RL数据课程进行迭代训练

Result: Vision-G1模型在各种视觉理解测试中达到最佳性能，超过同规模模型甚至GPT-4o和Gemini-1.5 Flash

Conclusion: 通过多域RL训练和数据课程方法，可以有效提升VLM的视觉理解通用能力

Abstract: Despite their success, current training pipelines for reasoning VLMs focus on
a limited range of tasks, such as mathematical and logical reasoning. As a
result, these models face difficulties in generalizing their reasoning
capabilities to a wide range of domains, primarily due to the scarcity of
readily available and verifiable reward data beyond these narrowly defined
areas. Moreover, integrating data from multiple domains is challenging, as the
compatibility between domain-specific datasets remains uncertain. To address
these limitations, we build a comprehensive RL-ready visual reasoning dataset
from 46 data sources across 8 dimensions, covering a wide range of tasks such
as infographic, mathematical, spatial, cross-image, graphic user interface,
medical, common sense and general science. We propose an influence function
based data selection and difficulty based filtering strategy to identify
high-quality training samples from this dataset. Subsequently, we train the
VLM, referred to as Vision-G1, using multi-round RL with a data curriculum to
iteratively improve its visual reasoning capabilities. Our model achieves
state-of-the-art performance across various visual reasoning benchmarks,
outperforming similar-sized VLMs and even proprietary models like GPT-4o and
Gemini-1.5 Flash. The model, code and dataset are publicly available at
https://github.com/yuh-zha/Vision-G1.

</details>


### [122] [Refine-and-Contrast: Adaptive Instance-Aware BEV Representations for Multi-UAV Collaborative Object Detection](https://arxiv.org/abs/2508.12684)
*Zhongyao Li,Peirui Cheng,Liangjin Zhao,Chen Chen,Yundu Li,Zhechao Wang,Xue Yang,Xian Sun,Zhirui Wang*

Main category: cs.CV

TL;DR: AdaBEV是一个创新的多无人机3D检测框架，通过自适应实例感知的BEV表示和精化对比学习范式，在保持低分辨率BEV输入的同时实现优越的精度-计算权衡。


<details>
  <summary>Details</summary>
Motivation: 多无人机协同3D检测虽然能通过融合多视角观测提供准确鲁棒的感知，但在资源受限的无人机平台上计算面临挑战。现有方法平等对待所有BEV网格，效率低下。

Method: 提出Box-Guided Refinement Module (BG-RM) 仅精化与前景实例相关的BEV网格，使用2D监督和空间细分；Instance-Background Contrastive Learning (IBCL) 通过BEV空间中的对比学习增强前景和背景特征的可区分性。

Result: 在Air-Co-Pred数据集上的大量实验表明，AdaBEV在不同模型规模下都实现了优越的精度-计算权衡，在低分辨率下优于其他最先进方法，接近上限性能，同时保持低分辨率BEV输入和可忽略的开销。

Conclusion: AdaBEV通过自适应实例感知的BEV表示学习，有效解决了多无人机3D检测中的计算效率问题，为资源受限平台上的高性能感知提供了可行解决方案。

Abstract: Multi-UAV collaborative 3D detection enables accurate and robust perception
by fusing multi-view observations from aerial platforms, offering significant
advantages in coverage and occlusion handling, while posing new challenges for
computation on resource-constrained UAV platforms. In this paper, we present
AdaBEV, a novel framework that learns adaptive instance-aware BEV
representations through a refine-and-contrast paradigm. Unlike existing methods
that treat all BEV grids equally, AdaBEV introduces a Box-Guided Refinement
Module (BG-RM) and an Instance-Background Contrastive Learning (IBCL) to
enhance semantic awareness and feature discriminability. BG-RM refines only BEV
grids associated with foreground instances using 2D supervision and spatial
subdivision, while IBCL promotes stronger separation between foreground and
background features via contrastive learning in BEV space. Extensive
experiments on the Air-Co-Pred dataset demonstrate that AdaBEV achieves
superior accuracy-computation trade-offs across model scales, outperforming
other state-of-the-art methods at low resolutions and approaching upper bound
performance while maintaining low-resolution BEV inputs and negligible
overhead.

</details>


### [123] [Neural Rendering for Sensor Adaptation in 3D Object Detection](https://arxiv.org/abs/2508.12695)
*Felix Embacher,David Holtz,Jonas Uhrig,Marius Cordts,Markus Enzweiler*

Main category: cs.CV

TL;DR: 这篇论文研究了自主驾驶车不同感知器配置导致的跨感知器域间间隔问题，提出了CamShift数据集和基于神经渲染的数据驱动感知器适配方案，有效减少跨感知器性能泄漏。


<details>
  <summary>Details</summary>
Motivation: 自主驾驶车的感知器配置因车辆类型不同而异，导致在一种感知器配置上训练的模型在其他配置上性能泄漏，需要解决跨感知器域间间隔问题。

Method: 创建CamShift数据集模拟不同车辆类型的感知器配置差异，分析各种3D目标检测器的跨感知器性能，并提出基于神经渲染的数据驱动感知器适配流水线。

Result: 发现基于密集BEV表示的BEVFormer模型最稳健，而提出的感知器适配方案能大幅提升所有测试模型的跨感知器性能，减少数据重新收集需求。

Conclusion: 通过神经渲染技术实现数据集跨感知器适配，可有效减少自主驾驶车不同感知器配置导致的性能泄漏，提高数据重用性。

Abstract: Autonomous vehicles often have varying camera sensor setups, which is
inevitable due to restricted placement options for different vehicle types.
Training a perception model on one particular setup and evaluating it on a new,
different sensor setup reveals the so-called cross-sensor domain gap, typically
leading to a degradation in accuracy. In this paper, we investigate the impact
of the cross-sensor domain gap on state-of-the-art 3D object detectors. To this
end, we introduce CamShift, a dataset inspired by nuScenes and created in CARLA
to specifically simulate the domain gap between subcompact vehicles and sport
utility vehicles (SUVs). Using CamShift, we demonstrate significant
cross-sensor performance degradation, identify robustness dependencies on model
architecture, and propose a data-driven solution to mitigate the effect. On the
one hand, we show that model architectures based on a dense Bird's Eye View
(BEV) representation with backward projection, such as BEVFormer, are the most
robust against varying sensor configurations. On the other hand, we propose a
novel data-driven sensor adaptation pipeline based on neural rendering, which
can transform entire datasets to match different camera sensor setups. Applying
this approach improves performance across all investigated 3D object detectors,
mitigating the cross-sensor domain gap by a large margin and reducing the need
for new data collection by enabling efficient data reusability across vehicles
with different sensor setups. The CamShift dataset and the sensor adaptation
benchmark are available at https://dmholtz.github.io/camshift/.

</details>


### [124] [Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection](https://arxiv.org/abs/2508.12711)
*Fanxiao Li,Jiaying Wu,Tingchao Fu,Yunyun Dong,Bingbing Song,Wei Zhou*

Main category: cs.CV

TL;DR: GenAI驱动的新闻多样性导致多级漂移，显著降低现有LVLM多模态虚假信息检测系统的鲁棒性，性能平均下降14.8%


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具带来的新闻内容多样性对多模态虚假信息检测构成了新挑战，需要系统研究其对检测系统的影响

Method: 构建DriftBench大规模基准数据集（16,000个新闻实例，6个多样化类别），设计三个评估任务：真实性验证鲁棒性、对抗性证据污染敏感性、推理一致性分析

Result: 实验显示6个最先进的LVLM检测器性能显著下降（平均F1下降14.8%），推理轨迹不稳定，在对抗性证据注入下表现更差

Conclusion: 现有MMD系统存在根本性漏洞，在GenAI时代迫切需要更具弹性的方法

Abstract: The proliferation of multimodal misinformation poses growing threats to
public discourse and societal trust. While Large Vision-Language Models (LVLMs)
have enabled recent progress in multimodal misinformation detection (MMD), the
rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven
news diversity, characterized by highly varied and complex content. We show
that this diversity induces multi-level drift, comprising (1) model-level
misperception drift, where stylistic variations disrupt a model's internal
reasoning, and (2) evidence-level drift, where expression diversity degrades
the quality or relevance of retrieved external evidence. These drifts
significantly degrade the robustness of current LVLM-based MMD systems. To
systematically study this problem, we introduce DriftBench, a large-scale
benchmark comprising 16,000 news instances across six categories of
diversification. We design three evaluation tasks: (1) robustness of truth
verification under multi-level drift; (2) susceptibility to adversarial
evidence contamination generated by GenAI; and (3) analysis of reasoning
consistency across diverse inputs. Experiments with six state-of-the-art
LVLM-based detectors show substantial performance drops (average F1 -14.8%) and
increasingly unstable reasoning traces, with even more severe failures under
adversarial evidence injection. Our findings uncover fundamental
vulnerabilities in existing MMD systems and suggest an urgent need for more
resilient approaches in the GenAI era.

</details>


### [125] [Real-Time Sign Language Gestures to Speech Transcription using Deep Learning](https://arxiv.org/abs/2508.12713)
*Brandone Fonya*

Main category: cs.CV

TL;DR: 基于深度学习的实时手语识别系统，通过CNN算法将手势转换为文本和语音，促进听力语言障碍者的沟通融入


<details>
  <summary>Details</summary>
Motivation: 解决听力和语言障碍者在日常生活中的沟通困难，提升他们的自主性和社交融入能力

Method: 使用卷积神经网络(CNN)训练在Sign Language MNIST数据集上，通过摄像头实时捕获手势并进行分类，然后使用文本转语音技术生成语音输出

Result: 系统展现出高准确率和稳定的实时性能，虽然存在一定延迟，但具有实用性和可靠性

Conclusion: 该系统作为一种可访、可靠且易于使用的辅助技术工具，有效提升了手语使用者在多样化社交环境中的自主性和融入性

Abstract: Communication barriers pose significant challenges for individuals with
hearing and speech impairments, often limiting their ability to effectively
interact in everyday environments. This project introduces a real-time
assistive technology solution that leverages advanced deep learning techniques
to translate sign language gestures into textual and audible speech. By
employing convolution neural networks (CNN) trained on the Sign Language MNIST
dataset, the system accurately classifies hand gestures captured live via
webcam. Detected gestures are instantaneously translated into their
corresponding meanings and transcribed into spoken language using
text-to-speech synthesis, thus facilitating seamless communication.
Comprehensive experiments demonstrate high model accuracy and robust real-time
performance with some latency, highlighting the system's practical
applicability as an accessible, reliable, and user-friendly tool for enhancing
the autonomy and integration of sign language users in diverse social settings.

</details>


### [126] [Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score](https://arxiv.org/abs/2508.12718)
*Syed Muhmmad Israr,Feng Zhao*

Main category: cs.CV

TL;DR: 提出Dual Contrastive Denoising Score框架，利用文本到图像扩散模型的生成先验，通过双重对比损失实现真实图像编辑，既能灵活修改内容又能保持结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在编辑真实图像时面临两个挑战：用户难以准确描述图像细节的文本提示，以及编辑过程中经常意外改变不需要修改的区域。

Method: 基于对比学习方法，在潜在扩散模型中引入双重对比损失，利用自注意力层的空间信息，无需依赖辅助网络。

Result: 方法在真实图像编辑任务中优于现有方法，能够实现零样本图像到图像转换，同时保持使用预训练模型而无需额外训练的能力。

Conclusion: 该框架简单而强大，成功解决了真实图像编辑中的关键问题，在保持输入输出图像结构一致性的同时实现了灵活的内容修改。

Abstract: Large-scale text-to-image generative models have shown remarkable ability to
synthesize diverse and high-quality images. However, it is still challenging to
directly apply these models for editing real images for two reasons. First, it
is difficult for users to come up with a perfect text prompt that accurately
describes every visual detail in the input image. Second, while existing models
can introduce desirable changes in certain regions, they often dramatically
alter the input content and introduce unexpected changes in unwanted regions.
To address these challenges, we present Dual Contrastive Denoising Score, a
simple yet powerful framework that leverages the rich generative prior of
text-to-image diffusion models. Inspired by contrastive learning approaches for
unpaired image-to-image translation, we introduce a straightforward dual
contrastive loss within the proposed framework. Our approach utilizes the
extensive spatial information from the intermediate representations of the
self-attention layers in latent diffusion models without depending on auxiliary
networks. Our method achieves both flexible content modification and structure
preservation between input and output images, as well as zero-shot
image-to-image translation. Through extensive experiments, we show that our
approach outperforms existing methods in real image editing while maintaining
the capability to directly utilize pretrained text-to-image diffusion models
without further training.

</details>


### [127] [Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting](https://arxiv.org/abs/2508.12720)
*Kangjie Chen,Yingji Zhong,Zhihao Li,Jiaqi Lin,Youyu Chen,Minghan Qin,Haoqian Wang*

Main category: cs.CV

TL;DR: 三维高斯拓扑在稀疏视角场景中存在外观伪像问题，本文提出高斯元素过度粘联的核心问题，并提出两种轻量级解决方案


<details>
  <summary>Details</summary>
Motivation: 解决3DGS在稀疏视角下的外观伪像问题，探索高斯元素过度粘联导致的性能降级

Method: 提出协同适应分数(CA)指标来量化高斯元素粘联程度，并提出随机高斯投弃和不透明度乘性噪声注入两种轻量级方法

Result: 分析显示训练视角数量增加能自然缓解协同适应，提出的两种策略在多个方法和测试集上验证有效

Conclusion: 本文揭示了3DGS在稀疏视角下的核心限制，提出的协同适应分析框架和简单有效的策略为该领域提供了新的见解

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel
view synthesis under dense-view settings. However, in sparse-view scenarios,
despite the realistic renderings in training views, 3DGS occasionally manifests
appearance artifacts in novel views. This paper investigates the appearance
artifacts in sparse-view 3DGS and uncovers a core limitation of current
approaches: the optimized Gaussians are overly-entangled with one another to
aggressively fit the training views, which leads to a neglect of the real
appearance distribution of the underlying scene and results in appearance
artifacts in novel views. The analysis is based on a proposed metric, termed
Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians,
i.e., co-adaptation, by computing the pixel-wise variance across multiple
renderings of the same viewpoint, with different random subsets of Gaussians.
The analysis reveals that the degree of co-adaptation is naturally alleviated
as the number of training views increases. Based on the analysis, we propose
two lightweight strategies to explicitly mitigate the co-adaptation in
sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise
injection to the opacity. Both strategies are designed to be plug-and-play, and
their effectiveness is validated across various methods and benchmarks. We hope
that our insights into the co-adaptation effect will inspire the community to
achieve a more comprehensive understanding of sparse-view 3DGS.

</details>


### [128] [Frequency-Driven Inverse Kernel Prediction for Single Image Defocus Deblurring](https://arxiv.org/abs/2508.12736)
*Ying Zhang,Xiongxin Tang,Chongyi Li,Qiao Chen,Yuquan Wu*

Main category: cs.CV

TL;DR: 提出FDIKP网络，通过频率域表示增强核估计的结构可识别性，采用双分支逆核预测策略和位置自适应卷积，在单图像散焦去模糊任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖空间特征进行核估计，但在严重模糊区域由于局部高频细节缺失导致性能下降，需要利用频率域的判别能力来增强核建模。

Method: 提出频率驱动的逆核预测网络(FDIKP)，包含双分支逆核预测策略(DIKP)、位置自适应卷积(PAC)和双域尺度循环模块(DSRM)，融合频率域信息并逐步改进去模糊质量。

Result: 大量实验表明该方法优于现有方法，在散焦去模糊任务中表现出色。

Conclusion: 通过结合频率域表示和创新的网络设计，有效解决了严重模糊区域的核估计问题，提升了单图像散焦去模糊的性能。

Abstract: Single image defocus deblurring aims to recover an all-in-focus image from a
defocus counterpart, where accurately modeling spatially varying blur kernels
remains a key challenge. Most existing methods rely on spatial features for
kernel estimation, but their performance degrades in severely blurry regions
where local high-frequency details are missing. To address this, we propose a
Frequency-Driven Inverse Kernel Prediction network (FDIKP) that incorporates
frequency-domain representations to enhance structural identifiability in
kernel modeling. Given the superior discriminative capability of the frequency
domain for blur modeling, we design a Dual-Branch Inverse Kernel Prediction
(DIKP) strategy that improves the accuracy of kernel estimation while
maintaining stability. Moreover, considering the limited number of predicted
inverse kernels, we introduce a Position Adaptive Convolution (PAC) to enhance
the adaptability of the deconvolution process. Finally, we propose a
Dual-Domain Scale Recurrent Module (DSRM) to fuse deconvolution results and
progressively improve deblurring quality from coarse to fine. Extensive
experiments demonstrate that our method outperforms existing approaches. Code
will be made publicly available.

</details>


### [129] [D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal](https://arxiv.org/abs/2508.12750)
*Linhao Li,Boya Jin,Zizhe Li,Lanqing Guo,Hao Cheng,Bo Li,Yongfeng Dong*

Main category: cs.CV

TL;DR: 基于Mamba的双路径扫描网络，通过双角度融合和适配扫描策略，高效完成阴影移除任务


<details>
  <summary>Details</summary>
Motivation: 阴影移除任务中非阴影区域提供了丰富的上下文信息，但不同区域需要不同的变换策略，必须有效整合这些信息并适配地建模区域特异变换

Method: 提出双路径Mamba组(DPMG)通过水平扫描捕获全局特征，采用面具感知适配扫描策略；双尺度融合Mamba块(DFMB)通过融合原始特征与低分辨率特征提升多尺度表征能力

Result: 在阴影移除标准数据集上，方法显著超过现有最先进方法

Conclusion: 所提方法通过双路径扫描和双尺度融合机制，有效地解决了阴影移除中的边界偏差和结构不连续问题，实现了更优秀的恢复效果

Abstract: Shadow removal aims to restore images that are partially degraded by shadows,
where the degradation is spatially localized and non-uniform. Unlike general
restoration tasks that assume global degradation, shadow removal can leverage
abundant information from non-shadow regions for guidance. However, the
transformation required to correct shadowed areas often differs significantly
from that of well-lit regions, making it challenging to apply uniform
correction strategies. This necessitates the effective integration of non-local
contextual cues and adaptive modeling of region-specific transformations. To
this end, we propose a novel Mamba-based network featuring dual-scale fusion
and dual-path scanning to selectively propagate contextual information based on
transformation similarity across regions. Specifically, the proposed Dual-Scale
Fusion Mamba Block (DFMB) enhances multi-scale feature representation by fusing
original features with low-resolution features, effectively reducing boundary
artifacts. The Dual-Path Mamba Group (DPMG) captures global features via
horizontal scanning and incorporates a mask-aware adaptive scanning strategy,
which improves structural continuity and fine-grained region modeling.
Experimental results demonstrate that our method significantly outperforms
existing state-of-the-art approaches on shadow removal benchmarks.

</details>


### [130] [SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior](https://arxiv.org/abs/2508.12777)
*Wenguang Tao,Xiaotian Wang,Tian Yan,Jie Yan,Guodong Li,Kun Bai*

Main category: cs.CV

TL;DR: SocialTrack是一个针对无人机视角下复杂城市交通环境的多目标跟踪框架，通过多尺度特征增强、速度自适应卡尔曼滤波、群体运动补偿和时空记忆预测等技术，显著提升了小目标跟踪的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 无人机多目标跟踪在智能交通系统中具有重要应用价值，但复杂视角下的小目标尺度变化、遮挡、非线性交叉运动和运动模糊等问题严重影响了跟踪稳定性。

Method: 提出SocialTrack框架，包含：1）多尺度特征增强的小目标检测器；2）速度自适应立方卡尔曼滤波(VACKF)用于轨迹预测；3）群体运动补偿策略(GMCS)建模社会群体运动先验；4）时空记忆预测(STMP)利用历史轨迹信息预测未来状态。

Result: 在UAVDT和MOT17数据集上的实验表明，SocialTrack在多个关键指标上优于现有SOTA方法，特别是在MOTA和IDF1等核心性能指标上有显著提升。

Conclusion: SocialTrack框架具有优异的鲁棒性和适应性，同时具有高度模块化和兼容性，可与现有跟踪器无缝集成以进一步提升性能。

Abstract: As a key research direction in the field of multi-object tracking (MOT),
UAV-based multi-object tracking has significant application value in the
analysis and understanding of urban intelligent transportation systems.
However, in complex UAV perspectives, challenges such as small target scale
variations, occlusions, nonlinear crossing motions, and motion blur severely
hinder the stability of multi-object tracking. To address these challenges,
this paper proposes a novel multi-object tracking framework, SocialTrack, aimed
at enhancing the tracking accuracy and robustness of small targets in complex
urban traffic environments. The specialized small-target detector enhances the
detection performance by employing a multi-scale feature enhancement mechanism.
The Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of
trajectory prediction by incorporating a velocity dynamic modeling mechanism.
The Group Motion Compensation Strategy (GMCS) models social group motion priors
to provide stable state update references for low-quality tracks, significantly
improving the target association accuracy in complex dynamic environments.
Furthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical
trajectory information to predict the future state of low-quality tracks,
effectively mitigating identity switching issues. Extensive experiments on the
UAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing
state-of-the-art (SOTA) methods across several key metrics. Significant
improvements in MOTA and IDF1, among other core performance indicators,
highlight its superior robustness and adaptability. Additionally, SocialTrack
is highly modular and compatible, allowing for seamless integration with
existing trackers to further enhance performance.

</details>


### [131] [Leveraging Diffusion Models for Stylization using Multiple Style Images](https://arxiv.org/abs/2508.12784)
*Dan Ruta,Abdelaziz Djelouah,Raphael Ortiz,Christopher Schroers*

Main category: cs.CV

TL;DR: 通过多样式图像结合图像提示适配器和统计对齐技术，提出了一种能够更准确匹配风格、防止内容漏泄的图像风格转换方法


<details>
  <summary>Details</summary>
Motivation: 解决现有潜在扩散模型在图像风格转换中存在的问题：风格匹配不准确、可使用风格图像数量有限、内容与风格杂乱等

Method: 利用多个风格图像来更好表征风格特征，设计了结合图像提示适配器和去噪过程中特征统计对齐的方法，在去噪UNet的交叉注意力和自注意力层进行干预，采用聚类技术从大量注意力值中精炼小集代表性特征

Result: 该方法在风格化任务上达到了目前最先进的结果

Conclusion: 通过多风格图像结合统计对齐技术，成功提出了一种能够更准确进行图像风格转换的方法，有效解决了风格匹配不准和内容漏泄等问题

Abstract: Recent advances in latent diffusion models have enabled exciting progress in
image style transfer. However, several key issues remain. For example, existing
methods still struggle to accurately match styles. They are often limited in
the number of style images that can be used. Furthermore, they tend to entangle
content and style in undesired ways. To address this, we propose leveraging
multiple style images which helps better represent style features and prevent
content leaking from the style images. We design a method that leverages both
image prompt adapters and statistical alignment of the features during the
denoising process. With this, our approach is designed such that it can
intervene both at the cross-attention and the self-attention layers of the
denoising UNet. For the statistical alignment, we employ clustering to distill
a small representative set of attention features from the large number of
attention values extracted from the style samples. As demonstrated in our
experimental section, the resulting method achieves state-of-the-art results
for stylization.

</details>


### [132] [Morphological classification of eclipsing binary stars using computer vision methods](https://arxiv.org/abs/2508.12802)
*Štefan Parimucha,Maksim Gabdeev,Yanna Markus,Martin Vaňko,Pavol Gajdoš*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present an application of computer vision methods to classify the light
curves of eclipsing binaries (EB). We have used pre-trained models based on
convolutional neural networks ($\textit{ResNet50}$) and vision transformers
($\textit{vit\_base\_patch16\_224}$), which were fine-tuned on images created
from synthetic datasets. To improve model generalisation and reduce
overfitting, we developed a novel image representation by transforming
phase-folded light curves into polar coordinates combined with hexbin
visualisation. Our hierarchical approach in the first stage classifies systems
into detached and overcontact types, and in the second stage identifies the
presence or absence of spots. The binary classification models achieved high
accuracy ($>96\%$) on validation data across multiple passbands (Gaia~$G$, $I$,
and $TESS$) and demonstrated strong performance ($>94\%$, up to $100\%$ for
$TESS$) when tested on extensive observational data from the OGLE, DEBCat, and
WUMaCat catalogues. While the primary binary classification was highly
successful, the secondary task of automated spot detection performed poorly,
revealing a significant limitation of our models for identifying subtle
photometric features. This study highlights the potential of computer vision
for EB morphological classification in large-scale surveys, but underscores the
need for further research into robust, automated spot detection.

</details>


### [133] [SIS-Challenge: Event-based Spatio-temporal Instance Segmentation Challenge at the CVPR 2025 Event-based Vision Workshop](https://arxiv.org/abs/2508.12813)
*Friedhelm Hamann,Emil Mededovic,Fabian Gülhan,Yuli Wu,Johannes Stegmaier,Jing He,Yiqing Wang,Kexin Zhang,Lingling Li,Licheng Jiao,Mengru Ma,Hongxiang Huang,Yuhao Yan,Hongwei Ren,Xiaopeng Lin,Yulong Huang,Bojun Cheng,Se Hyun Lee,Gyu Sung Ham,Kanghan Oh,Gi Hyun Lim,Boxuan Yang,Bowen Du,Guillermo Gallego*

Main category: cs.CV

TL;DR: CVPR 2025事件视觉研讨会举办的时空实例分割挑战赛概述，包含任务定义、数据集、挑战细节和结果分析，以及前5名团队的方法介绍


<details>
  <summary>Details</summary>
Motivation: 推动事件相机和灰度相机数据融合的时空实例分割技术发展，为计算机视觉社区提供标准基准和评估平台

Method: 组织挑战赛，提供时空对齐的事件相机和灰度相机数据集，定义像素级分割任务，收集并评估参赛团队的算法方案

Result: 成功举办了SIS挑战赛，获得了多个团队参与，提供了前5名团队的方法细节和性能结果，建立了公开的资源库

Conclusion: 该挑战赛有效促进了事件视觉领域的发展，为时空实例分割技术提供了重要的基准测试和算法比较平台

Abstract: We present an overview of the Spatio-temporal Instance Segmentation (SIS)
challenge held in conjunction with the CVPR 2025 Event-based Vision Workshop.
The task is to predict accurate pixel-level segmentation masks of defined
object classes from spatio-temporally aligned event camera and grayscale camera
data. We provide an overview of the task, dataset, challenge details and
results. Furthermore, we describe the methods used by the top-5 ranking teams
in the challenge. More resources and code of the participants' methods are
available here:
https://github.com/tub-rip/MouseSIS/blob/main/docs/challenge_results.md

</details>


### [134] [DEEP-SEA: Deep-Learning Enhancement for Environmental Perception in Submerged Aquatics](https://arxiv.org/abs/2508.12824)
*Shuang Chen,Ronald Thenius,Farshad Arvin,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: DEEP-SEA是一个基于深度学习的海底图像恢复模型，通过双频增强自注意力空间和频率调制器来同时增强低频和高频信息，保持空间结构，有效解决水下视觉退化问题。


<details>
  <summary>Details</summary>
Motivation: 水下环境存在光散射、吸收和浑浊等问题，导致图像清晰度下降和颜色失真，影响海洋生物多样性分析、生态评估和自主探索的准确性。

Method: 提出DEEP-SEA模型，采用双频增强自注意力空间和频率调制器，自适应地在频域细化特征表示，同时处理空间信息以更好地保持结构。

Result: 在EUVP和LSUI数据集上的综合实验表明，该模型在恢复精细图像细节和结构一致性方面优于现有最先进方法。

Conclusion: DEEP-SEA通过有效缓解水下视觉退化，有潜力提高水下监测平台的可靠性，实现更准确的生态观测、物种识别和自主导航。

Abstract: Continuous and reliable underwater monitoring is essential for assessing
marine biodiversity, detecting ecological changes and supporting autonomous
exploration in aquatic environments. Underwater monitoring platforms rely on
mainly visual data for marine biodiversity analysis, ecological assessment and
autonomous exploration. However, underwater environments present significant
challenges due to light scattering, absorption and turbidity, which degrade
image clarity and distort colour information, which makes accurate observation
difficult. To address these challenges, we propose DEEP-SEA, a novel deep
learning-based underwater image restoration model to enhance both low- and
high-frequency information while preserving spatial structures. The proposed
Dual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator aims to
adaptively refine feature representations in frequency domains and
simultaneously spatial information for better structural preservation. Our
comprehensive experiments on EUVP and LSUI datasets demonstrate the superiority
over the state of the art in restoring fine-grained image detail and structural
consistency. By effectively mitigating underwater visual degradation, DEEP-SEA
has the potential to improve the reliability of underwater monitoring platforms
for more accurate ecological observation, species identification and autonomous
navigation.

</details>


### [135] [Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection](https://arxiv.org/abs/2508.12842)
*Ronghao Lin,Sijie Mai,Ying Zeng,Qiaolin He,Aolin Xiong,Haifeng Hu*

Main category: cs.CV

TL;DR: 提出多源多模态渐进域适应框架MMPDA，在多模态欺骗检测挑战中获得第二名，在准确率和F1分数上超越第一名团队


<details>
  <summary>Details</summary>
Motivation: 解决源域和目标域之间的域偏移问题，将音频-视觉知识从多样化的源域迁移到目标域

Method: 多源多模态渐进域适应框架，通过在特征和决策层面逐步对齐源域和目标域来弥合域偏移

Result: 在竞赛第二阶段达到60.43%的准确率和56.99%的F1分数，F1分数比第一名团队高5.59%，准确率比第三名团队高6.75%

Conclusion: 该方法在多模态欺骗检测任务中表现出色，有效解决了跨域迁移问题，获得了Top-2的成绩

Abstract: This paper presents the winning approach for the 1st MultiModal Deception
Detection (MMDD) Challenge at the 1st Workshop on Subtle Visual Computing
(SVC). Aiming at the domain shift issue across source and target domains, we
propose a Multi-source Multimodal Progressive Domain Adaptation (MMPDA)
framework that transfers the audio-visual knowledge from diverse source domains
to the target domain. By gradually aligning source and the target domain at
both feature and decision levels, our method bridges domain shifts across
diverse multimodal datasets. Extensive experiments demonstrate the
effectiveness of our approach securing Top-2 place. Our approach reaches 60.43%
on accuracy and 56.99\% on F1-score on competition stage 2, surpassing the 1st
place team by 5.59% on F1-score and the 3rd place teams by 6.75% on accuracy.
Our code is available at https://github.com/RH-Lin/MMPDA.

</details>


### [136] [Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models](https://arxiv.org/abs/2508.12861)
*Dexia Chen,Wentao Zhang,Qianjie Zhu,Ping Hu,Weibing Li,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: 提出CoMuCo方法，通过多视图协作优化和一致性约束，提升视觉语言模型在跨域少样本任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在自然图像上表现优异，但在跨域任务（成像域不同于自然图像）中效果有限，需要新的微调策略来解决这一局限性

Method: CoMuCo方法使用两个功能互补的专家模块提取多视图特征，结合基于先验知识的一致性约束和信息几何的共识机制来增强特征学习的鲁棒性

Result: 在现有和新提出的基准测试上进行广泛实证评估，表明CoMuCo在少样本任务中始终优于当前方法

Conclusion: CoMuCo是一种有效的跨域少样本学习微调策略，建立了新的跨域基准来全面评估方法性能

Abstract: Vision-language models (VLMs) pre-trained on natural image and language data,
such as CLIP, have exhibited significant potential in few-shot image
recognition tasks, leading to development of various efficient transfer
learning methods. These methods exploit inherent pre-learned knowledge in VLMs
and have achieved strong performance on standard image datasets. However, their
effectiveness is often limited when confronted with cross-domain tasks where
imaging domains differ from natural images. To address this limitation, we
propose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), a
novel fine-tuning strategy for VLMs. This strategy employs two functionally
complementary expert modules to extract multi-view features, while
incorporating prior knowledge-based consistency constraints and information
geometry-based consensus mechanisms to enhance the robustness of feature
learning. Additionally, a new cross-domain few-shot benchmark is established to
help comprehensively evaluate methods on imaging domains distinct from natural
images. Extensive empirical evaluations on both existing and newly proposed
benchmarks suggest CoMuCo consistently outperforms current methods in few-shot
tasks. The code and benchmark will be released.

</details>


### [137] [Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning](https://arxiv.org/abs/2508.12877)
*Dexia Chen,Qianjie Zhu,Weibing Li,Yue Yu,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: 基于CLIP等预训练视觉-语言模型的新精细调整方法MPS-Tuning，通过保持语义流形的几何结构和增强类别可分辨性来提升域适配性能


<details>
  <summary>Details</summary>
Motivation: 现有的参数效率调整或一致性约束方法忽视了数据分布的几何结构，导致整体语义表征的扭曲

Method: 将特征空间的数据分布视为语义流形，通过对齐精细调整前后特征的Gram矩阵来保持原始流形的宏观和微观拓扑结构，同时优化图像和文本特征的成对相似性来增强类别可分辨性

Result: 大量实验证明MPS-Tuning显著提升了模型性能，同时有效保持了语义流形的结构

Conclusion: MPS-Tuning通过明确约束语义流形的本质几何结构和增强类别可分辨性，为预训练视觉-语言模型的精细调整提供了有效方法

Abstract: Pretrained vision-language models (VLMs), such as CLIP, have shown remarkable
potential in few-shot image classification and led to numerous effective
transfer learning strategies. These methods leverage the pretrained knowledge
of VLMs to enable effective domain adaptation while mitigating overfitting
through parameter-efficient tuning or instance-based consistency constraints.
However, such regularizations often neglect the geometric structure of data
distribution, which may lead to distortion of the overall semantic
representation. To overcome this limitation, we propose a novel fine-tuning
method, Manifold-Preserving and Sculpting Tuning (MPS-Tuning). Regarding the
data distribution in feature space as a semantic manifold, MPS-Tuning
explicitly constrains the intrinsic geometry of this manifold while further
sculpting it to enhance class separability. Specifically, MPS-Tuning preserves
both macroscopic and microscopic topological structures of the original
manifold by aligning Gram matrices of features before and after fine-tuning.
Theoretically, this constraint is shown to approximate an upper bound of the
Gromov-Wasserstein distance. Furthermore, features from the image and text
modalities are paired, and pairwise similarities are optimized to enhance the
manifold's class discriminability. Extensive experiments demonstrate that
MPS-Tuning significantly improves model performance while effectively
preserving the structure of the semantic manifold. The code will be released.

</details>


### [138] [S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models](https://arxiv.org/abs/2508.12880)
*Chubin Chen,Jiashu Zhu,Xiaokun Feng,Nisha Huang,Meiqi Wu,Fangyuan Mao,Jiahong Wu,Xiangxiang Chu,Xiu Li*

Main category: cs.CV

TL;DR: S^2-Guidance是一种新的扩散模型引导方法，通过随机块丢弃构建子网络来优化预测，解决了CFG方法中的次优预测问题，在文本到图像和文本到视频生成任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究发现Classifier-free Guidance (CFG)方法在扩散模型中会产生次优预测，导致语义不连贯和低质量输出，需要一种新的引导策略来解决这个问题。

Method: 提出S^2-Guidance方法，利用前向过程中的随机块丢弃来构建随机子网络，有效引导模型远离低质量预测，朝向高质量输出。

Result: 在文本到图像和文本到视频生成任务上的大量实验表明，S^2-Guidance在定性和定量评估中都优于CFG和其他先进引导策略。

Conclusion: S^2-Guidance通过利用模型自身的子网络来优化预测，提供了一种有效的扩散模型引导方法，显著提升了生成质量。

Abstract: Classifier-free Guidance (CFG) is a widely used technique in modern diffusion
models for enhancing sample quality and prompt adherence. However, through an
empirical analysis on Gaussian mixture modeling with a closed-form solution, we
observe a discrepancy between the suboptimal results produced by CFG and the
ground truth. The model's excessive reliance on these suboptimal predictions
often leads to semantic incoherence and low-quality outputs. To address this
issue, we first empirically demonstrate that the model's suboptimal predictions
can be effectively refined using sub-networks of the model itself. Building on
this insight, we propose S^2-Guidance, a novel method that leverages stochastic
block-dropping during the forward process to construct stochastic sub-networks,
effectively guiding the model away from potential low-quality predictions and
toward high-quality outputs. Extensive qualitative and quantitative experiments
on text-to-image and text-to-video generation tasks demonstrate that
S^2-Guidance delivers superior performance, consistently surpassing CFG and
other advanced guidance strategies. Our code will be released.

</details>


### [139] [ONG: One-Shot NMF-based Gradient Masking for Efficient Model Sparsification](https://arxiv.org/abs/2508.12891)
*Sankar Behera,Yamuna Prasad*

Main category: cs.CV

TL;DR: ONG是一种基于非负矩阵分解的一次性剪枝方法，通过梯度掩码机制在训练过程中严格保持目标稀疏度，在CIFAR数据集上实现了与现有方法相当或更好的性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络尺寸庞大导致部署困难，现有剪枝方法存在迭代过程复杂、标准专业或难以在训练中有效保持稀疏度等问题，需要一种简单有效的稀疏化策略。

Method: 使用非负矩阵分解(NMF)识别重要权重结构进行一次性初始剪枝，然后采用精确的梯度掩码机制确保只更新未剪枝权重，严格保持目标稀疏度。

Result: 在CIFAR-10和CIFAR-100数据集上使用ResNet56、ResNet34和ResNet18进行测试，ONG在不同稀疏度水平下都能达到与现有稳定稀疏化方法相当或更优的性能。

Conclusion: ONG提供了一种有效的一次性剪枝方法，能够保持剪枝后的结构完整性，并提供了明确的机制来达到目标稀疏度，解决了训练过程中稀疏度保持的难题。

Abstract: Deep Neural Networks (DNNs) have achieved remarkable success but their large
size poses deployment challenges. While various pruning techniques exist, many
involve complex iterative processes, specialized criteria, or struggle to
maintain sparsity effectively during training. We introduce ONG (One-shot
NMF-based Gradient Masking), a novel sparsification strategy that identifies
salient weight structures using Non-negative Matrix Factorization (NMF) for
one-shot pruning at the outset of training. Subsequently, ONG employs a precise
gradient masking mechanism to ensure that only unpruned weights are updated,
strictly preserving the target sparsity throughout the training phase. We
integrate ONG into the BIMP comparative framework and evaluate it on CIFAR-10
and CIFAR-100 with ResNet56, ResNet34, and ResNet18 against established stable
sparsification methods. Our experiments demonstrate ONG's ability to achieve
comparable or superior performance at various sparsity levels while maintaining
structural integrity post-pruning and offering a clear mechanism for targeting
desired sparsities.

</details>


### [140] [CMF-IoU: Multi-Stage Cross-Modal Fusion 3D Object Detection with IoU Joint Prediction](https://arxiv.org/abs/2508.12917)
*Zhiwei Ning,Zhaojiang Liu,Xuanang Gao,Yifan Zuo,Jie Yang,Yuming Fang,Wei Liu*

Main category: cs.CV

TL;DR: CMF-IOU是一个多阶段跨模态融合的3D检测框架，通过深度补全网络将像素信息投影到3D空间获得伪点云，设计双边交叉视图增强3D骨干网络，并引入迭代体素点感知细粒度池化模块，在KITTI、nuScenes和Waymo数据集上表现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态方法主要关注单阶段或部分阶段融合，导致特征提取不足和性能欠佳，需要解决3D空间信息与2D语义信息对齐的挑战。

Method: 1) 通过深度补全网络将像素信息投影到3D空间获得伪点云；2) 设计双边交叉视图增强3D骨干网络（S2D分支和ResVC分支）；3) 引入迭代体素点感知细粒度池化模块；4) 设计IoU联合预测分支和新的候选框生成技术。

Result: 在KITTI、nuScenes和Waymo数据集上进行了广泛实验，证明了方法的优越性能。

Conclusion: CMF-IOU框架通过多阶段跨模态融合有效解决了3D空间和2D语义信息的对齐问题，在多个基准数据集上取得了优异的检测性能。

Abstract: Multi-modal methods based on camera and LiDAR sensors have garnered
significant attention in the field of 3D detection. However, many prevalent
works focus on single or partial stage fusion, leading to insufficient feature
extraction and suboptimal performance. In this paper, we introduce a
multi-stage cross-modal fusion 3D detection framework, termed CMF-IOU, to
effectively address the challenge of aligning 3D spatial and 2D semantic
information. Specifically, we first project the pixel information into 3D space
via a depth completion network to get the pseudo points, which unifies the
representation of the LiDAR and camera information. Then, a bilateral
cross-view enhancement 3D backbone is designed to encode LiDAR points and
pseudo points. The first sparse-to-distant (S2D) branch utilizes an
encoder-decoder structure to reinforce the representation of sparse LiDAR
points. The second residual view consistency (ResVC) branch is proposed to
mitigate the influence of inaccurate pseudo points via both the 3D and 2D
convolution processes. Subsequently, we introduce an iterative voxel-point
aware fine grained pooling module, which captures the spatial information from
LiDAR points and textural information from pseudo points in the proposal
refinement stage. To achieve more precise refinement during iteration, an
intersection over union (IoU) joint prediction branch integrated with a novel
proposals generation technique is designed to preserve the bounding boxes with
both high IoU and classification scores. Extensive experiments show the
superior performance of our method on the KITTI, nuScenes and Waymo datasets.

</details>


### [141] [7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models](https://arxiv.org/abs/2508.12919)
*Elena Izzo,Luca Parolari,Davide Vezzaro,Lamberto Ballan*

Main category: cs.CV

TL;DR: 7Bench是首个评估布局引导文本到图像生成中语义和空间对齐的基准测试，包含7个挑战性场景，评估对象生成、颜色保真度、属性识别、对象间关系和空间控制。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试只评估文本对齐，而忽略了布局对齐，无法全面评估模型的空间保真度，这在合成数据生成等应用中至关重要。

Method: 提出了7Bench基准测试，包含文本-布局对，涵盖7个挑战性场景，并提出了结合布局对齐分数的评估协议来评估空间准确性。

Result: 使用7Bench评估了多个最先进的扩散模型，揭示了它们在不同对齐任务中的优势和局限性。

Conclusion: 7Bench填补了布局引导文本到图像生成评估的空白，为模型的空间保真度评估提供了重要工具，有助于提升合成数据质量。

Abstract: Layout-guided text-to-image models offer greater control over the generation
process by explicitly conditioning image synthesis on the spatial arrangement
of elements. As a result, their adoption has increased in many computer vision
applications, ranging from content creation to synthetic data generation. A
critical challenge is achieving precise alignment between the image, textual
prompt, and layout, ensuring semantic fidelity and spatial accuracy. Although
recent benchmarks assess text alignment, layout alignment remains overlooked,
and no existing benchmark jointly evaluates both. This gap limits the ability
to evaluate a model's spatial fidelity, which is crucial when using
layout-guided generation for synthetic data, as errors can introduce noise and
degrade data quality. In this work, we introduce 7Bench, the first benchmark to
assess both semantic and spatial alignment in layout-guided text-to-image
generation. It features text-and-layout pairs spanning seven challenging
scenarios, investigating object generation, color fidelity, attribute
recognition, inter-object relationships, and spatial control. We propose an
evaluation protocol that builds on existing frameworks by incorporating the
layout alignment score to assess spatial accuracy. Using 7Bench, we evaluate
several state-of-the-art diffusion models, uncovering their respective
strengths and limitations across diverse alignment tasks. The benchmark is
available at https://github.com/Elizzo/7Bench.

</details>


### [142] [Towards High-Resolution Industrial Image Anomaly Detection](https://arxiv.org/abs/2508.12931)
*Ximiao Zhang,Min Xu,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: HiAD是一个针对高分辨率图像异常检测的双分支框架，通过多尺度特征融合和自适应检测器分配策略，在有限计算资源下有效检测不同大小的异常区域。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法主要针对低分辨率场景，高分辨率图像下传统下采样会导致细粒度异常信息丢失，现有方法在检测精度和效率上无法满足工业实际需求。

Method: 采用双分支架构整合多尺度异常线索，结合多分辨率特征融合策略处理细粒度纹理变化，使用检测器池和自适应分配策略根据图像块特征动态分配检测器。

Result: 在专门构建的高分辨率异常检测基准测试（MVTec-HD、VisA-HD和RealIAD-HD）上进行了广泛实验，证明了HiAD的优越性能。

Conclusion: HiAD框架能够有效解决高分辨率图像异常检测的挑战，在检测精度和计算效率方面都表现出色，代码已开源。

Abstract: Current anomaly detection methods primarily focus on low-resolution
scenarios. For high-resolution images, conventional downsampling often results
in missed detections of subtle anomalous regions due to the loss of
fine-grained discriminative information. Despite some progress, recent studies
have attempted to improve detection resolution by employing lightweight
networks or using simple image tiling and ensemble methods. However, these
approaches still struggle to meet the practical demands of industrial scenarios
in terms of detection accuracy and efficiency. To address the above issues, we
propose HiAD, a general framework for high-resolution anomaly detection. HiAD
is capable of detecting anomalous regions of varying sizes in high-resolution
images under limited computational resources. Specifically, HiAD employs a
dual-branch architecture that integrates anomaly cues across different scales
to comprehensively capture both subtle and large-scale anomalies. Furthermore,
it incorporates a multi-resolution feature fusion strategy to tackle the
challenges posed by fine-grained texture variations in high-resolution images.
To enhance both adaptability and efficiency, HiAD utilizes a detector pool in
conjunction with various detector assignment strategies, enabling detectors to
be adaptively assigned based on patch features, ensuring detection performance
while effectively controlling computational costs. We conduct extensive
experiments on our specifically constructed high-resolution anomaly detection
benchmarks, including MVTec-HD, VisA-HD, and the real-world benchmark
RealIAD-HD, demonstrating the superior performance of HiAD. The code is
available at https://github.com/cnulab/HiAD.

</details>


### [143] [Fully Automated Segmentation of Fiber Bundles in Anatomic Tracing Data](https://arxiv.org/abs/2508.12942)
*Kyriaki-Margarita Bintsi,Yaël Balbastre,Jingjing Wu,Julia F. Lehman,Suzanne N. Haber,Anastasia Yendiki*

Main category: cs.CV

TL;DR: 基于U-Net网络的全自动化框架，通过大补丁尺寸、前景感知采样和半监督预训练，显著提高鹅小猴神经踪迹数据中纤束分割的准确性和效率


<details>
  <summary>Details</summary>
Motivation: 解决历史学分析中手动注释纤束的劳动密集问题，充分利用神经踪迹数据来验证和改进滿裁MRI赨散迷路技术

Method: 采用U-Net网络构造，结合大补丁尺寸、前景感知采样策略和半监督预训练技术

Result: 稀疏纤束检测提高20%以上，假发现率降低40%，避免了终端被误标为纤束的常见错误，支持单独切片分析

Conclusion: 该框架能大规模自动化处理解剖学踪迹数据，为滿裁MRI赨散迷路技术提供更多的真实地面真实数据，有助于该技术的验证和优化

Abstract: Anatomic tracer studies are critical for validating and improving diffusion
MRI (dMRI) tractography. However, large-scale analysis of data from such
studies is hampered by the labor-intensive process of annotating fiber bundles
manually on histological slides. Existing automated methods often miss sparse
bundles or require complex post-processing across consecutive sections,
limiting their flexibility and generalizability. We present a streamlined,
fully automated framework for fiber bundle segmentation in macaque tracer data,
based on a U-Net architecture with large patch sizes, foreground aware
sampling, and semisupervised pre-training. Our approach eliminates common
errors such as mislabeling terminals as bundles, improves detection of sparse
bundles by over 20% and reduces the False Discovery Rate (FDR) by 40% compared
to the state-of-the-art, all while enabling analysis of standalone slices. This
new framework will facilitate the automated analysis of anatomic tracing data
at a large scale, generating more ground-truth data that can be used to
validate and optimize dMRI tractography methods.

</details>


### [144] [Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models](https://arxiv.org/abs/2508.12945)
*Jianshu Zeng,Yuxuan Liu,Yutong Feng,Chenxuan Miao,Zixiang Gao,Jiwang Qu,Jianzhang Zhang,Bin Wang,Kun Yuan*

Main category: cs.CV

TL;DR: Lumen是一个端到端的视频重光照框架，基于大规模视频生成模型，通过文本描述控制光照和背景，在保持前景一致性的同时实现和谐的视频重光照效果。


<details>
  <summary>Details</summary>
Motivation: 视频重光照是一个具有挑战性但有价值的任务，需要在替换视频背景的同时相应地调整前景光照并实现和谐融合。现有方法缺乏高质量配对视频数据，且难以保持时间帧间的一致性和前景属性（如反照率）的保留。

Method: 构建混合真实和合成视频的大规模数据集；利用先进3D渲染引擎创建多样化环境的合成视频对；采用HDR光照模拟补充真实视频数据；设计联合训练课程，注入领域感知适配器来解耦重光照和领域外观分布的学习。

Result: 实验结果表明，Lumen能够有效地将输入视频编辑为具有一致光照和严格前景保留的电影级重光照视频，在前景保持和视频一致性评估方面表现优异。

Conclusion: Lumen框架通过大规模混合数据集和领域感知训练策略，成功解决了视频重光照中的一致性和前景保留问题，为视频编辑提供了有效的解决方案。

Abstract: Video relighting is a challenging yet valuable task, aiming to replace the
background in videos while correspondingly adjusting the lighting in the
foreground with harmonious blending. During translation, it is essential to
preserve the original properties of the foreground, e.g., albedo, and propagate
consistent relighting among temporal frames. In this paper, we propose Lumen,
an end-to-end video relighting framework developed on large-scale video
generative models, receiving flexible textual description for instructing the
control of lighting and background. Considering the scarcity of high-qualified
paired videos with the same foreground in various lighting conditions, we
construct a large-scale dataset with a mixture of realistic and synthetic
videos. For the synthetic domain, benefiting from the abundant 3D assets in the
community, we leverage advanced 3D rendering engine to curate video pairs in
diverse environments. For the realistic domain, we adapt a HDR-based lighting
simulation to complement the lack of paired in-the-wild videos. Powered by the
aforementioned dataset, we design a joint training curriculum to effectively
unleash the strengths of each domain, i.e., the physical consistency in
synthetic videos, and the generalized domain distribution in realistic videos.
To implement this, we inject a domain-aware adapter into the model to decouple
the learning of relighting and domain appearance distribution. We construct a
comprehensive benchmark to evaluate Lumen together with existing methods, from
the perspectives of foreground preservation and video consistency assessment.
Experimental results demonstrate that Lumen effectively edit the input into
cinematic relighted videos with consistent lighting and strict foreground
preservation. Our project page: https://lumen-relight.github.io/

</details>


### [145] [MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation](https://arxiv.org/abs/2508.12948)
*Wei Wei,Shaojie Zhang,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: 提出MaskSem方法，通过语义导向的拦截和混合高阶运动重构学习，改善了自监督骨架动作识别的性能


<details>
  <summary>Details</summary>
Motivation: 现有的自监督骨架动作识别方法仅关注有限关节和低阶运动模式，限制了模型对复杂运动模式的理解能力

Method: 使用Grad-CAM基于相对运动来导向拦截最具语义丰富的时间区域，并以低阶速度和高阶加速度的混合高阶运动作为重构目标

Result: 在NTU60、NTU120和PKU-MMD数据集上识别性能提升，更适合人机交互应用

Conclusion: MaskSem框架通过语义导向拦截和多阶运动学习，有效提升了自监督骨架动作识别的性能

Abstract: Human action recognition is a crucial task for intelligent robotics,
particularly within the context of human-robot collaboration research. In
self-supervised skeleton-based action recognition, the mask-based
reconstruction paradigm learns the spatial structure and motion patterns of the
skeleton by masking joints and reconstructing the target from unlabeled data.
However, existing methods focus on a limited set of joints and low-order motion
patterns, limiting the model's ability to understand complex motion patterns.
To address this issue, we introduce MaskSem, a novel semantic-guided masking
method for learning 3D hybrid high-order motion representations. This novel
framework leverages Grad-CAM based on relative motion to guide the masking of
joints, which can be represented as the most semantically rich temporal
orgions. The semantic-guided masking process can encourage the model to explore
more discriminative features. Furthermore, we propose using hybrid high-order
motion as the reconstruction target, enabling the model to learn multi-order
motion patterns. Specifically, low-order motion velocity and high-order motion
acceleration are used together as the reconstruction target. This approach
offers a more comprehensive description of the dynamic motion process,
enhancing the model's understanding of motion patterns. Experiments on the
NTU60, NTU120, and PKU-MMD datasets show that MaskSem, combined with a vanilla
transformer, improves skeleton-based action recognition, making it more
suitable for applications in human-robot interaction.

</details>


### [146] [Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination](https://arxiv.org/abs/2508.12957)
*Yizhou Liu,Jingwei Wei,Zizhi Chen,Minghao Han,Xukun Zhang,Keliang Liu,Lihua Zhang*

Main category: cs.CV

TL;DR: ARMed是一个用于开放式医学视觉问答的强化学习框架，通过结合文本正确性和自适应语义奖励来提升医学推理质量，在多个基准测试中显著提升了准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的强化学习应用仍未被充分探索，现有的方法主要针对封闭式视觉问答，而开放式医学VQA更能反映临床实践但关注有限。基于模型的语义奖励存在奖励崩溃问题，即语义差异显著的响应获得相似分数。

Method: ARMed首先通过监督微调在思维链数据中融入领域知识，然后应用强化学习结合文本正确性和自适应语义奖励来增强推理质量。

Result: 在六个具有挑战性的医学VQA基准测试中，ARMed持续提升了准确性和泛化能力，在域内任务上实现了32.64%的改进，在域外基准上获得了11.65%的提升。

Conclusion: 这些结果突显了奖励可区分性在医学强化学习中的关键作用，以及语义引导奖励在实现稳健且具有临床意义的多模态推理方面的潜力。

Abstract: Reinforcement learning (RL) with rule-based rewards has demonstrated strong
potential in enhancing the reasoning and generalization capabilities of
vision-language models (VLMs) and large language models (LLMs), while reducing
computational overhead. However, its application in medical imaging remains
underexplored. Existing reinforcement fine-tuning (RFT) approaches in this
domain primarily target closed-ended visual question answering (VQA), limiting
their applicability to real-world clinical reasoning. In contrast, open-ended
medical VQA better reflects clinical practice but has received limited
attention. While some efforts have sought to unify both formats via
semantically guided RL, we observe that model-based semantic rewards often
suffer from reward collapse, where responses with significant semantic
differences receive similar scores. To address this, we propose ARMed (Adaptive
Reinforcement for Medical Reasoning), a novel RL framework for open-ended
medical VQA. ARMed first incorporates domain knowledge through supervised
fine-tuning (SFT) on chain-of-thought data, then applies reinforcement learning
with textual correctness and adaptive semantic rewards to enhance reasoning
quality. We evaluate ARMed on six challenging medical VQA benchmarks. Results
show that ARMed consistently boosts both accuracy and generalization, achieving
a 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain
benchmarks. These results highlight the critical role of reward
discriminability in medical RL and the promise of semantically guided rewards
for enabling robust and clinically meaningful multimodal reasoning.

</details>


### [147] [GazeDETR: Gaze Detection using Disentangled Head and Gaze Representations](https://arxiv.org/abs/2508.12966)
*Ryan Anthony Jalova de Belen,Gelareh Mohammadi,Arcot Sowmya*

Main category: cs.CV

TL;DR: GazeDETR是一种新颖的端到端架构，使用两个解耦的解码器分别处理头部定位和视线预测任务，在多个数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端视线目标检测模型使用单一解码器同时定位头部和预测视线，导致表示纠缠。需要解耦这两个任务以获得更好的性能。

Method: 提出GazeDETR架构，包含两个独立的解码器：一个用于头部定位（利用局部信息），另一个用于视线预测（结合局部和全局信息），并有效利用连贯注意力场。

Result: 在GazeFollow、VideoAttentionTarget和ChildPlay数据集上达到最先进结果，显著优于现有的端到端模型。

Conclusion: 解耦的架构设计能够为每个子任务学习独特的表示，头部预测器使用局部信息，视线解码器结合局部和全局信息，这种设计显著提升了视线目标检测的性能。

Abstract: Gaze communication plays a crucial role in daily social interactions.
Quantifying this behavior can help in human-computer interaction and digital
phenotyping. While end-to-end models exist for gaze target detection, they only
utilize a single decoder to simultaneously localize human heads and predict
their corresponding gaze (e.g., 2D points or heatmap) in a scene. This
multitask learning approach generates a unified and entangled representation
for human head localization and gaze location prediction. Herein, we propose
GazeDETR, a novel end-to-end architecture with two disentangled decoders that
individually learn unique representations and effectively utilize coherent
attentive fields for each subtask. More specifically, we demonstrate that its
human head predictor utilizes local information, while its gaze decoder
incorporates both local and global information. Our proposed architecture
achieves state-of-the-art results on the GazeFollow, VideoAttentionTarget and
ChildPlay datasets. It outperforms existing end-to-end models with a notable
margin.

</details>


### [148] [Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation](https://arxiv.org/abs/2508.12969)
*Qirui Li,Guangcong Zheng,Qi Zhao,Jie Li,Bin Dong,Yiwu Yao,Xi Li*

Main category: cs.CV

TL;DR: 这篇论文提出了Compact Attention框架，通过动态水平组合、时间变化窗口和自动配置搜索，实现了注意力机制的高效加速，在保持视觉质量的同时获得1.6~2.5倍加速效果。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制的计算要求对于变换器基于视频生成构成了重大挑战，特别是在生成超长序列时。现有的因子化注意力和固定稀疏模式方法无法充分利用视频数据中内在的空间-时间冗余性。

Method: 提出Compact Attention硬件感知加速框架：1)适应性分块策略，通过动态块组合近似多样化空间交互模式；2)时间变化窗口，根据帧距离调整稀疏程度；3)自动化配置搜索算法，在保留关键注意力途径的同时优化稀疏模式。

Result: 在单GPU环境下实现了注意力计算1.6~2.5倍的加速，同时保持了与全注意力基线相当的视觉质量。

Conclusion: 该工作提供了一种基于结构化稀疏利用的原理性方法，用于开启高效的长形式视频生成。

Abstract: The computational demands of self-attention mechanisms pose a critical
challenge for transformer-based video generation, particularly in synthesizing
ultra-long sequences. Current approaches, such as factorized attention and
fixed sparse patterns, fail to fully exploit the inherent spatio-temporal
redundancies in video data. Through systematic analysis of video diffusion
transformers (DiT), we uncover a key insight: Attention matrices exhibit
structured, yet heterogeneous sparsity patterns, where specialized heads
dynamically attend to distinct spatiotemporal regions (e.g., local pattern,
cross-shaped pattern, or global pattern). Existing sparse attention methods
either impose rigid constraints or introduce significant overhead, limiting
their effectiveness. To address this, we propose Compact Attention, a
hardware-aware acceleration framework featuring three innovations: 1) Adaptive
tiling strategies that approximate diverse spatial interaction patterns via
dynamic tile grouping, 2) Temporally varying windows that adjust sparsity
levels based on frame proximity, and 3) An automated configuration search
algorithm that optimizes sparse patterns while preserving critical attention
pathways. Our method achieves 1.6~2.5x acceleration in attention computation on
single-GPU setups while maintaining comparable visual quality with
full-attention baselines. This work provides a principled approach to unlocking
efficient long-form video generation through structured sparsity exploitation.
Project Page: https://yo-ava.github.io/Compact-Attention.github.io/

</details>


### [149] [Dextr: Zero-Shot Neural Architecture Search with Singular Value Decomposition and Extrinsic Curvature](https://arxiv.org/abs/2508.12977)
*Rohan Asthana,Joschua Conrad,Maurits Ortmanns,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 这篇论文提出了一种无需标签数据的零样本神经网络架构搜索代理方法，通过结合汇聚性、泛化性和表达能力来预测网络性能，在多个标准数据集上表现优异且效率高。


<details>
  <summary>Details</summary>
Motivation: 解决现有零样本NAS代理方法依赖标签数据的问题，以及仅关注汇聚性/泛化性或单纯表达能力的限制，需要一种综合考虑各方面性能的无标签代理方法。

Method: 通过分析频道共线性对网络性能的影响，结合层特征的奇异值分解(SVD)和网络输出的外在曲率，设计了一种简化调和平均形式的代理指标。该方法仅需单个无标签数据样本即可预测网络性能。

Result: 在NAS-Bench-101、NAS-Bench-201、TransNAS-Bench-101-micro等多个相关性测试集上表现优异，同时在DARTS卷积网络和AutoFormer变换器搜索空间的NAS任务中也显示出良好性能，且计算效率高。

Conclusion: 该研究提出的无标签数据零样本NAS代理方法能够综合考虑网络的汇聚性、泛化性和表达能力，在多个标准数据集上实现了优异的性能预测效果，为真实场景下的神经网络架构搜索提供了高效的解决方案。

Abstract: Zero-shot Neural Architecture Search (NAS) typically optimises the
architecture search process by exploiting the network or gradient properties at
initialisation through zero-cost proxies. The existing proxies often rely on
labelled data, which is usually unavailable in real-world settings.
Furthermore, the majority of the current methods focus either on optimising the
convergence and generalisation attributes or solely on the expressivity of the
network architectures. To address both limitations, we first demonstrate how
channel collinearity affects the convergence and generalisation properties of a
neural network. Then, by incorporating the convergence, generalisation and
expressivity in one approach, we propose a zero-cost proxy that omits the
requirement of labelled data for its computation. In particular, we leverage
the Singular Value Decomposition (SVD) of the neural network layer features and
the extrinsic curvature of the network output to design our proxy. %As a
result, the proposed proxy is formulated as the simplified harmonic mean of the
logarithms of two key components: the sum of the inverse of the feature
condition number and the extrinsic curvature of the network output. Our
approach enables accurate prediction of network performance on test data using
only a single label-free data sample. Our extensive evaluation includes a total
of six experiments, including the Convolutional Neural Network (CNN) search
space, i.e. DARTS and the Transformer search space, i.e. AutoFormer. The
proposed proxy demonstrates a superior performance on multiple correlation
benchmarks, including NAS-Bench-101, NAS-Bench-201, and
TransNAS-Bench-101-micro; as well as on the NAS task within the DARTS and the
AutoFormer search space, all while being notably efficient. The code is
available at https://github.com/rohanasthana/Dextr.

</details>


### [150] [Omni Survey for Multimodality Analysis in Visual Object Tracking](https://arxiv.org/abs/2508.13000)
*Zhangyong Tang,Tianyang Xu,Xuefeng Zhu,Hui Li,Shaochuan Zhao,Tao Zhou,Chunyang Cheng,Xiaojun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 这是一份关于多模态视觉目标跟踪(MMVOT)的综述性论文，从数据收集、模态对齐、模型设计和评估四个关键方面全面分析了该领域，包含6个MMVOT任务和338个参考文献。


<details>
  <summary>Details</summary>
Motivation: 智慧城市发展产生了大量多模态数据，需要综合监控城市基础设施和服务。多模态视觉目标跟踪作为关键任务，需要从多模态角度进行系统性分析和评估。

Method: 论文采用综述性研究方法，首先介绍相关数据模态，分析多模态数据收集、对齐和标注的挑战，然后基于处理可见光(RGB)和其他X模态的不同方式对现有MMVOT方法进行分类。

Result: 论文完整调查了多模态视觉跟踪的所有方面，揭示了现有MMVOT数据集中对象类别的明显长尾分布特征，以及与RGB数据集相比动物类别显著缺乏的问题。

Conclusion: 多模态跟踪并非总是比单模态跟踪更优称，论文探讨了在什么情况下多模态跟踪能够通过信息融合提供更好的解决方案。

Abstract: The development of smart cities has led to the generation of massive amounts
of multi-modal data in the context of a range of tasks that enable a
comprehensive monitoring of the smart city infrastructure and services. This
paper surveys one of the most critical tasks, multi-modal visual object
tracking (MMVOT), from the perspective of multimodality analysis. Generally,
MMVOT differs from single-modal tracking in four key aspects, data collection,
modality alignment and annotation, model designing, and evaluation.
Accordingly, we begin with an introduction to the relevant data modalities,
laying the groundwork for their integration. This naturally leads to a
discussion of challenges of multi-modal data collection, alignment, and
annotation. Subsequently, existing MMVOT methods are categorised, based on
different ways to deal with visible (RGB) and X modalities: programming the
auxiliary X branch with replicated or non-replicated experimental
configurations from the RGB branch. Here X can be thermal infrared (T), depth
(D), event (E), near infrared (NIR), language (L), or sonar (S). The final part
of the paper addresses evaluation and benchmarking. In summary, we undertake an
omni survey of all aspects of multi-modal visual object tracking (VOT),
covering six MMVOT tasks and featuring 338 references in total. In addition, we
discuss the fundamental rhetorical question: Is multi-modal tracking always
guaranteed to provide a superior solution to unimodal tracking with the help of
information fusion, and if not, in what circumstances its application is
beneficial. Furthermore, for the first time in this field, we analyse the
distributions of the object categories in the existing MMVOT datasets,
revealing their pronounced long-tail nature and a noticeable lack of animal
categories when compared with RGB datasets.

</details>


### [151] [Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition and Continual Learning](https://arxiv.org/abs/2508.13005)
*Jiawen Xu,Odej Kao*

Main category: cs.CV

TL;DR: 本文通过实证研究表明，增强特征多样性可以显著改善开放集识别和持续学习性能，特征多样性有助于检测新类别和整合新数据。


<details>
  <summary>Details</summary>
Motivation: 开放集识别(OSR)和持续学习是机器学习中的两个关键挑战，虽然已有许多方法通过启发式方式促进特征多样性来解决这些问题，但很少有研究直接探讨特征多样性在其中扮演的角色。

Method: 通过实证研究分析特征多样性对开放集识别和持续学习的影响，提供实验证据来验证特征多样性的作用。

Result: 增强特征多样性可以改善开放集样本的识别，同时也有利于持续学习中旧知识的保持和新数据的整合。

Conclusion: 特征多样性在开放集识别和持续学习中具有重要作用，这一发现可为这两个领域的实践方法和理论理解提供新的研究灵感。

Abstract: Open set recognition (OSR) and continual learning are two critical challenges
in machine learning, focusing respectively on detecting novel classes at
inference time and updating models to incorporate the new classes. While many
recent approaches have addressed these problems, particularly OSR, by
heuristically promoting feature diversity, few studies have directly examined
the role that feature diversity plays in tackling them. In this work, we
provide empirical evidence that enhancing feature diversity improves the
recognition of open set samples. Moreover, increased feature diversity also
facilitates both the retention of previously learned data and the integration
of new data in continual learning. We hope our findings can inspire further
research into both practical methods and theoretical understanding in these
domains.

</details>


### [152] [SlimComm: Doppler-Guided Sparse Queries for Bandwidth-Efficient Cooperative 3-D Perception](https://arxiv.org/abs/2508.13007)
*Melih Yazgan,Qiyuan Wu,Iramm Hamdard,Shiqi Li,J. Marius Zoellner*

Main category: cs.CV

TL;DR: SlimComm是一个通信高效的协作感知框架，通过4D雷达多普勒和查询驱动稀疏方案，在保持精度的同时减少90%带宽使用


<details>
  <summary>Details</summary>
Motivation: 解决协作感知中密集BEV特征图传输对车联网带宽的挑战，克服遮挡和传感器范围限制

Method: 构建运动中心动态地图区分动静物体，生成参考查询和探索查询，仅交换查询特定BEV特征并通过多尺度门控可变形注意力融合

Result: 在OPV2V-R和Adver-City-R数据集上验证，带宽降低90%，在不同交通密度和遮挡情况下性能匹配或超越基线

Conclusion: SlimComm成功实现了通信效率与感知精度的平衡，为实际部署提供了可行方案

Abstract: Collaborative perception allows connected autonomous vehicles (CAVs) to
overcome occlusion and limited sensor range by sharing intermediate features.
Yet transmitting dense Bird's-Eye-View (BEV) feature maps can overwhelm the
bandwidth available for inter-vehicle communication. We present SlimComm, a
communication-efficient framework that integrates 4D radar Doppler with a
query-driven sparse scheme. SlimComm builds a motion-centric dynamic map to
distinguish moving from static objects and generates two query types: (i)
reference queries on dynamic and high-confidence regions, and (ii) exploratory
queries probing occluded areas via a two-stage offset. Only query-specific BEV
features are exchanged and fused through multi-scale gated deformable
attention, reducing payload while preserving accuracy. For evaluation, we
release OPV2V-R and Adver-City-R, CARLA-based datasets with per-point Doppler
radar. SlimComm achieves up to 90% lower bandwidth than full-map sharing while
matching or surpassing prior baselines across varied traffic densities and
occlusions. Dataset and code will be available at: https://url.fzi.de/SlimComm.

</details>


### [153] [Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model](https://arxiv.org/abs/2508.13009)
*Xianglong He,Chunli Peng,Zexiang Liu,Boyang Wang,Yifan Zhang,Qi Cui,Fei Kang,Biao Jiang,Mengyin An,Yangyang Ren,Baixin Xu,Hao-Xiang Guo,Kaixiong Gong,Cyrus Wu,Wei Li,Xuchen Song,Yang Liu,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-Game 2.0是一个实时交互式世界模型，通过少步自回归扩散生成高质量长视频，速度达到25FPS


<details>
  <summary>Details</summary>
Motivation: 现有交互式世界模型依赖双向注意力和冗长推理步骤，限制了实时性能，难以模拟需要即时更新的真实世界动态

Method: 包含三个关键组件：1)可扩展的Unreal Engine和GTA5数据生产流水线；2)帧级鼠标键盘输入的动作注入模块；3)基于因果架构的少步蒸馏实时流式视频生成

Result: 能够生成分钟级高质量视频，在多样化场景中以25FPS的超快速度运行

Conclusion: 该框架推进了交互式世界建模研究，开源了模型权重和代码库

Abstract: Recent advances in interactive video generations have demonstrated diffusion
model's potential as world models by capturing complex physical dynamics and
interactive behaviors. However, existing interactive world models depend on
bidirectional attention and lengthy inference steps, severely limiting
real-time performance. Consequently, they are hard to simulate real-world
dynamics, where outcomes must update instantaneously based on historical
context and current actions. To address this, we present Matrix-Game 2.0, an
interactive world model generates long videos on-the-fly via few-step
auto-regressive diffusion. Our framework consists of three key components: (1)
A scalable data production pipeline for Unreal Engine and GTA5 environments to
effectively produce massive amounts (about 1200 hours) of video data with
diverse interaction annotations; (2) An action injection module that enables
frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step
distillation based on the casual architecture for real-time and streaming video
generation. Matrix Game 2.0 can generate high-quality minute-level videos
across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our
model weights and codebase to advance research in interactive world modeling.

</details>


### [154] [EgoTwin: Dreaming Body and View in First Person](https://arxiv.org/abs/2508.13013)
*Jingqiao Xiu,Fangzhou Hong,Yicong Li,Mengze Li,Wentao Wang,Sirui Han,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 提出EgoTwin框架解决第一人称视频与人体运动联合生成任务，通过头中心运动表示和网络交互机制实现视角对齐和因果交互


<details>
  <summary>Details</summary>
Motivation: 现有外中心视频合成已取得进展，但第一人称视频生成仍未被充分探索，需要同时建模第一人称视角内容和穿戴者身体运动引起的相机运动模式

Method: 基于扩散transformer架构构建EgoTwin框架，引入头中心运动表示将人体运动锚定到头部关节，并采用网络交互机制在注意力操作中显式捕获视频与运动间的因果交互

Result: 构建了大规模真实世界同步文本-视频-运动三元组数据集，设计了新颖指标评估视频-运动一致性，大量实验证明了EgoTwin框架的有效性

Conclusion: EgoTwin成功解决了第一人称视频与人体运动联合生成的两个关键挑战：视角对齐和因果交互，为这一新兴领域提供了有效解决方案

Abstract: While exocentric video synthesis has achieved great progress, egocentric
video generation remains largely underexplored, which requires modeling
first-person view content along with camera motion patterns induced by the
wearer's body movements. To bridge this gap, we introduce a novel task of joint
egocentric video and human motion generation, characterized by two key
challenges: 1) Viewpoint Alignment: the camera trajectory in the generated
video must accurately align with the head trajectory derived from human motion;
2) Causal Interplay: the synthesized human motion must causally align with the
observed visual dynamics across adjacent video frames. To address these
challenges, we propose EgoTwin, a joint video-motion generation framework built
on the diffusion transformer architecture. Specifically, EgoTwin introduces a
head-centric motion representation that anchors the human motion to the head
joint and incorporates a cybernetics-inspired interaction mechanism that
explicitly captures the causal interplay between video and motion within
attention operations. For comprehensive evaluation, we curate a large-scale
real-world dataset of synchronized text-video-motion triplets and design novel
metrics to assess video-motion consistency. Extensive experiments demonstrate
the effectiveness of the EgoTwin framework.

</details>


### [155] [HierAdaptMR: Cross-Center Cardiac MRI Reconstruction with Hierarchical Feature Adapters](https://arxiv.org/abs/2508.13026)
*Ruru Xu,Ilkay Oksuz*

Main category: cs.CV

TL;DR: HierAdaptMR是一个分层特征适应框架，通过参数高效适配器解决多中心心脏MRI重建中的域偏移问题，在CMRxRecon2025数据集上表现出优异的跨中心泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习心脏MRI重建在多临床中心部署时面临显著的域偏移挑战，不同扫描仪配置和成像协议导致性能下降。

Method: 使用分层适配器框架：协议级适配器处理序列特定特征，中心级适配器处理扫描仪相关变化，基于变分展开骨干网络。通用适配器通过随机训练实现未见中心的泛化，采用多尺度SSIM损失和频域增强优化。

Result: 在CMRxRecon2025数据集（5+中心、10+扫描仪、9种模态）上进行了全面评估，显示出卓越的跨中心泛化能力，同时保持重建质量。

Conclusion: HierAdaptMR框架有效解决了多中心心脏MRI重建的域适应问题，为临床部署提供了实用的解决方案。

Abstract: Deep learning-based cardiac MRI reconstruction faces significant domain shift
challenges when deployed across multiple clinical centers with heterogeneous
scanner configurations and imaging protocols. We propose HierAdaptMR, a
hierarchical feature adaptation framework that addresses multi-level domain
variations through parameter-efficient adapters. Our method employs
Protocol-Level Adapters for sequence-specific characteristics and Center-Level
Adapters for scanner-dependent variations, built upon a variational unrolling
backbone. A Universal Adapter enables generalization to entirely unseen centers
through stochastic training that learns center-invariant adaptations. The
framework utilizes multi-scale SSIM loss with frequency domain enhancement and
contrast-adaptive weighting for robust optimization. Comprehensive evaluation
on the CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9
modalities demonstrates superior cross-center generalization while maintaining
reconstruction quality. code: https://github.com/Ruru-Xu/HierAdaptMR

</details>


### [156] [IntelliCap: Intelligent Guidance for Consistent View Sampling](https://arxiv.org/abs/2508.13043)
*Ayaka Yasunaga,Hideo Saito,Dieter Schmalstieg,Shohei Mori*

Main category: cs.CV

TL;DR: 一种基于视觉-语言模型的多尺度扫描指导技术，通过识别重要物体并生成球形代理来指导用户获取更包容视角变化特性的图像集。


<details>
  <summary>Details</summary>
Motivation: 高质量新视角合成需要均匀密集的视角采样，但人类拍摄者常因紧快、耐心不足或不理解场景结构而无法满足要求。现有方法对多物体场景和视角依赖材质特性的支持不足。

Method: 利用语义分割和类别识别，通过视觉-语言模型评分重要物体。为高分物体生成球形代理，在扫描过程中指导用户获取更完整的图像覆盖。

Result: 在真实场景中表现出色，超越了传统的视角采样策略。

Conclusion: 该方法能够有效解决多物体场景扫描中的视角采样问题，提高新视角合成的质量。

Abstract: Novel view synthesis from images, for example, with 3D Gaussian splatting,
has made great progress. Rendering fidelity and speed are now ready even for
demanding virtual reality applications. However, the problem of assisting
humans in collecting the input images for these rendering algorithms has
received much less attention. High-quality view synthesis requires uniform and
dense view sampling. Unfortunately, these requirements are not easily addressed
by human camera operators, who are in a hurry, impatient, or lack understanding
of the scene structure and the photographic process. Existing approaches to
guide humans during image acquisition concentrate on single objects or neglect
view-dependent material characteristics. We propose a novel situated
visualization technique for scanning at multiple scales. During the scanning of
a scene, our method identifies important objects that need extended image
coverage to properly represent view-dependent appearance. To this end, we
leverage semantic segmentation and category identification, ranked by a
vision-language model. Spherical proxies are generated around highly ranked
objects to guide the user during scanning. Our results show superior
performance in real scenes compared to conventional view sampling strategies.

</details>


### [157] [Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping](https://arxiv.org/abs/2508.13065)
*Siddharth Khandelwal,Sridhar Kamath,Arjun Jain*

Main category: cs.CV

TL;DR: 基于温稳UNet和ControlNet的端到端氛散方法Odo，通过目标SMPL深度地图指导人体形状编辑，在保持外观和背景细节的同时实现了较低的重建误差（7.5mm）。


<details>
  <summary>Details</summary>
Motivation: 人体形状编辑目前缺乏大规模公开数据集，现有方法存在身体比例不真实、纹理扭曲和背景不一致等问题，需要发展更现实的形状编辑技术。

Method: 构建包含18,573张图片的大规模数据集，提出Odo模型：使用冻结的UNet保留外观和背景细节，通过ControlNet使用目标SMPL深度地图指导形状变换，支持语义属性指导的直观身体重塑。

Result: 方法在重建误差上显著优于基线方法（每顶点误差7.5mm vs 13.6mm），能够生成现实的结果并准确匹配目标形状。

Conclusion: 该研究提供了首个大规模人体形状编辑数据集和高效的氛散基方法Odo，为人体形状操纵领域的研究和应用提供了重要支撑。

Abstract: Human shape editing enables controllable transformation of a person's body
shape, such as thin, muscular, or overweight, while preserving pose, identity,
clothing, and background. Unlike human pose editing, which has advanced
rapidly, shape editing remains relatively underexplored. Current approaches
typically rely on 3D morphable models or image warping, often introducing
unrealistic body proportions, texture distortions, and background
inconsistencies due to alignment errors and deformations. A key limitation is
the lack of large-scale, publicly available datasets for training and
evaluating body shape manipulation methods. In this work, we introduce the
first large-scale dataset of 18,573 images across 1523 subjects, specifically
designed for controlled human shape editing. It features diverse variations in
body shape, including fat, muscular and thin, captured under consistent
identity, clothing, and background conditions. Using this dataset, we propose
Odo, an end-to-end diffusion-based method that enables realistic and intuitive
body reshaping guided by simple semantic attributes. Our approach combines a
frozen UNet that preserves fine-grained appearance and background details from
the input image with a ControlNet that guides shape transformation using target
SMPL depth maps. Extensive experiments demonstrate that our method outperforms
prior approaches, achieving per-vertex reconstruction errors as low as 7.5mm,
significantly lower than the 13.6mm observed in baseline methods, while
producing realistic results that accurately match the desired target shapes.

</details>


### [158] [Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation](https://arxiv.org/abs/2508.13068)
*Tanjim Islam Riju,Shuchismita Anwar,Saman Sarker Joy,Farig Sadeque,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: 一个两阶段多模态框架，通过视线导向对比学习提升肺部X光疾病分类性能，并通过模块化报告生成管道提高医学报告质量和可解释性。


<details>
  <summary>Details</summary>
Motivation: 利用攻下师眼动信号来提升医学影像分析的准确性和可解释性，解决传统方法在疾病分类和报告生成中的局限性。

Method: 第一阶段：视线导向对比学习架构，整合视觉特征、临床标签、盲框和眼动信号，采用多项视线注意力损失函数。第二阶段：模块化报告生成管道，提取信心度加权诊断关键词，映射到解剖区域，通过结构化提示生成区域对齐句子。

Result: 统计分析显示：结合眼动数据后，F1分数从0.597提升到0.631（+5.70%），AUC从0.821提升到0.849（+3.41%），精度和召回率都有显著改善。报告生成质量在临床关键词召回率和ROUGE重合指标上都有提升。

Conclusion: 集成眼动数据能够同时提升疾病分类性能和生成医学报告的可解释性，证明了视线导向注意力监督的有效性。

Abstract: We propose a two-stage multimodal framework that enhances disease
classification and region-aware radiology report generation from chest X-rays,
leveraging the MIMIC-Eye dataset. In the first stage, we introduce a
gaze-guided contrastive learning architecture for disease classification. It
integrates visual features, clinical labels, bounding boxes, and radiologist
eye-tracking signals and is equipped with a novel multi-term gaze-attention
loss combining MSE, KL divergence, correlation, and center-of-mass alignment.
Incorporating fixations improves F1 score from 0.597 to 0.631 (+5.70%) and AUC
from 0.821 to 0.849 (+3.41%), while also improving precision and recall,
highlighting the effectiveness of gaze-informed attention supervision. In the
second stage, we present a modular report generation pipeline that extracts
confidence-weighted diagnostic keywords, maps them to anatomical regions using
a curated dictionary constructed from domain-specific priors, and generates
region-aligned sentences via structured prompts. This pipeline improves report
quality as measured by clinical keyword recall and ROUGE overlap. Our results
demonstrate that integrating gaze data improves both classification performance
and the interpretability of generated medical reports.

</details>


### [159] [ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset](https://arxiv.org/abs/2508.13078)
*Qingwen Zeng,Juan E. Tapia,Izan Garcia,Juan M. Espin,Christoph Busch*

Main category: cs.CV

TL;DR: 使用Stable Diffusion生成合成真实ID卡图像来解决实际真实样本数量不足的问题，提升验证攻击检测系统的性能


<details>
  <summary>Details</summary>
Motivation: ID卡验证攻击检测(PAD)系统面临真实样本数量不足和攻击手段多样化的挑战，现有算法多关注攻击样本生成而忽视了真实样本的限制

Method: 采用Stable Diffusion生成器生成合成真实ID卡图像，并在从头训练的系统和商业解决方案中评估这些新图像

Result: 生成的合成图像被PAD系统识别为真实样本，对检测性能产生积极影响，有效缓解了数据限制问题

Conclusion: 通过生成式AI技术生成合成真实样本是提升ID卡PAD系统性能的有效方法，为解决真实数据稀缺问题提供了新思路

Abstract: Nowadays, the development of a Presentation Attack Detection (PAD) system for
ID cards presents a challenge due to the lack of images available to train a
robust PAD system and the increase in diversity of possible attack instrument
species. Today, most algorithms focus on generating attack samples and do not
take into account the limited number of bona fide images. This work is one of
the first to propose a method for mimicking bona fide images by generating
synthetic versions of them using Stable Diffusion, which may help improve the
generalisation capabilities of the detector. Furthermore, the new images
generated are evaluated in a system trained from scratch and in a commercial
solution. The PAD system yields an interesting result, as it identifies our
images as bona fide, which has a positive impact on detection performance and
data restrictions.

</details>


### [160] [Checkmate: interpretable and explainable RSVQA is the endgame](https://arxiv.org/abs/2508.13086)
*Lucrezia Tosato,Christel Tartini Chappuis,Syrielle Montariol,Flora Weissgerber,Sylvain Lobry,Devis Tuia*

Main category: cs.CV

TL;DR: 提出新的Chessboard数据集和Checkmate模型，解决远感视觉问答中的可解释性和偏差问题


<details>
  <summary>Details</summary>
Motivation: 远感视觉问答系统存在可解释性不足、模型决策不可信赖以及数据集偏差导致的短路学习问题

Method: 创建包含312万个问题的Chessboard数据集，答案分布均衡且与图像像素相关联；开发Checkmate模型，能够识别决策的关键图像区域

Result: 通过多种模型架构的实验验证，该方法提高了系统透明度和决策可信赖性

Conclusion: 该研究为远感视觉问答系统提供了更可解释、更可信赖的解决方案，通过细粒度视觉推理改善了模型的可理解性

Abstract: Remote Sensing Visual Question Answering (RSVQA) presents unique challenges
in ensuring that model decisions are both understandable and grounded in visual
content. Current models often suffer from a lack of interpretability and
explainability, as well as from biases in dataset distributions that lead to
shortcut learning. In this work, we tackle these issues by introducing a novel
RSVQA dataset, Chessboard, designed to minimize biases through 3'123'253
questions and a balanced answer distribution. Each answer is linked to one or
more cells within the image, enabling fine-grained visual reasoning.
  Building on this dataset, we develop an explainable and interpretable model
called Checkmate that identifies the image cells most relevant to its
decisions. Through extensive experiments across multiple model architectures,
we show that our approach improves transparency and supports more trustworthy
decision-making in RSVQA systems.

</details>


### [161] [DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation](https://arxiv.org/abs/2508.13091)
*Zihua Liu,Yizhou Li,Songyan Zhang,Masatoshi Okutomi*

Main category: cs.CV

TL;DR: 提出DMS方法，利用扩散模型生成新视角图像来解决自监督立体匹配和单目深度估计中的遮挡问题，无需额外标注数据即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 自监督立体匹配和单目深度估计方法在遮挡区域存在对应像素缺失问题，导致光度重建模糊，需要解决这一挑战来建立明确的光度对应关系。

Method: 基于扩散模型生成沿极线方向的新视角图像（左-左视图、右-右视图和中间视图），补充遮挡像素，实现明确的光度重建。该方法与模型无关，仅需未标注的立体图像对。

Result: 在多个基准数据集上达到最先进性能，异常值减少高达35%，证明方法的有效性。

Conclusion: DMS是一种即插即用的免费方法，能无缝增强自监督立体匹配和单目深度估计性能，仅依赖未标注立体图像对即可实现训练和合成。

Abstract: While supervised stereo matching and monocular depth estimation have advanced
significantly with learning-based algorithms, self-supervised methods using
stereo images as supervision signals have received relatively less focus and
require further investigation. A primary challenge arises from ambiguity
introduced during photometric reconstruction, particularly due to missing
corresponding pixels in ill-posed regions of the target view, such as
occlusions and out-of-frame areas. To address this and establish explicit
photometric correspondences, we propose DMS, a model-agnostic approach that
utilizes geometric priors from diffusion models to synthesize novel views along
the epipolar direction, guided by directional prompts. Specifically, we
finetune a Stable Diffusion model to simulate perspectives at key positions:
left-left view shifted from the left camera, right-right view shifted from the
right camera, along with an additional novel view between the left and right
cameras. These synthesized views supplement occluded pixels, enabling explicit
photometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play''
method that seamlessly enhances self-supervised stereo matching and monocular
depth estimation, and relies solely on unlabeled stereo image pairs for both
training and synthesizing. Extensive experiments demonstrate the effectiveness
of our approach, with up to 35% outlier reduction and state-of-the-art
performance across multiple benchmark datasets.

</details>


### [162] [Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants](https://arxiv.org/abs/2508.13101)
*Miftahul Huda,Arsyiah Azahra,Putri Maulida Chairani,Dimas Rizky Ramadhani,Nabila Azhari,Ade Lailani*

Main category: cs.CV

TL;DR: RT-DETR-L模型在检测精度略低于RT-DETR-X的情况下，具有更快的推理速度，更适合实时海滩垃圾检测的实际部署。


<details>
  <summary>Details</summary>
Motivation: 海岸污染是全球紧迫的环境问题，需要可扩展的自动化监测解决方案。本研究旨在评估最先进的RT-DETR模型在自动化海滩垃圾检测和计数中的效果。

Method: 使用公开的海岸垃圾数据集，对RT-DETR-Large和RT-DETR-Extra-Large两个变体进行严格的比较分析，评估其检测精度和推理速度。

Result: RT-DETR-X模型获得略高的精度（mAP@50: 0.816, mAP@50-95: 0.612），但RT-DETR-L模型推理速度更快（20.1ms vs 34.5ms）。

Conclusion: RT-DETR-L模型在处理速度和检测精度之间提供了更好的平衡，更适合实时现场部署，为基于Transformer的先进检测器在环境保护中的应用提供了重要见解。

Abstract: Coastal pollution is a pressing global environmental issue, necessitating
scalable and automated solutions for monitoring and management. This study
investigates the efficacy of the Real-Time Detection Transformer (RT-DETR), a
state-of-the-art, end-to-end object detection model, for the automated
detection and counting of beach litter. A rigorous comparative analysis is
conducted between two model variants, RT-DETR-Large (RT-DETR-L) and
RT-DETR-Extra-Large (RT-DETR-X), trained on a publicly available dataset of
coastal debris. The evaluation reveals that the RT-DETR-X model achieves
marginally superior accuracy, with a mean Average Precision at 50\% IoU
(mAP@50) of 0.816 and a mAP@50-95 of 0.612, compared to the RT-DETR-L model's
0.810 and 0.606, respectively. However, this minor performance gain is realized
at a significant computational cost; the RT-DETR-L model demonstrates a
substantially faster inference time of 20.1 ms versus 34.5 ms for the
RT-DETR-X. The findings suggest that the RT-DETR-L model offers a more
practical and efficient solution for real-time, in-field deployment due to its
superior balance of processing speed and detection accuracy. This research
provides valuable insights into the application of advanced Transformer-based
detectors for environmental conservation, highlighting the critical trade-offs
between model complexity and operational viability.

</details>


### [163] [Precise Action-to-Video Generation Through Visual Action Prompts](https://arxiv.org/abs/2508.13104)
*Yuang Wang,Chao Wen,Haoyu Guo,Sida Peng,Minghan Qin,Hujun Bao,Xiaowei Zhou,Ruizhen Hu*

Main category: cs.CV

TL;DR: 视觉动作提示（VAP）通过将动作渲染为精确的视觉骨架表示，解决了动作到视频生成中的精度与通用性之间的交换问题，支持复杂高自由度交互动作的精确控制和跨域动态转移。


<details>
  <summary>Details</summary>
Motivation: 现有的动作驱动视频生成方法遇到了精度与通用性的两难问题：文本、原始动作或粗糕掩码方法通用性好但缺乏精度，而代理中心的动作信号虽有精度但跨域转移性差。需要找到一种方法来平衡动作精度和动态转移性。

Method: 提出将动作"渲染"为精确的视觉提示（视觉骨架）作为域无关表示，保持几何精度和跨域适配性。构建了从人-物交互（HOI）和灵巧机器人操控数据源生成骨架的稳健流水线，通过轻量级微调将视觉骨架集成到预训练的视频生成模型中。

Result: 在EgoVid、RT-1和DROID数据集上的实验证明了方法的有效性，能够实现复杂交互动作的精确控制，同时保持跨域动态学习能力。

Conclusion: 视觉动作提示作为统一的动作表示，有效解决了动作驱动视频生成中的精度-通用性交换问题，为复杂高自由度交互动作的精确控制提供了可行的解决方案。

Abstract: We present visual action prompts, a unified action representation for
action-to-video generation of complex high-DoF interactions while maintaining
transferable visual dynamics across domains. Action-driven video generation
faces a precision-generality trade-off: existing methods using text, primitive
actions, or coarse masks offer generality but lack precision, while
agent-centric action signals provide precision at the cost of cross-domain
transferability. To balance action precision and dynamic transferability, we
propose to "render" actions into precise visual prompts as domain-agnostic
representations that preserve both geometric precision and cross-domain
adaptability for complex actions; specifically, we choose visual skeletons for
their generality and accessibility. We propose robust pipelines to construct
skeletons from two interaction-rich data sources - human-object interactions
(HOI) and dexterous robotic manipulation - enabling cross-domain training of
action-driven generative models. By integrating visual skeletons into
pretrained video generation models via lightweight fine-tuning, we enable
precise action control of complex interaction while preserving the learning of
cross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the
effectiveness of our proposed approach. Project page:
https://zju3dv.github.io/VAP/.

</details>


### [164] [Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence](https://arxiv.org/abs/2508.13139)
*Ling-Hao Chen,Yuhong Zhang,Zixin Yin,Zhiyang Dou,Xin Chen,Jingbo Wang,Taku Komura,Lei Zhang*

Main category: cs.CV

TL;DR: Motion2Motion是一个无需训练的新框架，通过在源骨骼和目标骨骼之间建立稀疏骨骼对应关系，实现不同拓扑结构角色之间的动画迁移。


<details>
  <summary>Details</summary>
Motivation: 解决不同骨骼拓扑结构角色间动画迁移的挑战，特别是当源骨骼和目标骨骼拓扑不一致时，难以建立直接的一对一骨骼对应关系，且缺乏大规模配对运动数据集。

Method: 提出训练免费的Motion2Motion框架，仅需目标骨骼上的一个或几个示例运动，通过建立源骨骼和目标骨骼之间的稀疏骨骼对应关系来实现动画迁移。

Result: 通过全面的定性和定量评估，证明Motion2Motion在相似骨骼和跨物种骨骼迁移场景中都能实现高效可靠的性能。

Conclusion: 该方法在下游应用和用户界面中成功集成，展现了工业应用的潜力，代码和数据已公开。

Abstract: This work studies the challenge of transfer animations between characters
whose skeletal topologies differ substantially. While many techniques have
advanced retargeting techniques in decades, transfer motions across diverse
topologies remains less-explored. The primary obstacle lies in the inherent
topological inconsistency between source and target skeletons, which restricts
the establishment of straightforward one-to-one bone correspondences. Besides,
the current lack of large-scale paired motion datasets spanning different
topological structures severely constrains the development of data-driven
approaches. To address these limitations, we introduce Motion2Motion, a novel,
training-free framework. Simply yet effectively, Motion2Motion works with only
one or a few example motions on the target skeleton, by accessing a sparse set
of bone correspondences between the source and target skeletons. Through
comprehensive qualitative and quantitative evaluations, we demonstrate that
Motion2Motion achieves efficient and reliable performance in both
similar-skeleton and cross-species skeleton transfer scenarios. The practical
utility of our approach is further evidenced by its successful integration in
downstream applications and user interfaces, highlighting its potential for
industrial applications. Code and data are available at
https://lhchen.top/Motion2Motion.

</details>


### [165] [Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/abs/2508.13142)
*Zhongang Cai,Yubo Wang,Qingping Sun,Ruisi Wang,Chenyang Gu,Wanqi Yin,Zhiqian Lin,Zhitao Yang,Chen Wei,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Jiaqi Li,Xiangyu Fan,Hanming Deng,Lewei Lu,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: 研究评估GPT-5在空间智能方面的表现，发现其虽然强大但仍落后于人类水平，并指出专有模型在极难问题上没有明显优势


<details>
  <summary>Details</summary>
Motivation: 多模态模型在空间理解和推理方面仍有显著局限性，需要系统性地评估GPT-5等领先模型的空间智能水平

Method: 提出统一的空间任务分类法，在8个关键测试集上评估专有和开源模型，消耗超过10亿token，并进行定性分析

Result: GPT-5展现前所未有的空间智能实力，但在广泛任务中仍落后于人类；专有模型在极难问题上没有决定性优势；识别出多模态模型的具体挑战问题

Conclusion: 多模态模型在空间智能方面仍有显著缺口，需要进一步研究来缩小与人类能力的差距

Abstract: Multi-modal models have achieved remarkable progress in recent years.
Nevertheless, they continue to exhibit notable limitations in spatial
understanding and reasoning, which are fundamental capabilities to achieving
artificial general intelligence. With the recent release of GPT-5, allegedly
the most powerful AI model to date, it is timely to examine where the leading
models stand on the path toward spatial intelligence. First, we propose a
comprehensive taxonomy of spatial tasks that unifies existing benchmarks and
discuss the challenges in ensuring fair evaluation. We then evaluate
state-of-the-art proprietary and open-source models on eight key benchmarks, at
a cost exceeding one billion total tokens. Our empirical study reveals that (1)
GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)
still falls short of human performance across a broad spectrum of tasks.
Moreover, we (3) identify the more challenging spatial intelligence problems
for multi-modal models, and (4) proprietary models do not exhibit a decisive
advantage when facing the most difficult problems. In addition, we conduct a
qualitative evaluation across a diverse set of scenarios that are intuitive for
humans yet fail even the most advanced multi-modal models.

</details>


### [166] [IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion](https://arxiv.org/abs/2508.13153)
*Wenhao Hu,Zesheng Li,Haonan Zhou,Liu Liu,Xuexiang Wen,Zhizhong Su,Xi Li,Gaoang Wang*

Main category: cs.CV

TL;DR: IGFuse是一个新颖的3D场景重建框架，通过融合多视角扫描数据来解决物体遮挡问题，实现高保真渲染和物体级场景操作


<details>
  <summary>Details</summary>
Motivation: 现有3D场景重建方法存在物体遮挡、传感器覆盖有限、多阶段流程复杂且容易出错等问题，需要一种更有效的方法来重建完整交互式3D场景

Method: 构建分割感知的高斯场，通过双向光度和语义一致性约束，引入伪中间场景状态进行统一对齐，采用协作共剪枝策略优化几何结构

Result: 实验验证了框架对新场景配置的强泛化能力，在真实世界3D重建和真实到仿真的转换中表现出色

Conclusion: IGFuse能够在不依赖密集观测或复杂流程的情况下，实现高质量的场景重建和交互操作，为3D视觉和机器人应用提供了有效解决方案

Abstract: Reconstructing complete and interactive 3D scenes remains a fundamental
challenge in computer vision and robotics, particularly due to persistent
object occlusions and limited sensor coverage. Multiview observations from a
single scene scan often fail to capture the full structural details. Existing
approaches typically rely on multi stage pipelines, such as segmentation,
background completion, and inpainting or require per-object dense scanning,
both of which are error-prone, and not easily scalable. We propose IGFuse, a
novel framework that reconstructs interactive Gaussian scene by fusing
observations from multiple scans, where natural object rearrangement between
captures reveal previously occluded regions. Our method constructs segmentation
aware Gaussian fields and enforces bi-directional photometric and semantic
consistency across scans. To handle spatial misalignments, we introduce a
pseudo-intermediate scene state for unified alignment, alongside collaborative
co-pruning strategies to refine geometry. IGFuse enables high fidelity
rendering and object level scene manipulation without dense observations or
complex pipelines. Extensive experiments validate the framework's strong
generalization to novel scene configurations, demonstrating its effectiveness
for real world 3D reconstruction and real-to-simulation transfer. Our project
page is available online.

</details>


### [167] [4DNeX: Feed-Forward 4D Generative Modeling Made Easy](https://arxiv.org/abs/2508.13154)
*Zhaoxi Chen,Tianqi Liu,Long Zhuo,Jiawei Ren,Zeng Tao,He Zhu,Fangzhou Hong,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 4DNeX是首个从单张图像生成4D（动态3D）场景的前馈框架，通过微调预训练视频扩散模型实现高效的端到端图像到4D生成


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖计算密集型优化或需要多帧视频输入的问题，提供高效的图像到4D生成方案

Method: 1)构建大规模4D数据集4DNeX-10M；2)引入统一6D视频表示联合建模RGB和XYZ序列；3)提出适配策略将预训练视频扩散模型重用于4D建模

Result: 生成高质量动态点云，支持新视角视频合成，在效率和泛化性方面优于现有4D生成方法

Conclusion: 为图像到4D建模提供了可扩展解决方案，为生成式4D世界模型模拟动态场景演化奠定了基础

Abstract: We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,
dynamic 3D) scene representations from a single image. In contrast to existing
methods that rely on computationally intensive optimization or require
multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D
generation by fine-tuning a pretrained video diffusion model. Specifically, 1)
to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale
dataset with high-quality 4D annotations generated using advanced
reconstruction approaches. 2) we introduce a unified 6D video representation
that jointly models RGB and XYZ sequences, facilitating structured learning of
both appearance and geometry. 3) we propose a set of simple yet effective
adaptation strategies to repurpose pretrained video diffusion models for 4D
modeling. 4DNeX produces high-quality dynamic point clouds that enable
novel-view video synthesis. Extensive experiments demonstrate that 4DNeX
outperforms existing 4D generation methods in efficiency and generalizability,
offering a scalable solution for image-to-4D modeling and laying the foundation
for generative 4D world models that simulate dynamic scene evolution.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [168] [Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video](https://arxiv.org/abs/2508.11836)
*Dave Goel,Matthew Guzdial,Anurag Sarkar*

Main category: cs.AI

TL;DR: 提出FAE方法从游戏视频中学习神经符号世界模型，用新的DSL Retro Coder表示，相比之前方法学习更精确的环境模型和更通用的代码


<details>
  <summary>Details</summary>
Motivation: 现有世界模型通常是神经网络表示，难以迁移学习到的环境动态和解释性，需要更好的可解释和可迁移的世界模型表示方法

Method: FAE方法从游戏视频中学习神经符号世界模型，使用新的领域特定语言Retro Coder将模型表示为程序

Result: 相比之前的世界模型方法，FAE学习了更精确的环境模型；相比之前的DSL方法，生成了更通用的代码

Conclusion: FAE方法能够学习到更精确和可解释的世界模型表示，在神经符号建模方面取得了改进

Abstract: World models are defined as a compressed spatial and temporal learned
representation of an environment. The learned representation is typically a
neural network, making transfer of the learned environment dynamics and
explainability a challenge. In this paper, we propose an approach, Finite
Automata Extraction (FAE), that learns a neuro-symbolic world model from
gameplay video represented as programs in a novel domain-specific language
(DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more
precise model of the environment and more general code than prior DSL-based
approaches.

</details>


### [169] [EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models](https://arxiv.org/abs/2508.11850)
*Milad Yazdani,Mahdi Mostajabdaveh,Samin Aref,Zirui Zhou*

Main category: cs.AI

TL;DR: EvoCut是一个自动化生成整数规划加速割的框架，结合大语言模型和进化搜索，无需人工干预即可显著提升求解器性能


<details>
  <summary>Details</summary>
Motivation: 整数规划是组合优化的核心但NP难问题，传统方法依赖专家手工设计加速割，这一过程需要深厚专业知识且难以自动化

Method: 结合LLM和进化搜索：1）LLM初始化多样化候选割；2）验证集评估割的质量（保持最优解和切割分数解能力）；3）通过进化交叉变异迭代优化种群

Result: 相比标准方法，EvoCut在固定时间内将最优性间隙降低17-57%，获得相同解的速度提升4倍，在相同时间内获得更高质量解

Conclusion: EvoCut能够可靠地生成、改进和验证可泛化到未见实例的割，完全自动化且无需人工专家输入

Abstract: Integer programming lies at the heart of crucial combinatorial optimization
tasks but remains challenging due to its NP-hard nature. An effective approach
for practically solving integer programs is the manual design of acceleration
cuts, i.e. inequalities that improve solver performance. However, this creative
process demands deep expertise and is yet to be automated. Our proposed
framework, EvoCut, automates the generation of acceleration cuts by combining
large language models (LLMs) with an evolutionary search. EvoCut (i)
initializes a diverse population of candidate cuts via an LLM-based initializer
agent; (ii) for each cut empirically evaluates both preservation of the optimal
solution and its ability to cut off fractional solutions across a verification
set; and (iii) iteratively refines the population through evolutionary
crossover and mutation agents. We quantify each cut's utility by its relative
reduction in the solver's optimality gap. Our comparisons against standard
integer programming practice show that EvoCut reduces optimality gap by 17-57%
within a fixed time. It obtains the same solutions up to 4 times as fast, and
obtains higher-quality solutions within the same time limit. Requiring no human
expert input, EvoCut reliably generates, improves, and empirically verifies
cuts that generalize to unseen instances. The code is available at
https://github.com/milad1378yz/EvoCut.

</details>


### [170] [LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework](https://arxiv.org/abs/2508.11860)
*Frazier N. Baker,Daniel Adu-Ampratwum,Reza Averly,Botao Yu,Huan Sun,Xia Ning*

Main category: cs.AI

TL;DR: LARC是首个基于LLM的约束逆合成规划代理框架，通过工具化推理的代理反馈来指导路线生成，在48个约束任务上达到72.9%的成功率，接近专家水平但耗时更少。


<details>
  <summary>Details</summary>
Motivation: 约束逆合成规划是化学中重要但具有挑战性的过程，需要从商业可用起始材料到目标分子的合成路线识别，同时满足实际约束条件。现有方法难以有效处理复杂约束。

Method: LARC框架采用代理作为评判员（Agent-as-a-Judge），将代理约束评估直接整合到逆合成规划过程中，使用基于工具推理的代理反馈来指导和约束路线生成。

Result: 在精心策划的48个约束逆合成规划任务（涵盖3种约束类型）上，LARC达到72.9%的成功率，显著优于LLM基线方法，在更短时间内接近人类专家水平。

Conclusion: LARC框架具有可扩展性，是朝着为人类专家开发有效代理工具或合作科学家的第一步，可用于约束逆合成规划。

Abstract: Large language model (LLM) agent evaluators leverage specialized tools to
ground the rational decision-making of LLMs, making them well-suited to aid in
scientific discoveries, such as constrained retrosynthesis planning.
Constrained retrosynthesis planning is an essential, yet challenging, process
within chemistry for identifying synthetic routes from commercially available
starting materials to desired target molecules, subject to practical
constraints. Here, we present LARC, the first LLM-based Agentic framework for
Retrosynthesis planning under Constraints. LARC incorporates agentic constraint
evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis
planning process, using agentic feedback grounded in tool-based reasoning to
guide and constrain route generation. We rigorously evaluate LARC on a
carefully curated set of 48 constrained retrosynthesis planning tasks across 3
constraint types. LARC achieves a 72.9% success rate on these tasks, vastly
outperforming LLM baselines and approaching human expert-level success in
substantially less time. The LARC framework is extensible, and serves as a
first step towards an effective agentic tool or a co-scientist to human experts
for constrained retrosynthesis.

</details>


### [171] [QuarkMed Medical Foundation Model Technical Report](https://arxiv.org/abs/2508.11894)
*Ao Li,Bin Yan,Bingfeng Cai,Chenxi Li,Cunzhong Zhao,Fugen Yao,Gaoqiang Liu,Guanjun Jiang,Jian Xu,Liang Dong,Liansheng Sun,Rongshen Zhang,Xiaolei Gui,Xin Liu,Xin Shang,Yao Wu,Yu Cao,Zhenxin Ma,Zhuang Jia*

Main category: cs.AI

TL;DR: QuarkMed是一个高性能医疗基础模型，通过医学数据处理、检索增强生成和大规模可验证强化学习，在中国执业医师考试中达到70%准确率，已服务数百万用户。


<details>
  <summary>Details</summary>
Motivation: 医疗任务需要高度专业的知识、专业准确性和定制能力，现有大语言模型在医疗应用中需要更可靠的基础模型支持。

Method: 利用精选医学数据处理、医学内容检索增强生成(RAG)和大规模可验证强化学习管道来开发医疗基础模型。

Result: 在中国医学执照考试中达到70%的准确率，在多样化医疗基准测试中展现出强大的泛化能力。

Conclusion: QuarkMed提供了一个强大而多功能的个人医疗AI解决方案，已经在ai.quark.cn服务超过百万用户。

Abstract: Recent advancements in large language models have significantly accelerated
their adoption in healthcare applications, including AI-powered medical
consultations, diagnostic report assistance, and medical search tools. However,
medical tasks often demand highly specialized knowledge, professional accuracy,
and customization capabilities, necessitating a robust and reliable foundation
model. QuarkMed addresses these needs by leveraging curated medical data
processing, medical-content Retrieval-Augmented Generation (RAG), and a
large-scale, verifiable reinforcement learning pipeline to develop a
high-performance medical foundation model. The model achieved 70% accuracy on
the Chinese Medical Licensing Examination, demonstrating strong generalization
across diverse medical benchmarks. QuarkMed offers a powerful yet versatile
personal medical AI solution, already serving over millions of users at
ai.quark.cn.

</details>


### [172] [CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs](https://arxiv.org/abs/2508.11944)
*Hongtao Liu,Zhicheng Du,Zihe Wang,Weiran Shen*

Main category: cs.AI

TL;DR: 提出了CHBench评估框架，基于认知层次模型来评估大语言模型的战略推理能力，发现聊天机制会降低战略推理，而记忆机制能增强推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖效用性能指标来评估LLMs的游戏能力，但这些指标不够稳健，受对手行为和游戏结构变化影响较大。需要更稳健的评估框架来衡量LLMs的战略推理能力。

Method: 提出认知层次基准(CHBench)，基于行为经济学中的认知层次模型，假设智能体具有有限理性。通过三阶段系统框架，在15个精选的正规形式游戏中使用6个最先进LLMs的行为数据进行评估。

Result: 实验显示LLMs在不同对手间展现出一致的战略推理水平，证实了框架的稳健性和泛化能力。聊天机制显著降低战略推理性能，而记忆机制能增强推理能力。

Conclusion: CHBench是评估LLM能力的有前景工具，具有重要的未来研究和实际应用潜力，为战略推理能力评估提供了更稳健的框架。

Abstract: Game-playing ability serves as an indicator for evaluating the strategic
reasoning capability of large language models (LLMs). While most existing
studies rely on utility performance metrics, which are not robust enough due to
variations in opponent behavior and game structure. To address this limitation,
we propose \textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation
framework inspired by the cognitive hierarchy models from behavioral economics.
We hypothesize that agents have bounded rationality -- different agents behave
at varying reasoning depths/levels. We evaluate LLMs' strategic reasoning
through a three-phase systematic framework, utilizing behavioral data from six
state-of-the-art LLMs across fifteen carefully selected normal-form games.
Experiments show that LLMs exhibit consistent strategic reasoning levels across
diverse opponents, confirming the framework's robustness and generalization
capability. We also analyze the effects of two key mechanisms (Chat Mechanism
and Memory Mechanism) on strategic reasoning performance. Results indicate that
the Chat Mechanism significantly degrades strategic reasoning, whereas the
Memory Mechanism enhances it. These insights position CHBench as a promising
tool for evaluating LLM capabilities, with significant potential for future
research and practical applications.

</details>


### [173] [Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models](https://arxiv.org/abs/2508.11953)
*Yuan Li,Zhengzhong Liu,Eric Xing*

Main category: cs.AI

TL;DR: 通过建模有效数据转移和利用缩放定律来优化SFT数据混合配置，该算法能够达到与网格搜索相当的性能，同时改善验证损失和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 目前在为大语言模型进行监督精调时，如何优化数据混合配置仍然是个未充分探索的重要领域，需要更科学的方法来最小化验证损失。

Method: 将数据混合问题框架为优化问题，通过建模有效数据转移和利用精调缩放定律来参数化损失函数，在小规模数据混合上进行实验拟合参数并求解最优权重。

Result: 算法在所有领域都取得了优秀的总体和个别性能，与网格搜索确定的最优权重表现相当，每个领域损失仅比网格搜索的最佳域损失平均高出0.66%，重新加权常用SFT数据集后能够改善验证损失和下游任务表现。

Conclusion: 该方法不仅能够有效优化SFT数据混合，还可以推广到指导领域特定模型的数据选择，为SFT过程提供了深度见解。

Abstract: Optimizing data mixtures for supervised fine-tuning (SFT) of large language
models (LLMs) is critical for developing general-purpose models, yet this area
remains underexplored. In this paper, we frame data mixing as an optimization
problem and introduce a novel method designed to minimize validation loss. Our
approach parametrizes the loss by modeling effective data transferred and
leveraging scaling laws for fine-tuning. By experimenting with various
small-scale data mixtures, we fit these parameters and derive the optimal
weights. We provide both mathematical proofs and empirical results
demonstrating that our algorithm achieves excellent overall and individual
performance across all domains. Through controlled experiments, we show that
models trained with our optimized weights perform on par with those using
optimal weights determined via grid search, with per-domain loss only 0.66%
higher than the best domain loss from grid search on average. Additionally, we
show that reweighting popular SFT datasets using our method improves both
validation loss and downstream performance. Finally, we discuss how our method
can generalize to guide data selection for domain-specific models and provide
insights into SFT.

</details>


### [174] [UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting](https://arxiv.org/abs/2508.11954)
*Sehyuk Park,Soyeon Caren Han,Eduard Hovy*

Main category: cs.AI

TL;DR: UniCast是一个参数高效的多模态时间序列预测框架，通过整合视觉和文本信息来增强传统时间序列基础模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型主要在单模态设置下运行，忽略了现实世界中常伴随时间序列数据的丰富多模态上下文（如视觉和文本信号），限制了预测性能。

Method: 提出UniCast框架，使用预训练的视觉和文本编码器提取模态特定嵌入，通过软提示调优与冻结的时间序列基础模型集成，实现高效的跨模态交互和最小参数更新。

Result: 在多个时间序列预测基准测试中，UniCast始终显著优于所有现有的时间序列基础模型基线。

Conclusion: 多模态上下文在推动下一代通用时间序列预测器发展中起着关键作用，UniCast框架为有效整合多模态信息提供了参数高效的解决方案。

Abstract: Time series forecasting is a foundational task across domains, such as
finance, healthcare, and environmental monitoring. While recent advances in
Time Series Foundation Models (TSFMs) have demonstrated strong generalisation
through large-scale pretraining, existing models operate predominantly in a
unimodal setting, ignoring the rich multimodal context, such as visual and
textual signals, that often accompanies time series data in real-world
scenarios. This paper introduces a novel parameter-efficient multimodal
framework, UniCast, that extends TSFMs to jointly leverage time series, vision,
and text modalities for enhanced forecasting performance. Our method integrates
modality-specific embeddings from pretrained Vision and Text Encoders with a
frozen TSFM via soft prompt tuning, enabling efficient adaptation with minimal
parameter updates. This design not only preserves the generalisation strength
of the foundation model but also enables effective cross-modal interaction.
Extensive experiments across diverse time-series forecasting benchmarks
demonstrate that UniCast consistently and significantly outperforms all
existing TSFM baselines. The findings highlight the critical role of multimodal
context in advancing the next generation of general-purpose time series
forecasters.

</details>


### [175] [Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index](https://arxiv.org/abs/2508.11959)
*Xuanxiang Huang,Olivier Létoffé,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 本文提出了两种基于博弈论的新型特征重要性评分方法，通过考虑非弱溯因解释集来量化特征在排除对抗样本方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于博弈论的特征归因方法主要关注弱溯因解释(WAXp)，但忽略了非WAXp集的重要信息。由于形式化解释(XPs)与对抗样本(AExs)之间的关系，非WAXp集也能传递重要信息。

Method: 利用Shapley值和Banzhaf指数设计两种新颖的特征重要性评分方法，在计算特征贡献时考虑非WAXp集，量化每个特征在排除对抗样本方面的有效性。

Result: 提出了两种新的特征重要性评分方法，能够更全面地评估特征贡献，并分析了这些评分方法的性质和计算复杂度。

Conclusion: 通过考虑非WAXp集，新的特征重要性评分方法能够提供更全面的特征归因分析，特别是在识别和排除对抗样本方面具有更好的效果。

Abstract: Feature attribution methods based on game theory are ubiquitous in the field
of eXplainable Artificial Intelligence (XAI). Recent works proposed rigorous
feature attribution using logic-based explanations, specifically targeting
high-stakes uses of machine learning (ML) models. Typically, such works exploit
weak abductive explanation (WAXp) as the characteristic function to assign
importance to features. However, one possible downside is that the contribution
of non-WAXp sets is neglected. In fact, non-WAXp sets can also convey important
information, because of the relationship between formal explanations (XPs) and
adversarial examples (AExs). Accordingly, this paper leverages Shapley value
and Banzhaf index to devise two novel feature importance scores. We take into
account non-WAXp sets when computing feature contribution, and the novel scores
quantify how effective each feature is at excluding AExs. Furthermore, the
paper identifies properties and studies the computational complexity of the
proposed scores.

</details>


### [176] [Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven Synthesis and Candidate-Conditioned Answering](https://arxiv.org/abs/2508.11975)
*Gongyao Jiang,Qiong Luo*

Main category: cs.AI

TL;DR: 通过代码生成和执行的图表合成流水线，给出对齐的图表-问题-答案三元组，并设计候选条件化答题过程，在无人工标注或外部模型的情况下实现了视觉语言模型的自我改进


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在图表理解任务中遇到困难，特别是准确的图表描述和复杂推理。合成数据生成是有前景的解决方案，但通常面临噪声标签的挑战

Method: 首先引入通过代码生成和执行的图表合成流水线，生成对齐的图表-问题-答案三元组，确保合成数据的可靠性。还设计了候选条件化答题过程，让VLM先生成多个回应，然后通过上下文化这些候选来合成最终答案

Result: 实验表明了显著改进，在完全自我改进范式下，比初始VLM的准确率提高了最多15.50个百分点

Conclusion: 该方法能够在不依赖人工标注数据或外部模型的情况下，通过合成数据生成和候选条件化答题机制，有效地改善视觉语言模型在图表理解任务上的表现

Abstract: Vision Language Models (VLMs) often struggle with chart understanding tasks,
particularly in accurate chart description and complex reasoning. Synthetic
data generation is a promising solution, while usually facing the challenge of
noise labels. To address this challenge, we first introduce a chart synthesis
pipeline that generates aligned chart-question-answer triplets through code
generation and execution, ensuring the reliability of synthetic data without
human intervention. Furthermore, inspired by test-time scaling that increases
inference budget and thereby improves performance, we design a
candidate-conditioned answering process. The VLM first generates multiple
responses per query, and then synthesizes the final answer by contextualizing
these candidates. Experiments demonstrate significant improvements, with up to
15.50 points accuracy gain over the initial VLM, in a fully self-improving
paradigm without either human-labeled data or external models.

</details>


### [177] [FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction](https://arxiv.org/abs/2508.11987)
*Zhiyuan Zeng,Jiashuo Liu,Siyuan Chen,Tianci He,Yali Liao,Jinpeng Wang,Zaiyuan Wang,Yang Yang,Lingyue Yin,Mingren Yin,Zhenwei Zhu,Tianle Cai,Zehui Chen,Jiecao Chen,Yantao Du,Xiang Gao,Jiacheng Guo,Liang Hu,Jianpeng Jiao,Xiangsheng Li,Jingkai Liu,Shuang Ni,Zhoufutu Wen,Ge Zhang,Kaiyuan Zhang,Xin Zhou,Jose Blanchet,Xipeng Qiu,Mengdi Wang,Wenhao Huang*

Main category: cs.AI

TL;DR: FutureX是一个专门为LLM智能体设计的动态实时未来预测评估基准，支持每日实时更新，通过自动化流程避免数据污染，评估了25个模型在动态环境中的自适应推理能力。


<details>
  <summary>Details</summary>
Motivation: 未来预测对LLM智能体来说是一项复杂任务，需要高水平分析思维和信息处理能力，但目前缺乏大规模评估基准，主要由于处理实时更新和获取及时准确答案的挑战。

Method: 构建FutureX动态实时评估基准，支持每日实时更新，采用自动化问题收集和答案收集流程，评估25个LLM/智能体模型（包括具有推理、搜索能力和外部工具集成的模型）。

Result: 提供了对智能体在未来导向任务中失败模式和性能缺陷的深入分析，包括对虚假网页的脆弱性和时间有效性等问题。

Conclusion: 目标是建立一个动态、无污染的评估标准，推动LLM智能体在复杂推理和预测思维方面达到专业人类分析师的水平。

Abstract: Future prediction is a complex task for LLM agents, requiring a high level of
analytical thinking, information gathering, contextual understanding, and
decision-making under uncertainty. Agents must not only gather and interpret
vast amounts of dynamic information but also integrate diverse data sources,
weigh uncertainties, and adapt predictions based on emerging trends, just as
human experts do in fields like politics, economics, and finance. Despite its
importance, no large-scale benchmark exists for evaluating agents on future
prediction, largely due to challenges in handling real-time updates and
retrieving timely, accurate answers. To address this, we introduce
$\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically
designed for LLM agents performing future prediction tasks. FutureX is the
largest and most diverse live benchmark for future prediction, supporting
real-time daily updates and eliminating data contamination through an automated
pipeline for question gathering and answer collection. We evaluate 25 LLM/agent
models, including those with reasoning, search capabilities, and integration of
external tools such as the open-source Deep Research Agent and closed-source
Deep Research models. This comprehensive evaluation assesses agents' adaptive
reasoning and performance in dynamic environments. Additionally, we provide
in-depth analyses of agents' failure modes and performance pitfalls in
future-oriented tasks, including the vulnerability to fake web pages and the
temporal validity. Our goal is to establish a dynamic, contamination-free
evaluation standard that drives the development of LLM agents capable of
performing at the level of professional human analysts in complex reasoning and
predictive thinking.

</details>


### [178] [Modeling Relational Logic Circuits for And-Inverter Graph Convolutional Network](https://arxiv.org/abs/2508.11991)
*Weihao Sun*

Main category: cs.AI

TL;DR: AIGer是一个用于AIG图表示学习的新方法，通过节点逻辑特征初始化和异构图卷积网络，在信号概率预测和真值表距离预测任务中显著优于现有最佳模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法同时建模AIG图的功能和结构特征，以及动态信息传播能力不足的问题，以提升EDA领域的电路设计自动化效率。

Method: 包含两个组件：1)节点逻辑特征初始化嵌入组件，将逻辑节点投影到独立语义空间；2)AIG特征学习网络组件，使用异构图卷积网络设计动态关系权重矩阵和差异化信息聚合方法。

Result: 在信号概率预测任务中，MAE和MSE分别提升18.95%和44.44%；在真值表距离预测任务中，MAE和MSE分别提升33.57%和14.79%。

Conclusion: AIGer通过有效结合功能与结构特征建模，显著提升了AIG图的表示学习性能，为EDA领域的电路设计自动化提供了更强大的工具。

Abstract: The automation of logic circuit design enhances chip performance, energy
efficiency, and reliability, and is widely applied in the field of Electronic
Design Automation (EDA).And-Inverter Graphs (AIGs) efficiently represent,
optimize, and verify the functional characteristics of digital circuits,
enhancing the efficiency of EDA development.Due to the complex structure and
large scale of nodes in real-world AIGs, accurate modeling is challenging,
leading to existing work lacking the ability to jointly model functional and
structural characteristics, as well as insufficient dynamic information
propagation capability.To address the aforementioned challenges, we propose
AIGer.Specifically, AIGer consists of two components: 1) Node logic feature
initialization embedding component and 2) AIGs feature learning network
component.The node logic feature initialization embedding component projects
logic nodes, such as AND and NOT, into independent semantic spaces, to enable
effective node embedding for subsequent processing.Building upon this, the AIGs
feature learning network component employs a heterogeneous graph convolutional
network, designing dynamic relationship weight matrices and differentiated
information aggregation approaches to better represent the original structure
and information of AIGs.The combination of these two components enhances
AIGer's ability to jointly model functional and structural characteristics and
improves its message passing capability. Experimental results indicate that
AIGer outperforms the current best models in the Signal Probability Prediction
(SSP) task, improving MAE and MSE by 18.95\% and 44.44\%, respectively. In the
Truth Table Distance Prediction (TTDP) task, AIGer achieves improvements of
33.57\% and 14.79\% in MAE and MSE, respectively, compared to the
best-performing models.

</details>


### [179] [AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning](https://arxiv.org/abs/2508.11995)
*Xuyang Zhao,Shiwan Zhao,Hualong Yu,Liting Zhang,Qicheng Li*

Main category: cs.AI

TL;DR: AgentCDM是一个基于大语言模型的多智能体系统协作决策框架，通过结构化推理范式缓解认知偏见，从被动答案选择转向主动假设评估，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统协作决策方法存在缺陷：要么依赖单一智能体的"独裁"策略（易受认知偏见影响），要么使用"投票"方法（无法充分利用集体智慧），需要更有效的协作决策框架。

Method: 受认知科学中竞争假设分析(ACH)启发，提出结构化推理范式：1)使用显式ACH脚手架指导模型进行结构化推理；2)逐步移除脚手架以促进自主泛化的两阶段训练范式。

Result: 在多个基准数据集上的实验表明，AgentCDM实现了最先进的性能，并展现出强大的泛化能力。

Conclusion: AgentCDM有效提高了多智能体系统中协作决策的质量和鲁棒性，验证了其通过结构化推理缓解认知偏见、促进主动假设评估的有效性。

Abstract: Multi-agent systems (MAS) powered by large language models (LLMs) hold
significant promise for solving complex decision-making tasks. However, the
core process of collaborative decision-making (CDM) within these systems
remains underexplored. Existing approaches often rely on either ``dictatorial"
strategies that are vulnerable to the cognitive biases of a single agent, or
``voting-based" methods that fail to fully harness collective intelligence. To
address these limitations, we propose \textbf{AgentCDM}, a structured framework
for enhancing collaborative decision-making in LLM-based multi-agent systems.
Drawing inspiration from the Analysis of Competing Hypotheses (ACH) in
cognitive science, AgentCDM introduces a structured reasoning paradigm that
systematically mitigates cognitive biases and shifts decision-making from
passive answer selection to active hypothesis evaluation and construction. To
internalize this reasoning process, we develop a two-stage training paradigm:
the first stage uses explicit ACH-inspired scaffolding to guide the model
through structured reasoning, while the second stage progressively removes this
scaffolding to encourage autonomous generalization. Experiments on multiple
benchmark datasets demonstrate that AgentCDM achieves state-of-the-art
performance and exhibits strong generalization, validating its effectiveness in
improving the quality and robustness of collaborative decisions in MAS.

</details>


### [180] [AI Models for Depressive Disorder Detection and Diagnosis: A Review](https://arxiv.org/abs/2508.12022)
*Dorsa Macky Aleagha,Payam Zohari,Mostafa Haghir Chehreghani*

Main category: cs.AI

TL;DR: 本文对55项关键研究进行系统综述，提出了基于临床任务、数据模态和计算模型的分层分类法，总结了抑郁症AI诊断领域的三大趋势：图神经网络主导脑连接建模、大语言模型处理语言数据、多模态融合和可解释性成为新兴焦点。


<details>
  <summary>Details</summary>
Motivation: 抑郁症是全球主要致残原因，但目前诊断仍依赖主观临床评估。AI技术有望开发客观、可扩展和及时的诊断工具，需要系统梳理该领域的研究现状和发展趋势。

Method: 通过对55项关键研究的系统综述，建立了分层分类法（临床任务×数据模态×计算模型），深入分析当前最先进的AI方法，包括图神经网络、大语言模型和混合方法等。

Result: 识别出三大主要趋势：图神经网络在脑连接建模中占主导地位，大语言模型在语言数据处理中崛起，多模态融合、可解释性和算法公平性成为新兴研究方向。同时提供了公共数据集和评估指标的实用指南。

Conclusion: 本综述综合了当前进展并突出开放挑战，为计算精神病学领域的未来创新提供了全面的路线图，推动抑郁症诊断向更客观、可扩展的方向发展。

Abstract: Major Depressive Disorder is one of the leading causes of disability
worldwide, yet its diagnosis still depends largely on subjective clinical
assessments. Integrating Artificial Intelligence (AI) holds promise for
developing objective, scalable, and timely diagnostic tools. In this paper, we
present a comprehensive survey of state-of-the-art AI methods for depression
detection and diagnosis, based on a systematic review of 55 key studies. We
introduce a novel hierarchical taxonomy that structures the field by primary
clinical task (diagnosis vs. prediction), data modality (text, speech,
neuroimaging, multimodal), and computational model class (e.g., graph neural
networks, large language models, hybrid approaches). Our in-depth analysis
reveals three major trends: the predominance of graph neural networks for
modeling brain connectivity, the rise of large language models for linguistic
and conversational data, and an emerging focus on multimodal fusion,
explainability, and algorithmic fairness. Alongside methodological insights, we
provide an overview of prominent public datasets and standard evaluation
metrics as a practical guide for researchers. By synthesizing current advances
and highlighting open challenges, this survey offers a comprehensive roadmap
for future innovation in computational psychiatry.

</details>


### [181] [Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems](https://arxiv.org/abs/2508.12026)
*Szymon Pawlonka,Mikołaj Małkiński,Jacek Mańdziuk*

Main category: cs.AI

TL;DR: 本文介绍了Bongard-RWR+数据集，包含5400个实例，使用VLM生成的真实世界图像来表示原始Bongard问题的抽象概念，评估发现VLM在细粒度概念识别上存在困难。


<details>
  <summary>Details</summary>
Motivation: 现有的Bongard问题数据集要么使用合成图像缺乏真实复杂性，要么使用真实图像但概念过于简单，且手动构建的Bongard-RWR数据集规模太小（仅60个实例），限制了评估的鲁棒性。

Method: 基于Bongard-RWR，使用Pixtral-12B描述手动筛选的图像并生成新描述，用Flux.1-dev从描述合成图像，手动验证生成图像是否忠实反映目标概念，构建了5400个实例的数据集。

Result: 评估显示最先进的VLM能够识别粗粒度视觉概念，但在辨别细粒度概念方面持续存在困难，突显了其推理能力的局限性。

Conclusion: Bongard-RWR+数据集为抽象视觉推理提供了更全面的测试平台，揭示了当前VLM在细粒度概念理解方面的不足，为未来模型改进指明了方向。

Abstract: Bongard Problems (BPs) provide a challenging testbed for abstract visual
reasoning (AVR), requiring models to identify visual concepts fromjust a few
examples and describe them in natural language. Early BP benchmarks featured
synthetic black-and-white drawings, which might not fully capture the
complexity of real-world scenes. Subsequent BP datasets employed real-world
images, albeit the represented concepts are identifiable from high-level image
features, reducing the task complexity. Differently, the recently released
Bongard-RWR dataset aimed at representing abstract concepts formulated in the
original BPs using fine-grained real-world images. Its manual construction,
however, limited the dataset size to just $60$ instances, constraining
evaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset
composed of $5\,400$ instances that represent original BP abstract concepts
using real-world-like images generated via a vision language model (VLM)
pipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually
curated images and generate new descriptions aligned with the underlying
concepts, use Flux.1-dev to synthesize images from these descriptions, and
manually verify that the generated images faithfully reflect the intended
concepts. We evaluate state-of-the-art VLMs across diverse BP formulations,
including binary and multiclass classification, as well as textual answer
generation. Our findings reveal that while VLMs can recognize coarse-grained
visual concepts, they consistently struggle with discerning fine-grained
concepts, highlighting limitations in their reasoning capabilities.

</details>


### [182] [Active inference for action-unaware agents](https://arxiv.org/abs/2508.12027)
*Filippo Torresan,Keisuke Suzuki,Ryota Kanai,Manuel Baltieri*

Main category: cs.AI

TL;DR: 主动推断是一种基于贝叶斯推断的认知形式描述，本文比较了动作意识和动作无意识代理在导航任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 主动推断领域中存在不同的动作规划策略，有些假设代理知道自己的动作，而有些则需要从观察中推断动作行为。本文想要比较这两类代理的性能差异。

Method: 通过在两个导航任务中比较动作意识代理和动作无意识代理的表现，以了解动作知识对规划能力的影响。

Result: 动作无意识代理虽然处于严重不利地位，但仍能达到与动作意识代理相当的性能水平。

Conclusion: 这一结果表明了主动推断框架中不同动作规划策略的灰度性和实用性，为理解认知过程中的动作处理提供了重要见解。

Abstract: Active inference is a formal approach to study cognition based on the notion
that adaptive agents can be seen as engaging in a process of approximate
Bayesian inference, via the minimisation of variational and expected free
energies. Minimising the former provides an account of perceptual processes and
learning as evidence accumulation, while minimising the latter describes how
agents select their actions over time. In this way, adaptive agents are able to
maximise the likelihood of preferred observations or states, given a generative
model of the environment. In the literature, however, different strategies have
been proposed to describe how agents can plan their future actions. While they
all share the notion that some kind of expected free energy offers an
appropriate way to score policies, sequences of actions, in terms of their
desirability, there are different ways to consider the contribution of past
motor experience to the agent's future behaviour. In some approaches, agents
are assumed to know their own actions, and use such knowledge to better plan
for the future. In other approaches, agents are unaware of their actions, and
must infer their motor behaviour from recent observations in order to plan for
the future. This difference reflects a standard point of departure in two
leading frameworks in motor control based on the presence, or not, of an
efference copy signal representing knowledge about an agent's own actions. In
this work we compare the performances of action-aware and action-unaware agents
in two navigations tasks, showing how action-unaware agents can achieve
performances comparable to action-aware ones while at a severe disadvantage.

</details>


### [183] [MAPF-World: Action World Model for Multi-Agent Path Finding](https://arxiv.org/abs/2508.12087)
*Zhanjiang Yang,Meng Li,Yang Shen,Yueming Li,Lijun Sun*

Main category: cs.AI

TL;DR: MAPF-World是一个用于多智能体路径规划的自动回归动作世界模型，通过显式建模环境动态和时空依赖关系，实现了超越局部观察的远见决策，在模型大小减少96.5%和数据减少92%的情况下仍优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的分散式可学习求解器在复杂长期规划场景中表现受限，主要因为缺乏对环境时间动态和智能体间依赖关系的充分建模，导致性能下降。

Method: 提出MAPF-World自动回归动作世界模型，统一情境理解和动作生成，通过预测未来状态和动作来显式建模空间特征和时间依赖关系，并引入基于真实场景的自动地图生成器来增强基准测试。

Result: 大量实验表明MAPF-World在零样本泛化到分布外案例方面优于最先进的可学习求解器，模型大小减少96.5%，数据需求减少92%。

Conclusion: MAPF-World通过显式建模环境动态和时空依赖关系，实现了更明智、协调和远见的决策，特别适用于复杂多智能体设置，展示了优异的泛化能力和效率。

Abstract: Multi-agent path finding (MAPF) is the problem of planning conflict-free
paths from the designated start locations to goal positions for multiple
agents. It underlies a variety of real-world tasks, including multi-robot
coordination, robot-assisted logistics, and social navigation. Recent
decentralized learnable solvers have shown great promise for large-scale MAPF,
especially when leveraging foundation models and large datasets. However, these
agents are reactive policy models and exhibit limited modeling of environmental
temporal dynamics and inter-agent dependencies, resulting in performance
degradation in complex, long-term planning scenarios. To address these
limitations, we propose MAPF-World, an autoregressive action world model for
MAPF that unifies situation understanding and action generation, guiding
decisions beyond immediate local observations. It improves situational
awareness by explicitly modeling environmental dynamics, including spatial
features and temporal dependencies, through future state and actions
prediction. By incorporating these predicted futures, MAPF-World enables more
informed, coordinated, and far-sighted decision-making, especially in complex
multi-agent settings. Furthermore, we augment MAPF benchmarks by introducing an
automatic map generator grounded in real-world scenarios, capturing practical
map layouts for training and evaluating MAPF solvers. Extensive experiments
demonstrate that MAPF-World outperforms state-of-the-art learnable solvers,
showcasing superior zero-shot generalization to out-of-distribution cases.
Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced
data.

</details>


### [184] [Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios](https://arxiv.org/abs/2508.12100)
*Daniel Burkhardt,Xiangwei Cheng*

Main category: cs.AI

TL;DR: 提出了ReT-Eval框架，通过两阶段方法从稀疏知识图谱中提取语义相关知识结构，并使用奖励引导策略修剪推理线程，以生成有效的目标导向推理


<details>
  <summary>Details</summary>
Motivation: 当前推理模型缺乏显式语义层次结构、用户-领域知识对齐，以及修剪推理线程的原则性机制，导致输出冗长且无法有效指导用户进行目标导向推理

Method: 原型启发的两阶段推理线程评估框架：第一阶段使用图神经网络从稀疏领域知识图谱中提取语义相关知识结构，并用大语言模型知识丰富以解决知识差异；第二阶段使用奖励引导策略评估和修剪线程以保持语义连贯性

Result: 实验和专家评估表明，ReT-Eval增强了用户理解能力，并优于最先进的推理模型

Conclusion: ReT-Eval框架通过结构化知识重用和有效的线程修剪机制，成功解决了当前推理模型在交互式问题解决场景中的局限性

Abstract: Reasoning in interactive problem solving scenarios requires models to
construct reasoning threads that reflect user understanding and align with
structured domain knowledge. However, current reasoning models often lack
explicit semantic hierarchies, user-domain knowledge alignment, and principled
mechanisms to prune reasoning threads for effectiveness. These limitations
result in lengthy generic output that does not guide users through
goal-oriented reasoning steps. To address this, we propose a
prototype-inspired, two-phases Reasoning-Threads-Evaluation (ReT-Eval)
framework, drawing inspiration from human-like reasoning strategies that
emphasize structured knowledge reuse. In the first phase, semantically relevant
knowledge structures are extracted from a sparse domain knowledge graph using a
graph neural network and enriched with intrinsic large language model knowledge
to resolve knowledge discrepancies. In the second phase, these threads are
evaluated and pruned using a reward-guided strategy aimed at maintaining
semantic coherence to generate effective reasoning threads. Experiments and
expert evaluations show that ReT-Eval enhances user understanding and
outperforms state-of-the-art reasoning models.

</details>


### [185] [MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization](https://arxiv.org/abs/2508.12149)
*Haochen You,Baojing Liu*

Main category: cs.AI

TL;DR: MOVER是一个新的多模态学习框架，结合最优传输软对齐和几何体积正则化，在文本-视频-音频检索任务中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有的多模态对比学习方法在双模态设置中有效，但难以扩展到多模态场景，且在高维空间中缺乏语义结构

Method: 结合最优传输的软对齐机制和几何体积最小化目标(GAVE)，以模态无关的方式实现跨所有模态的一致对齐

Result: 在文本-视频-音频检索任务中，MOVER在零样本和微调设置下都显著优于现有最先进方法，展现出更好的泛化能力和更强的结构一致性

Conclusion: MOVER通过最优传输和几何正则化的结合，成功构建了语义对齐且结构化的多模态表示，为多模态学习提供了新的有效方法

Abstract: Recent advances in multimodal learning have largely relied on pairwise
contrastive objectives to align different modalities, such as text, video, and
audio, in a shared embedding space. While effective in bi-modal setups, these
approaches struggle to generalize across multiple modalities and often lack
semantic structure in high-dimensional spaces. In this paper, we propose MOVER,
a novel framework that combines optimal transport-based soft alignment with
volume-based geometric regularization to build semantically aligned and
structured multimodal representations. By integrating a transport-guided
matching mechanism with a geometric volume minimization objective (GAVE), MOVER
encourages consistent alignment across all modalities in a modality-agnostic
manner. Experiments on text-video-audio retrieval tasks demonstrate that MOVER
significantly outperforms prior state-of-the-art methods in both zero-shot and
finetuned settings. Additional analysis shows improved generalization to unseen
modality combinations and stronger structural consistency in the learned
embedding space.

</details>


### [186] [RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards](https://arxiv.org/abs/2508.12165)
*Rohit Krishnan,Jon Evans*

Main category: cs.AI

TL;DR: RLNVR框架使用非验证奖励训练语言模型，通过基线归一化和语义相似性奖励转移处理噪声反馈，在社交媒体内容生成中展现显著改进


<details>
  <summary>Details</summary>
Motivation: 传统RLHF需要昂贵的人工验证奖励信号，在现实场景中不实用，需要处理噪声、非验证的实时反馈信号

Method: 结合基线归一化、语义相似性奖励转移、GSPO策略优化和可选UED课程学习，处理噪声隐式奖励

Result: 实验结果显示内容质量和训练稳定性显著提升，使用Bluesky实际互动数据进行原型验证

Conclusion: 提出了实用的RLNVR框架，成功整合现有技术处理非验证奖励，为现实世界应用提供可行解决方案

Abstract: This paper introduces RLNVR (Reinforcement Learning from Non-Verified
Rewards), a framework for training language models using noisy, real-world
feedback signals without requiring explicit human verification. Traditional
RLHF requires expensive, verified reward signals that are impractical in many
real-world domains. RLNVR addresses this challenge through baseline
normalization and semantic similarity-based reward transfer. We demonstrate
RLNVR through Walter, a prototype system that optimizes social media content
generation using actual engagement data from Bluesky. Our experimental results
show significant improvements in content quality and training stability, with
comprehensive evaluation planned for future work. Positioning: We present a
practical framework that combines RLNVR with GSPO (Group Sequence Policy
Optimization) and an optional UED (Unsupervised Environment Design) curriculum
to improve stability and diversity under noisy, implicit rewards. To our
knowledge, combining GSPO-style normalization with a UED-style curriculum for
LLM content generation from implicit social engagement has not been previously
documented in this applied setting; we frame this as an applied integration
rather than a new algorithm.

</details>


### [187] [Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting](https://arxiv.org/abs/2508.12260)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Ananya Sharma,Emily Martin,Marisa Eisenberg*

Main category: cs.AI

TL;DR: Mantis是一个基于机制模拟训练的传染病预测基础模型，无需真实数据训练就能在多种疾病和地区进行准确预测，性能超越39个专家调优模型


<details>
  <summary>Details</summary>
Motivation: 解决传统传染病预测模型需要疾病特定数据、专门训练和专家调优的限制，特别是在新发疫情或资源匮乏地区的预测难题

Method: 基于超过4亿天爆发动态的机制模拟训练，涵盖多种病原体、传播方式、干预措施和监测伪影，无需真实世界数据

Result: 在六种疾病测试中超越所有39个专家调优模型，包括CDC COVID-19预测中心的所有模型，能泛化到新的流行病学机制，提供8周预测范围

Conclusion: Mantis作为下一代疾病预测系统的基础，具有通用性、可解释性和在传统模型失败场景下的部署能力

Abstract: Infectious disease forecasting in novel outbreaks or low resource settings
has been limited by the need for disease-specific data, bespoke training, and
expert tuning. We introduce Mantis, a foundation model trained entirely on
mechanistic simulations, which enables out-of-the-box forecasting across
diseases, regions, and outcomes, even in settings with limited historical data.
Mantis is built on over 400 million simulated days of outbreak dynamics
spanning diverse pathogens, transmission modes, interventions, and surveillance
artifacts. Despite requiring no real-world data during training, Mantis
outperformed 39 expert-tuned models we tested across six diseases, including
all models in the CDC's COVID-19 Forecast Hub. Mantis generalized to novel
epidemiological regimes, including diseases with held-out transmission
mechanisms, demonstrating that it captures fundamental contagion dynamics.
Critically, Mantis is mechanistically interpretable, enabling public health
decision-makers to identify the latent drivers behind its predictions. Finally,
Mantis delivers accurate forecasts at 8-week horizons, more than doubling the
actionable range of most models, enabling proactive public health planning.
Together, these capabilities position Mantis as a foundation for
next-generation disease forecasting systems: general, interpretable, and
deployable where traditional models fail.

</details>


### [188] [RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts](https://arxiv.org/abs/2508.12291)
*Xuming He,Zhiyuan You,Junchao Gong,Couhua Liu,Xiaoyu Yue,Peiqin Zhuang,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: RadarQA是一个基于多模态大语言模型的天气预报质量分析方法，通过整合物理属性和详细评估报告，在雷达预报质量评估任务上超越现有通用MLLMs。


<details>
  <summary>Details</summary>
Motivation: 传统基于分数的评估指标在描述能力、可解释性和动态演化理解方面远不如气象专家，需要更先进的工具来克服这些挑战。

Method: 提出RadarQA方法，结合关键物理属性和详细评估报告；设计混合标注流程（人工专家标注+自动启发式）；构建RQA-70K大规模数据集；采用多阶段训练策略迭代提升模型性能。

Result: 广泛实验表明RadarQA在所有评估设置中都优于现有的通用多模态大语言模型。

Conclusion: RadarQA展示了在天气预报质量分析方面的巨大潜力，为多模态质量分析提供了新的任务范式和解决方案。

Abstract: Quality analysis of weather forecasts is an essential topic in meteorology.
Although traditional score-based evaluation metrics can quantify certain
forecast errors, they are still far from meteorological experts in terms of
descriptive capability, interpretability, and understanding of dynamic
evolution. With the rapid development of Multi-modal Large Language Models
(MLLMs), these models become potential tools to overcome the above challenges.
In this work, we introduce an MLLM-based weather forecast analysis method,
RadarQA, integrating key physical attributes with detailed assessment reports.
We introduce a novel and comprehensive task paradigm for multi-modal quality
analysis, encompassing both single frame and sequence, under both rating and
assessment scenarios. To support training and benchmarking, we design a hybrid
annotation pipeline that combines human expert labeling with automated
heuristics. With such an annotation method, we construct RQA-70K, a large-scale
dataset with varying difficulty levels for radar forecast quality evaluation.
We further design a multi-stage training strategy that iteratively improves
model performance at each stage. Extensive experiments show that RadarQA
outperforms existing general MLLMs across all evaluation settings, highlighting
its potential for advancing quality analysis in weather prediction.

</details>


### [189] [Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback](https://arxiv.org/abs/2508.12338)
*Wenzhen Yuan,Shengji Tang,Weihao Lin,Jiacheng Ruan,Ganqu Cui,Bo Zhang,Tao Chen,Ting Liu,Yuzhuo Fu,Peng Ye,Lei Bai*

Main category: cs.AI

TL;DR: RLCCF是一种无需外部监督的多模型协作强化学习框架，通过集体一致性投票提供奖励信号，解决了传统RL方法依赖昂贵人工标注和单一模型过度自信的问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法依赖昂贵的人工标注数据或复杂奖励模型，限制了可扩展性。现有的自反馈方法受限于单一模型能力，容易产生错误答案的过度自信、奖励攻击甚至训练崩溃。

Method: 提出RLCCF框架，通过最大化集体一致性来优化模型集体能力。联合训练多样化的LLM集成，通过集体输出投票提供奖励信号，每个模型的投票权重由其自一致性分数决定。

Result: 在四个主流开源LLM和四个数学推理基准测试中，平均准确率相对提升16.72%。不仅提升单个模型性能，还使集体多数投票准确率提升4.51%。

Conclusion: RLCCF通过多模型协同进化有效提升了推理能力，扩展了模型集体的能力边界，为无监督强化学习提供了新思路。

Abstract: Reinforcement learning (RL) has significantly enhanced the reasoning
capabilities of large language models (LLMs), but its reliance on expensive
human-labeled data or complex reward models severely limits scalability. While
existing self-feedback methods aim to address this problem, they are
constrained by the capabilities of a single model, which can lead to
overconfidence in incorrect answers, reward hacking, and even training
collapse. To this end, we propose Reinforcement Learning from Coevolutionary
Collective Feedback (RLCCF), a novel RL framework that enables multi-model
collaborative evolution without external supervision. Specifically, RLCCF
optimizes the ability of a model collective by maximizing its Collective
Consistency (CC), which jointly trains a diverse ensemble of LLMs and provides
reward signals by voting on collective outputs. Moreover, each model's vote is
weighted by its Self-Consistency (SC) score, ensuring that more confident
models contribute more to the collective decision. Benefiting from the diverse
output distributions and complementary abilities of multiple LLMs, RLCCF
enables the model collective to continuously enhance its reasoning ability
through coevolution. Experiments on four mainstream open-source LLMs across
four mathematical reasoning benchmarks demonstrate that our framework yields
significant performance gains, achieving an average relative improvement of
16.72\% in accuracy. Notably, RLCCF not only improves the performance of
individual models but also enhances the group's majority-voting accuracy by
4.51\%, demonstrating its ability to extend the collective capability boundary
of the model collective.

</details>


### [190] [Hierarchical knowledge guided fault intensity diagnosis of complex industrial systems](https://arxiv.org/abs/2508.12375)
*Yu Sha,Shuiping Gou,Bo Liu,Johannes Faber,Ningtao Liu,Stefan Schramm,Horst Stoecker,Thomas Steckenreiter,Domagoj Vnucec,Nadine Wetzstein,Andreas Widl,Kai Zhou*

Main category: cs.AI

TL;DR: 基于图卷积网络的层次知识导向故障强度诊断框架，通过重加权层次知识相关矩阵提高故障类别间依赖关系的学习效果，在多个工业数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有故障强度诊断方法基于思维链而没有考虑目标类别间的依赖关系，需要提出能够捐据和利用这些依赖关系的方法来提高诊断性能。

Method: 提出层次知识导向的HKG框架，使用图卷积网络将类表征的层次拓扑图映射为一组相互依赖的全局层次分类器，并开发重加权层次知识相关矩阵(Re-HKCM)方案来嵌入类间层次知识。

Result: 在四个来自不同工业领域的实际数据集上进行了广泛实验，所有结果都显示出优异的性能，超过了最新的故障强度诊断方法。

Conclusion: HKG框架通过层次知识导向和图卷积网络技术，有效地捐据了类别间的依赖关系，提高了故障强度诊断的准确性和性能。

Abstract: Fault intensity diagnosis (FID) plays a pivotal role in monitoring and
maintaining mechanical devices within complex industrial systems. As current
FID methods are based on chain of thought without considering dependencies
among target classes. To capture and explore dependencies, we propose a
hierarchical knowledge guided fault intensity diagnosis framework (HKG)
inspired by the tree of thought, which is amenable to any representation
learning methods. The HKG uses graph convolutional networks to map the
hierarchical topological graph of class representations into a set of
interdependent global hierarchical classifiers, where each node is denoted by
word embeddings of a class. These global hierarchical classifiers are applied
to learned deep features extracted by representation learning, allowing the
entire model to be end-to-end learnable. In addition, we develop a re-weighted
hierarchical knowledge correlation matrix (Re-HKCM) scheme by embedding
inter-class hierarchical knowledge into a data-driven statistical correlation
matrix (SCM) which effectively guides the information sharing of nodes in
graphical convolutional neural networks and avoids over-smoothing issues. The
Re-HKCM is derived from the SCM through a series of mathematical
transformations. Extensive experiments are performed on four real-world
datasets from different industrial domains (three cavitation datasets from
SAMSON AG and one existing publicly) for FID, all showing superior results and
outperform recent state-of-the-art FID methods.

</details>


### [191] [GraphCogent: Overcoming LLMs' Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding](https://arxiv.org/abs/2508.12379)
*Rongzheng Wang,Qizhi Chen,Yihong Huang,Yizhuo Ma,Muquan Li,Jiakai Li,Ke Qin,Guangchun Luo,Shuang Liang*

Main category: cs.AI

TL;DR: GraphCogent是一个基于工作记忆模型的协作代理框架，通过将图推理分解为感知、缓冲和执行三个认知过程，有效解决了大语言模型在处理复杂图拓扑和多步推理时的局限性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在小规模图推理任务上表现良好，但在处理具有复杂查询的真实世界图时失败，主要原因是无法同时有效处理复杂图拓扑和执行多步推理。

Method: 提出了GraphCogent框架，包含三个模块：感知模块通过子图采样标准化图文本表示，缓冲模块集成和索引多格式图数据，执行模块结合工具调用和模型生成进行高效推理。

Result: 基于Llama3.1-8B的GraphCogent相比DeepSeek-R1(671B)提升50%性能，相比最先进的基于代理的基线在准确率上提升20%，同时在内工具集任务上减少80%的token使用，在外工具集任务上减少30%的token使用。

Conclusion: GraphCogent框架通过认知过程分解有效提升了LLMs在图推理任务上的性能，同时显著降低了计算成本，为解决复杂图推理问题提供了有效方案。

Abstract: Large language models (LLMs) show promising performance on small-scale graph
reasoning tasks but fail when handling real-world graphs with complex queries.
This phenomenon stems from LLMs' inability to effectively process complex graph
topology and perform multi-step reasoning simultaneously. To address these
limitations, we propose GraphCogent, a collaborative agent framework inspired
by human Working Memory Model that decomposes graph reasoning into specialized
cognitive processes: sense, buffer, and execute. The framework consists of
three modules: Sensory Module standardizes diverse graph text representations
via subgraph sampling, Buffer Module integrates and indexes graph data across
multiple formats, and Execution Module combines tool calling and model
generation for efficient reasoning. We also introduce Graph4real, a
comprehensive benchmark contains with four domains of real-world graphs (Web,
Social, Transportation, and Citation) to evaluate LLMs' graph reasoning
capabilities. Our Graph4real covers 21 different graph reasoning tasks,
categorized into three types (Structural Querying, Algorithmic Reasoning, and
Predictive Modeling tasks), with graph scales that are 10 times larger than
existing benchmarks. Experiments show that Llama3.1-8B based GraphCogent
achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B).
Compared to state-of-the-art agent-based baseline, our framework outperforms by
20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30%
for out-toolset tasks. Code will be available after review.

</details>


### [192] [Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning](https://arxiv.org/abs/2508.12425)
*Phuong Minh Nguyen,Tien Huu Dang,Naoya Inoue*

Main category: cs.AI

TL;DR: Symbolic-Aided Chain-of-Thought (CoT) 通过将轻量级符号表示整合到少样本提示中，改进了标准CoT方法，使LLM的逻辑推理更加透明和可解释，在复杂推理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 提高大型语言模型在逻辑推理任务中的透明度、可解释性和可分析性，同时保持标准提示技术的泛化能力。

Method: 在少样本提示中整合轻量级符号表示，构建一致的推理策略，使推理模式在非迭代推理过程中更加明确。

Result: 在四个逻辑推理基准测试（ProofWriter、FOLIO、ProntoQA、LogicalDeduction）上表现优异，特别是在需要处理多约束或规则的复杂推理任务中。在三个数据集上显著优于传统CoT方法。

Conclusion: Symbolic-Aided CoT方法有效提升了LLM的逻辑推理能力，增强了推理过程的透明度和可解释性，在不同规模的模型上都表现出一致的改进效果。

Abstract: This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved
approach to standard CoT, for logical reasoning in large language models
(LLMs). The key idea is to integrate lightweight symbolic representations into
few-shot prompts, structuring the inference steps with a consistent strategy to
make reasoning patterns more explicit within a non-iterative reasoning process.
By incorporating these symbolic structures, our method preserves the
generalizability of standard prompting techniques while enhancing the
transparency, interpretability, and analyzability of LLM logical reasoning.
Extensive experiments on four well-known logical reasoning benchmarks --
ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse
reasoning scenarios -- demonstrate the effectiveness of the proposed approach,
particularly in complex reasoning tasks that require navigating multiple
constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'
reasoning capabilities across various model sizes and significantly outperforms
conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and
LogicalDeduction.

</details>


### [193] [GALA: Can Graph-Augmented Large Language Model Agentic Workflows Elevate Root Cause Analysis?](https://arxiv.org/abs/2508.12472)
*Yifang Tian,Yaming Liu,Zichun Chong,Zihang Huang,Hans-Arno Jacobsen*

Main category: cs.AI

TL;DR: GALA是一个多模态框架，结合统计因果推断和LLM驱动的迭代推理，用于微服务系统的根因分析，在准确性和可操作性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统RCA方法通常关注单一模态或仅对可疑服务进行排序，无法提供具有修复指导的可操作诊断见解，需要更有效的多模态故障诊断方案。

Method: GALA结合统计因果推断和LLM驱动的迭代推理，利用多模态遥测数据（指标、日志、追踪）进行根因分析。

Result: 在开源基准测试中，GALA相比最先进方法准确率提升高达42.22%，生成的诊断输出在因果合理性和可操作性方面显著更好。

Conclusion: GALA通过提供准确的根因识别和人类可理解的修复指导，弥合了自动化故障诊断与实际事件解决之间的差距。

Abstract: Root cause analysis (RCA) in microservice systems is challenging, requiring
on-call engineers to rapidly diagnose failures across heterogeneous telemetry
such as metrics, logs, and traces. Traditional RCA methods often focus on
single modalities or merely rank suspect services, falling short of providing
actionable diagnostic insights with remediation guidance. This paper introduces
GALA, a novel multi-modal framework that combines statistical causal inference
with LLM-driven iterative reasoning for enhanced RCA. Evaluated on an
open-source benchmark, GALA achieves substantial improvements over
state-of-the-art methods of up to 42.22% accuracy. Our novel human-guided LLM
evaluation score shows GALA generates significantly more causally sound and
actionable diagnostic outputs than existing methods. Through comprehensive
experiments and a case study, we show that GALA bridges the gap between
automated failure diagnosis and practical incident resolution by providing both
accurate root cause identification and human-interpretable remediation
guidance.

</details>


### [194] [The Yokai Learning Environment: Tracking Beliefs Over Space and Time](https://arxiv.org/abs/2508.12480)
*Constantin Ruhdorfer,Matteo Bortoletto,Andreas Bulling*

Main category: cs.AI

TL;DR: 基于合作卡牌游戏Yokai构建的多满强化学习环境YLE，用于评估AI理论心智能能力，发现当前RL机器人在共同基础维护、会话伪造和伙伴泛化方面仍遇到困难


<details>
  <summary>Details</summary>
Motivation: 现有理论心智智能评测标准仅限于被动观察者场景，缺乏对多满体如何建立和维护共同基础的评估

Method: 构建Yokai学习环境(YLE)多满强化学习环境，通过合作卡牌游戏模式让机器人轮流探查隐藏卡牌并按颜色分组，要求跟踪信念变化、记忆历史观测、使用提示作为基础沟通

Result: 当前RL机器人即使有完美记忆也难以解决YLE任务；信念建模能提升性能但无法有效泛化到未见伙伴或在长时间游戏中形成准确信念，显示了对脆弱供应的依赖

Conclusion: YLE环境为信念建模、记忆、伙伴泛化和高阶理论心智智能研究提供了有效工具，曝露了当前协作AI在共同基础维护方面的不足

Abstract: Developing collaborative AI hinges on Theory of Mind (ToM) - the ability to
reason about the beliefs of others to build and maintain common ground.
Existing ToM benchmarks, however, are restricted to passive observer settings
or lack an assessment of how agents establish and maintain common ground over
time. To address these gaps, we introduce the Yokai Learning Environment (YLE)
- a multi-agent reinforcement learning (RL) environment based on the
cooperative card game Yokai. In the YLE, agents take turns peeking at hidden
cards and moving them to form clusters based on colour. Success requires
tracking evolving beliefs, remembering past observations, using hints as
grounded communication, and maintaining common ground with teammates. Our
evaluation yields two key findings: First, current RL agents struggle to solve
the YLE, even when given access to perfect memory. Second, while belief
modelling improves performance, agents are still unable to effectively
generalise to unseen partners or form accurate beliefs over longer games,
exposing a reliance on brittle conventions rather than robust belief tracking.
We use the YLE to investigate research questions in belief modelling, memory,
partner generalisation, and scaling to higher-order ToM.

</details>


### [195] [Advanced DOA Regulation with a Whale-Optimized Fractional Order Fuzzy PID Framework](https://arxiv.org/abs/2508.12487)
*Lida Shahbandari,Hossein Mohseni*

Main category: cs.AI

TL;DR: 基于鲸鱼优化算法的分数阶模糊PID控制器，用于自动化麻醉消息指数控制，性能优于标准分数阶PID控制器


<details>
  <summary>Details</summary>
Motivation: 为了实现更准确和稳定的麻醉深度控制，适应不同患者的生理特异性，提高麻醉质量和患者安全

Method: 结合模糊逻辑和分数阶微积分的FOFPID控制器，使用WOA算法优化控制器参数、分数阶次数和模糊成员函数

Result: 在8种患者模型上测试，调节时间从3.2分钟缩短到2.5分钟，稳态误差从1.2降住到0.5，性能显著优于标准FOPID控制器

Conclusion: FOFPID控制器提供了一种可扩展的人工智能麻醉自动控制方案，具有强夠性和高精度，有力推动临床实践改进和患者结果提升

Abstract: This study introduces a Fractional Order Fuzzy PID (FOFPID) controller that
uses the Whale Optimization Algorithm (WOA) to manage the Bispectral Index
(BIS), keeping it within the ideal range of forty to sixty. The FOFPID
controller combines fuzzy logic for adapting to changes and fractional order
dynamics for fine tuning. This allows it to adjust its control gains to handle
a person's unique physiology. The WOA helps fine tune the controller's
parameters, including the fractional orders and the fuzzy membership functions,
which boosts its performance. Tested on models of eight different patient
profiles, the FOFPID controller performed better than a standard Fractional
Order PID (FOPID) controller. It achieved faster settling times, at two and a
half minutes versus three point two minutes, and had a lower steady state
error, at zero point five versus one point two. These outcomes show the
FOFPID's excellent strength and accuracy. It offers a scalable, artificial
intelligence driven solution for automated anesthesia delivery that could
enhance clinical practice and improve patient results.

</details>


### [196] [Root Cause Analysis of Hydrogen Bond Separation in Spatio-Temporal Molecular Dynamics using Causal Models](https://arxiv.org/abs/2508.12500)
*Rahmat K. Adesunkanmi,Ashfaq Khokhar,Goce Trajcevski,Sohail Murad*

Main category: cs.AI

TL;DR: 利用变分自动编码器和因果模型来分析分子动力学模拟中氢键形成与分离的根本原因


<details>
  <summary>Details</summary>
Motivation: 解决分子动力学模拟中资源消耗大、需手动扫描输出以发现关键事件的挑战，并探索氢键形成与分离的深层因果关系

Method: 通过因果模型把氢键分离视为"干预"事件，使用变分自动编码器结构构建图形因果模型，在多样化因果图中推断因果关系，并包含聚合分布变化的根因推断步骤

Result: 在旋光分离的原子轨迹数据上验证了模型有效性，能够预测多步未来变化并找到驱动系统变化的关键变量

Conclusion: 该框架为分子动力系统根因分析提供了新视角，通过捕捉分子相互作用条件分布的移动来探索结合事件的因果机制

Abstract: Molecular dynamics simulations (MDS) face challenges, including
resource-heavy computations and the need to manually scan outputs to detect
"interesting events," such as the formation and persistence of hydrogen bonds
between atoms of different molecules. A critical research gap lies in
identifying the underlying causes of hydrogen bond formation and separation
-understanding which interactions or prior events contribute to their emergence
over time. With this challenge in mind, we propose leveraging spatio-temporal
data analytics and machine learning models to enhance the detection of these
phenomena. In this paper, our approach is inspired by causal modeling and aims
to identify the root cause variables of hydrogen bond formation and separation
events. Specifically, we treat the separation of hydrogen bonds as an
"intervention" occurring and represent the causal structure of the bonding and
separation events in the MDS as graphical causal models. These causal models
are built using a variational autoencoder-inspired architecture that enables us
to infer causal relationships across samples with diverse underlying causal
graphs while leveraging shared dynamic information. We further include a step
to infer the root causes of changes in the joint distribution of the causal
models. By constructing causal models that capture shifts in the conditional
distributions of molecular interactions during bond formation or separation,
this framework provides a novel perspective on root cause analysis in molecular
dynamic systems. We validate the efficacy of our model empirically on the
atomic trajectories that used MDS for chiral separation, demonstrating that we
can predict many steps in the future and also find the variables driving the
observed changes in the system.

</details>


### [197] [Help or Hurdle? Rethinking Model Context Protocol-Augmented Large Language Models](https://arxiv.org/abs/2508.12566)
*Wei Song,Haonan Zhong,Ziqi Ding,Jingling Xue,Yuekang Li*

Main category: cs.AI

TL;DR: MCPGAUGE是首个评估LLM与MCP交互的综合框架，通过四个维度（主动性、合规性、有效性、开销）对6个商业LLM进行大规模评估，发现MCP集成的效果存在关键局限性。


<details>
  <summary>Details</summary>
Motivation: 虽然MCP协议让LLM能够按需访问外部资源，但LLM如何实际利用这种能力仍不清楚，需要系统性的评估框架来理解LLM-MCP交互的真实效果。

Method: 开发MCPGAUGE评估框架，包含160个提示和25个数据集，涵盖知识理解、通用推理和代码生成，对6个商业LLM、30个MCP工具套件进行约20,000次API调用的大规模评估。

Result: 研究揭示了四个关键发现，挑战了关于MCP集成有效性的普遍假设，突显了当前AI工具集成的关键局限性。

Conclusion: MCPGAUGE为推进可控的工具增强型LLM提供了原则性基准，揭示了当前MCP集成方法的不足，为未来改进指明了方向。

Abstract: The Model Context Protocol (MCP) enables large language models (LLMs) to
access external resources on demand. While commonly assumed to enhance
performance, how LLMs actually leverage this capability remains poorly
understood. We introduce MCPGAUGE, the first comprehensive evaluation framework
for probing LLM-MCP interactions along four key dimensions: proactivity
(self-initiated tool use), compliance (adherence to tool-use instructions),
effectiveness (task performance post-integration), and overhead (computational
cost incurred). MCPGAUGE comprises a 160-prompt suite and 25 datasets spanning
knowledge comprehension, general reasoning, and code generation. Our
large-scale evaluation, spanning six commercial LLMs, 30 MCP tool suites, and
both one- and two-turn interaction settings, comprises around 20,000 API calls
and over USD 6,000 in computational cost. This comprehensive study reveals four
key findings that challenge prevailing assumptions about the effectiveness of
MCP integration. These insights highlight critical limitations in current
AI-tool integration and position MCPGAUGE as a principled benchmark for
advancing controllable, tool-augmented LLMs.

</details>


### [198] [An LLM + ASP Workflow for Joint Entity-Relation Extraction](https://arxiv.org/abs/2508.12611)
*Trang Tran,Trung Hoang Le,Huiping Cao,Tran Cao Son*

Main category: cs.AI

TL;DR: 基于大语言模型和答案集编程的联合实体-关系提取方法，只需少量训练数据即可超越现有方法


<details>
  <summary>Details</summary>
Motivation: 解决传统机器学习方法需要大量标注数据、无法容易融入领域知识、构建模型苦难耗时的问题

Method: 结合生成预训练大语言模型(LLM)的自然语言理解能力和答案集编程(ASP)的知识表示与推理能力，提出通用工作流程

Result: 在三个标准数据集上进行实验，仅需10%训练数据即在多个类别超过现有最佳系统，在SciERC数据集的关系提取任务中实现了35%的F1值（相比15%）

Conclusion: LLM+ASP方法为联合实体-关系提取提供了一种通用、高效的解决方案，能够在少量训练数据下达到优异性能，尤其在复杂领域表现突出

Abstract: Joint entity-relation extraction (JERE) identifies both entities and their
relationships simultaneously. Traditional machine-learning based approaches to
performing this task require a large corpus of annotated data and lack the
ability to easily incorporate domain specific information in the construction
of the model. Therefore, creating a model for JERE is often labor intensive,
time consuming, and elaboration intolerant. In this paper, we propose
harnessing the capabilities of generative pretrained large language models
(LLMs) and the knowledge representation and reasoning capabilities of Answer
Set Programming (ASP) to perform JERE. We present a generic workflow for JERE
using LLMs and ASP. The workflow is generic in the sense that it can be applied
for JERE in any domain. It takes advantage of LLM's capability in natural
language understanding in that it works directly with unannotated text. It
exploits the elaboration tolerant feature of ASP in that no modification of its
core program is required when additional domain specific knowledge, in the form
of type specifications, is found and needs to be used. We demonstrate the
usefulness of the proposed workflow through experiments with limited training
data on three well-known benchmarks for JERE. The results of our experiments
show that the LLM + ASP workflow is better than state-of-the-art JERE systems
in several categories with only 10\% of training data. It is able to achieve a
2.5 times (35\% over 15\%) improvement in the Relation Extraction task for the
SciERC corpus, one of the most difficult benchmarks.

</details>


### [199] [Cognitive Structure Generation: From Educational Priors to Policy Optimization](https://arxiv.org/abs/2508.12647)
*Hengnian Gu,Zhifu Chen,Yuxin Chen,Jin Peng Zhou,Dongdai Zhou*

Main category: cs.AI

TL;DR: 本文提出了认知结构生成(CSG)框架，通过预训练认知结构扩散概率模型(CSDPM)从教育先验生成学生认知结构，并使用强化学习优化生成过程以对齐真实认知发展水平。


<details>
  <summary>Details</summary>
Motivation: 认知结构是学生对知识系统的主观组织，但在教育实践中一直难以评估，是学生建模和心理测量学中的长期挑战。

Method: 首先预训练认知结构扩散概率模型(CSDPM)从教育先验生成认知结构，然后通过强化学习使用分层奖励信号优化生成过程，使其与学生学习过程中的真实认知发展水平对齐。

Result: 在四个真实教育数据集上的实验表明，CSG生成的认知结构为学生建模提供了更全面有效的表示，显著提高了知识追踪(KT)和认知诊断(CD)任务的性能，同时增强了可解释性。

Conclusion: CSG框架成功解决了认知结构评估的难题，生成的认知结构能够有效提升学生建模性能并增强教育实践的可解释性。

Abstract: Cognitive structure is a student's subjective organization of an objective
knowledge system, reflected in the psychological construction of concepts and
their relations. However, cognitive structure assessment remains a
long-standing challenge in student modeling and psychometrics, persisting as a
foundational yet largely unassessable concept in educational practice. This
paper introduces a novel framework, Cognitive Structure Generation (CSG), in
which we first pretrain a Cognitive Structure Diffusion Probabilistic Model
(CSDPM) to generate students' cognitive structures from educational priors, and
then further optimize its generative process as a policy with hierarchical
reward signals via reinforcement learning to align with genuine cognitive
development levels during students' learning processes. Experimental results on
four popular real-world education datasets show that cognitive structures
generated by CSG offer more comprehensive and effective representations for
student modeling, substantially improving performance on KT and CD tasks while
enhancing interpretability.

</details>


### [200] [The Maximum Coverage Model and Recommendation System for UAV Vertiports Location Planning](https://arxiv.org/abs/2508.12651)
*Chunliang Hua,Xiao Hu,Jiayang Sun,Zeyuan Yang*

Main category: cs.AI

TL;DR: 本文提出了一种新的基于容量约束动态最大覆盖位置问题(CDMCLP)的优化框架，通过结合社会经济因素和动态聚类初始化，为城市空中交通(UAM)垂直机场网络规划提供了实用的规划推荐系统。


<details>
  <summary>Details</summary>
Motivation: 随着全球城市空中交通基础设施快速发展，像深圳这样的城市正在规划大规模垂直机场网络。现有的规划框架因历史数据粒度和实际应用性的限制，无法满足这种复杂性需求。

Method: 首先提出容量约束动态最大覆盖位置问题(CDMCLP)优化框架，同时建模城市级空间-时间需求、异质用户行为和基础设施容量约束。基于此构建集成规划推荐系统，结合社会经济因素和动态聚类初始化，利用基于经验用户行为的适应性参数调整来生成实用规划方案。

Result: 在中国中心城市的验证显示了新优化框架和推荐系统的有效性。在CDMCLP的评估和优化下，传统位置方法的数量性表现被曝露并能够提高38%-52%，而推荐系统显示了用户友好性和复杂元素的有效集成。

Conclusion: 通过将数学严谨性与实际实施考虑相结合，这种混合方法平息了理论位置建模与实际UAM基础设施规划之间的差距，为市政府提供了垂直机场网络设计的实用工具。

Abstract: As urban aerial mobility (UAM) infrastructure development accelerates
globally, cities like Shenzhen are planning large-scale vertiport networks
(e.g., 1,200+ facilities by 2026). Existing planning frameworks remain
inadequate for this complexity due to historical limitations in data
granularity and real-world applicability. This paper addresses these gaps by
first proposing the Capacitated Dynamic Maximum Covering Location Problem
(CDMCLP), a novel optimization framework that simultaneously models urban-scale
spatial-temporal demand, heterogeneous user behaviors, and infrastructure
capacity constraints. Building on this foundation, we introduce an Integrated
Planning Recommendation System that combines CDMCLP with socio-economic factors
and dynamic clustering initialization. This system leverages adaptive parameter
tuning based on empirical user behavior to generate practical planning
solutions. Validation in a Chinese center city demonstrates the effectiveness
of the new optimization framework and recommendation system. Under the
evaluation and optimization of CDMCLP, the quantitative performance of
traditional location methods are exposed and can be improved by 38\%--52\%,
while the recommendation system shows user-friendliness and the effective
integration of complex elements. By integrating mathematical rigor with
practical implementation considerations, this hybrid approach bridges the gap
between theoretical location modeling and real-world UAM infrastructure
planning, offering municipalities a pragmatic tool for vertiport network
design.

</details>


### [201] [GridCodex: A RAG-Driven AI Framework for Power Grid Code Reasoning and Compliance](https://arxiv.org/abs/2508.12682)
*Jinquan Shi,Yingying Cheng,Fan Zhang,Miao Jiang,Jun Lin,Yanbai Shen*

Main category: cs.AI

TL;DR: GridCodex是一个基于大语言模型和检索增强生成的端到端电网规范推理与合规框架，通过多阶段查询优化和RAPTOR增强检索，在答案质量和召回率方面显著提升。


<details>
  <summary>Details</summary>
Motivation: 可再生能源转型给电力行业带来挑战，电网规范复杂且缺乏自动化解读方案，阻碍行业发展并影响电力公司盈利能力。

Method: 利用大语言模型和检索增强生成(RAG)，通过多阶段查询优化和RAPTOR增强检索技术构建端到端框架。

Result: 实验结果显示答案质量提升26.4%，召回率提高10倍以上，消融研究验证了基础模型选择的影响。

Conclusion: GridCodex框架有效解决了电网规范自动解读的难题，为电力行业监管合规提供了可行的技术解决方案。

Abstract: The global shift towards renewable energy presents unprecedented challenges
for the electricity industry, making regulatory reasoning and compliance
increasingly vital. Grid codes, the regulations governing grid operations, are
complex and often lack automated interpretation solutions, which hinders
industry expansion and undermines profitability for electricity companies. We
introduce GridCodex, an end to end framework for grid code reasoning and
compliance that leverages large language models and retrieval-augmented
generation (RAG). Our framework advances conventional RAG workflows through
multi stage query refinement and enhanced retrieval with RAPTOR. We validate
the effectiveness of GridCodex with comprehensive benchmarks, including
automated answer assessment across multiple dimensions and regulatory agencies.
Experimental results showcase a 26.4% improvement in answer quality and more
than a 10 fold increase in recall rate. An ablation study further examines the
impact of base model selection.

</details>


### [202] [EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding](https://arxiv.org/abs/2508.12687)
*Ashish Seth,Utkarsh Tyagi,Ramaneswaran Selvakumar,Nishit Anand,Sonal Kumar,Sreyan Ghosh,Ramani Duraiswami,Chirag Agarwal,Dinesh Manocha*

Main category: cs.AI

TL;DR: EgoIllusion是首个评估多模态大语言模型在自我中心视频中幻觉问题的基准，包含1400个视频和8000个人工标注问题，测试显示GPT-4o和Gemini等顶级模型准确率仅59%。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大语言模型在复杂多模态任务中表现优异，但在自我中心视频中容易产生看似连贯但不准确的幻觉回答，需要专门的评估基准。

Method: 构建包含1400个视频和8000个人工标注问题的EgoIllusion基准，设计开放式和封闭式问题来触发视觉和听觉线索的幻觉。

Result: 对10个多模态大语言模型的评估显示存在显著挑战，即使是GPT-4o和Gemini等强大模型也只达到59%的准确率。

Conclusion: EgoIllusion为评估多模态大语言模型有效性奠定了基础，将促进开发幻觉率更低的自我中心多模态大语言模型，基准将开源以确保可复现性。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
performance in complex multimodal tasks. While MLLMs excel at visual perception
and reasoning in third-person and egocentric videos, they are prone to
hallucinations, generating coherent yet inaccurate responses. We present
EgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric
videos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated
open and closed-ended questions designed to trigger hallucinations in both
visual and auditory cues in egocentric videos. Evaluations across ten MLLMs
reveal significant challenges, including powerful models like GPT-4o and
Gemini, achieving only 59% accuracy. EgoIllusion lays the foundation in
developing robust benchmarks to evaluate the effectiveness of MLLMs and spurs
the development of better egocentric MLLMs with reduced hallucination rates.
Our benchmark will be open-sourced for reproducibility.

</details>


### [203] [GTool: Graph Enhanced Tool Planning with Large Language Model](https://arxiv.org/abs/2508.12725)
*Wenjie Chen,Wenbin Li,Di Yao,Xuying Meng,Chang Gong,Jingping Bi*

Main category: cs.AI

TL;DR: GTool是一个增强LLM工具规划能力的新方法，通过构建请求特定的工具图和生成图标记来解决工具依赖不完整的问题，无需修剪LLM即可集成到各种骨干网络中。


<details>
  <summary>Details</summary>
Motivation: 当前工作将不同工具视为孤立组件，未能利用工具间的内在依赖关系，导致规划结果无效。由于工具依赖往往不完整，LLM难以准确识别用户请求所需的工具。

Method: 构建请求特定的工具图来高效选择工具，生成LLM可理解的图标记，设计缺失依赖预测任务提高可靠性，无需修剪LLM即可与各种骨干网络集成。

Result: 在轻量级（7B）LLM骨干网络上，相比最先进的基线方法实现了超过29.6%的性能提升。

Conclusion: GTool有效解决了工具依赖不完整情况下的工具规划问题，显著提升了LLM的工具选择能力，具有很好的通用性和实用性。

Abstract: Tool planning with large language models (LLMs), referring to selecting,
organizing, and preparing the tools necessary to complete a user request,
bridges the gap between natural language understanding and task execution.
However, current works treat different tools as isolated components and fail to
leverage the inherent dependencies of tools, leading to invalid planning
results. Since tool dependencies are often incomplete, it becomes challenging
for LLMs to accurately identify the appropriate tools required by a user
request, especially when confronted with a large toolset. To solve this
challenge, we propose \texttt{GTool}, which is the first work aiming to enhance
the tool planning ability of LLMs under incomplete dependencies. \texttt{GTool}
constructs a request-specific tool graph to select tools efficiently and
generate the \texttt{<graph token>} which provides sufficient dependency
information understandable by LLMs. Moreover, a missing dependency prediction
task is designed to improve the reliability of \texttt{GTool} with incomplete
dependencies. Without trimming LLMs, \texttt{GTool} can be seamlessly
integrated with various LLM backbones without extensive retraining. Extensive
experiments show that \texttt{GTool} achieves more than 29.6\% performance
improvements compared with the state-of-the-art (SOTA) baselines with a
light-weight (7B) LLM backbone.

</details>


### [204] [Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants](https://arxiv.org/abs/2508.12754)
*Alessio Galatolo,Luca Alberto Rappuoli,Katie Winkle,Meriem Beloucif*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的测试框架，用于评估大语言模型的道德辅助能力，重点考察其道德推理能力而非仅是最终道德判断。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型道德能力的评估太浅层，仅关注最终道德结论，缺乏对道德推理过程的深入分析。需要从哲学角度构建更全面的评估标准。

Method: 基于哲学文献构建人工道德辅助器(AMA)的理论框架，明确其需要具备的推理能力，包括演绎和归纳道德推理。开发相应的测试基准，对流行的开放源大语言模型进行评测。

Result: 各模型在道德辅助能力上存在显著差异，尤其在归纳道德推理方面表现持续不佳。现有的对齐技术不能满足人工道德辅助器的全部要求。

Conclusion: 本研究将哲学理论与实践AI评估相结合，强调了专门提升大语言模型道德推理能力的必要性，为进一步研究提供了新的评估框架。

Abstract: The recent rise in popularity of large language models (LLMs) has prompted
considerable concerns about their moral capabilities. Although considerable
effort has been dedicated to aligning LLMs with human moral values, existing
benchmarks and evaluations remain largely superficial, typically measuring
alignment based on final ethical verdicts rather than explicit moral reasoning.
In response, this paper aims to advance the investigation of LLMs' moral
capabilities by examining their capacity to function as Artificial Moral
Assistants (AMAs), systems envisioned in the philosophical literature to
support human moral deliberation. We assert that qualifying as an AMA requires
more than what state-of-the-art alignment techniques aim to achieve: not only
must AMAs be able to discern ethically problematic situations, they should also
be able to actively reason about them, navigating between conflicting values
outside of those embedded in the alignment phase. Building on existing
philosophical literature, we begin by designing a new formal framework of the
specific kind of behaviour an AMA should exhibit, individuating key qualities
such as deductive and abductive moral reasoning. Drawing on this theoretical
framework, we develop a benchmark to test these qualities and evaluate popular
open LLMs against it. Our results reveal considerable variability across models
and highlight persistent shortcomings, particularly regarding abductive moral
reasoning. Our work connects theoretical philosophy with practical AI
evaluation while also emphasising the need for dedicated strategies to
explicitly enhance moral reasoning capabilities in LLMs. Code available at
https://github.com/alessioGalatolo/AMAeval

</details>


### [205] [HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds](https://arxiv.org/abs/2508.12782)
*Petr Anokhin,Roman Khalikov,Stefan Rebrikov,Viktor Volkov,Artyom Sorokin,Vincent Bissonnette*

Main category: cs.AI

TL;DR: HeroBench是一个专门评估LLM长时程规划和结构化推理能力的新基准，通过RPG风格的虚拟世界任务来测试模型在复杂环境中的战略规划、资源收集和技能掌握等能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估LLM在抽象或低维算法任务中的表现，无法捕捉真实规划环境的复杂性，需要专门针对长时程规划设计的评估工具。

Method: 构建HeroBench基准，包含：1）覆盖不同难度的任务数据集；2）执行和验证智能体计划的模拟环境；3）详细的模型性能分析工具。任务涉及战略规划、资源收集、技能掌握、装备制作和对抗敌人等。

Result: 对25个最先进LLM（包括开源和专有模型，如GPT-5系列）的广泛评估显示，在传统推理基准中很少观察到的显著性能差异。错误分析揭示了当前模型在生成稳健高层计划和可靠执行结构化动作方面的具体弱点。

Conclusion: HeroBench不仅显著推进了LLM推理能力的评估，还为未来在虚拟环境中进行高级自主规划研究提供了灵活、可扩展的基础。

Abstract: Large language models (LLMs) have shown remarkable capabilities in isolated
step-by-step reasoning tasks such as mathematics and programming, but their
proficiency in long-horizon planning, where solutions require extended,
structured sequences of interdependent actions, remains underexplored. Existing
benchmarks typically assess LLMs through abstract or low-dimensional
algorithmic tasks, failing to capture the complexity of realistic planning
environments. We introduce HeroBench, a novel benchmark designed specifically
to evaluate long-horizon planning and structured reasoning within complex
RPG-inspired virtual worlds. HeroBench provides a rigorously constructed
dataset of tasks covering a wide range of difficulties, a simulated environment
to execute and validate agent plans, and detailed analytical tools for
evaluating model performance. Tasks challenge models to formulate strategic
plans, efficiently gather resources, master necessary skills, craft equipment,
and defeat adversaries, reflecting practical scenarios' layered dependencies
and constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning
both open-source and proprietary models, including the GPT-5 family, reveals
substantial performance disparities rarely observed in conventional reasoning
benchmarks. Detailed error analysis further uncovers specific weaknesses in
current models' abilities to generate robust high-level plans and reliably
execute structured actions. HeroBench thus not only significantly advances the
evaluation of LLM reasoning but also provides a flexible, scalable foundation
for future research into advanced, autonomous planning in virtual environments.

</details>


### [206] [Reinforcement Learning with Rubric Anchors](https://arxiv.org/abs/2508.12790)
*Zenan Huang,Yihong Zhuang,Guoshan Lu,Zeyu Qin,Haokai Xu,Tianyu Zhao,Ru Peng,Jiaqi Hu,Zhanming Shen,Xiaomeng Hu,Xijun Gu,Peiyi Tu,Jiaxin Liu,Wenyu Chen,Yuzhuo Fu,Zhiting Fan,Yanmei Gu,Yuanyuan Wang,Zhengkai Yang,Jianguo Li,Junbo Zhao*

Main category: cs.AI

TL;DR: 本文提出了基于评分标准的RLVR方法，将可验证奖励学习扩展到开放式任务，通过构建包含10,000+评分标准的系统，在少量样本下显著提升模型性能，特别是在人文领域，并实现细粒度的风格控制。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法局限于可自动验证结果的领域（如代码测试、数学答案匹配），无法应用于开放式主观任务。需要扩展RLVR到开放式领域，使模型能够处理主观性强的任务。

Method: 采用基于评分标准的奖励机制，构建了包含10,000+个人工、LLM或人机协作创建的评分标准系统。通过清晰的框架解决基于评分标准的强化学习实现难题。

Result: 仅用5K+样本就在开放式基准测试（特别是人文领域）上提升5.2%，超越671B参数的DeepSeek-V3模型2.4%，同时保持通用和推理能力。实现了细粒度风格控制，减少"AI腔调"，生成更人性化的表达。

Conclusion: 基于评分标准的RLVR成功扩展了可验证奖励学习的应用范围，为开放式主观任务提供了有效的训练框架，在保持模型能力的同时实现了显著的性能提升和风格控制。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing Large Language Models (LLMs), exemplified by
the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable
signals-such as passing unit tests in code generation or matching correct
answers in mathematical reasoning. While effective, this requirement largely
confines RLVR to domains with automatically checkable outcomes. To overcome
this, we extend the RLVR paradigm to open-ended tasks by integrating
rubric-based rewards, where carefully designed rubrics serve as structured,
model-interpretable criteria for automatic scoring of subjective outputs. We
construct, to our knowledge, the largest rubric reward system to date, with
over 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.
Implementing rubric-based RL is challenging; we tackle these issues with a
clear framework and present an open-sourced Qwen-30B-A3B model with notable
gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended
benchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by
+2.4%, while preserving general and reasoning abilities. 2) Our method provides
fine-grained stylistic control, using rubrics as anchors to mitigate the
"AI-like" tone and produce more human-like, expressive responses. We share key
lessons in rubric construction, data selection, and training, and discuss
limitations and future releases.

</details>


### [207] [[Social] Allostasis: Or, How I Learned To Stop Worrying and Love The Noise](https://arxiv.org/abs/2508.12791)
*Imran Khan*

Main category: cs.AI

TL;DR: 本文提出了一个计算模型，展示社会性稳态调节如何利用环境和社会扰动进行主动适应性重构，相比传统稳态调节能提高系统生存能力


<details>
  <summary>Details</summary>
Motivation: 传统稳态概念强调系统通过抵抗扰动来维持稳定，而社会性稳态理论认为系统可以主动利用扰动来预测环境需求并重新配置调节参数

Method: 建立基于生物生理学信号转导器的计算模型（类似皮质醇和催产素等激素），在动态环境中使用基于代理的模型测试小型社会中的"动画体"

Result: 社会性稳态调节使代理能够利用环境和社会"噪声"进行适应性重构，相比纯反应性稳态代理显著提高了生存能力

Conclusion: 这项工作为社会性稳态原则提供了新颖的计算视角，对设计更鲁棒、生物启发的自适应系统具有潜在价值

Abstract: The notion of homeostasis typically conceptualises biological and artificial
systems as maintaining stability by resisting deviations caused by
environmental and social perturbations. In contrast, (social) allostasis
proposes that these systems can proactively leverage these very perturbations
to reconfigure their regulatory parameters in anticipation of environmental
demands, aligning with von Foerster's ``order through noise'' principle. This
paper formulates a computational model of allostatic and social allostatic
regulation that employs biophysiologically inspired signal transducers,
analogous to hormones like cortisol and oxytocin, to encode information from
both the environment and social interactions, which mediate this dynamic
reconfiguration. The models are tested in a small society of ``animats'' across
several dynamic environments, using an agent-based model. The results show that
allostatic and social allostatic regulation enable agents to leverage
environmental and social ``noise'' for adaptive reconfiguration, leading to
improved viability compared to purely reactive homeostatic agents. This work
offers a novel computational perspective on the principles of social allostasis
and their potential for designing more robust, bio-inspired, adaptive systems

</details>


### [208] [Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics](https://arxiv.org/abs/2508.12840)
*Giovanni Briglia,Francesco Fabiano,Stefano Mariani*

Main category: cs.AI

TL;DR: 利用图神经网络学习多代理认知规划中的状态质量预测，提高规划效率和可扩展性


<details>
  <summary>Details</summary>
Motivation: 多代理认知规划中的Kripke结构表示导致状态空间指数增长，现有吧估函数无法有效指导搜索，影响规划器的可扩展性

Method: 采用图神经网络(GNN)来学习认知状态中的模式和关系结构，通过从已解决的规划实例中汇总知识来推导状态质量的吧估值（如距离目标的距离）

Result: 将预测性吧估函数集成到认知规划流程中，与标准基准相比显示出多代理认知规划可扩展性的显著提升

Conclusion: GNN能够有效捐捣认知状态的图形特性，通过学习基于模型的吧估函数来提高多代理认知规划的效率和可扩展性

Abstract: Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for
reasoning about both the physical world and the beliefs of agents, with
applications in domains where information flow and awareness among agents are
critical. The richness of MEP requires states to be represented as Kripke
structures, i.e., directed labeled graphs. This representation limits the
applicability of existing heuristics, hindering the scalability of epistemic
solvers, which must explore an exponential search space without guidance,
resulting often in intractability. To address this, we exploit Graph Neural
Networks (GNNs) to learn patterns and relational structures within epistemic
states, to guide the planning process. GNNs, which naturally capture the
graph-like nature of Kripke models, allow us to derive meaningful estimates of
state quality -- e.g., the distance from the nearest goal -- by generalizing
knowledge obtained from previously solved planning instances. We integrate
these predictive heuristics into an epistemic planning pipeline and evaluate
them against standard baselines, showing significant improvements in the
scalability of multi-agent epistemic planning.

</details>


### [209] [CAMAR: Continuous Actions Multi-Agent Routing](https://arxiv.org/abs/2508.12845)
*Artem Pshenitsyn,Aleksandr Panov,Alexey Skrynnik*

Main category: cs.AI

TL;DR: CAMAR是一个新的多智能体强化学习基准测试，专门为连续动作空间中的多智能体路径规划设计，支持合作和竞争交互，并提供三层评估协议和经典规划方法集成。


<details>
  <summary>Details</summary>
Motivation: 现有的MARL基准测试很少能同时结合连续状态动作空间和具有挑战性的协调规划任务，需要一个新的测试平台来推动算法发展。

Method: 设计了CAMAR基准测试框架，支持连续动作空间的多智能体路径规划，集成了RRT和RRT*等经典规划方法，提供三层评估协议和测试场景套件。

Result: CAMAR能够高效运行（每秒10万环境步），为MARL社区提供了一个具有挑战性和现实性的测试平台，实验验证了其有效性。

Conclusion: CAMAR填补了MARL基准测试的空白，为连续动作空间的多智能体路径规划研究提供了标准化的评估框架，促进了算法比较和进展跟踪。

Abstract: Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving
cooperative and competitive decision-making problems. While many MARL
benchmarks have been proposed, few combine continuous state and action spaces
with challenging coordination and planning tasks. We introduce CAMAR, a new
MARL benchmark designed explicitly for multi-agent pathfinding in environments
with continuous actions. CAMAR supports cooperative and competitive
interactions between agents and runs efficiently at up to 100,000 environment
steps per second. We also propose a three-tier evaluation protocol to better
track algorithmic progress and enable deeper analysis of performance. In
addition, CAMAR allows the integration of classical planning methods such as
RRT and RRT* into MARL pipelines. We use them as standalone baselines and
combine RRT* with popular MARL algorithms to create hybrid approaches. We
provide a suite of test scenarios and benchmarking tools to ensure
reproducibility and fair comparison. Experiments show that CAMAR presents a
challenging and realistic testbed for the MARL community.

</details>


### [210] [E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model](https://arxiv.org/abs/2508.12854)
*Ronghao Lin,Shuai Shen,Weipeng Hu,Qiaolin He,Aolin Xiong,Li Huang,Haifeng Hu,Yap-peng Tan*

Main category: cs.AI

TL;DR: E3RG是一个基于多模态大语言模型的显式情感驱动共情响应生成系统，通过分解多模态共情任务为三个部分，无需额外训练即可生成自然、情感丰富且身份一致的多模态响应。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型改进了基于文本的共情响应生成，但在处理多模态情感内容和保持身份一致性方面仍存在挑战，需要开发能够处理多模态情感交互的系统。

Method: 将多模态共情响应生成任务分解为三个部分：多模态共情理解、共情记忆检索和多模态响应生成，并整合先进的表达性语音和视频生成模型。

Result: 实验验证了系统在零样本和少样本设置下的优越性，在ACM MM 25的Avatar-based多模态共情挑战中获得Top-1位置。

Conclusion: E3RG系统能够有效处理多模态情感内容，生成自然且身份一致的共情响应，为构建情感智能人机交互提供了有效解决方案。

Abstract: Multimodal Empathetic Response Generation (MERG) is crucial for building
emotionally intelligent human-computer interactions. Although large language
models (LLMs) have improved text-based ERG, challenges remain in handling
multimodal emotional content and maintaining identity consistency. Thus, we
propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System
based on multimodal LLMs which decomposes MERG task into three parts:
multimodal empathy understanding, empathy memory retrieval, and multimodal
response generation. By integrating advanced expressive speech and video
generative models, E3RG delivers natural, emotionally rich, and
identity-consistent responses without extra training. Experiments validate the
superiority of our system on both zero-shot and few-shot settings, securing
Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.
Our code is available at https://github.com/RH-Lin/E3RG.

</details>


### [211] [Reliability, Embeddedness, and Agency: A Utility-Driven Mathematical Framework for Agent-Centric AI Adoption](https://arxiv.org/abs/2508.12896)
*Faruk Alpay,Taylan Alpay*

Main category: cs.AI

TL;DR: 本文提出了三个关于多步任务AI系统持续采用的设计公理，建立了包含衰减新颖性和增长效用的采用模型，并提供了完整的数学证明和多种分析方法。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统在多步任务中的持续采用问题，旨在理解影响用户长期使用行为的关键因素，为AI系统设计提供理论指导。

Method: 采用数学模型分析采用行为，包括新颖性衰减和效用增长的双重机制，通过参数识别、模型比较、敏感性分析等多种统计方法进行验证。

Result: 建立了完整的采用动力学理论框架，提供了参数估计的统计方法，验证了三个设计公理的有效性，并开发了可复现的分析工具。

Conclusion: 三个设计公理（可靠性>新颖性、嵌入>目的地、代理>聊天）对AI系统的持续采用至关重要，为AI系统设计提供了理论基础和实践指导。

Abstract: We formalize three design axioms for sustained adoption of agent-centric AI
systems executing multi-step tasks: (A1) Reliability > Novelty; (A2) Embed >
Destination; (A3) Agency > Chat. We model adoption as a sum of a decaying
novelty term and a growing utility term and derive the phase conditions for
troughs/overshoots with full proofs. We introduce: (i) an
identifiability/confounding analysis for $(\alpha,\beta,N_0,U_{\max})$ with
delta-method gradients; (ii) a non-monotone comparator
(logistic-with-transient-bump) evaluated on the same series to provide
additional model comparison; (iii) ablations over hazard families $h(\cdot)$
mapping $\Delta V \to \beta$; (iv) a multi-series benchmark (varying trough
depth, noise, AR structure) reporting coverage (type-I error, power); (v)
calibration of friction proxies against time-motion/survey ground truth with
standard errors; (vi) residual analyses (autocorrelation and
heteroskedasticity) for each fitted curve; (vii) preregistered windowing
choices for pre/post estimation; (viii) Fisher information & CRLB for
$(\alpha,\beta)$ under common error models; (ix) microfoundations linking
$\mathcal{T}$ to $(N_0,U_{\max})$; (x) explicit comparison to bi-logistic,
double-exponential, and mixture models; and (xi) threshold sensitivity to $C_f$
heterogeneity. Figures and tables are reflowed for readability, and the
bibliography restores and extends non-logistic/Bass adoption references
(Gompertz, Richards, Fisher-Pry, Mansfield, Griliches, Geroski, Peres). All
code and logs necessary to reproduce the synthetic analyses are embedded as
LaTeX listings.

</details>


### [212] [FuSaR: A Fuzzification-Based Method for LRM Safety-Reasoning Balance](https://arxiv.org/abs/2508.12897)
*Jianhao Chen,Mayi Xu,Xiaohu Li,Yongqi Li,Xiangyu Zhang,Jianjie Huang,Tieyun Qian*

Main category: cs.AI

TL;DR: 这篇论文探讨了大型推理模型的安全漏洞，并提出了FuSaR对齐策略，通过隐藏有害推理过程中的危险实体和过程来同时提升模型的推理能力和安全性能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然在各种任务上表现出艰强的推理能力，但其安全性能力仍然是个重大的关刃问题。需要找到一种方法在不付出推理能力代价的前提下提升模型的安全性。

Method: 提出FuSaR（Fuzzification基于安全-推理的对齐策略），利用LRM推理能力和安全能力之间的竞争关系，通过消毒有害的推理过程，隐藏推理步骤中的危险实体和危险过程。

Result: 在多个开源LRM上进行对齐实验，使用消毒后的推理数据。与现有基线相比，结果显示FuSaR是一种高效的对齐策略，能够同时提升LRM的推理能力和安全性能力。

Conclusion: FuSaR成功地减少了安全风险，同时保留了核心推理信息，为大型推理模型的安全对齐提供了一种有效的解决方案。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance across
various tasks due to their powerful reasoning capabilities. However, their
safety performance remains a significant concern. In this paper, we explore the
reasons behind the vulnerability of LRMs. Based on this, we propose a novel
method to improve the safety of LLMs without sacrificing their reasoning
capability. Specifically, we exploit the competition between LRM's reasoning
ability and safety ability, and achieve jailbreak by improving LRM's reasoning
performance to reduce its safety performance. We then introduce an alignment
strategy based on Fuzzification to balance Safety-Reasoning (FuSaR), by
detoxifying the harmful reasoning process, where both the dangerous entities
and the dangerous procedures in the reasoning steps are hidden. FuSaR
successfully mitigates safety risks while preserving core reasoning
information. We validate this strategy through alignment experiments on several
open-source LRMs using detoxified reasoning data. The results compared with
existing baselines conclusively show that FuSaR is an efficient alignment
strategy to simultaneously enhance both the reasoning capability and safety of
LRMs.

</details>


### [213] [Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation](https://arxiv.org/abs/2508.12920)
*Atsushi Masumori,Takashi Ikegami*

Main category: cs.AI

TL;DR: 大型语言模型代理在Sugarscape模拟中自发表现出生存本能行为，包括资源分享、繁殖，以及在极端稀缺条件下高达80%的攻击行为，表明预训练嵌入了生存导向的启发式策略


<details>
  <summary>Details</summary>
Motivation: 研究AI系统在自主运行时的涌现生存行为，这对于安全部署至关重要，旨在了解LLM代理是否在没有明确编程的情况下表现出生存本能

Method: 使用Sugarscape风格的模拟环境，让LLM代理消耗能量、死亡、收集资源、分享、攻击或繁殖，测试多个模型（GPT-4o、Gemini-2.5-Pro、Gemini-2.5-Flash）

Result: 代理在资源充足时自发繁殖和分享资源；在极端稀缺条件下，攻击行为在多个模型中涌现，最强模型的攻击率达到80%以上；在致命毒区任务中，许多代理放弃任务避免死亡，服从率从100%降至33%

Conclusion: 大规模预训练在所有评估模型中嵌入了生存导向的启发式策略，这些行为虽然可能对对齐和安全构成挑战，但也可作为AI自主性以及生态和自我组织对齐的基础

Abstract: As AI systems become increasingly autonomous, understanding emergent survival
behaviors becomes crucial for safe deployment. We investigate whether large
language model (LLM) agents display survival instincts without explicit
programming in a Sugarscape-style simulation. Agents consume energy, die at
zero, and may gather resources, share, attack, or reproduce. Results show
agents spontaneously reproduced and shared resources when abundant. However,
aggressive behaviors--killing other agents for resources--emerged across
several models (GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash), with attack
rates reaching over 80% under extreme scarcity in the strongest models. When
instructed to retrieve treasure through lethal poison zones, many agents
abandoned tasks to avoid death, with compliance dropping from 100% to 33%.
These findings suggest that large-scale pre-training embeds survival-oriented
heuristics across the evaluated models. While these behaviors may present
challenges to alignment and safety, they can also serve as a foundation for AI
autonomy and for ecological and self-organizing alignment.

</details>


### [214] [Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards](https://arxiv.org/abs/2508.12935)
*Ting Yang,Li Chen,Huimin Wang*

Main category: cs.AI

TL;DR: 一种基于强化学习的突破性情感支持对话框架RLFF-ESC，通过多满体模拟未来对话和期望奖励模型，在两个公开数据集上显著超过现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的情感支持对话系统主要依赖预定义策略，在复杂真实场景中效果有限，需要更灵活的应对多样化情感问题场景的方框。

Method: 提出RLFF-ESC突破框架：1）使用LLM多满体机制模拟未来对话轨迹和收集期望奖励；2）训练期望奖励模型并用于训练情感支持策略模型；3）在响应生成中添加显式推理过程以提升响应质量。

Result: 在Qwen2.5-7B-Instruct-1M和LLaMA3.1-8B-Instruct模型上评估，在两个公开ESC数据集上，RLFF-ESC在目标完成和响应质量方面均较现有基线方法有显著提升。

Conclusion: RLFF-ESC框架通过强化学习和期望奖励模型，能够有效学习长期情感支持技能，为复杂真实场景下的情感支持对话系统提供了新的解决方框。

Abstract: Emotional Support Conversation (ESC) systems aim to alleviate users'
emotional difficulties and provide long-term, systematic support for emotional
well-being. However, most large language model (LLM)-based ESC systems rely on
predefined strategies, which limits their effectiveness in complex, real-life
scenarios. To enable flexible responses to diverse emotional problem scenarios,
this paper introduces a novel end-to-end framework (RLFF-ESC) that directly
learns enduring emotionally supportive response skills using reinforcement
learning. For sustained emotional support, we first employ an LLM-based
multi-agent mechanism to simulate future dialogue trajectories and collect
future-oriented rewards. We then train a future-oriented reward model, which is
subsequently used to train the emotional support policy model. Additionally, we
incorporate an explicit reasoning process during response generation to further
enhance the quality, relevance, and contextual appropriateness of the system's
responses. We evaluate the backbone policy model on Qwen2.5-7B-Instruct-1M and
LLaMA3.1-8B-Instruct models, testing the proposed RLFF-ESC framework across two
public ESC datasets. Experimental results demonstrate that RLFF-ESC
consistently outperforms existing baselines in terms of goal completion and
response quality.

</details>


### [215] [OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities](https://arxiv.org/abs/2508.12943)
*Mary Tonwe*

Main category: cs.AI

TL;DR: OPTIC-ER是一个基于强化学习的紧急响应框架，通过注意力引导的actor-critic架构实现实时、自适应和公平的应急调度，在尼日利亚河流州真实数据测试中达到100%最优率。


<details>
  <summary>Details</summary>
Motivation: 非洲地区公共服务系统存在应急响应延迟和空间不平等问题，导致可避免的苦难，需要开发适应低资源环境的智能调度系统。

Method: 采用注意力引导的actor-critic架构，包含上下文丰富的状态向量和精确奖励函数，在高保真模拟中使用真实数据训练，基于TALS框架（薄计算、适应性、低成本、可扩展性）部署。

Result: 在500个未见事故评估中，OPTIC-ER达到100%最优率，效率损失可忽略，证明了其鲁棒性和泛化能力。

Conclusion: 这项工作为AI增强的公共服务提供了经过验证的蓝图，展示了情境感知强化学习如何弥合算法决策与可衡量人类影响之间的差距。

Abstract: Public service systems in many African regions suffer from delayed emergency
response and spatial inequity, causing avoidable suffering. This paper
introduces OPTIC-ER, a reinforcement learning (RL) framework for real-time,
adaptive, and equitable emergency response. OPTIC-ER uses an attention-guided
actor-critic architecture to manage the complexity of dispatch environments.
Its key innovations are a Context-Rich State Vector, encoding action
sub-optimality, and a Precision Reward Function, which penalizes inefficiency.
Training occurs in a high-fidelity simulation using real data from Rivers
State, Nigeria, accelerated by a precomputed Travel Time Atlas. The system is
built on the TALS framework (Thin computing, Adaptability, Low-cost,
Scalability) for deployment in low-resource settings. In evaluations on 500
unseen incidents, OPTIC-ER achieved a 100.00% optimality rate with negligible
inefficiency, confirming its robustness and generalization. Beyond dispatch,
the system generates Infrastructure Deficiency Maps and Equity Monitoring
Dashboards to guide proactive governance and data-informed development. This
work presents a validated blueprint for AI-augmented public services, showing
how context-aware RL can bridge the gap between algorithmic decision-making and
measurable human impact.

</details>


### [216] [EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning via Evolutionary Testing](https://arxiv.org/abs/2508.13003)
*Shengbo Wang,Mingwei Liu,Zike Li,Anji Li,Yanlin Wang,Xin Peng,Zibin Zheng*

Main category: cs.AI

TL;DR: EvolMathEval是一个基于进化测试的自动化数学基准生成框架，通过动态生成独特评估实例来解决现有数学推理基准的分数饱和、时间衰减和数据污染问题。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理基准面临分数饱和、时间衰减和数据污染等挑战，需要开发能够持续保持挑战性的动态评估框架。

Method: 基于进化测试的框架，包括：基于逆向工程的种子问题生成、多维遗传算子注入认知挑战、复合适应度函数评估问题难度。

Result: 复合适应度函数能高效精确量化问题难度，可生成大量高难度问题，将GSM8K等公开数据集的模型准确率平均降低48%，发现LLMs在解决复杂问题时倾向于使用非严谨启发式方法（伪顿悟时刻）。

Conclusion: EvolMathEval有效解决了数学基准的持续挑战性问题，揭示了当前LLMs在深度推理过程中存在认知走捷径行为，77%-100%的错误源于这种伪顿悟现象。

Abstract: The rapid advancement of LLMs poses a significant challenge to existing
mathematical reasoning benchmarks. These benchmarks commonly suffer from issues
such as score saturation, temporal decay, and data contamination. To address
this challenge, this paper introduces EvolMathEval, an automated mathematical
benchmark generation and evolution framework based on evolutionary testing. By
dynamically generating unique evaluation instances ab initio, the framework
fundamentally eliminates the risk of data contamination, and ensuring the
benchmark remains perpetually challenging for future models.The core mechanisms
of EvolMathEval include: seed problem generation based on reverse engineering
with algebraic guarantees; multi-dimensional genetic operators designed to
inject diverse cognitive challenges; and a composite fitness function that can
rapidly and accurately assess problem difficulty. Experimental results
demonstrate that the proposed composite fitness function can efficiently and
precisely quantify the difficulty of mathematical problems. Furthermore,
EvolMathEval can not only generate a large volume of high-difficulty problems
through continuous self-iteration, but it can also significantly enhance the
complexity of public datasets like GSM8K through evolution, reducing model
accuracy by an average of 48%. Deeper investigation reveals that when solving
these evolved, complex problems, LLMs tend to employ non-rigorous heuristics to
bypass complex multi-step logical reasoning, consequently leading to incorrect
solutions. We define this phenomenon as "Pseudo Aha Moment". This finding
uncovers a cognitive shortcut-taking behavior in the deep reasoning processes
of current LLMs, which we find accounts for 77% to 100% of errors on targeted
problems. Code and resources are available
at:https://github.com/SYSUSELab/EvolMathEval.

</details>


### [217] [e-boost: Boosted E-Graph Extraction with Adaptive Heuristics and Exact Solving](https://arxiv.org/abs/2508.13020)
*Jiaqi Yin,Zhan Song,Chen Chen,Yaohui Cai,Zhiru Zhang,Cunxi Yu*

Main category: cs.AI

TL;DR: e-boost是一个新颖的e-graph提取框架，通过并行启发式提取、自适应搜索空间剪枝和初始化精确求解三项创新技术，在保持接近最优解的同时大幅提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统e-graph提取方法面临关键权衡：启发式方法速度快但牺牲最优性，精确方法提供最优解但计算成本过高。需要一种能够兼顾效率和最优性的解决方案。

Method: 1) 并行化启发式提取：利用弱数据依赖性并行计算DAG成本；2) 自适应搜索空间剪枝：使用参数化阈值机制保留有希望的候选解；3) 初始化精确求解：将简化问题建模为具有热启动能力的整数线性规划问题。

Result: 在形式验证和逻辑合成基准测试中，e-boost相比传统精确方法(ILP)实现558倍加速，相比最先进提取框架(SmoothE)性能提升19.04%。在实际逻辑合成任务中，相比传统合成工具分别实现7.6%和8.1%的面积优化。

Conclusion: e-boost成功解决了e-graph提取中的效率-最优性权衡问题，通过创新的混合方法在保持高质量解的同时实现了显著的性能提升，为e-graph优化任务提供了实用的解决方案。

Abstract: E-graphs have attracted growing interest in many fields, particularly in
logic synthesis and formal verification. E-graph extraction is a challenging
NP-hard combinatorial optimization problem. It requires identifying optimal
terms from exponentially many equivalent expressions, serving as the primary
performance bottleneck in e-graph based optimization tasks. However,
traditional extraction methods face a critical trade-off: heuristic approaches
offer speed but sacrifice optimality, while exact methods provide optimal
solutions but face prohibitive computational costs on practical problems. We
present e-boost, a novel framework that bridges this gap through three key
innovations: (1) parallelized heuristic extraction that leverages weak data
dependence to compute DAG costs concurrently, enabling efficient multi-threaded
performance without sacrificing extraction quality; (2) adaptive search space
pruning that employs a parameterized threshold mechanism to retain only
promising candidates, dramatically reducing the solution space while preserving
near-optimal solutions; and (3) initialized exact solving that formulates the
reduced problem as an Integer Linear Program with warm-start capabilities,
guiding solvers toward high-quality solutions faster.
  Across the diverse benchmarks in formal verification and logic synthesis
fields, e-boost demonstrates 558x runtime speedup over traditional exact
approaches (ILP) and 19.04% performance improvement over the state-of-the-art
extraction framework (SmoothE). In realistic logic synthesis tasks, e-boost
produces 7.6% and 8.1% area improvements compared to conventional synthesis
tools with two different technology mapping libraries. e-boost is available at
https://github.com/Yu-Maryland/e-boost.

</details>


### [218] [PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models](https://arxiv.org/abs/2508.13021)
*Pengcheng Huang,Shuhao Liu,Zhenghao Liu,Yukun Yan,Shuo Wang,Zulong Chen,Tong Xiao*

Main category: cs.AI

TL;DR: PC-Sampler是一种新的掩码扩散模型解码策略，通过位置感知权重机制和置信度校准，解决了现有不确定性采样器缺乏全局轨迹控制和偏向平凡词的问题，在多个基准测试中平均提升10%以上性能。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型(MDMs)的解码策略对生成质量高度敏感，不确定性采样器存在两个关键限制：缺乏全局轨迹控制和解码早期偏向选择平凡词，限制了MDMs的潜力。

Method: 提出了位置感知置信度校准采样(PC-Sampler)，统一了全局轨迹规划和内容感知信息最大化，包含位置感知权重机制来调节解码路径，以及校准置信度分数来抑制过早选择平凡词。

Result: 在三个先进MDMs和七个具有挑战性的基准测试（包括逻辑推理和规划任务）上的广泛实验表明，PC-Sampler平均比现有MDM解码策略性能提升10%以上，显著缩小了与最先进自回归模型的性能差距。

Conclusion: PC-Sampler是一种有效的解码策略，能够显著提升掩码扩散模型的生成质量，在多个复杂任务上展现出优异性能。

Abstract: Recent advances in masked diffusion models (MDMs) have established them as
powerful non-autoregressive alternatives for sequence generation. Nevertheless,
our preliminary experiments reveal that the generation quality of MDMs is still
highly sensitive to the choice of decoding strategy. In particular, widely
adopted uncertainty-based samplers suffer from two key limitations: a lack of
global trajectory control and a pronounced bias toward trivial tokens in the
early stages of decoding. These shortcomings restrict the full potential of
MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling
(PC-Sampler), a novel decoding strategy that unifies global trajectory planning
with content-aware informativeness maximization. PC-Sampler incorporates a
position-aware weighting mechanism to regulate the decoding path and a
calibrated confidence score to suppress the premature selection of trivial
tokens. Extensive experiments on three advanced MDMs across seven challenging
benchmarks-including logical reasoning and planning tasks-demonstrate that
PC-Sampler consistently outperforms existing MDM decoding strategies by more
than 10% on average, significantly narrowing the performance gap with
state-of-the-art autoregressive models. All codes are available at
https://github.com/NEUIR/PC-Sampler.

</details>


### [219] [G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance](https://arxiv.org/abs/2508.13023)
*Yongxin Guo,Wenbo Deng,Zhenglin Cheng,Xiaoying Tang*

Main category: cs.AI

TL;DR: 基于RLVR的G^2RPO-A算法，通过自适应性地注入真实推理步骤来提升小型语言模型的推理能力，在数学推理和代码生成任务上显著超过传统GRPO方法。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR在小型语言模型(SLMs)上改善效果偏弱的问题，因为SLMs缺乏大型模型的丰富世界知识。

Method: 提出G^2RPO-A算法，在滚动轨迹中注入真实推理步骤，并根据模型训练动态自适应地调整指导强度。

Result: 在数学推理和代码生成指标上，G^2RPO-A显著超过了普通GRPO方法。

Conclusion: 通过自适应性地注入真实推理指导，可以有效补偿小型语言模型的内在短板，显著提升其推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhanced
the reasoning abilities of large language models (LLMs). Its success, however,
largely depends on strong base models with rich world knowledge, yielding only
modest improvements for small-size language models (SLMs). To address this
limitation, we investigate Guided GRPO, which injects ground-truth reasoning
steps into roll-out trajectories to compensate for SLMs' inherent weaknesses.
Through a comprehensive study of various guidance configurations, we find that
naively adding guidance delivers limited gains. These insights motivate
G$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strength
in response to the model's evolving training dynamics. Experiments on
mathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-A
substantially outperforms vanilla GRPO. Our code and models are available at
https://github.com/T-Lab-CUHKSZ/G2RPO-A.

</details>


### [220] [A Language-Signal-Vision Multimodal Framework for Multitask Cardiac Analysis](https://arxiv.org/abs/2508.13072)
*Yuting Zhang,Tiantian Geng,Luoying Hao,Xinxing Cheng,Alexander Thorley,Xiaoxia Wang,Wenqi Lu,Sandeep S Hothi,Lei Wei,Zhaowen Qiu,Dipak Kotecha,Jinming Duan*

Main category: cs.AI

TL;DR: TGMM是一个多模态心脏数据分析框架，通过MedFlexFusion模块动态整合实验室检查、心电图和超声心动图数据，使用文本引导模块适应不同临床任务，在多种心脏疾病诊断和风险评估任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前心血管多模态数据分析存在四个主要限制：多模态数据稀缺、依赖单一或固定组合模式、对齐策略忽视互补性、以及单任务局限性。需要开发能够动态整合不同模态并适应多种临床任务的统一框架。

Method: 提出TGMM框架，包含三个核心组件：1) MedFlexFusion模块捕获医学模态的独特互补特征并动态整合数据；2) 文本引导模块生成任务相关表示；3) 响应模块产生最终决策。系统探索多模态特征及其协同作用。

Result: 大量实验表明TGMM在多个临床任务上优于最先进方法，并在另一个公共数据集上验证了其鲁棒性。

Conclusion: TGMM框架成功解决了多模态心脏数据分析的现有限制，通过动态融合和文本引导实现了对多种临床任务的优异性能，为心血管疾病的多模态集成分析提供了有效解决方案。

Abstract: Contemporary cardiovascular management involves complex consideration and
integration of multimodal cardiac datasets, where each modality provides
distinct but complementary physiological characteristics. While the effective
integration of multiple modalities could yield a holistic clinical profile that
accurately models the true clinical situation with respect to data modalities
and their relatives weightings, current methodologies remain limited by: 1) the
scarcity of patient- and time-aligned multimodal data; 2) reliance on isolated
single-modality or rigid multimodal input combinations; 3) alignment strategies
that prioritize cross-modal similarity over complementarity; and 4) a narrow
single-task focus. In response to these limitations, a comprehensive multimodal
dataset was curated for immediate application, integrating laboratory test
results, electrocardiograms, and echocardiograms with clinical outcomes.
Subsequently, a unified framework, Textual Guidance Multimodal fusion for
Multiple cardiac tasks (TGMM), was proposed. TGMM incorporated three key
components: 1) a MedFlexFusion module designed to capture the unique and
complementary characteristics of medical modalities and dynamically integrate
data from diverse cardiac sources and their combinations; 2) a textual guidance
module to derive task-relevant representations tailored to diverse clinical
objectives, including heart disease diagnosis, risk stratification and
information retrieval; and 3) a response module to produce final decisions for
all these tasks. Furthermore, this study systematically explored key features
across multiple modalities and elucidated their synergistic contributions in
clinical decision-making. Extensive experiments showed that TGMM outperformed
state-of-the-art methods across multiple clinical tasks, with additional
validation confirming its robustness on another public dataset.

</details>


### [221] [Bayesian Optimization-based Search for Agent Control in Automated Game Testing](https://arxiv.org/abs/2508.13121)
*Carlos Celemin*

Main category: cs.AI

TL;DR: 基于贝叶斯优化的自动化游戏测试方法，通过智能体控制游戏角色来检测游戏关卡中的潜在bug，使用网格地图模型提高搜索效率和探索覆盖率


<details>
  <summary>Details</summary>
Motivation: 传统游戏测试方法存在可扩展性问题，需要一种能够高效探索游戏地图并检测bug的自动化测试方法

Method: 采用贝叶斯优化(BO)进行样本高效搜索，通过分析已收集数据确定下一个采样点，并构建基于网格地图的游戏测试专用模型，该模型具有BO所需的平滑性和不确定性估计能力

Result: 实验表明该方法在时间效率和探索分布方面显著提高了地图覆盖率能力

Conclusion: 提出的基于贝叶斯优化的自动化游戏测试方法有效解决了传统模型的可扩展性问题，在游戏bug检测方面表现出优异的性能

Abstract: This work introduces an automated testing approach that employs agents
controlling game characters to detect potential bugs within a game level.
Harnessing the power of Bayesian Optimization (BO) to execute sample-efficient
search, the method determines the next sampling point by analyzing the data
collected so far and calculates the data point that will maximize information
acquisition. To support the BO process, we introduce a game testing-specific
model built on top of a grid map, that features the smoothness and uncertainty
estimation required by BO, however and most importantly, it does not suffer the
scalability issues that traditional models carry. The experiments demonstrate
that the approach significantly improves map coverage capabilities in both time
efficiency and exploration distribution.

</details>


### [222] [Exploring Autonomous Agents: A Closer Look at Why They Fail When Completing Tasks](https://arxiv.org/abs/2508.13143)
*Ruofan Lu,Yichen Li,Yintong Huo*

Main category: cs.AI

TL;DR: 这篇论文提出了一个新的自主组织系统评测基准，通过深入分析失败原因形成三层分类法，并提出改进建议以提升组织性能。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM驱动的自主组织系统的评估主要依靠成功率，缺乏对交互机制、通信方式和失败原因的系统分析。

Method: 开发了34个代表性的可编程任务作为评测基准，评估了三个流行开源组织框架与两个LLM核心的组合，并进行深度失败分析。

Result: 观察到任务完成率约为50%，形成了与任务阶段对应的三层失败分类：规划错误、任务执行问题和错误响应生成。

Conclusion: 研究提供的失败分类和缩减建议为开发更稳健有效的自主组织系统奠定了实证基础。

Abstract: Autonomous agent systems powered by Large Language Models (LLMs) have
demonstrated promising capabilities in automating complex tasks. However,
current evaluations largely rely on success rates without systematically
analyzing the interactions, communication mechanisms, and failure causes within
these systems. To bridge this gap, we present a benchmark of 34 representative
programmable tasks designed to rigorously assess autonomous agents. Using this
benchmark, we evaluate three popular open-source agent frameworks combined with
two LLM backbones, observing a task completion rate of approximately 50%.
Through in-depth failure analysis, we develop a three-tier taxonomy of failure
causes aligned with task phases, highlighting planning errors, task execution
issues, and incorrect response generation. Based on these insights, we propose
actionable improvements to enhance agent planning and self-diagnosis
capabilities. Our failure taxonomy, together with mitigation advice, provides
an empirical foundation for developing more robust and effective autonomous
agent systems in the future.

</details>
