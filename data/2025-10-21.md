<div id=toc></div>

# Table of Contents

- [physics.ao-ph](#physics.ao-ph) [Total: 3]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.CV](#cs.CV) [Total: 181]
- [cs.AI](#cs.AI) [Total: 70]


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [1] [Asymptotically well-balanced geostrophic reconstruction finite volumes numerical schemes for the 2D rotating NLSWE in spherical coordinates](https://arxiv.org/abs/2510.16002)
*Alejandro González del Pino,Manuel Jesús Castro Díaz,Jorge Macías Sánchez*

Main category: physics.ao-ph

TL;DR: 开发了用于球坐标下二维旋转浅水方程的二阶和三阶有限体积数值格式，旨在在罗斯贝数趋近于零时保持地转平衡，用于模拟由大气压力扰动引起的海啸事件。


<details>
  <summary>Details</summary>
Motivation: 设计可靠高效的预报模型来模拟由大气压力扰动在海洋中产生的长波事件（气象海啸），这些扰动会产生小振幅长波，在接近海岸时逐渐放大。

Method: 开发了应用于球坐标下二维旋转浅水方程的二阶和三阶有限体积数值格式，这些格式专门设计用于在罗斯贝数趋近于零时保持地转平衡。

Result: 各种分析和实际测试案例的数值结果强调了随时间保持地转平衡的重要性。

Conclusion: 所开发的数值格式能够有效保持地转平衡，为模拟气象海啸提供了可靠的数值工具，验证了在长时间模拟中维持地转平衡的关键作用。

Abstract: The dynamics of large-scale geophysical fluids is primarily governed by the
balance between the Coriolis force and the pressure gradient.
  This phenomenon, known as geostrophic equilibrium, is the basis for the
geostrophic model, which has proven to be extremely useful for understanding
and forecasting large-scale atmospheric and oceanic dynamics.
  In the present work, we develop second- and third-order finite-volume
numerical schemes applied to the 2D rotating shallow-water equations in
spherical coordinates. These schemes are designed to preserve the geostrophic
equilibrium in the limit as the Rossby number tends to zero.
  The final goal is to design reliable and efficient forecasting models for
simulating meteotsunamis, long-wave events generated in the ocean by
atmospheric pressure disturbances. These disturbances produce long waves of
small amplitude that gradually amplify as they approach the coast.
  The numerical results for various analytical and real-world test cases
underscore the importance of maintaining geostrophic equilibrium over time.

</details>


### [2] [A Storm-Centric 250 m NEXRAD Level-II Dataset for High-Resolution ML Nowcasting](https://arxiv.org/abs/2510.16031)
*Andy Shi*

Main category: physics.ao-ph

TL;DR: 提出了Storm250-L2数据集，这是一个基于NEXRAD Level-II和GridRad-Severe数据的高分辨率（250米）风暴中心雷达数据集，用于改进极端天气的机器学习降水临近预报。


<details>
  <summary>Details</summary>
Motivation: 现有公共雷达数据集（如SEVIR、HKO-7、GridRad-Severe）分辨率较低（1-2公里），平滑了精细尺度结构，限制了能够预测极端天气的模型发展。

Method: 从GridRad-Severe风暴轨迹中算法裁剪固定高分辨率（250米）窗口，保留原生极坐标几何，提供时间一致的单次扫描和伪合成反射率产品序列。

Result: 数据集包含美国大陆数千个风暴事件，以HDF5张量形式打包，带有丰富的上下文元数据和可重现清单。

Conclusion: Storm250-L2填补了高分辨率风暴雷达数据的空白，为改进极端天气的机器学习预报模型提供了关键数据支持。

Abstract: Machine learning-based precipitation nowcasting relies on high-fidelity radar
reflectivity sequences to model the short-term evolution of convective storms.
However, the development of models capable of predicting extreme weather has
been constrained by the coarse resolution (1-2 km) of existing public radar
datasets, such as SEVIR, HKO-7, and GridRad-Severe, which smooth the fine-scale
structures essential for accurate forecasting. To address this gap, we
introduce Storm250-L2, a storm-centric radar dataset derived from NEXRAD
Level-II and GridRad-Severe data. We algorithmically crop a fixed,
high-resolution (250 m) window around GridRad-Severe storm tracks, preserve the
native polar geometry, and provide temporally consistent sequences of both
per-tilt sweeps and a pseudo-composite reflectivity product. The dataset
comprises thousands of storm events across the continental United States,
packaged in HDF5 tensors with rich context metadata and reproducible manifests.

</details>


### [3] [A purely analytical and physical wind turbine wake model accounting for atmospheric stratification](https://arxiv.org/abs/2510.17236)
*Emeline Noël,Erwan Jézéquel,Pierre-Antoine Joulin*

Main category: physics.ao-ph

TL;DR: 提出了一种基于大气湍流与风机动力学物理相互作用的纯解析尾流模型，无需经验假设或可调参数，仅需输入湍流强度和湍流积分时间尺度即可预测尾流亏损。


<details>
  <summary>Details</summary>
Motivation: 现有经验模型依赖假设的尾流亏损形状或可调系数，缺乏物理基础。本文旨在开发一个完全基于物理相互作用、能自然考虑大气层结影响的尾流模型。

Method: 推导纯解析尾流模型，仅使用可测量的入流特性（湍流强度和湍流积分时间尺度）作为输入，通过大涡模拟（LES）对IEA 15MW和NREL 5MW风机在不同大气条件下进行系统验证。

Result: 模型在稳定、中性和不稳定大气条件下与LES结果高度一致，对积分时间尺度敏感性较弱，确保在入流特性存在适度不确定性时的鲁棒性。与Super-Gaussian模型相比，在非稳定和中性层结条件下表现更优。

Conclusion: 通过纳入更丰富的入流物理特性获得的预测精度证明了需要更全面大气输入的必要性，为风能运营中基于物理、无需校准的尾流建模提供了清晰路径。

Abstract: A purely analytical wake model for wind turbines is derived, anchored
exclusively in physical interactions between atmospheric turbulence and turbine
dynamics, and thus inherently accounting for atmospheric stratification. Unlike
empirical models relying on assumed wake deficit shapes or tunable
coefficients, this model predicts the wake deficit solely from measurable
properties of the inflow, namely, turbulence intensity and the turbulence
integral time scale. Systematic validation against Large Eddy Simulations (LES)
for both IEA 15MW and NREL 5MW turbines, simulated in Meso-NH under stable,
neutral, and unstable conditions, demonstrates excellent agreement across
atmospheric regimes. Importantly, the model requires these specific turbulence
statistics as input but shows only weak sensitivity to the integral time scale,
ensuring robustness even with moderate uncertainties in inflow
characterisation. Comparative analysis with the state-of-the-art Super-Gaussian
analytical model highlights superior performance of the present approach,
particularly for unstable and neutral stratification. These results show that
the predictive accuracy gained by incorporating richer inflow physics justifies
the need for more comprehensive atmospheric inputs, providing a clear pathway
for physically grounded, calibration-free wake modeling in operational wind
energy contexts.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [4] [Call-Center Staff Scheduling Considering Performance Evolution under Emotional Stress](https://arxiv.org/abs/2510.16406)
*Yujun Zheng,Xinya Chen,Xueqin Lu,Weiguo Sheng,Shengyong Chen*

Main category: cs.NE

TL;DR: 提出了一种考虑情绪压力对员工工作表现影响的呼叫中心人员调度方法，结合情感压力驱动模型和模因优化算法来最大化客户服务水平。


<details>
  <summary>Details</summary>
Motivation: 现有人员调度方法通常忽视情绪压力对员工工作表现的显著影响，需要更真实地理解和利用人类行为进行调度。

Method: 建立情感压力驱动模型评估员工工作表现，提出结合全局变异和邻域搜索的模因优化算法，并利用深度强化学习辅助求解。

Result: 在银行呼叫中心实际案例上的实验结果表明，该方法在性能上优于所选流行的人员调度方法。

Conclusion: 通过显式建模和融入情绪压力，该方法在人员调度中体现了对人类行为更现实的理解和利用。

Abstract: Emotional stress often has a significant effect on the working performance of
staff, but this effect is commonly neglected in existing staff scheduling
methods. We study a call-center staff scheduling problem, which considers the
evolution of work performance of staff under emotional stress. First, we
present an emotional stress driven model that estimates the working performance
of call-center employees based on not only skill levels but also emotional
states. On the basis of the model, we formulate a combined short-term and
long-term call-center staff scheduling problem aiming at maximizing the
customer service level, which depends on the working performance of employees.
We then propose a memetic optimization algorithm combining global mutation and
neighborhood search assisted by deep reinforcement learning to efficiently
solve this problem. Experimental results on real-world problem instances of
bank call-center staff scheduling demonstrate the performance advantages of the
proposed method over selected popular staff scheduling methods. By explicitly
modeling and incorporating emotional stress, our method reflects a more
realistic understanding and utilization of human behavior in staff scheduling.

</details>


### [5] [Bombardier Beetle Optimizer: A Novel Bio-Inspired Algorithm for Global Optimization](https://arxiv.org/abs/2510.17005)
*Hisham A. Shehadeh,Mohd Yamani Idna Idris,Iqbal H. Jebril*

Main category: cs.NE

TL;DR: 提出了一种新的仿生优化算法——轰炸甲虫优化器(BBO)，灵感来自轰炸甲虫的防御和逃生机制，在CEC 2017测试套件上表现优于多种知名元启发式算法。


<details>
  <summary>Details</summary>
Motivation: 受轰炸甲虫智能防御和逃生行为的启发，开发一种新的优化算法。轰炸甲虫能够感知威胁并触发化学喷雾防御，同时计算与捕食者的距离并飞行逃离。

Method: BBO算法模拟轰炸甲虫的两种机制：防御机制（触发有毒化学喷雾）和逃生机制（计算距离并飞行逃离）。算法在CEC 2017测试套件上进行验证。

Result: BBO算法在收敛速度和解的质量方面均优于Chernobyl Disaster Optimizer、Grey Wolf Optimizer、Particle Swarm Optimization、Bermuda Triangle Optimizer、Sperm Swarm Optimization和Gravitational Search Algorithm等对比算法。

Conclusion: BBO算法通过模拟轰炸甲虫的智能行为，在优化问题上表现出色，证明了其有效性和优越性。

Abstract: In this paper, a novel bio-inspired optimization algorithm is proposed,
called Bombardier Beetle Optimizer (BBO). This type of species is very
intelligent, which has an ability to defense and escape from predators. The
principles of the former one is inspired by the defense mechanism of Bombardier
Beetle against the predators, which the Bombardier Beetle triggers a toxic
chemical spray when it feels threatened. This reaction occurs in a specialized
reaction chamber inside its abdomen and includes a well regulated enzymatic
mechanism, which comprises hot water vapor, oxygen, and irritating substances
like p-benzoquinones. In addition, the proposed BBO simulates also the escape
mechanism of Bombardier Beetle from predator, which it has the ability to
calculate its distance from predator and it can fly away. The BBO is tested
with optimizing Congress on Evolutionary Computation (CEC 2017) test bed
suites. Moreover, it is compared against well-known metaheuristic optimization
algorithms includes Chernobyl Disaster Optimizer (CDO), Grey Wolf Optimizer
(GWO), Particle Swarm Optimization (PSO), Bermuda Triangle Optimizer (BTO),
Sperm Swarm Optimization (SSO) and Gravitational Search Algorithm (GSA). The
outcomes of this paper prove the BBO's efficiency in which outperforms the
other algorithms in terms of convergence rate and quality of results.

</details>


### [6] [ReLACE: A Resource-Efficient Low-Latency Cortical Acceleration Engine](https://arxiv.org/abs/2510.17392)
*Sonu Kumar,Arjun S. Nair,Bhawna Chaudhary,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.NE

TL;DR: 提出了一种基于CORDIC的Hodgkin Huxley神经元模型（RCHH）和皮质神经池（CNP）架构，在FPGA上实现了资源高效、高速的脉冲神经网络实现。


<details>
  <summary>Details</summary>
Motivation: 为资源受限的边缘AI应用开发生物学准确、低资源的脉冲神经网络实现，解决现有方法在速度和资源效率方面的限制。

Method: 使用模块化和性能优化的CORDIC阶段构建RCHH神经元模型，采用延迟-面积权衡策略，并在FPGA上实现皮质神经池架构。

Result: RCHH神经元相比最先进设计减少24.5% LUT、提升35.2%速度，NRMSE改善70%；CNP架构相比等效CORDIC-based DNN引擎吞吐量提升2.85倍（12.69 GOPS），在MNIST数据集上仅损失0.35%准确率。

Conclusion: 该设计为资源受限的边缘AI应用提供了生物学准确、低资源的脉冲神经网络实现方案。

Abstract: We present a Cortical Neural Pool (CNP) architecture featuring a high-speed,
resource-efficient CORDIC-based Hodgkin Huxley (RCHH) neuron model. Unlike
shared CORDIC-based DNN approaches, the proposed neuron leverages modular and
performance-optimised CORDIC stages with a latency-area trade-off. The FPGA
implementation of the RCHH neuron shows 24.5% LUT reduction and 35.2% improved
speed, compared to SoTA designs, with 70% better normalised root mean square
error (NRMSE). Furthermore, the CNP exhibits 2.85x higher throughput (12.69
GOPS) compared to a functionally equivalent CORDIC-based DNN engine, with only
a 0.35% accuracy drop compared to the DNN counterpart on the MNIST dataset. The
overall results indicate that the design shows biologically accurate,
low-resource spiking neural network implementations for resource-constrained
edge AI applications.

</details>


### [7] [A Multi-Threading Kernel for Enabling Neuromorphic Edge Applications](https://arxiv.org/abs/2510.17745)
*Lars Niedermeier,Vyom Shah,Jeffrey L. Krichmar*

Main category: cs.NE

TL;DR: 提出了一种多线程内核，使脉冲神经网络能够在边缘设备上运行，相比单线程处理速度提升4倍，能效提升70%，并支持多核负载均衡。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络具有稀疏、事件驱动的处理特性，适合神经形态应用。但需要解决边缘设备上的高效运行问题，避免对云服务的依赖。

Method: 开发了一个多线程内核，能够在多核处理器（如ARM）上实现负载均衡，支持神经形态应用直接在边缘设备上处理感官输入。

Result: 在中等规模SNN上速度提升4倍，在Synfire网络上提升1.7倍，能效比静态核心分配提高70%，有效利用所有可用核心。

Conclusion: 该工作能够推动开发低SWaP（尺寸、重量和功耗）的边缘应用，并为神经形态芯片的集成提供原型支持。

Abstract: Spiking Neural Networks (SNNs) have sparse, event driven processing that can
leverage neuromorphic applications. In this work, we introduce a
multi-threading kernel that enables neuromorphic applications running at the
edge, meaning they process sensory input directly and without any up-link to or
dependency on a cloud service. The kernel shows speed-up gains over single
thread processing by a factor of four on moderately sized SNNs and 1.7X on a
Synfire network. Furthermore, it load-balances all cores available on
multi-core processors, such as ARM, which run today's mobile devices and is up
to 70% more energy efficient compared to statical core assignment. The present
work can enable the development of edge applications that have low Size,
Weight, and Power (SWaP), and can prototype the integration of neuromorphic
chips.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang,Amish Sethi,Matthew Kuo,Mayank Keoliya,Neelay Velingker,JungHo Jung,Ser-Nam Lim,Ziyang Li,Mayur Naik*

Main category: cs.CV

TL;DR: 提出了ESCA框架和SGClip模型，通过结构化时空理解来增强多模态大语言模型在具身智能体中的表现。SGClip是一种基于CLIP的可提示场景图生成模型，无需人工标注，在场景图生成和动作定位任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型主要依赖高层视觉-声音-文本对，缺乏像素级视觉内容与文本语义之间的细粒度结构化对齐，限制了具身智能体的性能。

Method: 提出ESCA框架，核心是SGClip模型。SGClip通过神经符号学习管道在8.7万+开放域视频上训练，利用视频-字幕对进行模型驱动的自监督学习和结构化推理，无需人工标注的场景图。

Result: SGClip在场景图生成和动作定位基准测试中表现出色。ESCA框架持续改进开源和商业多模态大语言模型，在两个具身环境中实现最先进性能，显著减少智能体感知错误，使开源模型超越专有基线。

Conclusion: ESCA框架通过结构化空间-时间理解有效提升了具身智能体的性能，SGClip模型展示了无需人工标注即可实现高质量场景图生成的可行性，为通用具身智能体发展提供了重要推动。

Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.

</details>


### [9] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

Main category: cs.CV

TL;DR: 提出CrossRay3D稀疏多模态检测器，通过Ray-Aware Supervision和Class-Balanced Supervision提升token表示质量，在nuScenes基准上达到SOTA性能，且对传感器缺失具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏检测器忽视token表示质量，导致前景质量次优和性能受限。研究发现几何结构保持和类别分布是提升稀疏检测器性能的关键。

Method: 提出Sparse Selector(SS)，核心模块包括：Ray-Aware Supervision(RAS)在训练阶段保持丰富几何信息，Class-Balanced Supervision自适应重加权类别语义显著性，以及Ray Positional Encoding解决LiDAR和图像模态间的分布差异。

Result: 在nuScenes基准上达到72.4 mAP和74.7 NDS的SOTA性能，运行速度比其他领先方法快1.84倍，且在LiDAR或相机数据部分或完全缺失的场景下表现出强鲁棒性。

Conclusion: CrossRay3D通过改进token表示质量，在保持稀疏检测器优势的同时显著提升了性能，证明了几何结构保持和类别平衡监督的重要性。

Abstract: The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.

</details>


### [10] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

TL;DR: 提出一个利用街头CCTV监控进行多缺陷检测和分割的完整管道，结合YOLO目标检测器和视觉语言模型生成结构化维护计划


<details>
  <summary>Details</summary>
Motivation: 智能城市基础设施监控需求增长，手动检查成本高且危险，现有自动系统通常只处理单一缺陷类型或输出非结构化结果，无法直接指导维护工作

Method: 使用YOLO系列目标检测器进行多缺陷检测和分割，然后将检测结果传递给视觉语言模型进行场景感知总结，生成包含事件描述、推荐工具、尺寸、维修计划和紧急警报的JSON格式结构化行动计划

Result: 在公共数据集和捕获的CCTV片段上的实验评估表明，系统能准确识别多种缺陷并生成连贯的总结

Conclusion: 讨论了将系统扩展到城市范围部署的挑战和方向

Abstract: Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [11] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li,Zitong Yu,Qilang Ye,Weicheng Xie,Wei Zhuo,Linlin Shen*

Main category: cs.CV

TL;DR: 提出IAD-GPT，一种基于多模态大语言模型的工业异常检测新范式，通过异常提示生成器、文本引导增强器和多掩码融合模块，结合图像级和像素级信息进行异常检测和分割。


<details>
  <summary>Details</summary>
Motivation: 传统工业异常检测方法缺乏多轮人机对话和详细描述能力，而基于大预训练模型的方法尚未充分发挥大模型在异常检测任务中的潜力。

Method: 使用异常提示生成器生成详细异常提示，通过文本引导增强器增强视觉定位能力，设计多掩码融合模块引入掩码作为专家知识。

Result: 在MVTec-AD和VisA数据集上的实验表明，该方法在自监督和少样本异常检测与分割任务上达到了最先进的性能。

Conclusion: IAD-GPT成功地将丰富的文本语义与图像级和像素级信息相结合，为工业异常检测提供了新的解决方案。

Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.

</details>


### [12] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

TL;DR: 比较三种放射学报告模式（自由文本、结构化报告、AI辅助结构化报告）对图像分析行为、诊断准确性、效率和用户体验的影响。AI辅助结构化报告在诊断准确性和效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 评估结构化报告和人工智能如何改变放射科医生与影像研究的交互方式，探索不同报告模式对诊断工作流程的影响。

Method: 前瞻性研究，8名读者（4名新手和4名非新手）使用定制化查看器和眼动追踪系统分析35张床边胸片，比较三种报告模式的诊断准确性、报告时间、眼动指标和用户体验。

Result: AI辅助结构化报告的诊断准确性最高（κ=0.71），报告时间最短（25±9秒），眼动指标显示视觉注意力更集中于图像区域，且是用户首选模式。

Conclusion: 结构化报告通过引导视觉注意力向图像而提高效率，AI预填充的结构化报告进一步提升了诊断准确性和用户满意度。

Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [13] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

TL;DR: 提出了一个数据驱动的框架IFEF来分析图像分类中的交叉偏见，并开发了BWA数据增强方法来缓解这些偏见，在Open Images V7数据集上显著提升了欠代表类别的准确率。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在非平衡数据集上训练时经常表现出交叉偏见，这些偏见源于多个属性（如物体类别和环境条件）的交互作用，需要系统性的分析和缓解方法。

Method: 提出了交叉公平性评估框架(IFEF)结合定量公平性指标和可解释性工具来识别偏见模式，并开发了基于子群分布统计的自适应数据增强策略BWA。

Result: 在Open Images V7数据集上，BWA将欠代表类别-环境交叉的准确率提升了最高24个百分点，同时将公平性指标差异减少了35%，统计显著性得到确认(p < 0.05)。

Conclusion: 该方法为分析和解决图像分类系统中的交叉偏见提供了一个可复现的框架。

Abstract: Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [14] [Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch](https://arxiv.org/abs/2510.16088)
*Zia Badar*

Main category: cs.CV

TL;DR: 提出了一种可微分的神经网络量化方法，支持多比特对数量化，在ImageNet数据集上使用ResNet18进行权重量化时，仅需15个训练周期就能达到与全精度模型相差不到1%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法存在两个主要问题：一是大多采用不可微分的方法，在反向传播时需要手动设置导数，影响学习能力；二是对数/移位量化要么避免激活量化，要么准确率较低。

Method: 开发了可微分的量化方法，支持n比特对数量化（形式为2^n），提供了收敛性证明，能够同时进行权重和激活量化。

Result: 在ImageNet图像分类任务中，仅权重量化时准确率与全精度模型相差不到1%；权重和激活同时量化时达到SOTA水平，仅需15个训练周期，推理成本略高于1比特量化但无需高精度乘法。

Conclusion: 该方法提供了一种高效的可微分量化方案，在保持高准确率的同时显著减少了计算和内存需求。

Abstract: Quantization of neural networks provides benefits of inference in less
compute and memory requirements. Previous work in quantization lack two
important aspects which this work provides. First almost all previous work in
quantization used a non-differentiable approach and for learning; the
derivative is usually set manually in backpropogation which make the learning
ability of algorithm questionable, our approach is not just differentiable, we
also provide proof of convergence of our approach to the optimal neural
network. Second previous work in shift/logrithmic quantization either have
avoided activation quantization along with weight quantization or achieved less
accuracy. Learning logrithmic quantize values of form $2^n$ requires the
quantization function can scale to more than 1 bit quantization which is
another benifit of our quantization that it provides $n$ bits quantization as
well. Our approach when tested with image classification task using imagenet
dataset, resnet18 and weight quantization only achieves less than 1 percent
accuracy compared to full precision accuracy while taking only 15 epochs to
train using shift bit quantization and achieves comparable to SOTA approaches
accuracy in both weight and activation quantization using shift bit
quantization in 15 training epochs with slightly higher(only higher cpu
instructions) inference cost compared to 1 bit quantization(without logrithmic
quantization) and not requiring any higher precision multiplication.

</details>


### [15] [StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection](https://arxiv.org/abs/2510.16115)
*Jianhan Lin,Yuchu Qin,Shuai Gao,Yikang Rui,Jie Liu,Yanjie Lv*

Main category: cs.CV

TL;DR: 提出StripRFNet网络用于道路表面损伤检测，通过三个模块解决形状多样性、细长裂缝检测和小目标识别问题，在RDD2022基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 道路表面损伤威胁交通安全并阻碍可持续城市发展，但准确检测面临损伤形状多样、细长裂缝难以捕获和小尺度损伤识别错误率高的挑战。

Method: 包含三个模块：形状感知模块(SPM)通过大分离核注意力增强形状判别；条带感受野模块(SRFM)使用大条带卷积和池化捕获细长裂缝特征；小尺度增强模块(SSEM)利用高分辨率P2特征图、专用检测头和动态上采样改进小目标检测。

Result: 在RDD2022基准测试中，中国子集的F1分数、mAP50和mAP50:95分别比基线提高4.4、2.9和3.4个百分点；完整数据集上达到80.33%的最高F1分数，同时保持有竞争力的推理速度。

Conclusion: StripRFNet实现了最先进的准确性和实时效率，为智能道路维护和可持续基础设施管理提供了有前景的工具。

Abstract: Well-maintained road networks are crucial for achieving Sustainable
Development Goal (SDG) 11. Road surface damage not only threatens traffic
safety but also hinders sustainable urban development. Accurate detection,
however, remains challenging due to the diverse shapes of damages, the
difficulty of capturing slender cracks with high aspect ratios, and the high
error rates in small-scale damage recognition. To address these issues, we
propose StripRFNet, a novel deep neural network comprising three modules: (1) a
Shape Perception Module (SPM) that enhances shape discrimination via large
separable kernel attention (LSKA) in multi-scale feature aggregation; (2) a
Strip Receptive Field Module (SRFM) that employs large strip convolutions and
pooling to capture features of slender cracks; and (3) a Small-Scale
Enhancement Module (SSEM) that leverages a high-resolution P2 feature map, a
dedicated detection head, and dynamic upsampling to improve small-object
detection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses
existing methods. On the Chinese subset, it improves F1-score, mAP50, and
mAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,
respectively. On the full dataset, it achieves the highest F1-score of 80.33%
compared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while
maintaining competitive inference speed. These results demonstrate that
StripRFNet achieves state-of-the-art accuracy and real-time efficiency,
offering a promising tool for intelligent road maintenance and sustainable
infrastructure management.

</details>


### [16] [ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles](https://arxiv.org/abs/2510.16118)
*Nishad Sahu,Shounak Sural,Aditya Satish Patil,Ragunathan,Rajkumar*

Main category: cs.CV

TL;DR: ObjectTransforms是一种用于在自动驾驶视觉目标检测中量化和减少不确定性的技术，通过在训练和推理时对目标进行特定变换来提高检测可靠性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中可靠的感知对安全决策至关重要，但基于视觉的目标检测神经网络容易受到数据偏差和分布偏移等不确定性因素的影响。

Method: 在训练时对单个目标进行颜色空间扰动，并使用扩散模型生成多样化的行人实例；在推理时对检测到的目标应用扰动，利用检测分数的方差实时量化预测不确定性。

Result: 在NuImages 10K数据集上的实验表明，该方法在所有目标类别上都带来了显著的准确率提升和不确定性降低，误报的不确定性值高于真报。

Conclusion: ObjectTransforms作为一种轻量级但有效的机制，分别在训练和推理时减少和量化基于视觉的感知不确定性，具有重要潜力。

Abstract: Reliable perception is fundamental for safety critical decision making in
autonomous driving. Yet, vision based object detector neural networks remain
vulnerable to uncertainty arising from issues such as data bias and
distributional shifts. In this paper, we introduce ObjectTransforms, a
technique for quantifying and reducing uncertainty in vision based object
detection through object specific transformations at both training and
inference times. At training time, ObjectTransforms perform color space
perturbations on individual objects, improving robustness to lighting and color
variations. ObjectTransforms also uses diffusion models to generate realistic,
diverse pedestrian instances. At inference time, object perturbations are
applied to detected objects and the variance of detection scores are used to
quantify predictive uncertainty in real time. This uncertainty signal is then
used to filter out false positives and also recover false negatives, improving
the overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K
dataset demonstrate that our method yields notable accuracy improvements and
uncertainty reduction across all object classes during training, while
predicting desirably higher uncertainty values for false positives as compared
to true positives during inference. Our results highlight the potential of
ObjectTransforms as a lightweight yet effective mechanism for reducing and
quantifying uncertainty in vision-based perception during training and
inference respectively.

</details>


### [17] [Aria Gen 2 Pilot Dataset](https://arxiv.org/abs/2510.16134)
*Chen Kong,James Fort,Aria Kang,Jonathan Wittmer,Simon Green,Tianwei Shen,Yipu Zhao,Cheng Peng,Gustavo Solaira,Andrew Berkovich,Nikhil Raina,Vijay Baiyya,Evgeniy Oleinik,Eric Huang,Fan Zhang,Julian Straub,Mark Schwesinger,Luis Pesqueira,Xiaqing Pan,Jakob Julian Engel,Carl Ren,Mingfei Yan,Richard Newcombe*

Main category: cs.CV

TL;DR: A2PD是一个使用Aria Gen 2眼镜采集的自我中心多模态开放数据集，包含清洁、烹饪、进食、玩耍和户外步行五种场景的原始传感器数据和感知算法输出。


<details>
  <summary>Details</summary>
Motivation: 为研究社区提供及时可用的多模态自我中心数据集，展示设备感知佩戴者、环境和交互的能力。

Method: 使用Aria Gen 2眼镜采集主要对象Dia'ane及其朋友的日常活动数据，提供原始传感器数据和多种机器感知算法的输出数据。

Result: 数据集已公开发布在projectaria.com，包含开源工具和使用示例，展示了设备在不同用户和条件下的稳健性能。

Conclusion: A2PD是一个逐步发布的开放数据集，为自我中心多模态研究提供了有价值的资源。

Abstract: The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset
captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely
access, A2PD is released incrementally with ongoing dataset enhancements. The
initial release features Dia'ane, our primary subject, who records her daily
activities alongside friends, each equipped with Aria Gen 2 glasses. It
encompasses five primary scenarios: cleaning, cooking, eating, playing, and
outdoor walking. In each of the scenarios, we provide comprehensive raw sensor
data and output data from various machine perception algorithms. These data
illustrate the device's ability to perceive the wearer, the surrounding
environment, and interactions between the wearer and the environment, while
maintaining robust performance across diverse users and conditions. The A2PD is
publicly available at projectaria.com, with open-source tools and usage
examples provided in Project Aria Tools.

</details>


### [18] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: 提出了一种无需训练的方法，通过引导预训练的整流流模型来将外观（图像或文本）转移到3D资产上，解决了几何差异大时的外观迁移问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在输入和外观对象的几何形状差异较大时效果不佳，直接应用3D生成模型会产生不理想的结果，需要更有效的外观迁移方法。

Method: 基于通用引导思想，在采样过程中定期添加可微损失函数作为引导，包括部分感知损失和自相似性损失，无需额外训练。

Result: 方法成功地将纹理和几何细节转移到3D资产，在定性和定量评估中均优于基线方法，并通过GPT评估系统验证了效果。

Conclusion: 该方法具有通用性，可扩展到不同类型的扩散模型和引导函数，为外观迁移任务提供了有效的解决方案。

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [19] [C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy](https://arxiv.org/abs/2510.16145)
*Ahmad Arrabi,Jay hwasung Jung,J Le,A Nguyen,J Reed,E Stahl,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出使用深度学习自动化血栓切除术关键环节的自监督框架，通过基于回归的预训练任务分类骨骼标志点，提升下游分类性能。


<details>
  <summary>Details</summary>
Motivation: 血栓切除术是缺血性中风最有效的治疗方法之一，但资源密集且人员需求高，希望通过深度学习自动化关键环节来提高效率和安全性。

Method: 引入自监督框架，使用基于回归的预训练任务来分类各种骨骼标志点。

Result: 实验表明该模型在回归和分类任务上均优于现有方法，位置预训练任务显著提升了下游分类性能。

Conclusion: 未来工作将扩展该框架以实现完全自主的C臂控制，优化从中骨盆到头部的轨迹，提升中风血栓切除术的手术效果。

Abstract: Thrombectomy is one of the most effective treatments for ischemic stroke, but
it is resource and personnel-intensive. We propose employing deep learning to
automate critical aspects of thrombectomy, thereby enhancing efficiency and
safety. In this work, we introduce a self-supervised framework that classifies
various skeletal landmarks using a regression-based pretext task. Our
experiments demonstrate that our model outperforms existing methods in both
regression and classification tasks. Notably, our results indicate that the
positional pretext task significantly enhances downstream classification
performance. Future work will focus on extending this framework toward fully
autonomous C-arm control, aiming to optimize trajectories from the pelvis to
the head during stroke thrombectomy procedures. All code used is available at
https://github.com/AhmadArrabi/C_arm_guidance

</details>


### [20] [DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization](https://arxiv.org/abs/2510.16146)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Vi Vu,Ba-Thinh Lam,Phat Huynh,Tianyang Wang,Xingjian Li,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: 提出DuetMatch，一种用于医学图像分割的双分支半监督框架，通过异步优化编码器和解码器，结合解耦丢弃扰动和配对CutMix交叉引导来提高模型鲁棒性和多样性。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注数据有限，半监督学习具有吸引力。现有师生框架联合优化整个网络会影响收敛和稳定性，特别是在挑战性场景中。

Method: 双分支异步优化架构，每个分支分别优化编码器或解码器；引入解耦丢弃扰动进行正则化；设计配对CutMix交叉引导增强模型多样性；提出一致性匹配减少噪声伪标签的确认偏差。

Result: 在ISLES2022和BraTS等脑MRI分割基准数据集上的广泛实验表明，DuetMatch始终优于最先进方法。

Conclusion: DuetMatch在不同半监督分割场景中展现出有效性和鲁棒性，为医学图像分割提供了一种强大的半监督学习解决方案。

Abstract: The limited availability of annotated data in medical imaging makes
semi-supervised learning increasingly appealing for its ability to learn from
imperfect supervision. Recently, teacher-student frameworks have gained
popularity for their training benefits and robust performance. However, jointly
optimizing the entire network can hinder convergence and stability, especially
in challenging scenarios. To address this for medical image segmentation, we
propose DuetMatch, a novel dual-branch semi-supervised framework with
asynchronous optimization, where each branch optimizes either the encoder or
decoder while keeping the other frozen. To improve consistency under noisy
conditions, we introduce Decoupled Dropout Perturbation, enforcing
regularization across branches. We also design Pair-wise CutMix Cross-Guidance
to enhance model diversity by exchanging pseudo-labels through augmented input
pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose
Consistency Matching, refining labels using stable predictions from frozen
teacher models. Extensive experiments on benchmark brain MRI segmentation
datasets, including ISLES2022 and BraTS, show that DuetMatch consistently
outperforms state-of-the-art methods, demonstrating its effectiveness and
robustness across diverse semi-supervised segmentation scenarios.

</details>


### [21] [Automated C-Arm Positioning via Conformal Landmark Localization](https://arxiv.org/abs/2510.16160)
*Ahmad Arrabi,Jay Hwasung Jung,Jax Luo,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出了一种自主导航C臂到预定义解剖标志的管道，利用X射线图像预测3D位移向量，并整合不确定性量化和保形预测以确保可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前临床工作流程依赖手动对齐C臂，这会增加辐射暴露和手术延迟，需要自动化解决方案来提高效率和安全性。

Method: 使用X射线图像预测3D位移向量，结合概率损失和骨骼姿态正则化，采用保形预测校准不确定性，生成3D置信区域。

Result: 在DeepDRR生成的合成X射线数据集上验证，显示出强大的定位精度和良好校准的预测边界。

Conclusion: 该管道有潜力成为安全可靠自主C臂系统的组成部分，能够减少辐射暴露并提高手术效率。

Abstract: Accurate and reliable C-arm positioning is essential for fluoroscopy-guided
interventions. However, clinical workflows rely on manual alignment that
increases radiation exposure and procedural delays. In this work, we present a
pipeline that autonomously navigates the C-arm to predefined anatomical
landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary
starting location on the operating table, the model predicts a 3D displacement
vector toward each target landmark along the body. To ensure reliable
deployment, we capture both aleatoric and epistemic uncertainties in the
model's predictions and further calibrate them using conformal prediction. The
derived prediction regions are interpreted as 3D confidence regions around the
predicted landmark locations. The training framework combines a probabilistic
loss with skeletal pose regularization to encourage anatomically plausible
outputs. We validate our approach on a synthetic X-ray dataset generated from
DeepDRR. Results show not only strong localization accuracy across multiple
architectures but also well-calibrated prediction bounds. These findings
highlight the pipeline's potential as a component in safe and reliable
autonomous C-arm systems. Code is available at
https://github.com/AhmadArrabi/C_arm_guidance_APAH

</details>


### [22] [Cost Savings from Automatic Quality Assessment of Generated Images](https://arxiv.org/abs/2510.16179)
*Xavier Giro-i-Nieto,Nefeli Andreou,Anqi Liang,Manel Baradad,Francesc Moreno-Noguer,Aleix Martinez*

Main category: cs.CV

TL;DR: 提出一个公式来估计图像质量评估(IQA)引擎的成本节省，应用于背景修复用例，通过AutoML解决方案实现51.61%的成本节省


<details>
  <summary>Details</summary>
Motivation: 当前深度生成模型生成的图像质量仍不如传统摄影方法，生产流程中需要人工图像质量评估，这个过程既慢又昂贵

Method: 引入自动预过滤阶段，使用公式估计成本节省，并在背景修复用例中应用简单的AutoML解决方案

Result: 在背景修复用例中实现了51.61%的成本节省

Conclusion: 自动预过滤可以有效减少人工图像质量评估的工作量，显著降低获取高质量图像的平均成本

Abstract: Deep generative models have shown impressive progress in recent years, making
it possible to produce high quality images with a simple text prompt or a
reference image. However, state of the art technology does not yet meet the
quality standards offered by traditional photographic methods. For this reason,
production pipelines that use generated images often include a manual stage of
image quality assessment (IQA). This process is slow and expensive, especially
because of the low yield of automatically generated images that pass the
quality bar. The IQA workload can be reduced by introducing an automatic
pre-filtering stage, that will increase the overall quality of the images sent
to review and, therefore, reduce the average cost required to obtain a high
quality image. We present a formula that estimates the cost savings depending
on the precision and pass yield of a generic IQA engine. This formula is
applied in a use case of background inpainting, showcasing a significant cost
saving of 51.61% obtained with a simple AutoML solution.

</details>


### [23] [Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI](https://arxiv.org/abs/2510.16196)
*Zheng Huang,Enpei Zhang,Yinghao Cai,Weikang Qiu,Carl Yang,Elynn Chen,Xiang Zhang,Rex Ying,Dawei Zhou,Yujun Yan*

Main category: cs.CV

TL;DR: 该论文提出PRISM模型，将fMRI信号映射到结构化文本空间作为中间表示，通过对象中心扩散和属性关系搜索模块重建视觉刺激，在真实数据集上比现有方法减少8%感知损失。


<details>
  <summary>Details</summary>
Motivation: 理解大脑如何编码视觉信息是神经科学和机器学习的核心挑战。当前方法通过fMRI信号重建图像，但不确定哪种潜在空间最适合这种转换以及如何有效组织来表示视觉刺激。

Method: 提出PRISM模型：将fMRI信号投影到结构化文本空间作为中间表示，包含对象中心扩散模块（通过组合单个对象生成图像以减少对象检测错误）和属性关系搜索模块（自动识别与神经活动最匹配的关键属性和关系）。

Result: 在真实世界数据集上的广泛实验表明，该框架优于现有方法，实现了高达8%的感知损失减少。

Conclusion: 研究结果表明，使用结构化文本作为中间空间来桥接fMRI信号和图像重建具有重要意义，fMRI信号与语言模型的文本空间更相似，而非基于视觉的空间或联合文本图像空间。

Abstract: Understanding how the brain encodes visual information is a central challenge
in neuroscience and machine learning. A promising approach is to reconstruct
visual stimuli, essentially images, from functional Magnetic Resonance Imaging
(fMRI) signals. This involves two stages: transforming fMRI signals into a
latent space and then using a pretrained generative model to reconstruct
images. The reconstruction quality depends on how similar the latent space is
to the structure of neural activity and how well the generative model produces
images from that space. Yet, it remains unclear which type of latent space best
supports this transformation and how it should be organized to represent visual
stimuli effectively. We present two key findings. First, fMRI signals are more
similar to the text space of a language model than to either a vision based
space or a joint text image space. Second, text representations and the
generative model should be adapted to capture the compositional nature of
visual stimuli, including objects, their detailed attributes, and
relationships. Building on these insights, we propose PRISM, a model that
Projects fMRI sIgnals into a Structured text space as an interMediate
representation for visual stimuli reconstruction. It includes an object centric
diffusion module that generates images by composing individual objects to
reduce object detection errors, and an attribute relationship search module
that automatically identifies key attributes and relationships that best align
with the neural activity. Extensive experiments on real world datasets
demonstrate that our framework outperforms existing methods, achieving up to an
8% reduction in perceptual loss. These results highlight the importance of
using structured text as the intermediate space to bridge fMRI signals and
image reconstruction.

</details>


### [24] [Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions](https://arxiv.org/abs/2510.16207)
*Mateus Pinto da Silva,Sabrina P. L. P. Correa,Hugo N. Oliveira,Ian M. Nunes,Jefersson A. dos Santos*

Main category: cs.CV

TL;DR: 本文提出采用数据为中心的人工智能方法来解决热带地区农业遥感制图的挑战，强调数据质量和整理是模型稳健性和可扩展性的关键因素。


<details>
  <summary>Details</summary>
Motivation: 热带地区农业遥感制图面临缺乏高质量标注数据、标注成本高、数据变异性和区域泛化等独特挑战，传统以模型为中心的方法受到限制。

Method: 采用数据为中心的人工智能视角和流程，重点应用自信学习、核心集选择、数据增强和主动学习等技术，提出了包含9种最成熟直接方法的实用流程。

Result: 识别并优先考虑了25种不同策略在大规模农业制图流程中的适用性，为热带农业动态现实提供了更好的AI模型训练方案。

Conclusion: 数据为中心的方法为热带农业遥感制图提供了实用的解决方案，通过数据整理和优化训练流程，能够更好地适应热带农业的动态特性。

Abstract: Mapping agriculture in tropical areas through remote sensing presents unique
challenges, including the lack of high-quality annotated data, the elevated
costs of labeling, data variability, and regional generalisation. This paper
advocates a Data-Centric Artificial Intelligence (DCAI) perspective and
pipeline, emphasizing data quality and curation as key drivers for model
robustness and scalability. It reviews and prioritizes techniques such as
confident learning, core-set selection, data augmentation, and active learning.
The paper highlights the readiness and suitability of 25 distinct strategies in
large-scale agricultural mapping pipelines. The tropical context is of high
interest, since high cloudiness, diverse crop calendars, and limited datasets
limit traditional model-centric approaches. This tutorial outlines practical
solutions as a data-centric approach for curating and training AI models better
suited to the dynamic realities of tropical agriculture. Finally, we propose a
practical pipeline using the 9 most mature and straightforward methods that can
be applied to a large-scale tropical agricultural mapping project.

</details>


### [25] [StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales](https://arxiv.org/abs/2510.16209)
*Nyle Siddiqui,Rohit Gupta,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出了一种针对视频状态空间模型（SSMs）的灵活训练方法StretchySnake，通过动态采样不同时空分辨率的视频并插值模型权重，解决了传统视频模型在未见时空分辨率下的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频理解训练方法主要针对transformer设计，未能充分利用SSMs的线性复杂度和隐藏状态递归特性。固定分辨率和视频长度的训练导致模型在未见时空分辨率下性能下降，限制了模型处理不同长度视频的能力。

Method: 提出灵活训练方法：在训练过程中采样不同时空分辨率的视频，动态插值模型权重以适应任意时空尺度。比较了五种不同的灵活训练变体，确定了最适合视频SSMs的策略。

Result: 在短动作（UCF-101、HMDB-51）和长动作（COIN、Breakfast）基准测试中，StretchySnake比transformer和SSM基线性能提升高达28%，在细粒度动作（SSV2、Diving-48）上表现出强适应性。

Conclusion: 该方法提供了一个简单的即插即用训练方案，使视频SSMs在各种动作识别场景中更加鲁棒、分辨率无关且高效。

Abstract: State space models (SSMs) have emerged as a competitive alternative to
transformers in various tasks. Their linear complexity and hidden-state
recurrence make them particularly attractive for modeling long sequences,
whereas attention becomes quadratically expensive. However, current training
methods for video understanding are tailored towards transformers and fail to
fully leverage the unique attributes of SSMs. For example, video models are
often trained at a fixed resolution and video length to balance the quadratic
scaling of attention cost against performance. Consequently, these models
suffer from degraded performance when evaluated on videos with spatial and
temporal resolutions unseen during training; a property we call spatio-temporal
inflexibility. In the context of action recognition, this severely limits a
model's ability to retain performance across both short- and long-form videos.
Therefore, we propose a flexible training method that leverages and improves
the inherent adaptability of SSMs. Our method samples videos at varying
temporal and spatial resolutions during training and dynamically interpolates
model weights to accommodate any spatio-temporal scale. This instills our SSM,
which we call StretchySnake, with spatio-temporal flexibility and enables it to
seamlessly handle videos ranging from short, fine-grained clips to long,
complex activities. We introduce and compare five different variants of
flexible training, and identify the most effective strategy for video SSMs. On
short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,
StretchySnake outperforms transformer and SSM baselines alike by up to 28%,
with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,
our method provides a simple drop-in training recipe that makes video SSMs more
robust, resolution-agnostic, and efficient across diverse action recognition
scenarios.

</details>


### [26] [VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction](https://arxiv.org/abs/2510.16220)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 提出VM-BeautyNet，一种融合Vision Transformer和Mamba视觉模型的异构集成架构，用于面部美观度预测，在SCUT-FBP5500数据集上取得最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有CNN模型难以捕捉对美观度判断至关重要的全局面部特征，而Vision Transformer虽然能建模长距离空间关系但存在二次复杂度问题。

Method: 设计异构集成架构，ViT主干捕获全局面部结构和对称性，Mamba主干以线性复杂度高效建模长距离依赖关系，专注于序列特征和纹理。

Result: 在SCUT-FBP5500数据集上取得PC=0.9212、MAE=0.2085、RMSE=0.2698的优异性能，并通过Grad-CAM可视化验证了两个主干网络的互补特征提取。

Conclusion: VM-BeautyNet为计算美学提供了一个强大的新架构范式，通过融合ViT和Mamba的互补优势，显著提升了面部美观度预测性能。

Abstract: Facial Beauty Prediction (FBP) is a complex and challenging computer vision
task, aiming to model the subjective and intricate nature of human aesthetic
perception. While deep learning models, particularly Convolutional Neural
Networks (CNNs), have made significant strides, they often struggle to capture
the global, holistic facial features that are critical to human judgment.
Vision Transformers (ViT) address this by effectively modeling long-range
spatial relationships, but their quadratic complexity can be a bottleneck. This
paper introduces a novel, heterogeneous ensemble architecture,
\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths
of a Vision Transformer and a Mamba-based Vision model, a recent advancement in
State-Space Models (SSMs). The ViT backbone excels at capturing global facial
structure and symmetry, while the Mamba backbone efficiently models long-range
dependencies with linear complexity, focusing on sequential features and
textures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our
proposed VM-BeautyNet achieves state-of-the-art performance, with a
\textbf{Pearson Correlation (PC) of 0.9212}, a \textbf{Mean Absolute Error
(MAE) of 0.2085}, and a \textbf{Root Mean Square Error (RMSE) of 0.2698}.
Furthermore, through Grad-CAM visualizations, we provide interpretability
analysis that confirms the complementary feature extraction of the two
backbones, offering new insights into the model's decision-making process and
presenting a powerful new architectural paradigm for computational aesthetics.

</details>


### [27] [Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection](https://arxiv.org/abs/2510.16235)
*Vishal Manikanden,Aniketh Bandlamudi,Daniel Haehn*

Main category: cs.CV

TL;DR: 使用卷积神经网络(CNN)结合图像采集硬件来早期检测口腔鳞状细胞癌(OCSCC)，研究图像分辨率对检测准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 口腔鳞状细胞癌(OCSCC)是头颈部最常见的癌症，由于早期症状不明显、生长缓慢且发生在深层隐蔽区域，往往难以早期发现，导致可预防的死亡。需要开发有效的早期检测方法。

Method: 训练CNN模型识别OCSCC，使用4293张训练图像（包括良性和恶性肿瘤以及阴性样本），设计物理硬件系统捕获和处理详细图像，测试不同分辨率图像对预测准确性的影响。

Result: 图像分辨率越高，预测准确性越高，但呈对数增长趋势，表明像素数量增加带来的收益递减。开发了应用程序便于测试和开放访问CNN。

Conclusion: CNN结合图像采集硬件可有效检测OCSCC，图像分辨率对检测准确性有重要影响，但存在收益递减效应。

Abstract: Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head
and neck cancer. Due to the subtle nature of its early stages, deep and hidden
areas of development, and slow growth, OCSCC often goes undetected, leading to
preventable deaths. However, properly trained Convolutional Neural Networks
(CNNs), with their precise image segmentation techniques and ability to apply
kernel matrices to modify the RGB values of images for accurate image pattern
recognition, would be an effective means for early detection of OCSCC. Pairing
this neural network with image capturing and processing hardware would allow
increased efficacy in OCSCC detection. The aim of our project is to develop a
Convolutional Neural Network trained to recognize OCSCC, as well as to design a
physical hardware system to capture and process detailed images, in order to
determine the image quality required for accurate predictions. A CNN was
trained on 4293 training images consisting of benign and malignant tumors, as
well as negative samples, and was evaluated for its precision, recall, and Mean
Average Precision (mAP) in its predictions of OCSCC. A testing dataset of
randomly assorted images of cancerous, non-cancerous, and negative images was
chosen, and each image was altered to represent 5 common resolutions. This test
data set was thoroughly analyzed by the CNN and predictions were scored on the
basis of accuracy. The designed enhancement hardware was used to capture
detailed images, and its impact was scored. An application was developed to
facilitate the testing process and bring open access to the CNN. Images of
increasing resolution resulted in higher-accuracy predictions on a logarithmic
scale, demonstrating the diminishing returns of higher pixel counts.

</details>


### [28] [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://arxiv.org/abs/2510.16258)
*Claire McLean,Makenzie Meendering,Tristan Swartz,Orri Gabbay,Alexandra Olsen,Rachel Jacobs,Nicholas Rosen,Philippe de Bree,Tony Garcia,Gadsden Merrill,Jake Sandakly,Julia Buffalini,Neham Jain,Steven Krenn,Moneish Kumar,Dejan Markovic,Evonne Ng,Fabian Prada,Andrew Saba,Siwei Zhang,Vasu Agrawal,Tim Godisart,Alexander Richard,Michael Zollhoefer*

Main category: cs.CV

TL;DR: Embody 3D是一个包含500小时3D运动数据的多模态数据集，来自439名参与者，涵盖单人和多人行为数据。


<details>
  <summary>Details</summary>
Motivation: 为研究人类行为和社交互动提供大规模、高质量的3D运动数据，支持多模态分析。

Method: 在多摄像头采集环境中收集数据，包括追踪的3D运动、手部追踪、身体形状、文本注释和单独音频轨道。

Result: 创建了包含54百万帧追踪3D运动数据的数据集，涵盖单人运动和多人社交互动场景。

Conclusion: Embody 3D数据集为人类行为和社交互动研究提供了丰富资源，支持多模态分析应用。

Abstract: The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of
500 individual hours of 3D motion data from 439 participants collected in a
multi-camera collection stage, amounting to over 54 million frames of tracked
3D motion. The dataset features a wide range of single-person motion data,
including prompted motions, hand gestures, and locomotion; as well as
multi-person behavioral and conversational data like discussions, conversations
in different emotional states, collaborative activities, and co-living
scenarios in an apartment-like space. We provide tracked human motion including
hand tracking and body shape, text annotations, and a separate audio track for
each participant.

</details>


### [29] [Proactive Scene Decomposition and Reconstruction](https://arxiv.org/abs/2510.16272)
*Baicheng Li,Zike Yan,Dong Wu,Hongbin Zha*

Main category: cs.CV

TL;DR: 提出了一种基于人-物交互的主动场景分解与重建方法，通过观察人类行为来动态优化场景分解和重建过程，解决了静态物体级重建中的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 人类行为是场景动态的主要来源，包含丰富的动态线索。传统静态物体级重建方法存在固有模糊性，需要利用人类与物体的交互意图来提升重建质量。

Method: 采用在线方法，通过人-物交互迭代分解和重建环境。结合高斯泼溅技术，实现准确一致的动态场景建模和高效逼真渲染。整合相机和物体姿态估计、实例分解、在线地图更新等多任务。

Result: 在多个真实场景中验证了方法的有效性，展现出显著优势。实现了准确、一致的动态场景建模，具有逼真和高效的渲染效果。

Conclusion: 该方法为传统物体级重建方法提供了灵活、渐进的替代方案，通过利用第一人称视角直播中的人-物交互线索，有效解决了动态环境中的重建挑战。

Abstract: Human behaviors are the major causes of scene dynamics and inherently contain
rich cues regarding the dynamics. This paper formalizes a new task of proactive
scene decomposition and reconstruction, an online approach that leverages
human-object interactions to iteratively disassemble and reconstruct the
environment. By observing these intentional interactions, we can dynamically
refine the decomposition and reconstruction process, addressing inherent
ambiguities in static object-level reconstruction. The proposed system
effectively integrates multiple tasks in dynamic environments such as accurate
camera and object pose estimation, instance decomposition, and online map
updating, capitalizing on cues from human-object interactions in egocentric
live streams for a flexible, progressive alternative to conventional
object-level reconstruction methods. Aided by the Gaussian splatting technique,
accurate and consistent dynamic scene modeling is achieved with photorealistic
and efficient rendering. The efficacy is validated in multiple real-world
scenarios with promising advantages.

</details>


### [30] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

TL;DR: Cerberus是一个用于实时视频异常检测的两级级联系统，通过轻量级过滤和细粒度视觉语言模型推理，在保持高精度的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的视频异常检测方法虽然具有优异的零样本检测能力，但计算成本巨大且视觉定位性能不稳定，难以实现实时部署。

Method: 采用两级级联系统：离线学习正常行为规则，在线推理时结合轻量级过滤和细粒度VLM推理。关键创新包括运动掩码提示和基于规则的偏差检测。

Result: 在四个数据集上的评估显示，Cerberus在NVIDIA L40S GPU上平均达到57.68 fps，加速151.79倍，准确率达到97.2%，与最先进的VLM-based VAD方法相当。

Conclusion: Cerberus为实时视频分析提供了一个实用的解决方案，在保持高精度的同时实现了显著的性能提升。

Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [31] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto,Xin Fan,Fuyuko Kido,Tsuneo Matsumoto,Hayato Yamana*

Main category: cs.CV

TL;DR: OpenLVLM-MIA是一个新的基准测试，揭示了大视觉语言模型成员推理攻击评估中的根本挑战，指出先前的高成功率源于数据集构建中的分布偏差而非真实成员状态识别。


<details>
  <summary>Details</summary>
Motivation: 现有研究在评估大视觉语言模型的成员推理攻击时存在分布偏差问题，导致攻击成功率被高估，需要建立一个无偏的基准测试来准确评估MIA方法的真实性能。

Method: 构建了一个包含6,000张图像的受控基准测试，其中成员和非成员样本的分布经过仔细平衡，并在三个不同训练阶段提供真实成员标签。

Result: 在无偏条件下，最先进的MIA方法性能收敛到随机猜测水平，表明先前的高成功率主要源于数据集偏差而非真正的攻击能力。

Conclusion: OpenLVLM-MIA为LVLM的MIA研究提供了透明无偏的基准，澄清了当前方法的局限性，并为开发更强的隐私保护技术奠定了基础。

Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.

</details>


### [32] [Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation](https://arxiv.org/abs/2510.16319)
*Rui Yang,Huining Li,Yiyi Long,Xiaojun Wu,Shengfeng He*

Main category: cs.CV

TL;DR: Stroke2Sketch是一个无需训练的框架，通过跨图像笔画注意力机制实现参考风格的笔画属性精确转移，同时保持语义结构和内容保真度。


<details>
  <summary>Details</summary>
Motivation: 生成参考风格引导的草图需要精确转移笔画属性（如线条粗细、变形和纹理稀疏度），同时保持语义结构和内容保真度。

Method: 提出跨图像笔画注意力机制嵌入自注意力层，建立细粒度语义对应关系；开发自适应对比度增强和语义聚焦注意力来强化内容保留和前景强调。

Result: 有效合成风格忠实的草图，与手工制作结果高度相似，在表达性笔画控制和语义连贯性方面优于现有方法。

Conclusion: Stroke2Sketch框架能够自适应地整合参考笔画特征到内容图像中，同时保持结构完整性，实现了高质量的笔画属性转移。

Abstract: Generating sketches guided by reference styles requires precise transfer of
stroke attributes, such as line thickness, deformation, and texture sparsity,
while preserving semantic structure and content fidelity. To this end, we
propose Stroke2Sketch, a novel training-free framework that introduces
cross-image stroke attention, a mechanism embedded within self-attention layers
to establish fine-grained semantic correspondences and enable accurate stroke
attribute transfer. This allows our method to adaptively integrate reference
stroke characteristics into content images while maintaining structural
integrity. Additionally, we develop adaptive contrast enhancement and
semantic-focused attention to reinforce content preservation and foreground
emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches
that closely resemble handcrafted results, outperforming existing methods in
expressive stroke control and semantic coherence. Codes are available at
https://github.com/rane7/Stroke2Sketch.

</details>


### [33] [Scaling Laws for Deepfake Detection](https://arxiv.org/abs/2510.16320)
*Wenhao Wang,Longqi Cai,Taihong Xiao,Yuxiao Wang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文系统研究了深度伪造检测的缩放定律，构建了最大数据集ScaleDF，发现检测错误率随真实图像域和伪造方法数量增加呈幂律衰减，类似于大语言模型的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 现有数据集规模不足以研究深度伪造检测的缩放规律，需要构建大规模数据集来分析模型性能与真实图像域数量、伪造方法数量和训练图像数量之间的关系。

Method: 构建ScaleDF数据集（包含580万张来自51个域的真实图像和880万张由102种伪造方法生成的假图像），系统分析模型性能与数据规模的关系，观察幂律缩放规律。

Result: 发现深度伪造检测的平均错误率随真实域数量或伪造方法数量增加呈可预测的幂律衰减，可以预测达到目标性能所需的额外数据量。

Conclusion: 深度伪造检测存在类似大语言模型的缩放规律，这为以数据为中心的方式对抗不断发展的深度伪造技术提供了新思路，同时研究了预训练和数据增强在缩放中的作用及其局限性。

Abstract: This paper presents a systematic study of scaling laws for the deepfake
detection task. Specifically, we analyze the model performance against the
number of real image domains, deepfake generation methods, and training images.
Since no existing dataset meets the scale requirements for this research, we
construct ScaleDF, the largest dataset to date in this field, which contains
over 5.8 million real images from 51 different datasets (domains) and more than
8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we
observe power-law scaling similar to that shown in large language models
(LLMs). Specifically, the average detection error follows a predictable
power-law decay as either the number of real domains or the number of deepfake
methods increases. This key observation not only allows us to forecast the
number of additional real domains or deepfake methods required to reach a
target performance, but also inspires us to counter the evolving deepfake
technology in a data-centric manner. Beyond this, we examine the role of
pre-training and data augmentations in deepfake detection under scaling, as
well as the limitations of scaling itself.

</details>


### [34] [Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention](https://arxiv.org/abs/2510.16325)
*Yuyao Zhang,Yu-Wing Tai*

Main category: cs.CV

TL;DR: Scale-DiT是一个用于超高分辨率文本到图像生成的扩散框架，通过分层局部注意力和低分辨率全局引导，实现了4K分辨率的高效图像合成，无需额外的高分辨率训练数据。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型受限于注意力机制的二次复杂性和4K训练数据的稀缺，难以实现超高分辨率图像生成。需要一种既能保持全局语义一致性又能高效处理高分辨率图像的方法。

Method: 提出分层局部注意力机制，将高分辨率潜在空间划分为固定大小的局部窗口以减少计算复杂度；使用低分辨率潜在空间提供全局语义引导；通过LoRA适配器连接全局和局部路径；采用Hilbert曲线重排令牌序列和融合内核优化推理效率。

Result: Scale-DiT在4K分辨率下比密集注意力基线推理速度快2倍以上，内存使用更低，在FID、IS、CLIP Score等指标上表现优异，能够生成全局一致且局部细节清晰的图像。

Conclusion: 分层局部注意力配合引导性低分辨率锚点是推进超高分辨率图像生成的有效方法，Scale-DiT框架在不依赖额外4K训练数据的情况下实现了高质量的4K图像生成。

Abstract: Ultra-high-resolution text-to-image generation demands both fine-grained
texture synthesis and globally coherent structure, yet current diffusion models
remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive
quadratic complexity of attention and the scarcity of native $4K$ training
data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces
hierarchical local attention with low-resolution global guidance, enabling
efficient, scalable, and semantically coherent image synthesis at ultra-high
resolutions. Specifically, high-resolution latents are divided into fixed-size
local windows to reduce attention complexity from quadratic to near-linear,
while a low-resolution latent equipped with scaled positional anchors injects
global semantics. A lightweight LoRA adaptation bridges global and local
pathways during denoising, ensuring consistency across structure and detail. To
maximize inference efficiency, we repermute token sequence in Hilbert curve
order and implement a fused-kernel for skipping masked operations, resulting in
a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT
achieves more than $2\times$ faster inference and lower memory usage compared
to dense attention baselines, while reliably scaling to $4K \times 4K$
resolution without requiring additional high-resolution training data. On both
quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,
Scale-DiT delivers superior global coherence and sharper local detail, matching
or outperforming state-of-the-art methods that rely on native 4K training.
Taken together, these results highlight hierarchical local attention with
guided low-resolution anchors as a promising and effective approach for
advancing ultra-high-resolution image generation.

</details>


### [35] [DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution](https://arxiv.org/abs/2510.16326)
*Yi Wei,Shunpu Tang,Liang Zhao,Qiangian Yang*

Main category: cs.CV

TL;DR: 提出了DiffusionX云边协同框架，通过轻量级设备端模型快速生成预览图像，云端模型进行最终优化，结合噪声水平预测器动态平衡计算负载，在保持图像质量的同时显著降低生成时间。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成过程计算密集，用户需要多次迭代优化提示词，增加了延迟和云端资源负担。

Method: 采用云边协同框架，设备端轻量扩散模型快速生成预览，云端高容量模型进行最终优化，并引入噪声水平预测器动态平衡计算负载。

Result: 相比Stable Diffusion v1.5减少15.8%的平均生成时间，仅比Tiny-SD慢0.9%但图像质量显著提升。

Conclusion: DiffusionX在保持图像质量的同时显著提升了效率，展示了云边协同在扩散模型中的可行性和优势。

Abstract: Recent advances in diffusion models have driven remarkable progress in image
generation. However, the generation process remains computationally intensive,
and users often need to iteratively refine prompts to achieve the desired
results, further increasing latency and placing a heavy burden on cloud
resources. To address this challenge, we propose DiffusionX, a cloud-edge
collaborative framework for efficient multi-round, prompt-based generation. In
this system, a lightweight on-device diffusion model interacts with users by
rapidly producing preview images, while a high-capacity cloud model performs
final refinements after the prompt is finalized. We further introduce a noise
level predictor that dynamically balances the computation load, optimizing the
trade-off between latency and cloud workload. Experiments show that DiffusionX
reduces average generation time by 15.8% compared with Stable Diffusion v1.5,
while maintaining comparable image quality. Moreover, it is only 0.9% slower
than Tiny-SD with significantly improved image quality, thereby demonstrating
efficiency and scalability with minimal overhead.

</details>


### [36] [TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement](https://arxiv.org/abs/2510.16332)
*Haiyue Sun,Qingdong He,Jinlong Peng,Peng Tang,Jiangning Zhang,Junwei Zhu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: 提出了TokenAR框架，通过token级增强机制解决多参考图像生成中的身份混淆问题，包含token索引嵌入、指导token注入和身份token解缠策略。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在多参考图像生成中难以解耦不同参考身份，存在身份混淆问题。

Method: TokenAR框架包含三个token级增强组件：1）Token Index Embedding聚类token索引以更好表示相同参考图像；2）Instruct Token Injection作为额外视觉特征容器注入详细补充先验；3）身份token解缠策略（ITD）显式引导token表示独立表示每个身份特征。

Result: 该方法显著增强了现有基于AR的条件图像生成方法的能力，在保持高质量背景重建的同时实现了良好的身份一致性。在多个参考图像生成任务中超越了当前最先进模型。

Conclusion: TokenAR框架通过token级增强有效解决了多参考图像生成中的身份混淆问题，并引入了首个开源大规模多参考输入数据集InstructAR，为多主题生成提供了高质量解决方案。

Abstract: Autoregressive Model (AR) has shown remarkable success in conditional image
generation. However, these approaches for multiple reference generation
struggle with decoupling different reference identities. In this work, we
propose the TokenAR framework, specifically focused on a simple but effective
token-level enhancement mechanism to address reference identity confusion
problem. Such token-level enhancement consists of three parts, 1). Token Index
Embedding clusters the tokens index for better representing the same reference
images; 2). Instruct Token Injection plays as a role of extra visual feature
container to inject detailed and complementary priors for reference tokens; 3).
The identity-token disentanglement strategy (ITD) explicitly guides the token
representations toward independently representing the features of each
identity.This token-enhancement framework significantly augments the
capabilities of existing AR based methods in conditional image generation,
enabling good identity consistency while preserving high quality background
reconstruction. Driven by the goal of high-quality and high-diversity in
multi-subject generation, we introduce the InstructAR Dataset, the first
open-source, large-scale, multi-reference input, open domain image generation
dataset that includes 28K training pairs, each example has two reference
subjects, a relative prompt and a background with mask annotation, curated for
multiple reference image generation training and evaluating. Comprehensive
experiments validate that our approach surpasses current state-of-the-art
models in multiple reference image generation task. The implementation code and
datasets will be made publicly. Codes are available, see
https://github.com/lyrig/TokenAR

</details>


### [37] [RL makes MLLMs see better than SFT](https://arxiv.org/abs/2510.16333)
*Junha Song,Sangdoo Yun,Dongyoon Han,Jaegul Choo,Byeongho Heo*

Main category: cs.CV

TL;DR: 该研究发现强化学习(RL)比监督微调(SFT)在多模态语言模型(MLLM)中能产生更强、更精确的视觉表示，提出了PIVOT方法，用不到1%的计算成本就能训练出优于更大模型的视觉编码器。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM研究过度关注LLM主干而忽视视觉编码器的作用，特别是在训练范式从SFT转向RL后，缺乏对视觉编码器如何被重塑的分析。

Method: 通过多样化的深度实验分析MLLM视觉编码器，包括ImageNet分类、分割和梯度可视化，比较SFT和RL训练策略的效果。

Result: RL训练产生更强且更精确定位的视觉表示，基于此提出的PIVOT方法训练的视觉编码器性能优于更大模型，计算成本仅为标准视觉预训练的1%。

Conclusion: RL训练能显著提升MLLM视觉编码器的能力，PIVOT为MLLM视觉骨干的发展提供了高效路径。

Abstract: A dominant assumption in Multimodal Language Model (MLLM) research is that
its performance is largely inherited from the LLM backbone, given its immense
parameter scale and remarkable capabilities. This has created a void in the
understanding of the vision encoder, which determines how MLLMs perceive
images. The recent shift in MLLM training paradigms, from Supervised Finetuning
(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the
significant lack of analysis on how such training reshapes the vision encoder
as well as the MLLM. To address this, we first investigate the impact of
training strategies on MLLMs, where RL shows a clear advantage over SFT in
strongly vision-related VQA benchmarks. Motivated by this, we conduct a
critical yet under-explored analysis of the vision encoder of MLLMs through
diverse and in-depth experiments, ranging from ImageNet classification and
segmentation to gradient visualization. Our results demonstrate that MLLM's
post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on
MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual
representations. Specifically, the key finding of our study is that RL produces
stronger and precisely localized visual representations compared to SFT,
boosting the ability of the vision encoder for MLLM. We then reframe our
findings into a simple recipe for building strong vision encoders for MLLMs,
Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,
a PIVOT-trained vision encoder outperforms even larger and more heavily-trained
counterparts, despite requiring less than 1% of the computational cost of
standard vision pretraining. This result opens an effective and efficient path
for advancing the vision backbones of MLLMs. Project page available at
https://june-page.github.io/pivot/

</details>


### [38] [On the Provable Importance of Gradients for Language-Assisted Image Clustering](https://arxiv.org/abs/2510.16335)
*Bo Peng,Jie Lu,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper investigates the recently emerged problem of Language-assisted
Image Clustering (LaIC), where textual semantics are leveraged to improve the
discriminability of visual representations to facilitate image clustering. Due
to the unavailability of true class names, one of core challenges of LaIC lies
in how to filter positive nouns, i.e., those semantically close to the images
of interest, from unlabeled wild corpus data. Existing filtering strategies are
predominantly based on the off-the-shelf feature space learned by CLIP;
however, despite being intuitive, these strategies lack a rigorous theoretical
foundation. To fill this gap, we propose a novel gradient-based framework,
termed as GradNorm, which is theoretically guaranteed and shows strong
empirical performance. In particular, we measure the positiveness of each noun
based on the magnitude of gradients back-propagated from the cross-entropy
between the predicted target distribution and the softmax output.
Theoretically, we provide a rigorous error bound to quantify the separability
of positive nouns by GradNorm and prove that GradNorm naturally subsumes
existing filtering strategies as extremely special cases of itself.
Empirically, extensive experiments show that GradNorm achieves the
state-of-the-art clustering performance on various benchmarks.

</details>


### [39] [MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization](https://arxiv.org/abs/2510.16370)
*Pulin Li,Guocheng Wu,Li Yin,Yuxin Zheng,Wei Zhang,Yanjie Zhou*

Main category: cs.CV

TL;DR: 提出了MIRAD数据集，这是首个专门针对社交制造环境中异常检测的基准数据集，包含多样化定制产品、来自六个地理分散制造节点的数据以及显著的成像异质性。


<details>
  <summary>Details</summary>
Motivation: 社交制造模式在实现大规模个性化的同时，带来了质量控制方面的重大挑战，特别是在缺陷检测方面。主要困难包括产品配置高度定制化、生产涉及碎片化小批量订单以及分布式站点成像环境差异显著。

Method: 引入MIRAD数据集，该数据集捕获了社交制造领域的三个关键维度：多样化个性化产品、来自六个地理分散制造节点的数据以及显著的成像异质性。然后在MIRAD上对最先进的异常检测方法进行了广泛评估。

Result: 结果显示，与传统基准相比，所有模型都出现了显著的性能下降，突显了现实世界个性化生产中缺陷检测尚未解决的复杂性。

Conclusion: MIRAD通过弥合工业需求和学术研究，为开发工业5.0所需的稳健质量控制解决方案提供了现实基础。

Abstract: Social manufacturing leverages community collaboration and scattered
resources to realize mass individualization in modern industry. However, this
paradigm shift also introduces substantial challenges in quality control,
particularly in defect detection. The main difficulties stem from three
aspects. First, products often have highly customized configurations. Second,
production typically involves fragmented, small-batch orders. Third, imaging
environments vary considerably across distributed sites. To overcome the
scarcity of real-world datasets and tailored algorithms, we introduce the Mass
Individualization Robust Anomaly Detection (MIRAD) dataset. As the first
benchmark explicitly designed for anomaly detection in social manufacturing,
MIRAD captures three critical dimensions of this domain: (1) diverse
individualized products with large intra-class variation, (2) data collected
from six geographically dispersed manufacturing nodes, and (3) substantial
imaging heterogeneity, including variations in lighting, background, and motion
conditions. We then conduct extensive evaluations of state-of-the-art (SOTA)
anomaly detection methods on MIRAD, covering one-class, multi-class, and
zero-shot approaches. Results show a significant performance drop across all
models compared with conventional benchmarks, highlighting the unresolved
complexities of defect detection in real-world individualized production. By
bridging industrial requirements and academic research, MIRAD provides a
realistic foundation for developing robust quality control solutions essential
for Industry 5.0. The dataset is publicly available at
https://github.com/wu33learn/MIRAD.

</details>


### [40] [Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis](https://arxiv.org/abs/2510.16371)
*Mohammad Javad Ahmadi,Iman Gandomi,Parisa Abdi,Seyed-Farzad Mohammadi,Amirhossein Taslimi,Mehdi Khodaparast,Hassan Hashemi,Mahdi Tavakoli,Hamid D. Taghirad*

Main category: cs.CV

TL;DR: 提出了一个包含3000个白内障手术视频的数据集，具有四个注释层：手术阶段、实例分割、器械-组织交互追踪和技能评分，用于训练可泛化的深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 当前白内障手术数据集缺乏多样性和深度注释，无法训练泛化性强的深度学习模型。

Method: 收集来自两个手术中心的3000个白内障手术视频，包含四个注释层：时间手术阶段、器械和解剖结构实例分割、器械-组织交互追踪、基于ICO-OSCAR标准的技能评分。

Result: 通过基准实验验证了数据集在手术工作流识别、场景分割和自动技能评估等关键任务中的技术质量，并建立了领域适应的基准。

Conclusion: 该数据集为白内障手术AI系统开发提供了高质量、多样化的训练资源，支持多个关键手术AI任务。

Abstract: The development of computer-assisted surgery systems depends on large-scale,
annotated datasets. Current resources for cataract surgery often lack the
diversity and annotation depth needed to train generalizable deep-learning
models. To address this gap, we present a dataset of 3,000 phacoemulsification
cataract surgery videos from two surgical centers, performed by surgeons with a
range of experience levels. This resource is enriched with four annotation
layers: temporal surgical phases, instance segmentation of instruments and
anatomical structures, instrument-tissue interaction tracking, and quantitative
skill scores based on the established competency rubrics like the ICO-OSCAR.
The technical quality of the dataset is supported by a series of benchmarking
experiments for key surgical AI tasks, including workflow recognition, scene
segmentation, and automated skill assessment. Furthermore, we establish a
domain adaptation baseline for the phase recognition task by training a model
on a subset of surgical centers and evaluating its performance on a held-out
center. The dataset and annotations are available in Google Form
(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).

</details>


### [41] [iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance](https://arxiv.org/abs/2510.16375)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoadv2是一个用于实时坑洞检测、GPS地理标记和道路健康可视化的全自动端到端平台，专门针对印度多样化道路网络设计。


<details>
  <summary>Details</summary>
Motivation: 印度道路坑洞带来严重安全隐患和维护挑战，需要自动化解决方案来改善道路基础设施维护。

Method: 使用自标注的7000多张印度道路图像数据集微调YOLO模型进行坑洞检测，结合OCR提取时间戳和外部GPS日志进行精确定位，通过优化后端数据库管理道路段和承包商信息。

Result: 开发了具有智能治理功能的完整平台，能够自动向承包商和官员发送警报，支持自动化问责和保修执行。

Conclusion: iWatchRoadv2通过自动化完整的坑洞监测生命周期，实现了数据驱动的智慧城市管理、透明治理和道路基础设施维护的可持续改进。

Abstract: Road potholes pose significant safety hazards and maintenance challenges,
particularly on India's diverse and under-maintained road networks. This paper
presents iWatchRoadv2, a fully automated end-to-end platform for real-time
pothole detection, GPS-based geotagging, and dynamic road health visualization
using OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000
dashcam frames capturing diverse Indian road conditions, weather patterns, and
lighting scenarios, which we used to fine-tune the Ultralytics YOLO model for
accurate pothole detection. The system synchronizes OCR-extracted video
timestamps with external GPS logs to precisely geolocate each detected pothole,
enriching detections with comprehensive metadata, including road segment
attribution and contractor information managed through an optimized backend
database. iWatchRoadv2 introduces intelligent governance features that enable
authorities to link road segments with contract metadata through a secure login
interface. The system automatically sends alerts to contractors and officials
when road health deteriorates, supporting automated accountability and warranty
enforcement. The intuitive web interface delivers actionable analytics to
stakeholders and the public, facilitating evidence-driven repair planning,
budget allocation, and quality assessment. Our cost-effective and scalable
solution streamlines frame processing and storage while supporting seamless
public engagement for urban and rural deployments. By automating the complete
pothole monitoring lifecycle, from detection to repair verification,
iWatchRoadv2 enables data-driven smart city management, transparent governance,
and sustainable improvements in road infrastructure maintenance. The platform
and live demonstration are accessible at
https://smlab.niser.ac.in/project/iwatchroad.

</details>


### [42] [Demeter: A Parametric Model of Crop Plant Morphology from the Real World](https://arxiv.org/abs/2510.16377)
*Tianhang Cheng,Albert J. Zhai,Evan Z. Chen,Rui Zhou,Yawen Deng,Zitong Li,Kejie Zhao,Janice Shiu,Qianyu Zhao,Yide Xu,Xinlei Wang,Yuan Shen,Sheng Wang,Lisa Ainsworth,Kaiyu Guan,Shenlong Wang*

Main category: cs.CV

TL;DR: Demeter是一个数据驱动的参数化模型，用于学习植物的3D形态，包括拓扑结构、形状、关节和变形，能够处理不同物种的拓扑变化并模拟三种形状变化来源。


<details>
  <summary>Details</summary>
Motivation: 虽然存在强大的人类和动物3D参数化模型，但同样表达能力强的植物建模方法仍然缺乏，特别是在作物植物建模方面。

Method: 提出了Demeter参数化模型，将植物形态的关键因素编码为紧凑的学习表示，处理不同物种的拓扑变化，并建模关节、子组件形状变化和非刚性变形三种形状变化来源。

Result: 在大规模大豆农场数据集上验证，Demeter能够有效合成形状、重建结构并模拟生物物理过程。

Conclusion: Demeter为植物建模提供了强大的参数化方法，在作物植物建模方面具有重要应用价值。

Abstract: Learning 3D parametric shape models of objects has gained popularity in
vision and graphics and has showed broad utility in 3D reconstruction,
generation, understanding, and simulation. While powerful models exist for
humans and animals, equally expressive approaches for modeling plants are
lacking. In this work, we present Demeter, a data-driven parametric model that
encodes key factors of a plant morphology, including topology, shape,
articulation, and deformation into a compact learned representation. Unlike
previous parametric models, Demeter handles varying shape topology across
various species and models three sources of shape variation: articulation,
subcomponent shape variation, and non-rigid deformation. To advance crop plant
modeling, we collected a large-scale, ground-truthed dataset from a soybean
farm as a testbed. Experiments show that Demeter effectively synthesizes
shapes, reconstructs structures, and simulates biophysical processes. Code and
data is available at https://tianhang-cheng.github.io/Demeter/.

</details>


### [43] [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)
*Yeh Keng Hao,Hsu Tzu Wei,Sun Min*

Main category: cs.CV

TL;DR: 提出了一个轻量级框架，采用编码器-解码器架构，通过稀疏卷积、SPLite解码器和量化感知训练，在保持精度的同时显著提升AR/VR设备上的推理效率。


<details>
  <summary>Details</summary>
Motivation: 随着AR/VR设备的普及，边缘设备需要实时推理、低功耗和最小延迟，但现有方法在效率和性能之间难以平衡。

Method: 使用ResNet-18骨干网络应用稀疏卷积，提出SPLite解码器架构，并采用量化感知训练来优化性能。

Result: 端到端效率提升42%，解码帧率在树莓派5上提升3.1倍，内存使用减少，精度损失极小（PA-MPJPE从9.0mm仅增至9.1mm），整体速度提升2.98倍。

Conclusion: 该方法在保持与最先进方法相当精度的同时，显著提升了计算效率，适用于AR/VR边缘设备部署。

Abstract: With the increasing ubiquity of AR/VR devices, the deployment of deep
learning models on edge devices has become a critical challenge. These devices
require real-time inference, low power consumption, and minimal latency. Many
framework designers face the conundrum of balancing efficiency and performance.
We design a light framework that adopts an encoder-decoder architecture and
introduces several key contributions aimed at improving both efficiency and
accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the
inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency
improvement. Moreover, we propose our SPLite decoder. This new architecture
significantly boosts the decoding process's frame rate by 3.1x on the Raspberry
Pi 5, while maintaining accuracy on par. To further optimize performance, we
apply quantization-aware training, reducing memory usage while preserving
accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on
FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5
CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on
compound benchmark datasets, demonstrating comparable accuracy to
state-of-the-art approaches while significantly enhancing computational
efficiency.

</details>


### [44] [REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting](https://arxiv.org/abs/2510.16410)
*Changyue Shi,Minghao Chen,Yiping Mao,Chuxiao Yang,Xinyuan Hu,Jiajun Ding,Zhou Yu*

Main category: cs.CV

TL;DR: REALM是一个创新的MLLM-agent框架，能够在3D高斯泼溅表示上直接进行开放世界的基于推理的分割，无需大量3D特定后训练。


<details>
  <summary>Details</summary>
Motivation: 弥合复杂人类指令与精确3D对象定位之间的差距是视觉和机器人领域的重大挑战。现有3D分割方法难以解释模糊的基于推理的指令，而擅长此类推理的2D视觉语言模型又缺乏内在的3D空间理解。

Method: 在3D高斯泼溅表示上直接进行分割，利用其渲染逼真新视图的能力。提出全局到局部空间定位策略：首先并行输入多个全局视图到MLLM进行粗粒度定位，然后合成多个对象特写视图进行细粒度局部分割。

Result: 在LERF、3D-OVS和新引入的REALM3D基准测试中，REALM在解释显式和隐式指令方面表现出卓越性能。该框架还无缝支持对象移除、替换和风格转换等3D交互任务。

Conclusion: REALM展示了在3D场景中理解复杂人类指令的实际效用和多功能性，为3D视觉语言理解提供了有效的解决方案。

Abstract: Bridging the gap between complex human instructions and precise 3D object
grounding remains a significant challenge in vision and robotics. Existing 3D
segmentation methods often struggle to interpret ambiguous, reasoning-based
instructions, while 2D vision-language models that excel at such reasoning lack
intrinsic 3D spatial understanding. In this paper, we introduce REALM, an
innovative MLLM-agent framework that enables open-world reasoning-based
segmentation without requiring extensive 3D-specific post-training. We perform
segmentation directly on 3D Gaussian Splatting representations, capitalizing on
their ability to render photorealistic novel views that are highly suitable for
MLLM comprehension. As directly feeding one or more rendered views to the MLLM
can lead to high sensitivity to viewpoint selection, we propose a novel
Global-to-Local Spatial Grounding strategy. Specifically, multiple global views
are first fed into the MLLM agent in parallel for coarse-level localization,
aggregating responses to robustly identify the target object. Then, several
close-up novel views of the object are synthesized to perform fine-grained
local segmentation, yielding accurate and consistent 3D masks. Extensive
experiments show that REALM achieves remarkable performance in interpreting
both explicit and implicit instructions across LERF, 3D-OVS, and our newly
introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly
supports a range of 3D interaction tasks, including object removal,
replacement, and style transfer, demonstrating its practical utility and
versatility. Project page: https://ChangyueShi.github.io/REALM.

</details>


### [45] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo,Runyu Zhou,Yifei Wang,Qi Zhang,Chenheng Zhang,Stefanie Jegelka,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Yisen Wang*

Main category: cs.CV

TL;DR: SSL4RL是一个利用自监督学习任务作为可验证奖励的新框架，通过强化学习微调视觉语言模型，无需人工偏好数据或不可靠的AI评估器。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在视觉中心任务中过度依赖语言先验，在推理过程中使用文本捷径，而强化学习应用因缺乏可扩展可靠的奖励机制而受限。

Method: 将自监督学习目标（如图像旋转预测、掩码补丁重建）重新表述为密集的自动奖励信号，用于强化学习微调。

Result: SSL4RL显著提升了视觉中心和视觉语言推理基准的性能，并在图学习中也取得了显著收益。

Conclusion: SSL4RL建立了一个使用可验证自监督目标对齐多模态模型的通用有效范式，为未来工作提供了新的设计原则。

Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.

</details>


### [46] [LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching](https://arxiv.org/abs/2510.16438)
*Aidyn Ubingazhibov,Rémi Pautrat,Iago Suárez,Shaohui Liu,Marc Pollefeys,Viktor Larsson*

Main category: cs.CV

TL;DR: LightGlueStick是一个轻量级的点和线段匹配器，通过注意力线消息传递(ALMP)组件显式暴露线段连通性，在保持高精度的同时实现实时性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法将点和线匹配视为独立任务，GlueStick虽然实现了联合匹配但架构过重，无法满足实时应用或边缘设备部署需求。

Method: 提出轻量级匹配器LightGlueStick，核心创新是注意力线消息传递(ALMP)组件，显式暴露线段连通性，实现节点间高效通信。

Result: 在多个基准测试中建立了新的最先进性能，同时保持轻量级架构。

Conclusion: LightGlueStick通过ALMP组件实现了高效的点和线段联合匹配，在精度和效率上都优于现有方法，适合实时应用。

Abstract: Lines and points are complementary local features, whose combination has
proven effective for applications such as SLAM and Structure-from-Motion. The
backbone of these pipelines are the local feature matchers, establishing
correspondences across images. Traditionally, point and line matching have been
treated as independent tasks. Recently, GlueStick proposed a GNN-based network
that simultaneously operates on points and lines to establish matches. While
running a single joint matching reduced the overall computational complexity,
the heavy architecture prevented real-time applications or deployment to edge
devices.
  Inspired by recent progress in point matching, we propose LightGlueStick, a
lightweight matcher for points and line segments. The key novel component in
our architecture is the Attentional Line Message Passing (ALMP), which
explicitly exposes the connectivity of the lines to the network, allowing for
efficient communication between nodes. In thorough experiments we show that
LightGlueStick establishes a new state-of-the-art across different benchmarks.
The code is available at https://github.com/aubingazhib/LightGlueStick.

</details>


### [47] [EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning](https://arxiv.org/abs/2510.16442)
*Haoran Sun,Chen Cai,Huiping Zhuang,Kong Aik Lee,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 提出了可解释深度伪造视频检测任务和EDVD-LLaMA多模态大语言模型推理框架，通过时空特征提取和细粒度思维链机制，在准确检测的同时提供可追溯的推理过程和可信解释。


<details>
  <summary>Details</summary>
Motivation: 传统深度伪造视频检测方法缺乏透明度且泛化能力不足，需要能够识别伪造内容并提供可验证推理解释的检测器。

Method: 使用时空细微信息标记化提取全局和局部跨帧深度伪造特征，构建细粒度多模态思维链机制引入面部特征作为硬约束，实现像素级时空视频定位。

Result: 在检测准确性、可解释性以及处理跨伪造方法和跨数据集场景方面表现出色，相比先前方法提供更可解释和优越的解决方案。

Conclusion: EDVD-LLaMA框架在深度伪造检测中实现了准确检测与可解释推理的结合，为应对不断演变的伪造技术提供了有效解决方案。

Abstract: The rapid development of deepfake video technology has not only facilitated
artistic creation but also made it easier to spread misinformation. Traditional
deepfake video detection (DVD) methods face issues such as a lack of
transparency in their principles and insufficient generalization capabilities
to cope with evolving forgery techniques. This highlights an urgent need for
detectors that can identify forged content and provide verifiable reasoning
explanations. This paper proposes the explainable deepfake video detection
(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model
(MLLM) reasoning framework, which provides traceable reasoning processes
alongside accurate detection results and trustworthy explanations. Our approach
first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)
to extract and fuse global and local cross-frame deepfake features, providing
rich spatio-temporal semantic information input for MLLM reasoning. Second, we
construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which
introduces facial feature data as hard constraints during the reasoning process
to achieve pixel-level spatio-temporal video localization, suppress
hallucinated outputs, and enhance the reliability of the chain of thought. In
addition, we build an Explainable Reasoning FF++ benchmark dataset
(ER-FF++set), leveraging structured data to annotate videos and ensure quality
control, thereby supporting dual supervision for reasoning and detection.
Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding
performance and robustness in terms of detection accuracy, explainability, and
its ability to handle cross-forgery methods and cross-dataset scenarios.
Compared to previous DVD methods, it provides a more explainable and superior
solution. The source code and dataset will be publicly available.

</details>


### [48] [RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba](https://arxiv.org/abs/2510.16444)
*Kunyu Peng,Di Wen,Jia Fu,Jiamin Wu,Kailun Yang,Junwei Zheng,Ruiping Liu,Yufan Chen,Yuqian Fu,Danda Pani Paudel,Luc Van Gool,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本文提出了RefAVA++数据集和RefAtomNet++框架，用于解决基于语言描述的原子级视频动作识别任务，通过多层级语义对齐的跨模态注意力机制和Mamba建模，显著提升了细粒度动作识别的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的动作识别方法在复杂多人场景中难以精确识别特定人物的细粒度原子动作，特别是在需要语言引导的情况下。RefAtomNet虽然引入了代理注意力机制，但在跨模态信息对齐和检索方面仍有局限。

Method: 提出RefAtomNet++框架，采用多层级语义对齐的跨注意力机制，结合部分关键词、场景属性和整体句子三个层级的Mamba建模。通过动态选择最近视觉空间token构建扫描轨迹，实现更有效的时空token聚合。

Result: 实验表明RefAtomNet++在RefAVA++数据集上取得了最先进的性能，显著超越了包括RefAtomNet在内的多个基线方法。

Conclusion: RefAtomNet++通过创新的多层级跨模态注意力机制和Mamba建模，有效解决了原子级视频动作识别中的跨模态对齐问题，为复杂多人场景下的细粒度动作理解提供了有力解决方案。

Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize
fine-grained, atomic-level actions of a specific person of interest conditioned
on natural language descriptions. Distinct from conventional action recognition
and detection tasks, RAVAR emphasizes precise language-guided action
understanding, which is particularly critical for interactive human action
analysis in complex multi-person scenarios. In this work, we extend our
previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million
frames and >75.1k annotated persons in total. We benchmark this dataset using
baselines from multiple related domains, including atomic action localization,
video question answering, and text-video retrieval, as well as our earlier
model, RefAtomNet. Although RefAtomNet surpasses other baselines by
incorporating agent attention to highlight salient features, its ability to
align and retrieve cross-modal information remains limited, leading to
suboptimal performance in localizing the target person and predicting
fine-grained actions. To overcome the aforementioned limitations, we introduce
RefAtomNet++, a novel framework that advances cross-modal token aggregation
through a multi-hierarchical semantic-aligned cross-attention mechanism
combined with multi-trajectory Mamba modeling at the partial-keyword,
scene-attribute, and holistic-sentence levels. In particular, scanning
trajectories are constructed by dynamically selecting the nearest visual
spatial tokens at each timestep for both partial-keyword and scene-attribute
levels. Moreover, we design a multi-hierarchical semantic-aligned
cross-attention strategy, enabling more effective aggregation of spatial and
temporal tokens across different semantic hierarchies. Experiments show that
RefAtomNet++ establishes new state-of-the-art results. The dataset and code are
released at https://github.com/KPeng9510/refAVA2.

</details>


### [49] [Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance](https://arxiv.org/abs/2510.16445)
*Chien Thai,Mai Xuan Trang,Huong Ninh,Hoang Hiep Ly,Anh Son Le*

Main category: cs.CV

TL;DR: 提出一种改进的损失函数，利用高斯边界框表示和Bhattacharyya距离来增强旋转物体检测的准确性和鲁棒性，采用各向异性高斯表示解决类方形物体的各向同性方差问题。


<details>
  <summary>Details</summary>
Motivation: 传统物体检测框架在旋转物体场景下表现不佳，因为它们难以捕捉方向变化，特别是在航空影像、遥感和自动驾驶等应用中。

Method: 提出旋转不变的损失函数，结合高斯边界框表示和Bhattacharyya距离，采用各向异性高斯表示，并将其集成到最先进的深度学习旋转物体检测器中。

Result: 大量实验显示，在平均精度指标上相比现有方法有显著提升。

Conclusion: 该方法有潜力在旋转物体检测领域建立新的基准，对需要精确可靠物体定位的应用具有广泛意义。

Abstract: Detecting rotated objects accurately and efficiently is a significant
challenge in computer vision, particularly in applications such as aerial
imagery, remote sensing, and autonomous driving. Although traditional object
detection frameworks are effective for axis-aligned objects, they often
underperform in scenarios involving rotated objects due to their limitations in
capturing orientation variations. This paper introduces an improved loss
function aimed at enhancing detection accuracy and robustness by leveraging the
Gaussian bounding box representation and Bhattacharyya distance. In addition,
we advocate for the use of an anisotropic Gaussian representation to address
the issues associated with isotropic variance in square-like objects. Our
proposed method addresses these challenges by incorporating a
rotation-invariant loss function that effectively captures the geometric
properties of rotated objects. We integrate this proposed loss function into
state-of-the-art deep learning-based rotated object detection detectors, and
extensive experiments demonstrated significant improvements in mean Average
Precision metrics compared to existing methods. The results highlight the
potential of our approach to establish new benchmark in rotated object
detection, with implications for a wide range of applications requiring precise
and reliable object localization irrespective of orientation.

</details>


### [50] [VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion](https://arxiv.org/abs/2510.16446)
*Jaekyun Park,Hye Won Chung*

Main category: cs.CV

TL;DR: VIPAMIN是一种视觉提示初始化策略，通过将提示与嵌入空间中的语义信息区域对齐，并在预训练子空间之外注入新的表示方向，来增强自监督模型的适应性。


<details>
  <summary>Details</summary>
Motivation: 在基础模型时代，完全微调预训练网络对每个下游任务来说资源消耗巨大。现有的视觉提示调优方法在应用于自监督骨干网络时，往往无法专门化提示或丰富表示空间，特别是在具有挑战性的任务和数据稀缺设置中。

Method: VIPAMIN通过两个关键步骤增强自监督模型的适应能力：(1) 将提示与嵌入空间中的语义信息区域对齐；(2) 在预训练子空间之外注入新的表示方向。该方法仅需单次前向传播和轻量级操作。

Result: VIPAMIN在各种任务和数据集大小上持续提升性能，在视觉提示调优领域达到了新的最先进水平。

Conclusion: VIPAMIN提供了一种简单而有效的视觉提示初始化策略，能够显著增强自监督模型在下游任务中的适应能力，特别是在具有挑战性的场景中。

Abstract: In the era of large-scale foundation models, fully fine-tuning pretrained
networks for each downstream task is often prohibitively resource-intensive.
Prompt tuning offers a lightweight alternative by introducing tunable prompts
while keeping the backbone frozen. However, existing visual prompt tuning
methods often fail to specialize the prompts or enrich the representation
space--especially when applied to self-supervised backbones. We show that these
limitations become especially pronounced in challenging tasks and data-scarce
settings, where effective adaptation is most critical. In this work, we
introduce VIPAMIN, a visual prompt initialization strategy that enhances
adaptation of self-supervised models by (1) aligning prompts with semantically
informative regions in the embedding space, and (2) injecting novel
representational directions beyond the pretrained subspace. Despite its
simplicity--requiring only a single forward pass and lightweight
operations--VIPAMIN consistently improves performance across diverse tasks and
dataset sizes, setting a new state of the art in visual prompt tuning. Our code
is available at https://github.com/iamjaekyun/vipamin.

</details>


### [51] [Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy](https://arxiv.org/abs/2510.16450)
*Shan Xiong,Jiabao Chen,Ye Wang,Jialin Peng*

Main category: cs.CV

TL;DR: 提出了一种用于线粒体分割的弱监督域自适应方法，利用稀疏点标注和新型多任务学习框架，显著提升跨域分割性能。


<details>
  <summary>Details</summary>
Motivation: 电子显微镜图像中线粒体实例分割标注成本高，无监督域自适应方法在实际应用中性能有限，需要开发更高效的弱监督方法。

Method: 采用多任务学习框架，联合进行分割和中心检测，引入交叉教学机制、类聚焦跨域对比学习以及基于实例感知的伪标签选择策略。

Result: 在多个挑战性数据集上验证，方法优于现有UDA和WDA方法，显著缩小与全监督方法的性能差距，在UDA设置下也取得显著改进。

Conclusion: 该方法通过有效利用稀疏点标注和创新的学习策略，实现了高效的跨域线粒体分割，为生物医学研究提供了实用的解决方案。

Abstract: Annotation-efficient segmentation of the numerous mitochondria instances from
various electron microscopy (EM) images is highly valuable for biological and
neuroscience research. Although unsupervised domain adaptation (UDA) methods
can help mitigate domain shifts and reduce the high costs of annotating each
domain, they typically have relatively low performance in practical
applications. Thus, we investigate weakly supervised domain adaptation (WDA)
that utilizes additional sparse point labels on the target domain, which
require minimal annotation effort and minimal expert knowledge. To take full
use of the incomplete and imprecise point annotations, we introduce a multitask
learning framework that jointly conducts segmentation and center detection with
a novel cross-teaching mechanism and class-focused cross-domain contrastive
learning. While leveraging unlabeled image regions is essential, we introduce
segmentation self-training with a novel instance-aware pseudo-label (IPL)
selection strategy. Unlike existing methods that typically rely on pixel-wise
pseudo-label filtering, the IPL semantically selects reliable and diverse
pseudo-labels with the help of the detection task. Comprehensive validations
and comparisons on challenging datasets demonstrate that our method outperforms
existing UDA and WDA methods, significantly narrowing the performance gap with
the supervised upper bound. Furthermore, under the UDA setting, our method also
achieves substantial improvements over other UDA techniques.

</details>


### [52] [NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://arxiv.org/abs/2510.16457)
*Peiran Xu,Xicheng Gong,Yadong MU*

Main category: cs.CV

TL;DR: 提出一种前瞻性的视觉语言导航方法，通过Q-learning从大规模无标签轨迹数据中学习室内场景布局和物体关系知识，生成描述潜在未来信息的Q特征，结合导航指令进行A*式搜索策略。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要基于历史信息做决策，忽视了行动的未来影响和长期结果，需要开发具有前瞻性的导航智能体。

Method: 使用Q-learning训练Q模型学习场景知识，生成候选动作的Q特征；通过跨模态未来编码器将任务无关的Q特征与导航指令结合，产生反映未来前景的动作分数；结合历史分数实现A*式搜索策略。

Result: 在广泛使用的目标导向VLN数据集上进行了大量实验，验证了所提方法的有效性。

Conclusion: 该方法成功实现了前瞻性导航，通过结合未来信息和历史信息，能够更有效地探索可能通往目的地的区域。

Abstract: In this work we concentrate on the task of goal-oriented Vision-and-Language
Navigation (VLN). Existing methods often make decisions based on historical
information, overlooking the future implications and long-term outcomes of the
actions. In contrast, we aim to develop a foresighted agent. Specifically, we
draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory
data, in order to learn the general knowledge regarding the layout and object
relations within indoor scenes. This model can generate a Q-feature, analogous
to the Q-value in traditional Q-network, for each candidate action, which
describes the potential future information that may be observed after taking
the specific action. Subsequently, a cross-modal future encoder integrates the
task-agnostic Q-feature with navigation instructions to produce a set of action
scores reflecting future prospects. These scores, when combined with the
original scores based on history, facilitate an A*-style searching strategy to
effectively explore the regions that are more likely to lead to the
destination. Extensive experiments conducted on widely used goal-oriented VLN
datasets validate the effectiveness of the proposed method.

</details>


### [53] [HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars](https://arxiv.org/abs/2510.16463)
*Haocheng Tang,Ruoke Yan,Xinhui Yin,Qi Zhang,Xinfeng Zhang,Siwei Ma,Wen Gao,Chuanmin Jia*

Main category: cs.CV

TL;DR: 提出HGC-Avatar，一种用于动态虚拟人高效传输和高质量渲染的分层高斯压缩框架，通过结构层和运动层的解耦设计，支持分层压缩和渐进解码。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的压缩方法缺乏人体先验知识，导致在数字人编码传输中比特率效率和重建质量不理想，限制了其在可流式3D虚拟人系统中的应用。

Method: 将高斯表示解耦为结构层（通过StyleUNet生成器将姿态映射到高斯）和运动层（利用SMPL-X模型紧凑表示时序姿态变化），采用分层设计和面部注意力机制。

Result: 实验结果表明HGC-Avatar为快速3D虚拟人渲染提供了可流式解决方案，在视觉质量和压缩效率方面显著优于现有方法。

Conclusion: HGC-Avatar通过分层高斯压缩框架成功解决了动态虚拟人的高效传输和高质量渲染问题，特别在面部真实感保持方面表现出色。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,
photorealistic rendering of dynamic 3D scenes, showing strong potential in
immersive communication. However, in digital human encoding and transmission,
the compression methods based on general 3DGS representations are limited by
the lack of human priors, resulting in suboptimal bitrate efficiency and
reconstruction quality at the decoder side, which hinders their application in
streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical
Gaussian Compression framework designed for efficient transmission and
high-quality rendering of dynamic avatars. Our method disentangles the Gaussian
representation into a structural layer, which maps poses to Gaussians via a
StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model
to represent temporal pose variations compactly and semantically. This
hierarchical design supports layer-wise compression, progressive decoding, and
controllable rendering from diverse pose inputs such as video sequences or
text. Since people are most concerned with facial realism, we incorporate a
facial attention mechanism during StyleUNet training to preserve identity and
expression details under low-bitrate constraints. Experimental results
demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar
rendering, while significantly outperforming prior methods in both visual
quality and compression efficiency.

</details>


### [54] [PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies](https://arxiv.org/abs/2510.16505)
*Lukas Selch,Yufang Hou,M. Jehanzeb Mirza,Sivan Doveh,James Glass,Rogerio Feris,Wei Lin*

Main category: cs.CV

TL;DR: PRISMM-Bench是首个基于真实科学论文中审稿人标记不一致性的多模态基准测试，包含262个来自242篇论文的不一致案例，评估模型在识别、纠正和推理多模态不一致性的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试要么隔离单一模态，要么依赖合成错误，无法捕捉真实科学论文中多模态不一致的复杂性，这影响了科学研究的清晰度、可重复性和可信度。

Method: 通过多阶段流程（审稿挖掘、LLM辅助过滤和人工验证）构建基准，设计三个任务：不一致性识别、纠正和配对匹配，并引入结构化JSON答案表示以减少语言偏见。

Result: 对21个领先LMMs的基准测试显示性能极低（26.1-54.2%），包括大型开源模型和专有模型，突显了多模态科学推理的挑战。

Conclusion: 当前LMMs在理解科学论文多模态复杂性方面存在显著不足，需要进一步开发可信赖的科学助手。

Abstract: Large Multimodal Models (LMMs) are increasingly applied to scientific
research, yet it remains unclear whether they can reliably understand and
reason over the multimodal complexity of papers. A central challenge lies in
detecting and resolving inconsistencies across text, figures, tables, and
equations, issues that are often subtle, domain-specific, and ultimately
undermine clarity, reproducibility, and trust. Existing benchmarks overlook
this issue, either isolating single modalities or relying on synthetic errors
that fail to capture real-world complexity. We introduce PRISMM-Bench
(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first
benchmark grounded in real reviewer-flagged inconsistencies in scientific
papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering
and human verification, we curate 262 inconsistencies from 242 papers. Based on
this set, we design three tasks, namely inconsistency identification, remedy
and pair matching, which assess a model's capacity to detect, correct, and
reason over inconsistencies across different modalities. Furthermore, to
address the notorious problem of choice-only shortcuts in multiple-choice
evaluation, where models exploit answer patterns without truly understanding
the question, we further introduce structured JSON-based answer representations
that minimize linguistic biases by reducing reliance on superficial stylistic
cues. We benchmark 21 leading LMMs, including large open-weight models
(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5
with high reasoning). Results reveal strikingly low performance (26.1-54.2%),
underscoring the challenge of multimodal scientific reasoning and motivating
progress towards trustworthy scientific assistants.

</details>


### [55] [OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks](https://arxiv.org/abs/2510.16508)
*Franko Šikić,Sven Lončarić*

Main category: cs.CV

TL;DR: OOS-DSD是一种基于深度学习的缺货检测方法，通过辅助学习在YOLOv8架构上添加卷积分支，同时进行缺货检测、产品分割和深度估计，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 缺货检测是重要的零售验证流程，需要准确推断货架上产品的不可用性。现有方法性能有待提升。

Method: 扩展YOLOv8目标检测架构，添加卷积分支进行多任务学习：缺货检测、产品分割和深度估计。深度分支使用Depth Anything V2生成的伪标签训练，并提出了深度归一化方法。

Result: 提出的方法在平均精度(mAP)上比现有最优方法提升了1.8%。消融研究显示辅助学习提升mAP 3.7%，深度归一化提升4.2%。

Conclusion: 辅助学习和深度归一化能有效提升缺货检测性能，多任务学习框架具有实用价值。

Abstract: Out-of-stock (OOS) detection is a very important retail verification process
that aims to infer the unavailability of products in their designated areas on
the shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based
method that advances OOS detection through auxiliary learning. In particular,
we extend a well-established YOLOv8 object detection architecture with
additional convolutional branches to simultaneously detect OOS, segment
products, and estimate scene depth. While OOS detection and product
segmentation branches are trained using ground truth data, the depth estimation
branch is trained using pseudo-labeled annotations produced by the
state-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,
since the aforementioned pseudo-labeled depth estimates display relative depth,
we propose an appropriate depth normalization procedure that stabilizes the
training process. The experimental results show that the proposed method
surpassed the performance of the SOTA OOS detection methods by 1.8% of the mean
average precision (mAP). In addition, ablation studies confirm the
effectiveness of auxiliary learning and the proposed depth normalization
procedure, with the former increasing mAP by 3.7% and the latter by 4.2%.

</details>


### [56] [Image Categorization and Search via a GAT Autoencoder and Representative Models](https://arxiv.org/abs/2510.16514)
*Duygu Sap,Martin Lotz,Connor Mattinson*

Main category: cs.CV

TL;DR: 提出了一种基于图注意力网络自编码器的图像分类和检索方法，通过构建图像和类别的代表性模型来进行分类和检索。


<details>
  <summary>Details</summary>
Motivation: 开发一种代表性中心的图像分类和检索方法，利用图结构捕捉图像间的相似关系，并通过注意力机制突出重要特征。

Method: 构建图像节点图，使用图注意力网络自编码器生成上下文感知的潜在表示，从中提取类别代表，通过比较查询图像代表与类别代表进行分类，并在识别类别内检索最相似图像。

Result: 通过实验验证了该代表性中心方法的有效性，比较了图注意力网络自编码器和标准基于特征的技术。

Conclusion: 提出的代表性中心方法在图像分类和检索任务中表现有效，能够利用图结构和注意力机制构建更好的图像表示。

Abstract: We propose a method for image categorization and retrieval that leverages
graphs and a graph attention network (GAT)-based autoencoder. Our approach is
representative-centric, that is, we execute the categorization and retrieval
process via the representative models we construct for the images and image
categories. We utilize a graph where nodes represent images (or their
representatives) and edges capture similarity relationships. GAT highlights
important features and relationships between images, enabling the autoencoder
to construct context-aware latent representations that capture the key features
of each image relative to its neighbors. We obtain category representatives
from these embeddings and categorize a query image by comparing its
representative to the category representatives. We then retrieve the most
similar image to the query image within its identified category. We demonstrate
the effectiveness of our representative-centric approach through experiments
with both the GAT autoencoders and standard feature-based techniques.

</details>


### [57] [Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions](https://arxiv.org/abs/2510.16540)
*Jihoon Kwon,Kyle Min,Jy-yong Sohn*

Main category: cs.CV

TL;DR: READ是一种微调方法，通过添加token级重建和句子级对齐两个辅助目标来增强视觉语言模型的组合推理能力，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在组合推理方面表现不佳，主要原因是文本编码器倾向于关注单个单词而非它们之间的关系，这种局限性被对比训练进一步加强。

Method: READ方法在对比学习基础上添加两个辅助目标：(1) token级重建目标：使用冻结的预训练解码器基于原始标题的嵌入重建替代标题；(2) 句子级对齐目标：在嵌入空间中显式对齐释义句子。

Result: READ-CLIP在五个主要组合推理基准测试中达到最先进性能，比最强传统微调基线提升达4.1%。应用于现有CLIP变体（如NegCLIP和FSC-CLIP）也能提升这些基准测试的性能。

Conclusion: 重建和对齐目标提供互补优势：重建鼓励编码器捕捉标题内单词间的关系，而对齐确保不同措辞的释义具有一致表示。

Abstract: Despite recent advances, vision-language models trained with standard
contrastive objectives still struggle with compositional reasoning -- the
ability to understand structured relationships between visual and linguistic
elements. This shortcoming is largely due to the tendency of the text encoder
to focus on individual words rather than their relations, a limitation
reinforced by contrastive training that primarily aligns words with visual
objects. In this paper, we introduce REconstruction and Alignment of text
Descriptions (READ), a fine-tuning method designed to enhance compositional
reasoning by adding two auxiliary objectives to the contrastive learning: (1) a
token-level reconstruction objective, where a frozen pre-trained decoder
reconstructs alternative captions based on the embedding of the original
caption; and (2) a sentence-level alignment objective, which explicitly aligns
paraphrased sentences in the embedding space. We show that READ-CLIP, a model
derived by applying the READ method to the pre-trained CLIP model, achieves the
state-of-the-art performance across five major compositional reasoning
benchmarks, outperforming the strongest conventional fine-tuning baseline by up
to 4.1%. Furthermore, applying the READ to existing CLIP variants (including
NegCLIP and FSC-CLIP) also improves performance on these benchmarks.
Quantitative and qualitative analyses reveal that our proposed objectives --
reconstruction and alignment -- offer complementary benefits: the former
encourages the encoder to capture relationships between words within a caption,
while the latter ensures consistent representations for paraphrases expressed
with different wording.

</details>


### [58] [Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition](https://arxiv.org/abs/2510.16541)
*Binyuan Huang,Yongdong Luo,Xianda Guo,Xiawu Zheng,Zheng Zhu,Jiahui Pan,Chengju Zhou*

Main category: cs.CV

TL;DR: 提出了GaitRDAE框架，通过动态搜索运动区域并分配自适应时间尺度来改进步态识别，解决了现有方法使用预定义区域和固定时间尺度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别方法使用预定义区域和固定时间尺度，难以建模随时间动态变化的运动区域和适应其特定模式。

Method: 引入区域感知动态聚合与激励框架(GaitRDAE)，包含两个核心模块：RDA模块动态搜索每个区域的最佳时间感受野，RDE模块强调包含稳定行为模式的运动区域学习。

Result: 在多个基准数据集上实现了最先进的性能。

Conclusion: GaitRDAE通过动态区域搜索和自适应时间尺度分配，有效提升了步态识别的准确性。

Abstract: Deep learning-based gait recognition has achieved great success in various
applications. The key to accurate gait recognition lies in considering the
unique and diverse behavior patterns in different motion regions, especially
when covariates affect visual appearance. However, existing methods typically
use predefined regions for temporal modeling, with fixed or equivalent temporal
scales assigned to different types of regions, which makes it difficult to
model motion regions that change dynamically over time and adapt to their
specific patterns. To tackle this problem, we introduce a Region-aware Dynamic
Aggregation and Excitation framework (GaitRDAE) that automatically searches for
motion regions, assigns adaptive temporal scales and applies corresponding
attention. Specifically, the framework includes two core modules: the
Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the
optimal temporal receptive field for each region, and the Region-aware Dynamic
Excitation (RDE) module, which emphasizes the learning of motion regions
containing more stable behavior patterns while suppressing attention to static
regions that are more susceptible to covariates. Experimental results show that
GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

</details>


### [59] [Fit for Purpose? Deepfake Detection in the Real World](https://arxiv.org/abs/2510.16556)
*Guangyu Lin,Li Lin,Christina P. Walker,Daniel S. Schiff,Shu Hu*

Main category: cs.CV

TL;DR: 本文提出了首个基于真实世界政治深度伪造事件的基准测试，评估了学术界、政府和工业界的最先进深度伪造检测器，发现现有检测器在真实政治深度伪造上的泛化能力不足，呼吁开发更具政治情境化的检测框架。


<details>
  <summary>Details</summary>
Motivation: AI生成内容的快速扩散增加了虚假信息的风险，特别是政治深度伪造会扭曲真相并破坏对政治机构的信任。现有检测模型大多在实验室控制的合成数据集上训练，难以泛化到社交媒体上传播的真实政治深度伪造。

Method: 基于政治深度伪造事件数据库构建系统基准，对学术界、政府和工业界的最先进深度伪造检测器进行全面评估，包括免费和付费工具。

Result: 学术界和政府开发的检测器表现相对较差，付费检测工具性能相对较高，但所有评估的检测器都难以有效泛化到真实政治深度伪造，且对简单操作（特别是视频领域）易受攻击。

Conclusion: 需要开发更具政治情境化的深度伪造检测框架，以在真实世界环境中更好地保护公众。

Abstract: The rapid proliferation of AI-generated content, driven by advances in
generative adversarial networks, diffusion models, and multimodal large
language models, has made the creation and dissemination of synthetic media
effortless, heightening the risks of misinformation, particularly political
deepfakes that distort truth and undermine trust in political institutions. In
turn, governments, research institutions, and industry have strongly promoted
deepfake detection initiatives as solutions. Yet, most existing models are
trained and validated on synthetic, laboratory-controlled datasets, limiting
their generalizability to the kinds of real-world political deepfakes
circulating on social platforms that affect the public. In this work, we
introduce the first systematic benchmark based on the Political Deepfakes
Incident Database, a curated collection of real-world political deepfakes
shared on social media since 2018. Our study includes a systematic evaluation
of state-of-the-art deepfake detectors across academia, government, and
industry. We find that the detectors from academia and government perform
relatively poorly. While paid detection tools achieve relatively higher
performance than free-access models, all evaluated detectors struggle to
generalize effectively to authentic political deepfakes, and are vulnerable to
simple manipulations, especially in the video domain. Results urge the need for
politically contextualized deepfake detection frameworks to better safeguard
the public in real-world settings.

</details>


### [60] [SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](https://arxiv.org/abs/2510.16596)
*Yiyang Huang,Liang Shi,Yitian Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: SHIELD是一个无需训练的训练框架，通过重新加权视觉标记、引入噪声派生标记和应用对抗攻击来缓解大型视觉语言模型中的物体幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在跨模态任务中表现出色，但物体幻觉问题（模型生成看似合理但不准确的物体描述）仍然是一个重大挑战。本文首次将LVLM幻觉追溯到视觉编码器，并识别出三个关键问题：统计偏差、固有偏差和脆弱性。

Method: 提出SHIELD框架，采用三种策略：1）重新加权视觉标记以减少统计偏差；2）引入噪声派生标记来对抗固有偏差；3）应用带有对比解码的对抗攻击来解决脆弱性问题。

Result: 实验表明，SHIELD在不同基准测试和LVLM家族中有效缓解了物体幻觉问题。此外，SHIELD在通用LVLM基准测试中表现出色，显示了其广泛的适用性。

Conclusion: SHIELD是一个有效的训练免费框架，能够显著减少大型视觉语言模型中的物体幻觉，同时保持良好的通用性能。

Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.
However, object hallucination, where models produce plausible but inaccurate
object descriptions, remains a significant challenge. In contrast to previous
work focusing on LLM components, this paper is the first to trace LVLM
hallucinations to visual encoders and identifies three key issues: statistical
bias, inherent bias, and vulnerability. To address these challenges, we propose
SHIELD, a training-free framework that mitigates hallucinations through three
strategies: re-weighting visual tokens to reduce statistical bias, introducing
noise-derived tokens to counter inherent bias, and applying adversarial attacks
with contrastive decoding to address vulnerability. Experiments demonstrate
that SHIELD effectively mitigates object hallucinations across diverse
benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on
the general LVLM benchmark, highlighting its broad applicability. Code will be
released.

</details>


### [61] [VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)
*Jiaying Zhu,Yurui Zhu,Xin Lu,Wenrui Yan,Dong Li,Kunlin Liu,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: VisionSelector是一个轻量级插件框架，通过可学习的决策过程解决MLLMs中视觉令牌压缩问题，使用可微分Top-K机制和课程退火策略实现高效令牌选择。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理高分辨率图像或多图像输入时面临计算和内存瓶颈，现有令牌压缩技术受限于启发式规则，可能丢弃关键信息并存在注意力下沉等偏差。

Method: 提出VisionSelector评分器模块，与MLLM主干解耦，包含可微分Top-K机制和课程退火策略，实现端到端可学习的令牌压缩决策。

Result: 仅12.85M可训练参数，在MME基准上30%保留预算下保持100%准确率，10%保留预算下比先前方法提升12.14%，预填充速度翻倍。

Conclusion: VisionSelector展示了在各种压缩率下的泛化能力，能够自适应识别关键令牌，在所有压缩预算下实现优越性能。

Abstract: Multimodal Large Language Models (MLLMs) encounter significant computational
and memory bottlenecks from the massive number of visual tokens generated by
high-resolution images or multi-image inputs. Previous token compression
techniques are often constrained by heuristic rules that risk discarding
critical information. They may suffer from biases, such as attention sinks,
that lead to sharp performance drops under aggressive compression ratios. To
address these limitations, we reformulate token compression as a lightweight
plug-and-play framework that reformulates token compression into an end-to-end
learnable decision process. To be specific, we propose VisionSelector, a scorer
module decoupled from the MLLM backbone that incorporates a differentiable
Top-K mechanism and a curriculum annealing strategy to bridge the
training-inference gap, enabling efficient and adaptive token selection various
arbitrary compression rates. Remarkably lightweight with only 12.85M trainable
parameters, VisionSelector demonstrates generalization across various
compression rates and adaptively identifying critical tokens. This leads to
superior performance across all compression budgets, evidenced by preserving
100% accuracy on MME with 30% retention budget, outperforming prior methods by
12.14% at 10% retention budget, and doubling prefill speed. Our code is
available at https://github.com/JulietChoo/VisionSelector .

</details>


### [62] [A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications](https://arxiv.org/abs/2510.16611)
*Melika Filvantorkaman,Maral Filvan Torkaman*

Main category: cs.CV

TL;DR: 提出了一种用于实时医学图像分析的深度学习框架，整合U-Net、EfficientNet和Transformer等先进神经网络架构，通过模型剪枝、量化和GPU加速实现实时优化，在多种成像模态上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像处理技术缺乏实时临床使用所需的精度、鲁棒性和速度，且高分辨率放射学数据解读耗时且存在临床医生间的变异性。

Method: 整合U-Net、EfficientNet和Transformer等神经网络架构，采用模型剪枝、量化和GPU加速等实时优化策略，支持边缘设备、本地服务器和云基础设施的灵活部署。

Result: 在公共基准数据集上达到分类准确率92%以上、分割Dice分数91%以上、推理时间低于80毫秒的优异性能，并通过Grad-CAM和分割叠加等可视化工具增强透明度。

Conclusion: 该框架可显著加速诊断工作流程，减少临床医生工作量，支持在时间关键的医疗环境中可信赖的AI集成。

Abstract: Medical imaging plays a vital role in modern diagnostics; however,
interpreting high-resolution radiological data remains time-consuming and
susceptible to variability among clinicians. Traditional image processing
techniques often lack the precision, robustness, and speed required for
real-time clinical use. To overcome these limitations, this paper introduces a
deep learning framework for real-time medical image analysis designed to
enhance diagnostic accuracy and computational efficiency across multiple
imaging modalities, including X-ray, CT, and MRI. The proposed system
integrates advanced neural network architectures such as U-Net, EfficientNet,
and Transformer-based models with real-time optimization strategies including
model pruning, quantization, and GPU acceleration. The framework enables
flexible deployment on edge devices, local servers, and cloud infrastructures,
ensuring seamless interoperability with clinical systems such as PACS and EHR.
Experimental evaluations on public benchmark datasets demonstrate
state-of-the-art performance, achieving classification accuracies above 92%,
segmentation Dice scores exceeding 91%, and inference times below 80
milliseconds. Furthermore, visual explanation tools such as Grad-CAM and
segmentation overlays enhance transparency and clinical interpretability. These
results indicate that the proposed framework can substantially accelerate
diagnostic workflows, reduce clinician workload, and support trustworthy AI
integration in time-critical healthcare environments.

</details>


### [63] [Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs](https://arxiv.org/abs/2510.16624)
*Sebastian Mocanu,Emil Slusanschi,Marius Leordeanu*

Main category: cs.CV

TL;DR: 提出了一种仅使用视觉的无人机自主飞行系统，结合语义分割和单目深度估计，在室内环境中实现避障、场景探索和自主安全着陆，无需GPS或昂贵传感器。


<details>
  <summary>Details</summary>
Motivation: 解决小型无人机在受控室内环境中自主飞行的挑战，避免对GPS和昂贵传感器（如LiDAR）的依赖，通过纯视觉方法实现精确导航。

Method: 采用知识蒸馏框架，使用基于颜色的SVM教师网络生成训练数据，训练轻量级U-Net学生网络进行实时语义分割；开发自适应尺度因子算法将非度量深度预测转换为准确度量距离。

Result: 在5x4米实验室环境中测试，平均距离误差14.4厘米；30次真实环境飞行测试和100次数字孪生环境测试显示，该方法增加了监视距离，减少了任务时间，保持100%成功率；端到端学习实现87.5%自主任务成功率。

Conclusion: 该工作推进了结构化环境中基于视觉的无人机导航实践，解决了度量深度估计和计算效率挑战，可在资源受限平台上部署。

Abstract: This paper presents a vision-only autonomous flight system for small UAVs
operating in controlled indoor environments. The system combines semantic
segmentation with monocular depth estimation to enable obstacle avoidance,
scene exploration, and autonomous safe landing operations without requiring GPS
or expensive sensors such as LiDAR. A key innovation is an adaptive scale
factor algorithm that converts non-metric monocular depth predictions into
accurate metric distance measurements by leveraging semantic ground plane
detection and camera intrinsic parameters, achieving a mean distance error of
14.4 cm. The approach uses a knowledge distillation framework where a
color-based Support Vector Machine (SVM) teacher generates training data for a
lightweight U-Net student network (1.6M parameters) capable of real-time
semantic segmentation. For more complex environments, the SVM teacher can be
replaced with a state-of-the-art segmentation model. Testing was conducted in a
controlled 5x4 meter laboratory environment with eight cardboard obstacles
simulating urban structures. Extensive validation across 30 flight tests in a
real-world environment and 100 flight tests in a digital-twin environment
demonstrates that the combined segmentation and depth approach increases the
distance traveled during surveillance and reduces mission time while
maintaining 100% success rates. The system is further optimized through
end-to-end learning, where a compact student neural network learns complete
flight policies from demonstration data generated by our best-performing
method, achieving an 87.5% autonomous mission success rate. This work advances
practical vision-based drone navigation in structured environments,
demonstrating solutions for metric depth estimation and computational
efficiency challenges that enable deployment on resource-constrained platforms.

</details>


### [64] [MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models](https://arxiv.org/abs/2510.16641)
*Young-Jun Lee,Byung-Kwan Lee,Jianshu Zhang,Yechan Hwang,Byungsoo Ko,Han-Gyu Kim,Dongyu Yao,Xuankun Rong,Eojin Joo,Seung-Ho Han,Bowon Ko,Ho-Jin Choi*

Main category: cs.CV

TL;DR: 提出了MultiVerse多轮对话基准测试，包含647个对话，涵盖12个VLM评估基准的484个任务，使用GPT-4o作为自动评估器，评估18个VLMs发现最强模型在复杂多轮对话中成功率仅50%。


<details>
  <summary>Details</summary>
Motivation: 现有多轮对话数据集无法充分捕捉真实世界对话场景的广度和深度，需要更全面的基准测试来评估VLMs的多轮交互能力。

Method: 从12个流行的VLM评估基准中提取647个对话，每个对话平均4轮，使用基于检查表的评估方法，利用GPT-4o作为自动评估器，测量37个关键方面的性能。

Result: 评估18个VLMs显示，即使最强模型（如GPT-4o）在复杂多轮对话中成功率仅为50%，提供完整对话上下文能显著提升较小或较弱模型的性能。

Conclusion: MultiVerse是评估VLMs多轮交互能力的全面基准，揭示了当前模型在多轮对话中的局限性，强调了上下文学习的重要性。

Abstract: Vision-and-Language Models (VLMs) have shown impressive capabilities on
single-turn benchmarks, yet real-world applications often demand more intricate
multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only
partially capture the breadth and depth of conversational scenarios encountered
by users. In this work, we introduce MultiVerse, a novel multi-turn
conversation benchmark featuring 647 dialogues - each averaging four turns -
derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484
tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from
factual knowledge and perception to advanced reasoning tasks such as
mathematics and coding. To facilitate robust assessment, we propose a
checklist-based evaluation method that leverages GPT-4o as the automated
evaluator, measuring performance across 37 key aspects, including perceptual
accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on
MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve
only a 50% success rate in complex multi-turn conversations, highlighting the
dataset's challenging nature. Notably, we find that providing full dialogue
context significantly enhances performance for smaller or weaker models,
emphasizing the importance of in-context learning. We believe MultiVerse is a
landscape of evaluating multi-turn interaction abilities for VLMs.

</details>


### [65] [Structured Interfaces for Automated Reasoning with 3D Scene Graphs](https://arxiv.org/abs/2510.16643)
*Aaron Ray,Jacob Arkin,Harel Biggie,Chuchu Fan,Luca Carlone,Nicholas Roy*

Main category: cs.CV

TL;DR: 提出使用检索增强生成方法，通过Cypher查询语言作为LLM与3D场景图交互的接口，解决大规模3D场景图自然语言接地问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法将场景图序列化为文本放入LLM上下文窗口，但这种方法无法扩展到大型或丰富的3D场景图。

Method: 使用图数据库编码3D场景图，为LLM提供Cypher查询语言工具，使其能够检索与任务相关的数据。

Result: 在指令跟随和场景问答任务上的评估显示，Cypher接口方法在大规模丰富图上显著优于基线方法，同时大幅减少场景图内容的token数量。

Conclusion: 使用Cypher作为3D场景图接口的方法在扩展性和性能上都有显著提升，适用于基于本地和云端的模型。

Abstract: In order to provide a robot with the ability to understand and react to a
user's natural language inputs, the natural language must be connected to the
robot's underlying representations of the world. Recently, large language
models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for
grounding natural language and representing the world. In this work, we address
the challenge of using LLMs with 3DSGs to ground natural language. Existing
methods encode the scene graph as serialized text within the LLM's context
window, but this encoding does not scale to large or rich 3DSGs. Instead, we
propose to use a form of Retrieval Augmented Generation to select a subset of
the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide
a query language interface (Cypher) as a tool to the LLM with which it can
retrieve relevant data for language grounding. We evaluate our approach on
instruction following and scene question-answering tasks and compare against
baseline context window and code generation methods. Our results show that
using Cypher as an interface to 3D scene graphs scales significantly better to
large, rich graphs on both local and cloud-based models. This leads to large
performance improvements in grounded language tasks while also substantially
reducing the token count of the scene graph content. A video supplement is
available at https://www.youtube.com/watch?v=zY_YI9giZSA.

</details>


### [66] [Universal and Transferable Attacks on Pathology Foundation Models](https://arxiv.org/abs/2510.16660)
*Yuntian Wang,Xilin Yang,Che-Yung Shen,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 提出了针对病理学基础模型的通用可转移对抗扰动(UTAP)，这是一种固定且微弱的噪声模式，能够系统性地破坏多个病理学基础模型的特征表示能力，导致下游任务性能下降。


<details>
  <summary>Details</summary>
Motivation: 揭示病理学基础模型的关键脆弱性，建立高标准的模型鲁棒性评估基准，推动防御机制发展，确保AI在病理学中的安全可靠部署。

Method: 使用深度学习优化生成固定的微弱噪声模式(UTAP)，该扰动可添加到病理图像中，系统性地破坏基础模型的特征表示能力。

Result: UTAP在多个最先进的病理学基础模型和数据集上进行了系统评估，通过在输入图像中添加视觉上难以察觉的固定噪声模式，显著降低了模型性能。

Conclusion: UTAP构成了对各种新兴病理学基础模型及其应用的广泛威胁，其开发为模型鲁棒性评估建立了关键的高标准基准，强调了推进防御机制的必要性。

Abstract: We introduce Universal and Transferable Adversarial Perturbations (UTAP) for
pathology foundation models that reveal critical vulnerabilities in their
capabilities. Optimized using deep learning, UTAP comprises a fixed and weak
noise pattern that, when added to a pathology image, systematically disrupts
the feature representation capabilities of multiple pathology foundation
models. Therefore, UTAP induces performance drops in downstream tasks that
utilize foundation models, including misclassification across a wide range of
unseen data distributions. In addition to compromising the model performance,
we demonstrate two key features of UTAP: (1) universality: its perturbation can
be applied across diverse field-of-views independent of the dataset that UTAP
was developed on, and (2) transferability: its perturbation can successfully
degrade the performance of various external, black-box pathology foundation
models - never seen before. These two features indicate that UTAP is not a
dedicated attack associated with a specific foundation model or image dataset,
but rather constitutes a broad threat to various emerging pathology foundation
models and their applications. We systematically evaluated UTAP across various
state-of-the-art pathology foundation models on multiple datasets, causing a
significant drop in their performance with visually imperceptible modifications
to the input images using a fixed noise pattern. The development of these
potent attacks establishes a critical, high-standard benchmark for model
robustness evaluation, highlighting a need for advancing defense mechanisms and
potentially providing the necessary assets for adversarial training to ensure
the safe and reliable deployment of AI in pathology.

</details>


### [67] [HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications](https://arxiv.org/abs/2510.16664)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: 本文提出HYDRA框架，通过教师-学生知识蒸馏方法实现从三通道自然图像到高光谱图像的高质量重建，在精度和推理速度上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多尺度注意力方法只能处理稀疏光谱，而现代高光谱传感器包含数百个通道，需要更通用的光谱重建方法。

Method: 使用教师模型编码潜在高光谱数据，学生模型学习从自然图像到教师编码域的映射，结合新颖的训练方法实现知识蒸馏。

Result: 在所有指标上达到SOTA性能，精度提升18%，在不同通道深度下推理速度均优于当前SOTA模型。

Conclusion: HYDRA框架成功解决了先前光谱重建模型的关键限制，实现了高质量、高效率的高光谱图像重建。

Abstract: Hyperspectral images (HSI) promise to support a range of new applications in
computer vision. Recent research has explored the feasibility of generalizable
Spectral Reconstruction (SR), the problem of recovering a HSI from a natural
three-channel color image in unseen scenarios.
  However, previous Multi-Scale Attention (MSA) works have only demonstrated
sufficient generalizable results for very sparse spectra, while modern HSI
sensors contain hundreds of channels.
  This paper introduces a novel approach to spectral reconstruction via our
HYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).
  Using a Teacher model that encapsulates latent hyperspectral image data and a
Student model that learns mappings from natural images to the Teacher's encoded
domain, alongside a novel training method, we achieve high-quality spectral
reconstruction.
  This addresses key limitations of prior SR models, providing SOTA performance
across all metrics, including an 18\% boost in accuracy, and faster inference
times than current SOTA models at various channel depths.

</details>


### [68] [Pursuing Minimal Sufficiency in Spatial Reasoning](https://arxiv.org/abs/2510.16688)
*Yejie Guo,Yunzhong Hou,Wufei Ma,Meng Tang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: MSSR是一个双智能体框架，通过构建最小充分信息集来解决VLMs在3D空间推理中的挑战，显著提升了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在3D空间推理中的两个基本瓶颈：基于2D预训练导致的3D理解能力不足，以及冗余3D信息引发的推理失败。

Method: 提出双智能体框架：感知智能体使用感知工具箱程序化查询3D场景提取充分信息（包括新颖的SOG模块），推理智能体迭代精炼信息以追求最小化，在闭环中修剪冗余细节并请求缺失信息，直到构建出最小充分集。

Result: 在两个具有挑战性的基准测试中显著提高了准确性并达到最先进性能，同时产生可解释的推理路径。

Conclusion: 通过明确追求充分性和最小化，该方法有效解决了3D空间推理问题，并为未来模型提供了高质量训练数据来源。

Abstract: Spatial reasoning, the ability to ground language in 3D understanding,
remains a persistent challenge for Vision-Language Models (VLMs). We identify
two fundamental bottlenecks: inadequate 3D understanding capabilities stemming
from 2D-centric pre-training, and reasoning failures induced by redundant 3D
information. To address these, we first construct a Minimal Sufficient Set
(MSS) of information before answering a given question: a compact selection of
3D perception results from \textit{expert models}. We introduce MSSR (Minimal
Sufficient Spatial Reasoner), a dual-agent framework that implements this
principle. A Perception Agent programmatically queries 3D scenes using a
versatile perception toolbox to extract sufficient information, including a
novel SOG (Situated Orientation Grounding) module that robustly extracts
language-grounded directions. A Reasoning Agent then iteratively refines this
information to pursue minimality, pruning redundant details and requesting
missing ones in a closed loop until the MSS is curated. Extensive experiments
demonstrate that our method, by explicitly pursuing both sufficiency and
minimality, significantly improves accuracy and achieves state-of-the-art
performance across two challenging benchmarks. Furthermore, our framework
produces interpretable reasoning paths, offering a promising source of
high-quality training data for future models. Source code is available at
https://github.com/gyj155/mssr.

</details>


### [69] [SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation](https://arxiv.org/abs/2510.16702)
*Huy Minh Nhat Nguyen,Triet Hoang Minh Dao,Chau Vinh Hoang Truong,Cuong Tuan Nguyen*

Main category: cs.CV

TL;DR: 提出SDPA++框架，仅使用噪声OCT图像进行自监督去噪，通过自融合生成伪真实图像，并采用基于块的策略训练去噪模型。


<details>
  <summary>Details</summary>
Motivation: OCT图像分析对眼科疾病诊断至关重要，但获取配对的干净和噪声图像数据集存在挑战，因为固有的散斑噪声和临床成像环境的实际限制。

Method: SDPA++框架仅使用噪声OCT图像，通过自融合和自监督去噪生成伪真实图像，然后使用基于块的策略训练去噪模型集合。

Result: 在IEEE SPS VIP Cup真实世界数据集上验证，通过CNR、MSR、TP和EP等指标显示性能提升，该数据集仅包含真实噪声OCT图像而无干净参考。

Conclusion: 该方法在临床实践中具有改善图像质量和诊断结果的潜力，特别是在缺乏干净参考图像的情况下。

Abstract: Optical Coherence Tomography (OCT) is a widely used non-invasive imaging
technique that provides detailed three-dimensional views of the retina, which
are essential for the early and accurate diagnosis of ocular diseases.
Consequently, OCT image analysis and processing have emerged as key research
areas in biomedical imaging. However, acquiring paired datasets of clean and
real-world noisy OCT images for supervised denoising models remains a
formidable challenge due to intrinsic speckle noise and practical constraints
in clinical imaging environments. To address these issues, we propose SDPA++: A
General Framework for Self-Supervised Denoising with Patch Aggregation. Our
novel approach leverages only noisy OCT images by first generating
pseudo-ground-truth images through self-fusion and self-supervised denoising.
These refined images then serve as targets to train an ensemble of denoising
models using a patch-based strategy that effectively enhances image clarity.
Performance improvements are validated via metrics such as Contrast-to-Noise
Ratio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge
Preservation (EP) on the real-world dataset from the IEEE SPS Video and Image
Processing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT
images without clean references, highlighting our method's potential for
improving image quality and diagnostic outcomes in clinical practice.

</details>


### [70] [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://arxiv.org/abs/2510.16704)
*Tianxin Wei,Yifan Chen,Xinrui He,Wenxuan Bao,Jingrui He*

Main category: cs.CV

TL;DR: 提出了一种新的领域连接对比学习（DCCL）方法来解决领域泛化问题，通过增强跨领域的概念连接性来提高模型在未见目标域上的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 实践中训练和测试样本之间的分布偏移经常发生，阻碍模型泛化性能。直接应用对比学习反而会降低领域泛化性能，因为缺乏领域内类内连接性。

Method: 提出DCCL方法：在数据侧使用更激进的数据增强和跨领域正样本来改善类内连接性；在模型侧提出模型锚定来利用预训练表示中的类内连接性，并通过生成变换损失进行补充。

Result: 在五个标准领域泛化基准测试上的广泛实验表明，DCCL即使没有领域监督也优于最先进的基线方法。

Conclusion: DCCL通过增强跨领域的概念连接性，成功解决了对比学习在领域泛化中的性能下降问题，获得了可泛化的表示。

Abstract: Distribution shifts between training and testing samples frequently occur in
practice and impede model generalization performance. This crucial challenge
thereby motivates studies on domain generalization (DG), which aim to predict
the label on unseen target domain data by solely using data from source
domains. It is intuitive to conceive the class-separated representations
learned in contrastive learning (CL) are able to improve DG, while the reality
is quite the opposite: users observe directly applying CL deteriorates the
performance. We analyze the phenomenon with the insights from CL theory and
discover lack of intra-class connectivity in the DG setting causes the
deficiency. We thus propose a new paradigm, domain-connecting contrastive
learning (DCCL), to enhance the conceptual connectivity across domains and
obtain generalizable representations for DG. On the data side, more aggressive
data augmentation and cross-domain positive samples are introduced to improve
intra-class connectivity. On the model side, to better embed the unseen test
domains, we propose model anchoring to exploit the intra-class connectivity in
pre-trained representations and complement the anchoring with generative
transformation loss. Extensive experiments on five standard DG benchmarks are
performed. The results verify that DCCL outperforms state-of-the-art baselines
even without domain supervision. The detailed model implementation and the code
are provided through https://github.com/weitianxin/DCCL

</details>


### [71] [HumanCM: One Step Human Motion Prediction](https://arxiv.org/abs/2510.16709)
*Liu Haojie,Gao Suixiang*

Main category: cs.CV

TL;DR: HumanCM是一个基于一致性模型的一步人体运动预测框架，通过单步生成实现高效推理，性能媲美扩散模型但推理速度提升两个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型需要多步去噪过程，推理效率较低。本文旨在开发一种高效的单步生成方法，在保持预测精度的同时大幅提升推理速度。

Method: 基于一致性模型，学习噪声与干净运动状态之间的自一致映射。采用基于Transformer的时空架构，结合时间嵌入来建模长程依赖关系和保持运动连贯性。

Result: 在Human3.6M和HumanEva-I数据集上的实验表明，HumanCM达到了与最先进扩散模型相当或更优的精度，同时将推理步骤减少了最多两个数量级。

Conclusion: HumanCM证明了通过一致性模型实现高效单步运动预测的可行性，在保持高质量预测的同时显著提升了推理效率。

Abstract: We present HumanCM, a one-step human motion prediction framework built upon
consistency models. Instead of relying on multi-step denoising as in
diffusion-based methods, HumanCM performs efficient single-step generation by
learning a self-consistent mapping between noisy and clean motion states. The
framework adopts a Transformer-based spatiotemporal architecture with temporal
embeddings to model long-range dependencies and preserve motion coherence.
Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves
comparable or superior accuracy to state-of-the-art diffusion models while
reducing inference steps by up to two orders of magnitude.

</details>


### [72] [Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes](https://arxiv.org/abs/2510.16714)
*Xiongkun Linghu,Jiangyong Huang,Ziyu Zhu,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: 提出了SCENECOT框架，通过将复杂推理任务分解为更简单的问题，并基于多模态专家模块构建视觉线索，实现了3D场景中的接地推理。开发了首个大规模接地CoT推理数据集SCENECOT-185K，在多个复杂3D场景推理基准上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D大语言模型在接地问答方面仍存在困难，主要原因是缺乏对人类场景-对象接地推理机制的深入探索。

Method: 提出了接地思维链推理方法(SCENECOT)，将复杂推理任务分解为更简单的问题，利用多模态专家模块构建视觉线索。开发了包含18.5万高质量实例的SCENECOT-185K数据集。

Result: 在多个复杂3D场景推理基准测试中表现出色，具有高度的接地问答一致性。

Conclusion: 这是首次成功将CoT推理应用于3D场景理解，实现了逐步的人类式推理，并显示出扩展到更广泛3D场景理解场景的潜力。

Abstract: Existing research on 3D Large Language Models (LLMs) still struggles to
achieve grounded question-answering, primarily due to the under-exploration of
the mech- anism of human-like scene-object grounded reasoning. This paper
bridges the gap by presenting a novel framework. We first introduce a grounded
Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a
complex reasoning task into simpler and manageable problems, and building
corresponding visual clues based on multimodal expert modules. To enable such a
method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning
dataset, consisting of 185K high-quality instances. Extensive experiments
across various complex 3D scene reasoning benchmarks demonstrate that our new
framework achieves strong performance with high grounding-QA coherence. To the
best of our knowledge, this is the first successful application of CoT
reasoning to 3D scene understanding, enabling step-by-step human-like reasoning
and showing potential for extension to broader 3D scene understanding
scenarios.

</details>


### [73] [Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models](https://arxiv.org/abs/2510.16729)
*Jianbiao Mei,Yu Yang,Xuemeng Yang,Licheng Wen,Jiajun Lv,Botian Shi,Yong Liu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: End-to-end autonomous driving systems increasingly rely on vision-centric
world models to understand and predict their environment. However, a common
ineffectiveness in these models is the full reconstruction of future scenes,
which expends significant capacity on redundantly modeling static backgrounds.
To address this, we propose IR-WM, an Implicit Residual World Model that
focuses on modeling the current state and evolution of the world. IR-WM first
establishes a robust bird's-eye-view representation of the current state from
the visual observation. It then leverages the BEV features from the previous
timestep as a strong temporal prior and predicts only the "residual", i.e., the
changes conditioned on the ego-vehicle's actions and scene context. To
alleviate error accumulation over time, we further apply an alignment module to
calibrate semantic and dynamic misalignments. Moreover, we investigate
different forecasting-planning coupling schemes and demonstrate that the
implicit future state generated by world models substantially improves planning
accuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D
occupancy forecasting and trajectory planning.

</details>


### [74] [UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid](https://arxiv.org/abs/2510.16730)
*Tianyang Dou,Ming Li,Jiangying Qin,Xuan Liao,Jiageng Zhong,Armin Gruen,Mengyi Deng*

Main category: cs.CV

TL;DR: UKANFormer是一种新颖的语义分割模型，能够在来自Allen Coral Atlas的噪声监督下实现高精度珊瑚礁映射，通过架构设计缓解标签噪声问题。


<details>
  <summary>Details</summary>
Motivation: 全球珊瑚礁产品如Allen Coral Atlas在空间精度和语义一致性方面存在局限，特别是在需要精细边界划分的区域，需要解决这些挑战以实现有效的珊瑚礁保护。

Method: 基于UKAN架构，UKANFormer在解码器中加入了全局-局部变换器（GL-Trans）块，能够同时提取全局语义结构和局部边界细节。

Result: UKANFormer在珊瑚类IoU达到67.00%，像素精度达到83.98%，在相同噪声标签设置下优于传统基线方法，产生的预测在视觉和结构上比训练使用的噪声标签更准确。

Conclusion: 研究结果表明数据质量并不直接限制模型性能，架构设计可以缓解标签噪声，支持在非完美监督下进行可扩展的映射，为生态监测提供了基础。

Abstract: Coral reefs are vital yet fragile ecosystems that require accurate
large-scale mapping for effective conservation. Although global products such
as the Allen Coral Atlas provide unprecedented coverage of global coral reef
distri-bution, their predictions are frequently limited in spatial precision
and semantic consistency, especially in regions requiring fine-grained boundary
delineation. To address these challenges, we propose UKANFormer, a novel
se-mantic segmentation model designed to achieve high-precision mapping under
noisy supervision derived from Allen Coral Atlas. Building upon the UKAN
architecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)
block in the decoder, enabling the extraction of both global semantic
structures and local boundary details. In experiments, UKANFormer achieved a
coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming
conventional baselines under the same noisy labels setting. Remarkably, the
model produces predictions that are visually and structurally more accurate
than the noisy labels used for training. These results challenge the notion
that data quality directly limits model performance, showing that architectural
design can mitigate label noise and sup-port scalable mapping under imperfect
supervision. UKANFormer provides a foundation for ecological monitoring where
reliable labels are scarce.

</details>


### [75] [A Comprehensive Survey on World Models for Embodied AI](https://arxiv.org/abs/2510.16732)
*Xinqing Li,Xin He,Le Zhang,Yun Liu*

Main category: cs.CV

TL;DR: 这篇论文提出了一个关于具身AI中世界模型的统一框架，包括问题形式化、学习目标和三轴分类法，系统整理了数据资源和评估指标，并对现有模型进行了定量比较，指出了关键开放挑战。


<details>
  <summary>Details</summary>
Motivation: 具身AI需要能够感知、行动并预测行动如何重塑未来世界状态的智能体。世界模型作为内部模拟器，能够捕捉环境动态，支持感知、预测和决策制定。

Method: 提出了一个三轴分类法：(1) 功能性：决策耦合vs通用目的；(2) 时序建模：序列模拟与推理vs全局差异预测；(3) 空间表示：全局潜在向量、令牌特征序列、空间潜在网格和分解渲染表示。

Result: 系统整理了机器人学、自动驾驶和通用视频设置中的数据资源和评估指标，覆盖像素预测质量、状态级理解和任务性能，并对最先进模型进行了定量比较。

Conclusion: 指出了关键开放挑战：统一数据集的稀缺性、需要评估物理一致性而非像素保真度的指标、模型性能与实时控制计算效率的权衡，以及实现长期时序一致性同时减轻误差累积的核心建模难度。

Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions
reshape future world states. World models serve as internal simulators that
capture environment dynamics, enabling forward and counterfactual rollouts to
support perception, prediction, and decision making. This survey presents a
unified framework for world models in embodied AI. Specifically, we formalize
the problem setting and learning objectives, and propose a three-axis taxonomy
encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)
Temporal Modeling, Sequential Simulation and Inference vs. Global Difference
Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature
Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We
systematize data resources and metrics across robotics, autonomous driving, and
general video settings, covering pixel prediction quality, state-level
understanding, and task performance. Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation. Finally, we maintain a curated
bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.

</details>


### [76] [Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling](https://arxiv.org/abs/2510.16751)
*Erik Riise,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: 研究表明，离散自回归模型通过beam search在图像生成中优于扩散模型，2B参数模型超越12B扩散模型，关键在于离散token空间支持早期剪枝和计算重用。


<details>
  <summary>Details</summary>
Motivation: 将推理时搜索策略从语言模型成功应用到图像生成领域面临困难，现有方法在连续扩散模型中效果有限，需要探索更适合图像生成的架构。

Method: 使用离散自回归视觉模型，应用beam search策略进行图像生成，利用离散token空间特性实现早期剪枝和计算重用。

Result: 2B参数自回归模型在文本到图像生成任务中超越12B参数扩散模型，beam search显著提升生成质量，系统消融实验验证了离散token空间的优势。

Conclusion: 模型架构（而不仅仅是规模）对于视觉生成中的推理时优化至关重要，离散自回归模型为图像生成中的搜索策略提供了有效框架。

Abstract: While inference-time scaling through search has revolutionized Large Language
Models, translating these gains to image generation has proven difficult.
Recent attempts to apply search strategies to continuous diffusion models show
limited benefits, with simple random sampling often performing best. We
demonstrate that the discrete, sequential nature of visual autoregressive
models enables effective search for image generation. We show that beam search
substantially improves text-to-image generation, enabling a 2B parameter
autoregressive model to outperform a 12B parameter diffusion model across
benchmarks. Systematic ablations show that this advantage comes from the
discrete token space, which allows early pruning and computational reuse, and
our verifier analysis highlights trade-offs between speed and reasoning
capability. These findings suggest that model architecture, not just scale, is
critical for inference-time optimization in visual generation.

</details>


### [77] [Region in Context: Text-condition Image editing with Human-like semantic reasoning](https://arxiv.org/abs/2510.16772)
*Thuy Phuong Vu,Dinh-Cuong Hoang,Minhhuy Le,Phan Xuan Tan*

Main category: cs.CV

TL;DR: 提出了Region in Context框架，通过多层级语义对齐实现文本条件图像编辑，确保局部编辑与全局场景的协调性


<details>
  <summary>Details</summary>
Motivation: 现有方法将图像区域孤立处理，仅依赖局部线索，导致编辑不一致、过渡不自然或整体连贯性丧失

Method: 引入双层级引导机制：区域在完整图像上下文中表示并与详细区域级描述对齐，同时整个图像与视觉语言模型生成的场景级描述匹配

Result: 实验表明该方法能产生更连贯且与指令对齐的结果

Conclusion: 通过理解区域在全局图像上下文中的作用，实现了精确协调的图像编辑

Abstract: Recent research has made significant progress in localizing and editing image
regions based on text. However, most approaches treat these regions in
isolation, relying solely on local cues without accounting for how each part
contributes to the overall visual and semantic composition. This often results
in inconsistent edits, unnatural transitions, or loss of coherence across the
image. In this work, we propose Region in Context, a novel framework for
text-conditioned image editing that performs multilevel semantic alignment
between vision and language, inspired by the human ability to reason about
edits in relation to the whole scene. Our method encourages each region to
understand its role within the global image context, enabling precise and
harmonized changes. At its core, the framework introduces a dual-level guidance
mechanism: regions are represented with full-image context and aligned with
detailed region-level descriptions, while the entire image is simultaneously
matched to a comprehensive scene-level description generated by a large
vision-language model. These descriptions serve as explicit verbal references
of the intended content, guiding both local modifications and global structure.
Experiments show that it produces more coherent and instruction-aligned
results. Code is available at:
https://github.com/thuyvuphuong/Region-in-Context.git

</details>


### [78] [Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution](https://arxiv.org/abs/2510.16752)
*Ivan Molodetskikh,Kirill Malyshev,Mark Mirgaleev,Nikita Zagainov,Evgeney Bogatyrev,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: 提出了一个包含1302个SR方法生成图像伪影的数据集，每个伪影都带有众包显著性评分，并训练了一个轻量级回归器来生成空间显著性热图。


<details>
  <summary>Details</summary>
Motivation: 生成式图像超分辨率模型在视觉质量和细节恢复方面进步迅速，但随着模型容量扩大，它们倾向于产生伪影。这些伪影对人类观察者的感知影响各不相同，应该根据其显著性而非统一处理为二元缺陷。

Method: 构建了一个包含11种当代图像SR方法产生的1302个伪影示例的数据集，每个伪影都配有众包显著性评分。基于此数据集训练了一个轻量级回归器来生成空间显著性热图。

Result: 训练的回归器在检测显著伪影方面优于现有方法，能够生成空间显著性热图。

Conclusion: 发布数据集和代码以促进基于显著性的SR伪影评估和缓解。

Abstract: Generative image super-resolution (SR) is rapidly advancing in visual quality
and detail restoration. As the capacity of SR models expands, however, so does
their tendency to produce artifacts: incorrect, visually disturbing details
that reduce perceived quality. Crucially, their perceptual impact varies: some
artifacts are barely noticeable while others strongly degrade the image. We
argue that artifacts should be characterized by their prominence to human
observers rather than treated as uniform binary defects. Motivated by this, we
present a novel dataset of 1302 artifact examples from 11 contemporary image-SR
methods, where each artifact is paired with a crowdsourced prominence score.
Building on this dataset, we train a lightweight regressor that produces
spatial prominence heatmaps and outperforms existing methods at detecting
prominent artifacts. We release the dataset and code to facilitate
prominence-aware evaluation and mitigation of SR artifacts.

</details>


### [79] [EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation](https://arxiv.org/abs/2510.16776)
*Mingzheng Zhang,Jinfeng Gao,Dan Xu,Jiangrui Yu,Yuhan Qiao,Lan Chen,Jin Tang,Xiao Wang*

Main category: cs.CV

TL;DR: 提出了EMRRG框架，利用参数高效方法微调预训练的Mamba网络，用于X射线医疗报告生成，在基准数据集上取得优异结果。


<details>
  <summary>Details</summary>
Motivation: 现有医疗报告生成模型主要依赖LLM，对预训练视觉基础模型和高级微调技术探索有限，主流框架忽视交叉注意力机制增强潜力，且非Transformer架构在医疗报告生成中研究不足。

Method: 将X射线图像分块、标记化，通过SSM-based视觉骨干进行特征提取，使用Partial LoRA获得最佳性能，结合混合解码器的LLM生成医疗报告，实现端到端训练。

Result: 在三个广泛使用的基准数据集上进行大量实验，充分验证了所提出策略对X射线医疗报告生成的有效性。

Conclusion: EMRRG框架通过参数高效微调预训练Mamba网络，在X射线医疗报告生成任务中表现出色，为相关研究提供了新思路。

Abstract: X-ray image-based medical report generation (MRG) is a pivotal area in
artificial intelligence that can significantly reduce diagnostic burdens for
clinicians and patient wait times. Existing MRG models predominantly rely on
Large Language Models (LLMs) to improve report generation, with limited
exploration of pre-trained vision foundation models or advanced fine-tuning
techniques. Mainstream frameworks either avoid fine-tuning or utilize
simplistic methods like LoRA, often neglecting the potential of enhancing
cross-attention mechanisms. Additionally, while Transformer-based models
dominate vision-language tasks, non-Transformer architectures, such as the
Mamba network, remain underexplored for medical report generation, presenting a
promising avenue for future research. In this paper, we propose EMRRG, a novel
X-ray report generation framework that fine-tunes pre-trained Mamba networks
using parameter-efficient methods. Specifically, X-ray images are divided into
patches, tokenized, and processed by an SSM-based vision backbone for feature
extraction, with Partial LoRA yielding optimal performance. An LLM with a
hybrid decoder generates the medical report, enabling end-to-end training and
achieving strong results on benchmark datasets. Extensive experiments on three
widely used benchmark datasets fully validated the effectiveness of our
proposed strategies for the X-ray MRG. The source code of this paper will be
released on https://github.com/Event-AHU/Medical_Image_Analysis.

</details>


### [80] [WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement](https://arxiv.org/abs/2510.16765)
*Shengyu Zhu,Fan,Fuxuan Zhang*

Main category: cs.CV

TL;DR: 提出WaMaIR框架，通过全局多尺度小波变换卷积扩大感受野，结合Mamba通道感知模块捕获长程依赖，并使用多尺度纹理增强损失，在图像恢复任务中实现更好的纹理细节重建和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有CNN方法在恢复精细纹理细节方面存在挑战，主要受限于CNN结构的小感受野和缺乏通道特征建模。

Method: 1.全局多尺度小波变换卷积(GMWTConvs)扩大感受野；2.Mamba通道感知模块(MCAM)捕获长程依赖；3.多尺度纹理增强损失(MTELoss)指导纹理保持。

Result: 大量实验证实WaMaIR优于最先进方法，在图像恢复和计算性能方面表现更好。

Conclusion: WaMaIR框架通过扩大感受野和增强通道建模，有效提升了图像恢复中纹理细节的重建质量，同时保持了计算效率。

Abstract: Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.

</details>


### [81] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: 提出无需训练的零样本视频理解框架，通过结合预训练视觉语言模型的语义先验和传统机器学习算法，将视频理解重构为高维语义特征空间中的自监督时空聚类问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型依赖大量标注数据和任务特定训练，成本高且扩展性差。大型视觉语言模型在静态图像上的零样本推理能力尚未充分应用于视频领域。

Method: 首先使用预训练VLM的冻结视觉编码器将视频流转换为语义特征轨迹，然后使用核时间分割算法将连续特征流分割为语义连贯的事件片段，最后通过无监督密度聚类识别重复的宏观场景和主题。

Result: 通过从每个发现的聚类中选择代表性关键帧并利用VLM的生成能力进行文本描述，自动生成视频内容的结构化多模态摘要。

Conclusion: 该方法为零样本、自动化的视频内容结构分析提供了一条有效、可解释且模型无关的途径。

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [82] [ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification](https://arxiv.org/abs/2510.16822)
*Yahia Battach,Abdulwahab Felemban,Faizan Farooq Khan,Yousef A. Radwan,Xiang Li,Fabio Marchese,Sara Beery,Burton H. Jones,Francesca Benzoni,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: ReefNet是一个大规模公开珊瑚礁图像数据集，包含约925,000个属级硬珊瑚标注，映射到世界海洋物种名录，提供细粒度、全球尺度的珊瑚分类基准测试。


<details>
  <summary>Details</summary>
Motivation: 由于气候变化等人为压力导致珊瑚礁迅速衰退，急需可扩展的自动化监测方法。现有数据集在规模、地理覆盖和标签粒度方面存在局限，无法满足机器学习需求。

Method: 整合76个CoralNet来源和红海Al Wajh站点的图像，提供专家验证的属级标注。提出两种评估设置：源内基准测试和跨源基准测试，分别测试局部性能和领域泛化能力。

Result: 监督学习在源内表现良好，但跨域性能显著下降；零样本模型在所有情况下表现均较差，特别是对于稀有和视觉相似属。

Conclusion: ReefNet为领域泛化和细粒度珊瑚分类提供了具有挑战性的基准，旨在推动稳健、领域自适应的全球珊瑚礁监测和保护技术的发展。

Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as
climate change, underscoring the urgent need for scalable, automated
monitoring. We introduce ReefNet, a large public coral reef image dataset with
point-label annotations mapped to the World Register of Marine Species (WoRMS).
ReefNet aggregates imagery from 76 curated CoralNet sources and an additional
site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level
hard coral annotations with expert-verified labels. Unlike prior datasets,
which are often limited by size, geography, or coarse labels and are not
ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global
scale to WoRMS. We propose two evaluation settings: (i) a within-source
benchmark that partitions each source's images for localized evaluation, and
(ii) a cross-source benchmark that withholds entire sources to test domain
generalization. We analyze both supervised and zero-shot classification
performance on ReefNet and find that while supervised within-source performance
is promising, supervised performance drops sharply across domains, and
performance is low across the board for zero-shot models, especially for rare
and visually similar genera. This provides a challenging benchmark intended to
catalyze advances in domain generalization and fine-grained coral
classification. We will release our dataset, benchmarking code, and pretrained
models to advance robust, domain-adaptive, global coral reef monitoring and
conservation.

</details>


### [83] [ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification](https://arxiv.org/abs/2510.16854)
*Akhila Kambhatla,Taminul Islam,Khaled R Ahmed*

Main category: cs.CV

TL;DR: ArmFormer是一个轻量级基于transformer的语义分割框架，集成了CBAM注意力模块和MixVisionTransformer架构，用于武器检测，在保持计算效率的同时实现高精度分割。


<details>
  <summary>Details</summary>
Motivation: 传统武器检测方法只能提供粗略的边界框定位，缺乏细粒度分割能力，而现有分割模型要么牺牲精度换取效率，要么计算资源需求过高，不适合边缘部署。

Method: 结合CBAM增强的编码器骨干网络和注意力集成的hamburger解码器，实现五类武器（手枪、步枪、刀、左轮手枪、人）的多类别分割。

Result: ArmFormer达到80.64% mIoU和89.13% mFscore的SOTA性能，实时推理速度82.26 FPS，仅需4.886G FLOPs和3.66M参数，比重量级模型节省48倍计算量。

Conclusion: ArmFormer是部署在便携安全摄像头、监控无人机和嵌入式AI加速器上的最优解决方案，在分布式安全基础设施中具有重要应用价值。

Abstract: The escalating threat of weapon-related violence necessitates automated
detection systems capable of pixel-level precision for accurate threat
assessment in real-time security applications. Traditional weapon detection
approaches rely on object detection frameworks that provide only coarse
bounding box localizations, lacking the fine-grained segmentation required for
comprehensive threat analysis. Furthermore, existing semantic segmentation
models either sacrifice accuracy for computational efficiency or require
excessive computational resources incompatible with edge deployment scenarios.
This paper presents ArmFormer, a lightweight transformer-based semantic
segmentation framework that strategically integrates Convolutional Block
Attention Module (CBAM) with MixVisionTransformer architecture to achieve
superior accuracy while maintaining computational efficiency suitable for
resource-constrained edge devices. Our approach combines CBAM-enhanced encoder
backbone with attention-integrated hamburger decoder to enable multi-class
weapon segmentation across five categories: handgun, rifle, knife, revolver,
and human. Comprehensive experiments demonstrate that ArmFormer achieves
state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while
maintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M
parameters, ArmFormer outperforms heavyweight models requiring up to 48x more
computation, establishing it as the optimal solution for deployment on portable
security cameras, surveillance drones, and embedded AI accelerators in
distributed security infrastructure.

</details>


### [84] [GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation](https://arxiv.org/abs/2510.16777)
*Junbo Li,Weimin Yuan,Yinuo Wang,Yue Zeng,Shihao Shu,Cai Meng,Xiangzhi Bai*

Main category: cs.CV

TL;DR: GS2POSE是一种基于3D高斯泼溅的6D物体姿态估计新方法，通过束调整原理和Lie代数优化姿态，在无纹理物体和光照变化场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于2D-3D特征对应方法在无纹理物体和光照变化条件下的困难。

Method: 提出基于束调整原理的姿态回归算法，利用Lie代数扩展3DGS构建姿态可微渲染管道，通过迭代优化输入图像与渲染图像的差异来优化姿态，同时更新3DGS模型的颜色参数以适应光照变化。

Result: 在T-LESS、LineMod-Occlusion和LineMod数据集上分别实现了1.4%、2.8%和2.5%的精度提升。

Conclusion: GS2POSE通过结合3DGS和束调整原理，有效提升了在挑战性场景下的6D姿态估计精度。

Abstract: Accurate 6D pose estimation of 3D objects is a fundamental task in computer
vision, and current research typically predicts the 6D pose by establishing
correspondences between 2D image features and 3D model features. However, these
methods often face difficulties with textureless objects and varying
illumination conditions. To overcome these limitations, we propose GS2POSE, a
novel approach for 6D object pose estimation. GS2POSE formulates a pose
regression algorithm inspired by the principles of Bundle Adjustment (BA). By
leveraging Lie algebra, we extend the capabilities of 3DGS to develop a
pose-differentiable rendering pipeline, which iteratively optimizes the pose by
comparing the input image to the rendered image. Additionally, GS2POSE updates
color parameters within the 3DGS model, enhancing its adaptability to changes
in illumination. Compared to previous models, GS2POSE demonstrates accuracy
improvements of 1.4\%, 2.8\% and 2.5\% on the T-LESS, LineMod-Occlusion and
LineMod datasets, respectively.

</details>


### [85] [Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis](https://arxiv.org/abs/2510.16973)
*Praveenbalaji Rajendran,Mojtaba Safari,Wenfeng He,Mingzhe Hu,Shansong Wang,Jun Zhou,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 这篇综述文章系统分析了医学图像分析中的基础模型，包括视觉专用和视觉语言模型，讨论了架构演变、训练策略、临床应用，以及面临的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在医学图像分析中快速发展，但该领域仍缺乏对架构演变、训练范式和临床应用的统一系统分析，需要填补这一空白。

Method: 将研究系统分类为视觉专用和视觉语言基础模型，分析其架构基础、训练策略和下游临床任务，并进行定量元分析以表征数据集利用和应用领域的时间趋势。

Result: 通过系统分析揭示了基础模型在医学图像分析中的架构演变、训练范式和应用趋势，识别了关键挑战和新兴解决方案。

Conclusion: 基础模型在医学图像分析中具有巨大潜力，但需要解决领域适应、高效微调、计算约束和可解释性等挑战，未来研究方向应集中在增强鲁棒性、可解释性和临床整合上。

Abstract: Recent advancements in artificial intelligence (AI), particularly foundation
models (FMs), have revolutionized medical image analysis, demonstrating strong
zero- and few-shot performance across diverse medical imaging tasks, from
segmentation to report generation. Unlike traditional task-specific AI models,
FMs leverage large corpora of labeled and unlabeled multimodal datasets to
learn generalized representations that can be adapted to various downstream
clinical applications with minimal fine-tuning. However, despite the rapid
proliferation of FM research in medical imaging, the field remains fragmented,
lacking a unified synthesis that systematically maps the evolution of
architectures, training paradigms, and clinical applications across modalities.
To address this gap, this review article provides a comprehensive and
structured analysis of FMs in medical image analysis. We systematically
categorize studies into vision-only and vision-language FMs based on their
architectural foundations, training strategies, and downstream clinical tasks.
Additionally, a quantitative meta-analysis of the studies was conducted to
characterize temporal trends in dataset utilization and application domains. We
also critically discuss persistent challenges, including domain adaptation,
efficient fine-tuning, computational constraints, and interpretability along
with emerging solutions such as federated learning, knowledge distillation, and
advanced prompting. Finally, we identify key future research directions aimed
at enhancing the robustness, explainability, and clinical integration of FMs,
thereby accelerating their translation into real-world medical practice.

</details>


### [86] [One-step Diffusion Models with Bregman Density Ratio Matching](https://arxiv.org/abs/2510.16983)
*Yuanzhi Zhu,Eleftherios Tsonis,Lucas Degeorge,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 提出了Di-Bregman框架，通过Bregman散度密度比匹配来统一扩散蒸馏方法，实现了高效的一步生成。


<details>
  <summary>Details</summary>
Motivation: 扩散和流模型生成质量高但计算昂贵，现有蒸馏方法缺乏统一的理论基础。

Method: 将扩散蒸馏表述为基于Bregman散度的密度比匹配问题，提供凸分析视角。

Result: 在CIFAR-10和文生图任务上，相比反向KL蒸馏获得更好的一步FID，同时保持高视觉保真度。

Conclusion: Bregman密度比匹配是通向高效一步扩散生成的理论基础扎实且实用的路径。

Abstract: Diffusion and flow models achieve high generative quality but remain
computationally expensive due to slow multi-step sampling. Distillation methods
accelerate them by training fast student generators, yet most existing
objectives lack a unified theoretical foundation. In this work, we propose
Di-Bregman, a compact framework that formulates diffusion distillation as
Bregman divergence-based density-ratio matching. This convex-analytic view
connects several existing objectives through a common lens. Experiments on
CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves
improved one-step FID over reverse-KL distillation and maintains high visual
fidelity compared to the teacher model. Our results highlight Bregman
density-ratio matching as a practical and theoretically-grounded route toward
efficient one-step diffusion generation.

</details>


### [87] [Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs](https://arxiv.org/abs/2510.16785)
*Jiazhen Liu,Long Chen*

Main category: cs.CV

TL;DR: LENS是一种新颖的即插即用解决方案，通过为冻结的MLLM附加轻量级可训练头部，利用注意力图中的空间线索提取关键点，实现像素级分割，同时完全保留模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法需要微调模型以产生与掩码解码器兼容的输出，这会改变模型的输出空间并损害其内在泛化能力，违背了构建统一模型的目标。

Method: 在完全冻结的MLLM上附加轻量级可训练头部，通过精炼注意力图中的空间线索来提取关键点，并将其描述为与掩码解码器直接兼容的点级特征。

Result: LENS实现了与基于重新训练方法相竞争甚至更优的分割性能，同时完全保留了MLLM的泛化能力，而微调方法会显著降低这种能力。

Conclusion: LENS的可附加设计为扩展MLLM建立了一个高效且强大的范式，为构建真正多才多艺的统一模型铺平了道路。

Abstract: Integrating diverse visual capabilities into a unified model is a significant
trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion
of segmentation poses a distinct set of challenges. To equip MLLMs with
pixel-level segmentation abilities, prevailing methods require finetuning the
model to produce specific outputs compatible with a mask decoder. This process
typically alters the model's output space and compromises its intrinsic
generalization, which undermines the goal of building a unified model. We
introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel
plug-and-play solution. LENS attaches a lightweight, trainable head to a
completely frozen MLLM. By refining the spatial cues embedded in attention
maps, LENS extracts keypoints and describes them into point-wise features
directly compatible with the mask decoder. Extensive experiments validate our
approach: LENS achieves segmentation performance competitive with or superior
to that of retraining-based methods. Crucially, it does so while fully
preserving the MLLM's generalization capabilities, which are significantly
degraded by finetuning approaches. As such, the attachable design of LENS
establishes an efficient and powerful paradigm for extending MLLMs, paving the
way for truly multi-talented, unified models.

</details>


### [88] [CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams](https://arxiv.org/abs/2510.16988)
*Junhao Zhao,Zishuai Liu,Ruili Fang,Jin Lu,Linghan Zhang,Fei Dou*

Main category: cs.CV

TL;DR: 提出了CARE框架，通过序列-图像对比对齐方法解决日常活动识别中序列和图像表示的对齐问题，在三个CASAS数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的日常活动识别方法存在表示层面的限制：序列方法保持时间顺序但对噪声敏感且缺乏空间意识；图像方法捕获全局模式但压缩时间动态并扭曲传感器布局；简单的特征融合方法无法有效对齐不同表示视图。

Method: CARE框架包含：(1)时间感知、抗噪声的序列编码；(2)空间感知和频率敏感的图像表示；(3)联合对比-分类目标函数，通过序列-图像对比对齐(SICA)和交叉熵分类进行端到端学习。

Result: 在三个CASAS数据集上取得最优性能：Milan数据集89.8%、Cairo数据集88.9%、Kyoto7数据集73.3%，并表现出对传感器故障和布局变化的鲁棒性。

Conclusion: CARE框架通过对比对齐有效利用了序列和图像表示的互补优势，实现了可靠且鲁棒的日常活动识别，在智能家居环境中具有重要应用潜力。

Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered
ambient sensors is an essential task in Ambient Assisted Living, yet existing
methods remain constrained by representation-level limitations. Sequence-based
approaches preserve temporal order of sensor activations but are sensitive to
noise and lack spatial awareness, while image-based approaches capture global
patterns and implicit spatial correlations but compress fine-grained temporal
dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)
fail to enforce alignment between sequence- and image-based representation
views, underutilizing their complementary strengths. We propose Contrastive
Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an
end-to-end framework that jointly optimizes representation learning via
Sequence-Image Contrastive Alignment (SICA) and classification via
cross-entropy, ensuring both cross-representation alignment and task-specific
discriminability. CARE integrates (i) time-aware, noise-resilient sequence
encoding with (ii) spatially-informed and frequency-sensitive image
representations, and employs (iii) a joint contrastive-classification objective
for end-to-end learning of aligned and discriminative embeddings. Evaluated on
three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on
Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to
sensor malfunctions and layout variability, highlighting its potential for
reliable ADL recognition in smart homes.

</details>


### [89] [Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry](https://arxiv.org/abs/2510.16790)
*Sara Hatami Rostami,Behrooz Nasihatkon*

Main category: cs.CV

TL;DR: 提出了一种完全无监督的二元道路分割方法，利用场景几何和时间一致性来区分道路与非道路区域，无需人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 消除对昂贵人工标注数据集的依赖，为自动驾驶开发可扩展的无监督道路分割解决方案。

Method: 首先基于几何先验生成弱标签（地平线以上为非道路，车辆前方四边形为道路），然后通过跨帧跟踪局部特征点并利用互信息最大化来强制时间一致性。

Result: 在Cityscapes数据集上达到0.82的交并比(IoU)，表现出高精度和良好的时间稳定性。

Conclusion: 几何约束与时间一致性相结合的方法在无监督道路分割中具有巨大潜力，能够实现简单设计下的高精度分割。

Abstract: This paper presents a fully unsupervised approach for binary road
segmentation (road vs. non-road), eliminating the reliance on costly manually
labeled datasets. The method leverages scene geometry and temporal cues to
distinguish road from non-road regions. Weak labels are first generated from
geometric priors, marking pixels above the horizon as non-road and a predefined
quadrilateral in front of the vehicle as road. In a refinement stage, temporal
consistency is enforced by tracking local feature points across frames and
penalizing inconsistent label assignments using mutual information
maximization. This enhances both precision and temporal stability. On the
Cityscapes dataset, the model achieves an Intersection-over-Union (IoU) of
0.82, demonstrating high accuracy with a simple design. These findings
demonstrate the potential of combining geometric constraints and temporal
consistency for scalable unsupervised road segmentation in autonomous driving.

</details>


### [90] [Video Reasoning without Training](https://arxiv.org/abs/2510.17045)
*Deepak Sridhar,Kartikeya Bhardwaj,Jeya Pradha Jeyaraj,Nuno Vasconcelos,Ankita Nayak,Harris Teague*

Main category: cs.CV

TL;DR: 提出V-Reason方法，通过基于熵的控制器优化LMM的推理过程，无需RL训练即可显著提升视频推理性能，同时大幅减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有视频推理模型依赖昂贵的强化学习和冗长的思维链，计算成本高且推理过程控制机制有限。

Method: 利用模型输出熵作为信号，通过小型可训练控制器优化LMM的值缓存，采用基于熵的目标函数调整推理过程中的微观探索和利用行为。

Result: 在多个视频推理数据集上显著超越基础指令调优模型，与RL训练模型差距缩小至0.6%平均准确率，同时输出token减少58.6%。

Conclusion: 基于熵的推理过程控制能有效提升LMM的视频推理能力，无需额外训练即可实现高效推理。

Abstract: Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.

</details>


### [91] [Personalized Image Filter: Mastering Your Photographic Style](https://arxiv.org/abs/2510.16791)
*Chengxuan Zhu,Shuchen Weng,Jiacong Fang,Peixuan Zhang,Si Li,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: 提出个性化图像滤镜PIF，基于预训练文本到图像扩散模型，通过文本反转技术学习参考图像的摄影风格，实现高质量的摄影风格提取和迁移。


<details>
  <summary>Details</summary>
Motivation: 摄影风格作为摄影概念的组合，是著名摄影师魅力的体现。但学习和迁移摄影风格需要深刻理解照片从未知原始外观的编辑过程。现有方法要么无法从参考图像中学习有意义的摄影概念，要么无法保持内容图像的内容。

Method: 基于预训练文本到图像扩散模型，利用生成先验学习摄影概念的平均外观以及如何根据文本提示调整它们。通过文本反转技术优化摄影概念的提示，学习参考图像的摄影风格。

Result: PIF在提取和迁移各种摄影风格方面表现出色性能。

Conclusion: PIF能够有效解决摄影风格学习和迁移中的关键问题，展现出优秀的风格提取和迁移能力。

Abstract: Photographic style, as a composition of certain photographic concepts, is the
charm behind renowned photographers. But learning and transferring photographic
style need a profound understanding of how the photo is edited from the unknown
original appearance. Previous works either fail to learn meaningful
photographic concepts from reference images, or cannot preserve the content of
the content image. To tackle these issues, we proposed a Personalized Image
Filter (PIF). Based on a pretrained text-to-image diffusion model, the
generative prior enables PIF to learn the average appearance of photographic
concepts, as well as how to adjust them according to text prompts. PIF then
learns the photographic style of reference images with the textual inversion
technique, by optimizing the prompts for the photographic concepts. PIF shows
outstanding performance in extracting and transferring various kinds of
photographic style. Project page: https://pif.pages.dev/

</details>


### [92] [GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2510.17131)
*Xin Gao,Jiyao Liu,Guanghao Li,Yueming Lyu,Jianxiong Gao,Weichen Yu,Ningsheng Xu,Liang Wang,Caifeng Shan,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: 提出了GOOD框架，通过图像级和特征级双重引导，直接指导扩散采样轨迹生成OOD样本，提升OOD检测性能


<details>
  <summary>Details</summary>
Motivation: 现有方法通过扰动文本条件嵌入生成OOD样本，存在语义不稳定和偏移多样性不足的问题，限制了在真实OOD场景中的泛化能力

Method: 使用现成的ID分类器，采用双级引导：图像级引导基于对数分割梯度减少输入似然，特征级引导基于k-NN距离在分类器潜在空间中促进特征稀疏区域采样

Result: GOOD生成的样本能显著提升OOD检测性能，通过定量和定性分析验证了其有效性

Conclusion: GOOD框架通过可控且多样化的OOD样本生成，为OOD检测提供了更有效的解决方案

Abstract: Recent advancements have explored text-to-image diffusion models for
synthesizing out-of-distribution (OOD) samples, substantially enhancing the
performance of OOD detection. However, existing approaches typically rely on
perturbing text-conditioned embeddings, resulting in semantic instability and
insufficient shift diversity, which limit generalization to realistic OOD. To
address these challenges, we propose GOOD, a novel and flexible framework that
directly guides diffusion sampling trajectories towards OOD regions using
off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level
guidance: (1) Image-level guidance based on the gradient of log partition to
reduce input likelihood, drives samples toward low-density regions in pixel
space. (2) Feature-level guidance, derived from k-NN distance in the
classifier's latent space, promotes sampling in feature-sparse regions. Hence,
this dual-guidance design enables more controllable and diverse OOD sample
generation. Additionally, we introduce a unified OOD score that adaptively
combines image and feature discrepancies, enhancing detection robustness. We
perform thorough quantitative and qualitative analyses to evaluate the
effectiveness of GOOD, demonstrating that training with samples generated by
GOOD can notably enhance OOD detection performance.

</details>


### [93] [An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting](https://arxiv.org/abs/2510.16800)
*Zhenpeng Zhang,Yi Wang,Shanglei Chai,Yingying Liu,Zekai Xie,Wenhao Huang,Pengyu Li,Zipei Luo,Dajiang Lu,Yibin Tian*

Main category: cs.CV

TL;DR: 构建了一个荔枝检测和成熟度分类的数据集，包含11,414张图像，涵盖不同品种、天气条件和成熟阶段，用于开发基于视觉的荔枝采摘机器人。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏在自然生长环境中具有一致性和全面性标注的开源荔枝数据集，而高质量数据对于开发荔枝采摘机器人至关重要。

Method: 采集了多种荔枝品种在不同天气条件和时间段的彩色图像，通过数据增强扩展数据集，并由多人独立标注后统一验证，确保标注一致性。

Result: 数据集包含8,780张增强RGB图像、878张原始RGB图像和1,756张深度图像，共标注了9,658对荔枝检测和成熟度分类标签。

Conclusion: 该数据集为荔枝检测和成熟度分类研究提供了高质量资源，并通过三个深度学习模型验证了其有效性，已公开发布供学术使用。

Abstract: Lychee is a high-value subtropical fruit. The adoption of vision-based
harvesting robots can significantly improve productivity while reduce reliance
on labor. High-quality data are essential for developing such harvesting
robots. However, there are currently no consistently and comprehensively
annotated open-source lychee datasets featuring fruits in natural growing
environments. To address this, we constructed a dataset to facilitate lychee
detection and maturity classification. Color (RGB) images were acquired under
diverse weather conditions, and at different times of the day, across multiple
lychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset
encompasses three different ripeness stages and contains 11,414 images,
consisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth
images. The images are annotated with 9,658 pairs of lables for lychee
detection and maturity classification. To improve annotation consistency, three
individuals independently labeled the data, and their results were then
aggregated and verified by a fourth reviewer. Detailed statistical analyses
were done to examine the dataset. Finally, we performed experiments using three
representative deep learning models to evaluate the dataset. It is publicly
available for academic

</details>


### [94] [GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image](https://arxiv.org/abs/2510.17157)
*Yinghui Wang,Xinyu Zhang,Peng Du*

Main category: cs.CV

TL;DR: GACO-CAD是一个两阶段后训练框架，通过深度和法线图作为几何先验，结合强化学习的组长度奖励，从单张图像生成可编辑的CAD模型，提高几何精度和建模简洁性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型从2D图像推断3D几何时空间推理能力有限的问题，降低工业概念设计的门槛。

Method: 两阶段框架：监督微调阶段使用深度和法线图作为多通道输入；强化学习阶段引入组长度奖励来鼓励简洁建模序列。

Result: 在DeepCAD和Fusion360数据集上达到最先进性能，在代码有效性、几何精度和建模简洁性方面优于现有方法。

Conclusion: GACO-CAD通过几何先验和简洁性奖励有效提升了从单图像生成CAD模型的几何准确性和建模效率。

Abstract: Generating editable, parametric CAD models from a single image holds great
potential to lower the barriers of industrial concept design. However, current
multi-modal large language models (MLLMs) still struggle with accurately
inferring 3D geometry from 2D images due to limited spatial reasoning
capabilities. We address this limitation by introducing GACO-CAD, a novel
two-stage post-training framework. It is designed to achieve a joint objective:
simultaneously improving the geometric accuracy of the generated CAD models and
encouraging the use of more concise modeling procedures. First, during
supervised fine-tuning, we leverage depth and surface normal maps as dense
geometric priors, combining them with the RGB image to form a multi-channel
input. In the context of single-view reconstruction, these priors provide
complementary spatial cues that help the MLLM more reliably recover 3D geometry
from 2D observations. Second, during reinforcement learning, we introduce a
group length reward that, while preserving high geometric fidelity, promotes
the generation of more compact and less redundant parametric modeling
sequences. A simple dynamic weighting strategy is adopted to stabilize
training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD
achieves state-of-the-art performance under the same MLLM backbone,
consistently outperforming existing methods in terms of code validity,
geometric accuracy, and modeling conciseness.

</details>


### [95] [Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring](https://arxiv.org/abs/2510.17179)
*Yingzi Han,Jiakai He,Chuanlong Xie,Jianping Li*

Main category: cs.CV

TL;DR: 本文针对浮游生物识别中的分布偏移问题，首次系统性地构建了OoD基准并评估了22种检测方法，发现ViM方法在远OoD场景中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 浮游生物识别模型在实际部署中面临分布偏移挑战，由于浮游生物形态复杂、物种多样性高且不断发现新物种，导致推理时出现不可预测错误。该领域缺乏最新计算机视觉技术的系统整合和大规模评估基准。

Method: 基于DYB-PlanktonNet数据集精心设计了一系列模拟不同分布偏移场景的OoD基准，系统评估了22种OoD检测方法。

Result: 大量实验结果表明，ViM方法在构建的基准中显著优于其他方法，特别是在远OoD场景中关键指标有大幅提升。

Conclusion: 这项全面评估为自动浮游生物识别中的算法选择提供了可靠参考，并为浮游生物OoD检测的未来研究奠定了坚实基础。这是浮游生物识别领域首次大规模、系统性的OoD检测方法评估分析。

Abstract: Automated plankton recognition models face significant challenges during
real-world deployment due to distribution shifts (Out-of-Distribution, OoD)
between training and test data. This stems from plankton's complex
morphologies, vast species diversity, and the continuous discovery of novel
species, which leads to unpredictable errors during inference. Despite rapid
advancements in OoD detection methods in recent years, the field of plankton
recognition still lacks a systematic integration of the latest computer vision
developments and a unified benchmark for large-scale evaluation. To address
this, this paper meticulously designed a series of OoD benchmarks simulating
various distribution shift scenarios based on the DYB-PlanktonNet dataset
\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection
methods. Extensive experimental results demonstrate that the ViM
\cite{wang2022vim} method significantly outperforms other approaches in our
constructed benchmarks, particularly excelling in Far-OoD scenarios with
substantial improvements in key metrics. This comprehensive evaluation not only
provides a reliable reference for algorithm selection in automated plankton
recognition but also lays a solid foundation for future research in plankton
OoD detection. To our knowledge, this study marks the first large-scale,
systematic evaluation and analysis of Out-of-Distribution data detection
methods in plankton recognition. Code is available at
https://github.com/BlackJack0083/PlanktonOoD.

</details>


### [96] [Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction](https://arxiv.org/abs/2510.16832)
*Abdur Rahman,Mohammad Marufuzzaman,Jason Street,Haifeng Wang,Veera G. Gude,Randy Buchanan*

Main category: cs.CV

TL;DR: 提出AdaptMoist域适应方法，通过纹理特征实现不同来源木屑数据的知识迁移，解决数据分布变化问题，在跨域预测中准确率提升23%达到80%。


<details>
  <summary>Details</summary>
Motivation: 现有木屑水分检测方法存在局限性：直接方法耗时且破坏样品，间接方法在材料来源多样时准确性不足。需要一种能有效应对来源变异性的鲁棒方法。

Method: 综合分析五种纹理特征预测水分含量，提出AdaptMoist域适应方法，利用纹理特征实现跨域知识迁移，并基于调整互信息提出模型保存标准。

Result: 组合五种纹理特征达到95%准确率；AdaptMoist方法在跨域预测中平均准确率达80%，比非适应模型提升23%。

Conclusion: AdaptMoist是跨域木屑水分含量估计的有效鲁棒解决方案，对依赖木屑的行业具有应用潜力。

Abstract: Accurate and quick prediction of wood chip moisture content is critical for
optimizing biofuel production and ensuring energy efficiency. The current
widely used direct method (oven drying) is limited by its longer processing
time and sample destructiveness. On the other hand, existing indirect methods,
including near-infrared spectroscopy-based, electrical capacitance-based, and
image-based approaches, are quick but not accurate when wood chips come from
various sources. Variability in the source material can alter data
distributions, undermining the performance of data-driven models. Therefore,
there is a need for a robust approach that effectively mitigates the impact of
source variability. Previous studies show that manually extracted texture
features have the potential to predict wood chip moisture class. Building on
this, in this study, we conduct a comprehensive analysis of five distinct
texture feature types extracted from wood chip images to predict moisture
content. Our findings reveal that a combined feature set incorporating all five
texture features achieves an accuracy of 95% and consistently outperforms
individual texture features in predicting moisture content. To ensure robust
moisture prediction, we propose a domain adaptation method named AdaptMoist
that utilizes the texture features to transfer knowledge from one source of
wood chip data to another, addressing variability across different domains. We
also proposed a criterion for model saving based on adjusted mutual
information. The AdaptMoist method improves prediction accuracy across domains
by 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted
models. These results highlight the effectiveness of AdaptMoist as a robust
solution for wood chip moisture content estimation across domains, making it a
potential solution for wood chip-reliant industries.

</details>


### [97] [ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models](https://arxiv.org/abs/2510.17197)
*Pu Zhang,Yuwei Li,Xingyuan Xian,Guoming Tang*

Main category: cs.CV

TL;DR: 提出了一种零样本的视觉令牌剪枝方法，通过平衡任务相关性和信息多样性，在剪枝高达90%令牌的情况下保持性能，同时显著降低GPU内存占用和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型处理能力增强，视觉令牌冗余导致推理成本急剧上升，现有剪枝方法忽视文本提示的指导，无法优先考虑任务相关性。

Method: 采用分层方法：首先选择任务相关的核心视觉令牌集，然后补充多样性令牌以保留更广泛的上下文信息。

Result: 在多个模型和基准测试中，该方法在剪枝高达90%令牌的情况下，性能达到或超过最先进水平，仅产生最小精度损失，同时显著降低GPU内存占用和推理延迟。

Conclusion: 提出的提示感知视觉令牌剪枝方法有效平衡了任务相关性和信息多样性，在保持性能的同时大幅降低了视觉语言模型的推理成本。

Abstract: As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.

</details>


### [98] [From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display](https://arxiv.org/abs/2510.16833)
*Xiangyu Mu,Dongliang Zhou,Jie Hou,Haijun Zhang,Weili Guan*

Main category: cs.CV

TL;DR: 提出M2HVideo框架，将假人模特视频转换为身份可控、逼真的人类视频，解决头部与身体运动不对齐和身份漂移问题。


<details>
  <summary>Details</summary>
Motivation: 假人模特展示服装成本低但缺乏真实感和表现细节，需要将假人视频转换为逼真人类视频以提升在线时尚展示效果。

Method: 使用动态姿态感知头部编码器融合面部语义和身体姿态，引入镜像损失和分布感知适配器，基于DDIM一步去噪增强时间一致性。

Result: 在UBC时尚数据集、自建ASOS数据集和现场采集的MannequinVideos数据集上，M2HVideo在服装一致性、身份保持和视频保真度方面优于现有方法。

Conclusion: M2HVideo框架有效解决了假人到人类视频生成中的关键挑战，实现了高质量的身份可控视频生成。

Abstract: Mannequin-based clothing displays offer a cost-effective alternative to
real-model showcases for online fashion presentation, but lack realism and
expressive detail. To overcome this limitation, we introduce a new task called
mannequin-to-human (M2H) video generation, which aims to synthesize
identity-controllable, photorealistic human videos from footage of mannequins.
We propose M2HVideo, a pose-aware and identity-preserving video generation
framework that addresses two key challenges: the misalignment between head and
body motion, and identity drift caused by temporal modeling. In particular,
M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial
semantics with body pose to produce consistent identity embeddings across
frames. To address the loss of fine facial details due to latent space
compression, we introduce a mirror loss applied in pixel space through a
denoising diffusion implicit model (DDIM)-based one-step denoising.
Additionally, we design a distribution-aware adapter that aligns statistical
distributions of identity and clothing features to enhance temporal coherence.
Extensive experiments on the UBC fashion dataset, our self-constructed ASOS
dataset, and the newly collected MannequinVideos dataset captured on-site
demonstrate that M2HVideo achieves superior performance in terms of clothing
consistency, identity preservation, and video fidelity in comparison to
state-of-the-art methods.

</details>


### [99] [From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh](https://arxiv.org/abs/2510.17198)
*M Saifuzzaman Rafat,Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Jungpil Shin*

Main category: cs.CV

TL;DR: 使用Segment Anything Model (SAM)来精确监测孟加拉国河流侵蚀，通过微调SAM的掩码解码器识别河岸侵蚀特征，在消失的定居点数据集上取得了86.30%的IoU和92.60%的Dice分数。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国河流每年吞噬村庄和农田，造成大规模破坏和人口流离失所，传统人工监测方法极其困难。

Method: 首先使用简单的颜色通道分析进行粗略的土地和水域分割，然后微调SAM的掩码解码器来识别河岸侵蚀的细微特征。

Result: 模型在河岸侵蚀识别方面表现优异，平均IoU达到86.30%，Dice分数达到92.60%，显著优于传统方法和现成的深度学习模型。

Conclusion: 该研究提供了首个孟加拉国因河流侵蚀消失定居点的标注数据集、专门针对此任务的微调AI模型，以及量化土地损失的方法，为政策制定者和灾害管理机构提供了强大的监测工具。

Abstract: The great rivers of Bangladesh, arteries of commerce and sustenance, are also
agents of relentless destruction. Each year, they swallow whole villages and
vast tracts of farmland, erasing communities from the map and displacing
thousands of families. To track this slow-motion catastrophe has, until now,
been a Herculean task for human analysts. Here we show how a powerful
general-purpose vision model, the Segment Anything Model (SAM), can be adapted
to this task with remarkable precision. To do this, we assembled a new dataset
- a digital chronicle of loss compiled from historical Google Earth imagery of
Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur
Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,
this dataset is the first to include manually annotated data on the settlements
that have vanished beneath the water. Our method first uses a simple
color-channel analysis to provide a rough segmentation of land and water, and
then fine-tunes SAM's mask decoder to recognize the subtle signatures of
riverbank erosion. The resulting model demonstrates a keen eye for this
destructive process, achieving a mean Intersection over Union of 86.30% and a
Dice score of 92.60% - a performance that significantly surpasses traditional
methods and off-the-shelf deep learning models. This work delivers three key
contributions: the first annotated dataset of disappeared settlements in
Bangladesh due to river erosion; a specialized AI model fine-tuned for this
critical task; and a method for quantifying land loss with compelling visual
evidence. Together, these tools provide a powerful new lens through which
policymakers and disaster management agencies can monitor erosion, anticipate
its trajectory, and ultimately protect the vulnerable communities in its path.

</details>


### [100] [2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting](https://arxiv.org/abs/2510.16837)
*Haofan Ren,Qingsong Yan,Ming Lu,Rongfeng Lu,Zunjie Zhu*

Main category: cs.CV

TL;DR: 2DGS-R是一种改进的2D高斯泼溅方法，通过分层训练策略在保持几何精度的同时提升渲染质量，仅增加1%存储和少量训练时间开销。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅难以准确表示表面，而2D高斯泼溅虽然几何保真度提升但渲染质量受损，目前无法在单阶段训练中同时优化几何和渲染质量。

Method: 采用分层训练：首先使用法向一致性正则化训练原始2D高斯；然后选择渲染质量不足的2D高斯进行原位克隆操作；最后冻结不透明度进行微调。

Result: 相比原始2DGS，仅增加1%存储和最小额外训练时间，实现了高质量渲染结果同时保持精细几何结构。

Conclusion: 该方法有效平衡了效率与性能，在视觉保真度和几何重建精度方面均有提升。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced
neural fields, as it enables high-fidelity rendering with impressive visual
quality. However, 3DGS has difficulty accurately representing surfaces. In
contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian
disks. Despite advancements in geometric fidelity, rendering quality remains
compromised, highlighting the challenge of achieving both high-quality
rendering and precise geometric structures. This indicates that optimizing both
geometric and rendering quality in a single training stage is currently
unfeasible. To overcome this limitation, we present 2DGS-R, a new method that
uses a hierarchical training approach to improve rendering quality while
maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians
with the normal consistency regularization. Then 2DGS-R selects the 2D
Gaussians with inadequate rendering quality and applies a novel in-place
cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R
model with opacity frozen. Experimental results show that compared to the
original 2DGS, our method requires only 1\% more storage and minimal additional
training time. Despite this negligible overhead, it achieves high-quality
rendering results while preserving fine geometric structures. These findings
indicate that our approach effectively balances efficiency with performance,
leading to improvements in both visual fidelity and geometric reconstruction
accuracy.

</details>


### [101] [Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis](https://arxiv.org/abs/2510.17199)
*Nirai Hayakawa,Kazumasa Shimari,Kazuma Yamasaki,Hirotatsu Hoshikawa,Rikuto Tsuchida,Kenichi Matsumoto*

Main category: cs.CV

TL;DR: 基于TimeSformer视频识别模型，通过分析VALORANT比赛录像中的小地图信息，提取战术特征来预测回合结果，在数据增强后达到约81%的预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有电竞比赛结果预测研究多基于比赛日志数据和统计信息，本研究针对需要复杂策略的FPS游戏VALORANT，旨在通过分析比赛录像中的小地图信息来构建回合结果预测模型。

Method: 基于TimeSformer视频识别模型，通过分析比赛录像中的小地图信息，提取角色位置信息等详细战术特征，并利用数据增强技术提升预测准确性。

Result: 在增强战术事件标签的数据集上训练的模型达到约81%的预测准确率，特别是在回合中后期阶段，显著优于仅使用小地图信息本身训练的模型。

Conclusion: 利用比赛录像中的战术特征对于预测VALORANT回合结果非常有效，证明了从视觉数据中提取战术信息的方法具有重要价值。

Abstract: Recently, research on predicting match outcomes in esports has been actively
conducted, but much of it is based on match log data and statistical
information. This research targets the FPS game VALORANT, which requires
complex strategies, and aims to build a round outcome prediction model by
analyzing minimap information in match footage. Specifically, based on the
video recognition model TimeSformer, we attempt to improve prediction accuracy
by incorporating detailed tactical features extracted from minimap information,
such as character position information and other in-game events. This paper
reports preliminary results showing that a model trained on a dataset augmented
with such tactical event labels achieved approximately 81% prediction accuracy,
especially from the middle phases of a round onward, significantly
outperforming a model trained on a dataset with the minimap information itself.
This suggests that leveraging tactical features from match footage is highly
effective for predicting round outcomes in VALORANT.

</details>


### [102] [When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions](https://arxiv.org/abs/2510.17218)
*Zhuo Cao,Heming Du,Bingqing Zhang,Xin Yu,Xue Li,Sen Wang*

Main category: cs.CV

TL;DR: 该论文提出了多时刻检索（MMR）任务，构建了QV-M^2数据集和FlashMMR框架，解决了现有单时刻检索方法在真实场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有时刻检索方法主要关注单时刻检索，但真实应用中一个查询可能对应多个相关时刻，这使得现有数据集和方法不足以支持视频时序定位。

Method: 提出了FlashMMR框架，包含多时刻后验证模块来精炼时刻边界，采用约束时序调整和验证模块重新评估候选片段，通过精细过滤管道修剪低置信度提议。

Result: 在QV-M^2数据集上，FlashMMR相比之前SOTA方法在G-mAP上提升3.00%，在mAP@3+tgt上提升2.70%，在mR@3上提升2.56%。

Conclusion: QV-M^2数据集和FlashMMR方法为推进更现实和具有挑战性的视频时序定位研究奠定了基础。

Abstract: Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval
(SMR). However, one query can correspond to multiple relevant moments in
real-world applications. This makes the existing datasets and methods
insufficient for video temporal grounding. By revisiting the gap between
current MR tasks and real-world applications, we introduce a high-quality
datasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new
evaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists
of 2,212 annotations covering 6,384 video segments. Building on existing
efforts in MMR, we propose a framework called FlashMMR. Specifically, we
propose a Multi-moment Post-verification module to refine the moment
boundaries. We introduce constrained temporal adjustment and subsequently
leverage a verification module to re-evaluate the candidate segments. Through
this sophisticated filtering pipeline, low-confidence proposals are pruned, and
robust multi-moment alignment is achieved. We retrain and evaluate 6 existing
MR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.
Results show that QV-M$^2$ serves as an effective benchmark for training and
evaluating MMR models, while FlashMMR provides a strong baseline. Specifically,
on QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,
2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method
establish a foundation for advancing research in more realistic and challenging
video temporal grounding scenarios. Code is released at
https://github.com/Zhuo-Cao/QV-M2.

</details>


### [103] [BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation](https://arxiv.org/abs/2510.16863)
*Shujian Gao,Yuan Wang,Zekuan Yu*

Main category: cs.CV

TL;DR: BARL是一个半监督医学图像分割框架，通过在表示空间和标签空间的双边对齐，结合双路径正则化和渐进认知偏差校正，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督医学图像分割方法主要依赖标签空间一致性，但忽视了表示空间对齐的重要性，导致模型难以学习到既具有判别性又空间一致的表示。

Method: 提出BARL框架，包含两个协作分支：1) 标签空间对齐：使用双路径正则化和渐进认知偏差校正实现细粒度跨分支一致性；2) 表示空间对齐：在分支间进行区域级和病变实例级匹配，捕捉医学图像中常见的碎片化复杂病理模式。

Result: 在四个公共基准数据集和一个私有CBCT数据集上的实验表明，BARL持续超越最先进的半监督医学图像分割方法。消融研究验证了各组件的贡献。

Conclusion: BARL通过双边对齐策略有效提升了半监督医学图像分割性能，证明了表示空间对齐与标签空间一致性同等重要。

Abstract: Semi-supervised medical image segmentation (SSMIS) seeks to match fully
supervised performance while sharply reducing annotation cost. Mainstream SSMIS
methods rely on \emph{label-space consistency}, yet they overlook the equally
critical \emph{representation-space alignment}. Without harmonizing latent
features, models struggle to learn representations that are both discriminative
and spatially coherent. To this end, we introduce \textbf{Bilateral Alignment
in Representation and Label spaces (BARL)}, a unified framework that couples
two collaborative branches and enforces alignment in both spaces. For
label-space alignment, inspired by co-training and multi-scale decoding, we
devise \textbf{Dual-Path Regularization (DPR)} and \textbf{Progressively
Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch
consistency while mitigating error accumulation from coarse to fine scales. For
representation-space alignment, we conduct region-level and lesion-instance
matching between branches, explicitly capturing the fragmented, complex
pathological patterns common in medical imagery. Extensive experiments on four
public benchmarks and a proprietary CBCT dataset demonstrate that BARL
consistently surpasses state-of-the-art SSMIS methods. Ablative studies further
validate the contribution of each component. Code will be released soon.

</details>


### [104] [FineVision: Open Data Is All You Need](https://arxiv.org/abs/2510.17269)
*Luis Wiedmann,Orr Zohar,Amir Mahla,Xiaohan Wang,Rui Li,Thibaud Frere,Leandro von Werra,Aritra Roy Gosthipaty,Andrés Marafioti*

Main category: cs.CV

TL;DR: FineVision是一个精心收集、整理和统一的数据集，包含2400万个样本，是同类中最大的开放资源。它通过半自动化流程整合了200多个来源，并进行严格去重和去污染处理，训练出的模型在各种评估中都优于现有开放数据集。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的发展受到零散、不一致和受污染公共数据集的阻碍，需要构建一个高质量、统一的大规模数据集来推动研究。

Method: 采用半自动化、人工参与的工作流程：自动化进行批量摄取和模式映射，人工审核员检查映射和抽样验证，确保注释的忠实消费、适当格式化和多样性，并进行严格的去重和去污染处理。

Result: 在FineVision上训练的模型在广泛的评估套件中持续优于现有开放混合数据集训练的模型，证明了规模、数据卫生以及平衡自动化与人工监督的好处。

Conclusion: FineVision数据集和整理工具的发布将加速以数据为中心的视觉语言模型研究，强调了高质量数据收集和整理的重要性。

Abstract: The advancement of vision-language models (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
FineVision, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation
performs bulk ingestion and schema mapping, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous de-duplication within and across
sources and decontamination against 66 public benchmarks. FineVision also
encompasses agentic/GUI tasks with a unified action space; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on FineVision consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.

</details>


### [105] [Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection](https://arxiv.org/abs/2510.16865)
*Yuyang Yu,Zhengwei Chen,Xuemiao Xu,Lei Zhang,Haoxin Yang,Yongwei Nie,Shengfeng He*

Main category: cs.CV

TL;DR: 提出了一种基于配准的旋转不变特征提取框架，将点云配准与基于内存库的异常检测相结合，通过联合优化配准和表示学习来解决现有方法在特征变换不一致和判别能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于内存库的3D异常检测方法存在特征变换不一致和判别能力有限的问题，特别是在捕捉局部几何细节和实现旋转不变性方面。当配准失败时，这些限制会导致不可靠的检测结果。

Method: 提出配准诱导的旋转不变特征提取框架，将点云配准目标与基于内存的异常检测集成。通过将特征提取嵌入到配准学习过程中，联合优化对齐和表示学习。

Result: 在Anomaly-ShapeNet和Real3D-AD数据集上的大量实验表明，该方法在有效性和泛化性方面持续优于现有方法。

Conclusion: 点云配准不仅在几何结构对齐中起关键作用，还能引导特征提取获得旋转不变和局部判别性表示，通过联合优化配准和异常检测任务可以显著提升3D异常检测性能。

Abstract: 3D anomaly detection in point-cloud data is critical for industrial quality
control, aiming to identify structural defects with high reliability. However,
current memory bank-based methods often suffer from inconsistent feature
transformations and limited discriminative capacity, particularly in capturing
local geometric details and achieving rotation invariance. These limitations
become more pronounced when registration fails, leading to unreliable detection
results. We argue that point-cloud registration plays an essential role not
only in aligning geometric structures but also in guiding feature extraction
toward rotation-invariant and locally discriminative representations. To this
end, we propose a registration-induced, rotation-invariant feature extraction
framework that integrates the objectives of point-cloud registration and
memory-based anomaly detection. Our key insight is that both tasks rely on
modeling local geometric structures and leveraging feature similarity across
samples. By embedding feature extraction into the registration learning
process, our framework jointly optimizes alignment and representation learning.
This integration enables the network to acquire features that are both robust
to rotations and highly effective for anomaly detection. Extensive experiments
on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method
consistently outperforms existing approaches in effectiveness and
generalizability.

</details>


### [106] [CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration](https://arxiv.org/abs/2510.17330)
*Gyuhwan Park,Kihyun Na,Injung Kim*

Main category: cs.CV

TL;DR: 提出CharDiff框架，通过字符级引导的扩散模型恢复和识别严重退化的车牌图像，在Roboflow-LP数据集上相比最佳基线模型实现了28%的CER相对降低。


<details>
  <summary>Details</summary>
Motivation: 车牌图像恢复不仅对LPR系统预处理很重要，还能提高证据价值、增强视觉界面清晰度和促进车牌图像的进一步利用。

Method: 提出CharDiff框架，利用外部分割和OCR模块提取细粒度字符级先验，并引入CHARM模块确保每个字符的引导仅限制在其自身区域。

Result: 在实验中显著优于基线恢复模型，在恢复质量和识别准确率方面都有提升，在Roboflow-LP数据集上实现了28%的CER相对降低。

Conclusion: 结构化字符引导条件化有效增强了基于扩散的车牌恢复和识别在实际部署场景中的鲁棒性。

Abstract: The significance of license plate image restoration goes beyond the
preprocessing stage of License Plate Recognition (LPR) systems, as it also
serves various purposes, including increasing evidential value, enhancing the
clarity of visual interface, and facilitating further utilization of license
plate images. We propose a novel diffusion-based framework with character-level
guidance, CharDiff, which effectively restores and recognizes severely degraded
license plate images captured under realistic conditions. CharDiff leverages
fine-grained character-level priors extracted through external segmentation and
Optical Character Recognition (OCR) modules tailored for low-quality license
plate images. For precise and focused guidance, CharDiff incorporates a novel
Character-guided Attention through Region-wise Masking (CHARM) module, which
ensures that each character's guidance is restricted to its own region, thereby
avoiding interference with other regions. In experiments, CharDiff
significantly outperformed the baseline restoration models in both restoration
quality and recognition accuracy, achieving a 28% relative reduction in CER on
the Roboflow-LP dataset, compared to the best-performing baseline model. These
results indicate that the structured character-guided conditioning effectively
enhances the robustness of diffusion-based license plate restoration and
recognition in practical deployment scenarios.

</details>


### [107] [Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding](https://arxiv.org/abs/2510.16870)
*Yudan Ren,Xinlong Wang,Kexin Wang,Tian Xia,Zihan Ma,Zhaowei Li,Xiangrong Bi,Xiao Li,Xiaowei He*

Main category: cs.CV

TL;DR: 提出了一个神经元级别的分析框架，通过人类大脑活动来研究视觉语言模型中的多模态信息处理机制，揭示了ANN与生物神经元在功能网络、冗余性、极性模式和架构影响等方面的相似性。


<details>
  <summary>Details</summary>
Motivation: 当前对人工神经网络与人类大脑处理之间相似性的理解有限：单模态ANN研究无法捕捉大脑固有的多模态处理能力，而多模态ANN研究主要关注高层模型输出，忽略了单个神经元的关键作用。

Method: 提出了一个新颖的神经元级别分析框架，结合精细的人工神经元分析和基于fMRI的体素编码，研究了CLIP和METER两种架构不同的视觉语言模型。

Result: 发现：(1) ANs能成功预测多个功能网络中BNs的活动；(2) ANs和BNs都表现出功能冗余；(3) ANs表现出与BNs平行的极性模式；(4) CLIP和METER的架构驱动不同的BNs。

Conclusion: 这些结果为视觉语言模型中存在类脑层次处理提供了有力证据，表明在神经元级别上存在共享的表征机制和相似的信息处理特性。

Abstract: While brain-inspired artificial intelligence(AI) has demonstrated promising
results, current understanding of the parallels between artificial neural
networks (ANNs) and human brain processing remains limited: (1) unimodal ANN
studies fail to capture the brain's inherent multimodal processing
capabilities, and (2) multimodal ANN research primarily focuses on high-level
model outputs, neglecting the crucial role of individual neurons. To address
these limitations, we propose a novel neuron-level analysis framework that
investigates the multimodal information processing mechanisms in
vision-language models (VLMs) through the lens of human brain activity. Our
approach uniquely combines fine-grained artificial neuron (AN) analysis with
fMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP
and METER. Our analysis reveals four key findings: (1) ANs successfully predict
biological neurons (BNs) activities across multiple functional networks
(including language, vision, attention, and default mode), demonstrating shared
representational mechanisms; (2) Both ANs and BNs demonstrate functional
redundancy through overlapping neural representations, mirroring the brain's
fault-tolerant and collaborative information processing mechanisms; (3) ANs
exhibit polarity patterns that parallel the BNs, with oppositely activated BNs
showing mirrored activation trends across VLM layers, reflecting the complexity
and bidirectional nature of neural information processing; (4) The
architectures of CLIP and METER drive distinct BNs: CLIP's independent branches
show modality-specific specialization, whereas METER's cross-modal design
yields unified cross-modal activation, highlighting the architecture's
influence on ANN brain-like properties. These results provide compelling
evidence for brain-like hierarchical processing in VLMs at the neuronal level.

</details>


### [108] [SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries](https://arxiv.org/abs/2510.17482)
*Chenxu Dang,Haiyan Liu,Guangjun Bao,Pei An,Xinyue Tang,Jie Ma,Bingchuan Sun,Yan Wang*

Main category: cs.CV

TL;DR: SparseWorld是一个新颖的4D占用世界模型，通过稀疏动态查询实现灵活、自适应和高效的场景理解，在感知、预测和规划任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有占用世界模型依赖静态固定嵌入或网格，限制了感知的灵活性，且其网格上的"原地分类"与真实场景的动态连续性存在潜在不匹配。

Method: 提出Range-Adaptive Perception模块（通过车辆状态调制可学习查询并增强时空关联），State-Conditioned Forecasting模块（用回归引导公式替代基于分类的预测），以及Temporal-Aware Self-Scheduling训练策略。

Result: 在感知、预测和规划任务中达到最先进性能，验证了在灵活性、适应性和效率方面的优势。

Conclusion: SparseWorld通过稀疏动态查询成功解决了现有占用世界模型的局限性，为4D环境理解提供了更灵活、自适应和高效的解决方案。

Abstract: Semantic occupancy has emerged as a powerful representation in world models
for its ability to capture rich spatial semantics. However, most existing
occupancy world models rely on static and fixed embeddings or grids, which
inherently limit the flexibility of perception. Moreover, their ``in-place
classification" over grids exhibits a potential misalignment with the dynamic
and continuous nature of real scenarios.In this paper, we propose SparseWorld,
a novel 4D occupancy world model that is flexible, adaptive, and efficient,
powered by sparse and dynamic queries. We propose a Range-Adaptive Perception
module, in which learnable queries are modulated by the ego vehicle states and
enriched with temporal-spatial associations to enable extended-range
perception. To effectively capture the dynamics of the scene, we design a
State-Conditioned Forecasting module, which replaces classification-based
forecasting with regression-guided formulation, precisely aligning the dynamic
queries with the continuity of the 4D environment. In addition, We specifically
devise a Temporal-Aware Self-Scheduling training strategy to enable smooth and
efficient training. Extensive experiments demonstrate that SparseWorld achieves
state-of-the-art performance across perception, forecasting, and planning
tasks. Comprehensive visualizations and ablation studies further validate the
advantages of SparseWorld in terms of flexibility, adaptability, and
efficiency. The code is available at https://github.com/MSunDYY/SparseWorld.

</details>


### [109] [Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis](https://arxiv.org/abs/2510.16887)
*Nusrat Munia,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出Class-N-Diff模型，将分类器集成到扩散模型中，实现皮肤病图像的同步生成和分类，提高生成图像的质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统类别条件生成模型在生成特定医学类别图像时存在困难，限制了其在皮肤癌诊断等应用中的实用性。

Method: 在扩散模型中集成分类器，基于类别条件引导图像生成，实现更好的类别控制。

Result: 模型能够生成更真实和多样化的皮肤病图像，同时分类器在下游诊断任务中表现出改进的性能。

Conclusion: Class-N-Diff通过独特的集成方法，成为增强基于扩散模型的合成皮肤病图像生成质量和实用性的强大工具。

Abstract: Generative models, especially Diffusion Models, have demonstrated remarkable
capability in generating high-quality synthetic data, including medical images.
However, traditional class-conditioned generative models often struggle to
generate images that accurately represent specific medical categories, limiting
their usefulness for applications such as skin cancer diagnosis. To address
this problem, we propose a classification-induced diffusion model, namely,
Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our
Class-N-Diff model integrates a classifier within a diffusion model to guide
image generation based on its class conditions. Thus, the model has better
control over class-conditioned image synthesis, resulting in more realistic and
diverse images. Additionally, the classifier demonstrates improved performance,
highlighting its effectiveness for downstream diagnostic tasks. This unique
integration in our Class-N-Diff makes it a robust tool for enhancing the
quality and utility of diffusion model-based synthetic dermoscopic image
generation. Our code is available at https://github.com/Munia03/Class-N-Diff.

</details>


### [110] [Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization](https://arxiv.org/abs/2510.17501)
*Yuanli Wu,Long Zhang,Yue Du,Bin Li*

Main category: cs.CV

TL;DR: 提出了一种基于评分标准引导的伪标签提示框架，通过将少量真实标注转化为高置信度伪标签，构建结构化、数据集自适应的评分标准来指导可解释的场景评估，实现无需训练的零样本视频摘要。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法标注成本高且跨数据集泛化能力有限，无监督方法难以捕捉高级语义和细粒度叙事线索，零样本提示方法对人工提示模板和数据集特定分数归一化高度敏感。

Method: 使用评分标准引导的伪标签提示框架，将少量真实标注转化为伪标签并聚合成结构化评分标准。推理时基于描述对首尾片段评分，中间片段则结合相邻场景的上下文摘要评估叙事进展和冗余度。

Result: 在SumMe和TVSum数据集上分别达到57.58和63.05的F1分数，超越了无监督和先前的零样本基线方法，接近监督方法的性能。

Conclusion: 评分标准引导的伪标签方法有效稳定了基于LLM的评分，为视频摘要建立了一个通用、可解释的零样本范式。

Abstract: With the rapid proliferation of video content across social media,
surveillance, and education platforms, efficiently summarizing long videos into
concise yet semantically faithful surrogates has become increasingly vital.
Existing supervised methods achieve strong in-domain accuracy by learning from
dense annotations but suffer from high labeling costs and limited cross-dataset
generalization, while unsupervised approaches, though label-free, often fail to
capture high-level human semantics and fine-grained narrative cues. More
recently, zero-shot prompting pipelines have leveraged large language models
(LLMs) for training-free video summarization, yet remain highly sensitive to
handcrafted prompt templates and dataset-specific score normalization. To
overcome these limitations, we introduce a rubric-guided, pseudo-labeled
prompting framework that transforms a small subset of ground-truth annotations
into high-confidence pseudo labels, which are aggregated into structured,
dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During
inference, first and last segments are scored based solely on their
descriptions, whereas intermediate ones incorporate brief contextual summaries
of adjacent scenes to assess narrative progression and redundancy. This
contextual prompting enables the LLM to balance local salience and global
coherence without parameter tuning. On SumMe and TVSum, our method achieves F1
scores of \textbf{57.58} and \textbf{63.05}, surpassing unsupervised and prior
zero-shot baselines while approaching supervised performance. The results
demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based
scoring and establishes a general, interpretable zero-shot paradigm for video
summarization.

</details>


### [111] [Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback](https://arxiv.org/abs/2510.16888)
*Zongjian Li,Zheyuan Liu,Qihui Zhang,Bin Lin,Shenghai Yuan,Zhiyuan Yan,Yang Ye,Wangbo Yu,Yuwei Niu,Li Yuan*

Main category: cs.CV

TL;DR: Edit-R1是一个基于策略优化的指令图像编辑后训练框架，通过DiffusionNFT方法和MLLM奖励模型解决监督微调过拟合问题，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 传统基于监督微调的指令图像编辑模型容易过拟合到标注模式，限制了其在训练分布之外的泛化能力。

Method: 采用Diffusion Negative-aware Finetuning (DiffusionNFT)策略优化方法，使用多模态大语言模型作为训练无关的奖励模型，并设计了低方差组过滤机制来减少评分噪声。

Result: UniWorld-V2在ImgEdit和GEdit-Bench基准测试中分别获得4.49和7.83分，达到最先进水平，且该框架可应用于多种基础模型。

Conclusion: Edit-R1是一个模型无关的后训练框架，能显著提升指令图像编辑模型的性能，具有广泛适用性。

Abstract: Instruction-based image editing has achieved remarkable progress; however,
models solely trained via supervised fine-tuning often overfit to annotated
patterns, hindering their ability to explore and generalize beyond training
distributions. To this end, we introduce Edit-R1, a novel post-training
framework for instruction-based image editing based on policy optimization.
Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a
likelihood-free policy optimization method consistent with the flow matching
forward process, thereby enabling the use of higher-order samplers and more
efficient training. Another key challenge here is the absence of a universal
reward model, resulting from the diverse nature of editing instructions and
tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)
as a unified, training-free reward model, leveraging its output logits to
provide fine-grained feedback. Furthermore, we carefully design a low-variance
group filtering mechanism to reduce MLLM scoring noise and stabilize
optimization. UniWorld-V2, trained with this framework, achieves
\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,
scoring 4.49 and 7.83, respectively. Crucially, our framework is
model-agnostic, delivering substantial performance gains when applied to
diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its
wide applicability. Code and models are publicly available at
https://github.com/PKU-YuanGroup/UniWorld-V2.

</details>


### [112] [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)
*Yongshun Zhang,Zhongyi Fan,Yonghang Zhang,Zhangzikang Li,Weifeng Chen,Zhongwei Feng,Chaoyue Wang,Peng Hou,Anxiang Zeng*

Main category: cs.CV

TL;DR: 提出了一个优化四个支柱的训练框架，用于大规模视频生成模型，显著提高了效率和性能，并开源了完整的训练代码和模型


<details>
  <summary>Details</summary>
Motivation: 解决大规模视频生成模型训练面临的挑战，包括跨模态文本-视频对齐、长序列和复杂时空依赖等问题

Method: 优化数据预处理、模型架构、训练策略和基础设施四个支柱，包括视频压缩、参数缩放、课程式预训练和对齐后训练

Result: 开发的MUG-V 10B模型在整体上匹配最先进的视频生成器，在电商视频生成任务上超越开源基线，并实现了高效的多节点扩展

Conclusion: 成功构建了高效的大规模视频生成训练框架，并开源完整技术栈，为社区提供了重要的资源

Abstract: In recent years, large-scale generative models for visual content
(\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable
progress. However, training large-scale video generation models remains
particularly challenging and resource-intensive due to cross-modal text-video
alignment, the long sequences involved, and the complex spatiotemporal
dependencies. To address these challenges, we present a training framework that
optimizes four pillars: (i) data processing, (ii) model architecture, (iii)
training strategy, and (iv) infrastructure for large-scale video generation
models. These optimizations delivered significant efficiency gains and
performance improvements across all stages of data preprocessing, video
compression, parameter scaling, curriculum-based pretraining, and
alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent
state-of-the-art video generators overall and, on e-commerce-oriented video
generation tasks, surpasses leading open-source baselines in human evaluations.
More importantly, we open-source the complete stack, including model weights,
Megatron-Core-based large-scale training code, and inference pipelines for
video generation and enhancement. To our knowledge, this is the first public
release of large-scale video generation training code that exploits
Megatron-Core to achieve high training efficiency and near-linear multi-node
scaling, details are available in
\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.

</details>


### [113] [Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data](https://arxiv.org/abs/2510.16891)
*Ramon Dalmau,Gabriel Jarry,Philippe Very*

Main category: cs.CV

TL;DR: 本文提出了一种使用地面相机进行凝结尾迹到航班归因的模块化框架，通过高时空分辨率捕捉凝结尾迹，解决了卫星归因的局限性。


<details>
  <summary>Details</summary>
Motivation: 航空业非CO2效应中凝结尾迹对气候影响显著，但现有卫星归因方法因时空分辨率限制难以准确追踪漂移变形的凝结尾迹。

Method: 利用地面可见相机凝结尾迹序列数据集，开发模块化框架，结合几何表示、距离度量、时间平滑和概率分配策略，将观测凝结尾迹与理论凝结尾迹进行匹配。

Result: 建立了凝结尾迹到航班归因的强基线，提供了可扩展的模块化框架。

Conclusion: 地面相机方法为凝结尾迹归因研究提供了有效替代方案，为未来研究奠定了坚实基础。

Abstract: Aviation's non-CO2 effects, particularly contrails, are a significant
contributor to its climate impact. Persistent contrails can evolve into
cirrus-like clouds that trap outgoing infrared radiation, with radiative
forcing potentially comparable to or exceeding that of aviation's CO2
emissions. While physical models simulate contrail formation, evolution and
dissipation, validating and calibrating these models requires linking observed
contrails to the flights that generated them, a process known as
contrail-to-flight attribution. Satellite-based attribution is challenging due
to limited spatial and temporal resolution, as contrails often drift and deform
before detection. In this paper, we evaluate an alternative approach using
ground-based cameras, which capture contrails shortly after formation at high
spatial and temporal resolution, when they remain thin, linear, and visually
distinct. Leveraging the ground visible camera contrail sequences (GVCCS)
dataset, we introduce a modular framework for attributing contrails observed
using ground-based cameras to theoretical contrails derived from aircraft
surveillance and meteorological data. The framework accommodates multiple
geometric representations and distance metrics, incorporates temporal
smoothing, and enables flexible probability-based assignment strategies. This
work establishes a strong baseline and provides a modular framework for future
research in linking contrails to their source flight.

</details>


### [114] [MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation](https://arxiv.org/abs/2510.17529)
*Yovin Yahathugoda,Davide Prezzi,Piyalitt Ittichaiwong,Vicky Goh,Sebastien Ourselin,Michela Antonelli*

Main category: cs.CV

TL;DR: 提出了MambaX-Net，一种用于前列腺癌主动监测的半监督双扫描3D分割架构，通过利用先前时间点的MRI和分割掩码来改进当前时间点的分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习分割模型在纵向主动监测分析中的局限性，这些模型通常基于单时间点和专家标注数据训练，无法有效处理多时间点和专家标注稀缺的情况。

Method: 采用Mamba增强的交叉注意力模块来捕捉时间演化和长距离空间依赖，结合形状提取器模块编码先前分割掩码为潜在解剖表示，并引入半监督自训练策略利用伪标签进行学习。

Result: 在纵向主动监测数据集上的评估显示，MambaX-Net显著优于最先进的U-Net和Transformer模型，即使在有限和噪声数据下也能实现优异的前列腺区域分割。

Conclusion: MambaX-Net为前列腺癌主动监测提供了一种有效的半监督分割解决方案，能够利用时间序列信息改进分割精度，减少对专家标注的依赖。

Abstract: Active Surveillance (AS) is a treatment option for managing low and
intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while
monitoring disease progression through serial MRI and clinical follow-up.
Accurate prostate segmentation is an important preliminary step for automating
this process, enabling automated detection and diagnosis of PCa. However,
existing deep-learning segmentation models are often trained on
single-time-point and expertly annotated datasets, making them unsuitable for
longitudinal AS analysis, where multiple time points and a scarcity of expert
labels hinder their effective fine-tuning. To address these challenges, we
propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation
architecture that computes the segmentation for time point t by leveraging the
MRI and the corresponding segmentation mask from the previous time point. We
introduce two new components: (i) a Mamba-enhanced Cross-Attention Module,
which integrates the Mamba block into cross attention to efficiently capture
temporal evolution and long-range spatial dependencies, and (ii) a Shape
Extractor Module that encodes the previous segmentation mask into a latent
anatomical representation for refined zone delination. Moreover, we introduce a
semi-supervised self-training strategy that leverages pseudo-labels generated
from a pre-trained nnU-Net, enabling effective learning without expert
annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results
showed that it significantly outperforms state-of-the-art U-Net and
Transformer-based models, achieving superior prostate zone segmentation even
when trained on limited and noisy data.

</details>


### [115] [Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation](https://arxiv.org/abs/2510.16913)
*Akhila Kambhatla,Ahmed R Khaled*

Main category: cs.CV

TL;DR: 本文评估了四种基于Transformer的架构在热成像武器分割任务上的表现，发现SegFormer-b5在mIoU和像素精度上表现最佳，而SegFormer-b0在推理速度上最优。


<details>
  <summary>Details</summary>
Motivation: 热成像武器分割在低光照和视觉遮挡条件下对监控和安全应用至关重要。虽然CNN在热成像分割文献中占主导地位，但其捕捉长距离依赖和精细结构细节的能力有限。ViT在RGB分割任务中表现出色，但在热成像武器分割中的潜力尚未充分探索。

Method: 使用自定义热成像数据集（9,711张图像，来自真实监控视频，使用SAM2自动标注），在MMSegmentation框架中采用标准增强策略，评估四种基于Transformer的架构：SegFormer、DeepLabV3+、SegNeXt和Swin Transformer。

Result: SegFormer-b5达到最高mIoU（94.15%）和像素精度（97.04%），SegFormer-b0提供最快的推理速度（98.32 FPS）和竞争性mIoU（90.84%）。SegNeXt-mscans提供平衡性能（85.12 FPS和92.24% mIoU），DeepLabV3+ R101-D8达到92.76% mIoU（29.86 FPS）。

Conclusion: Transformer架构在低光照和遮挡热成像环境中表现出强大的泛化能力，具有灵活的精度-速度权衡，适用于各种实时安全应用。

Abstract: Thermal weapon segmentation is crucial for surveillance and security
applications, enabling robust detection under lowlight and visually obscured
conditions where RGB-based systems fail. While convolutional neural networks
(CNNs) dominate thermal segmentation literature, their ability to capture
long-range dependencies and fine structural details is limited. Vision
Transformers (ViTs), with their global context modeling capabilities, have
achieved state-of-the-art results in RGB segmentation tasks, yet their
potential in thermal weapon segmentation remains underexplored. This work
adapts and evaluates four transformer-based architectures SegFormer,
DeepLabV3\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a
custom thermal dataset comprising 9,711 images collected from real world
surveillance videos and automatically annotated using SAM2. We employ standard
augmentation strategies within the MMSegmentation framework to ensure robust
model training and fair architectural comparison. Experimental results
demonstrate significant improvements in segmentation performance: SegFormer-b5
achieves the highest mIoU (94.15\%) and Pixel Accuracy (97.04\%), while
SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive
mIoU (90.84\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and
92.24\% mIoU, and DeepLabV3\+ R101-D8 reaches 92.76\% mIoU at 29.86 FPS. The
transformer architectures demonstrate robust generalization capabilities for
weapon detection in low-light and occluded thermal environments, with flexible
accuracy-speed trade-offs suitable for diverse real-time security applications.

</details>


### [116] [CaMiT: A Time-Aware Car Model Dataset for Classification and Generation](https://arxiv.org/abs/2510.17626)
*Frédéric LIN,Biruk Abere Ambaw,Adrian Popescu,Hejer Ammar,Romaric Audigier,Hervé Le Borgne*

Main category: cs.CV

TL;DR: CaMiT数据集捕捉汽车模型的时间演变，支持监督和自监督学习。静态预训练在域内数据上表现良好但跨年测试精度下降。提出时间增量分类设置和两种策略来提升时间鲁棒性，并探索时间感知图像生成。


<details>
  <summary>Details</summary>
Motivation: AI系统需要适应不断变化的视觉环境，特别是在物体外观随时间变化的领域。汽车模型作为技术制品的代表性类别，其时间演变需要被研究。

Method: 引入CaMiT数据集，包含787K标注样本和5.1M未标注样本。提出时间增量分类设置，评估时间增量预训练和时间增量分类器学习两种策略。探索时间感知图像生成方法。

Result: 静态预训练在域内数据上达到竞争性性能且更资源高效，但跨年测试精度下降。时间增量策略改善了时间鲁棒性。时间感知图像生成产生更真实的输出。

Conclusion: CaMiT为研究细粒度视觉识别和生成中的时间适应提供了丰富的基准，展示了时间增量学习策略的有效性。

Abstract: AI systems must adapt to evolving visual environments, especially in domains
where object appearances change over time. We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts. CaMiT includes 787K labeled
samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),
supporting both supervised and self-supervised learning. Static pretraining on
in-domain data achieves competitive performance with large-scale generalist
models while being more resource-efficient, yet accuracy declines when models
are tested across years. To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes. We evaluate two strategies:
time-incremental pretraining, which updates the backbone, and time-incremental
classifier learning, which updates only the final layer, both improving
temporal robustness. Finally, we explore time-aware image generation that
leverages temporal metadata during training, yielding more realistic outputs.
CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained
visual recognition and generation.

</details>


### [117] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li,Zhicai Wang,Yuan Sheng,Xingyu Zhu,Yanbin Hao,Xiang Wang*

Main category: cs.CV

TL;DR: 提出了Res-Bench基准测试，用于评估多模态大语言模型在不同输入分辨率下的性能稳定性，包含14,400个样本和12个分辨率级别，并引入了新的鲁棒性评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型支持动态图像分辨率，但现有评估主要关注语义性能，忽略了分辨率鲁棒性——即性能在不同输入分辨率下是否保持稳定。

Method: 设计了Res-Bench基准测试，包含6个核心能力维度和12个分辨率级别；提出了新的评估框架，包括Spearman相关性、绝对/相对连续误差等鲁棒性指标；对领先的MLLMs进行了大规模评估。

Result: 通过新框架评估了主要MLLMs，进行了模型中心化、任务中心化的鲁棒性分析，研究了填充和超分辨率等预处理策略，探索了微调对稳定性的提升效果。

Conclusion: Res-Bench填补了MLLMs分辨率鲁棒性评估的空白，为模型开发和优化提供了重要的评估工具和洞见。

Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.

</details>


### [118] [Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs](https://arxiv.org/abs/2510.17651)
*Sébastien Thuau,Siba Haidar,Ayush Bajracharya,Rachid Chelouah*

Main category: cs.CV

TL;DR: 比较了两种节俭联邦学习方法在暴力检测中的应用：零样本和联邦微调的视觉语言模型(VLMs)与个性化训练的紧凑3D卷积神经网络(CNN3D)，重点关注能效和环境指标。


<details>
  <summary>Details</summary>
Motivation: 研究如何在非独立同分布的现实环境中，通过节俭的联邦学习方法实现高效的暴力检测，同时考虑能源消耗和环境影响。

Method: 使用LLaVA-7B和65.8M参数的CNN3D作为代表案例，评估准确率、校准和能源使用。对比零样本/联邦微调的VLMs与个性化CNN3D训练策略。

Result: 两种方法准确率均超过90%。CNN3D在ROC AUC和log loss上略优于LoRA微调的VLMs，且能耗更低。VLMs在上下文推理和多模态推理方面仍具优势。

Conclusion: 提出混合模型：轻量级CNN用于常规分类，选择性激活VLM处理复杂场景。为视频监控中负责任、资源感知的AI提供了可复现的基准框架。

Abstract: We examine frugal federated learning approaches to violence detection by
comparing two complementary strategies: (i) zero-shot and federated fine-tuning
of vision-language models (VLMs), and (ii) personalized training of a compact
3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter
CNN3D as representative cases, we evaluate accuracy, calibration, and energy
usage under realistic non-IID settings. Both approaches exceed 90% accuracy.
CNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and
log loss, while using less energy. VLMs remain favorable for contextual
reasoning and multimodal inference. We quantify energy and CO$_2$ emissions
across training and inference, and analyze sustainability trade-offs for
deployment. To our knowledge, this is the first comparative study of LoRA-tuned
vision-language models and personalized CNNs for federated violence detection,
with an emphasis on energy efficiency and environmental metrics. These findings
support a hybrid model: lightweight CNNs for routine classification, with
selective VLM activation for complex or descriptive scenarios. The resulting
framework offers a reproducible baseline for responsible, resource-aware AI in
video surveillance, with extensions toward real-time, multimodal, and
lifecycle-aware systems.

</details>


### [119] [PICABench: How Far Are We from Physically Realistic Image Editing?](https://arxiv.org/abs/2510.17681)
*Yuandong Pu,Le Zhuo,Songhao Han,Jinbo Xing,Kaiwen Zhu,Shuo Cao,Bin Fu,Si Liu,Hongsheng Li,Yu Qiao,Wenlong Zhang,Xi Chen,Yihao Liu*

Main category: cs.CV

TL;DR: 提出了PICABench基准测试和PICAEval评估协议，系统评估图像编辑的物理真实性，发现当前模型在物理一致性方面仍有很大改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型主要关注指令完成度，但忽略了编辑操作伴随的物理效应（如阴影、反射、物体间相互作用），这对生成真实性至关重要。

Method: 构建PICABench基准测试，涵盖8个物理子维度（光学、力学、状态转换等）和常见编辑操作；提出PICAEval评估协议，使用VLM作为评判器，结合区域级人工标注；构建PICA-100K训练数据集从视频中学习物理知识。

Result: 评估主流模型后发现，物理真实性仍然是一个具有挑战性的问题，存在很大的改进空间。

Conclusion: 该研究为从简单内容编辑向物理一致的真实性转变提供了基础，希望基准测试和解决方案能推动未来工作的发展。

Abstract: Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.

</details>


### [120] [Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model](https://arxiv.org/abs/2510.17684)
*Xinwei Zhang,Hu Chen,Zhe Yuan,Sukun Tian,Peng Feng*

Main category: cs.CV

TL;DR: 提出IC-MoE模型，通过混合专家架构和语义引导对比学习增强医学图像分割基础模型的高层特征表示能力，同时保持预训练权重的结构完整性。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割基础模型微调方法存在两个主要问题：高层特征表示不足，以及微调过程破坏预训练权重的结构完整性。

Method: 1) 构建基础专家、语义专家和自适应专家，采用像素概率自适应投票策略进行专家选择和融合；2) 提出语义引导对比学习方法解决对比学习中弱监督问题。

Result: 在三个公共医学图像分割数据集上的实验表明，IC-MoE优于其他最先进模型，并验证了其在多样化医学图像分割场景中的优越泛化能力。

Conclusion: IC-MoE有效补充了基础医学图像分割模型的高层特征表示能力，同时保持了预训练权重的结构完整性。

Abstract: Foundation models for medical image segmentation have achieved remarkable
performance. Adaptive fine-tuning of natural image segmentation foundation
models is crucial for medical image segmentation tasks. However, some
limitations exist in existing fine-tuning methods: 1) insufficient
representation of high-level features and 2) the fine-tuning process disrupts
the structural integrity of pretrained weights. Inspired by these critical
problems, we propose an intelligent communication mixture-of-experts
boosted-medical image segmentation foundation model, named IC-MoE, with twofold
ideas: 1) We construct basic experts, semantic experts, and adaptive experts.
Moreover, we implement a pixel probability adaptive voting strategy, which
enables expert selection and fusion through label consistency and load
balancing. This approach preliminarily enhances the representation capability
of high-level features while preserving the structural integrity of pretrained
weights. 2) We propose a semantic-guided contrastive learning method to address
the issue of weak supervision in contrastive learning. This method further
enhances the representation capability of high-level features while preserving
the structural integrity of pretrained weights. Extensive experiments across
three public medical image segmentation datasets demonstrate that the IC-MoE
outperforms other SOTA models. Consequently, the proposed IC-MoE effectively
supplements foundational medical image segmentation models with high-level
features and pretrained structural integrity. We also validate the superior
generalizability of the IC-MoE across diverse medical image segmentation
scenarios.

</details>


### [121] [Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning](https://arxiv.org/abs/2510.17685)
*Min Cao,Xinyu Zhou,Ding Jiang,Bo Du,Mang Ye,Min Zhang*

Main category: cs.CV

TL;DR: 提出了多语言文本到图像行人检索任务，开发了Bi-IRRA框架，通过双向隐式关系推理和多维全局对齐来解决模态异质性问题，在多语言TIPR数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像行人检索方法存在模态异质性挑战，全局方法忽略细粒度差异，局部方法需要先验信息，且当前方法主要面向英语，限制了在多语言环境的应用。

Method: 提出Bi-IRRA框架，包含双向隐式关系推理模块（通过掩码图像和文本的双向预测来隐式增强跨语言和跨模态的局部关系建模）和多维全局对齐模块（桥接模态异质性）。

Result: 在所有多语言TIPR数据集上取得了新的最先进结果。

Conclusion: Bi-IRRA框架有效解决了多语言文本到图像行人检索中的模态异质性问题，为多语言环境下的行人检索提供了有效解决方案。

Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.

</details>


### [122] [Training-free Online Video Step Grounding](https://arxiv.org/abs/2510.16989)
*Luca Zanella,Massimiliano Mancini,Yiming Wang,Alessio Tonioni,Elisa Ricci*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、在线执行的视频步骤定位方法BaGLM，利用大型多模态模型的零样本能力，结合贝叶斯滤波原理，在仅访问部分帧的情况下超越基于训练的离线方法。


<details>
  <summary>Details</summary>
Motivation: 传统视频步骤定位方法需要带标签的训练数据和离线处理整个视频，成本高且无法满足在线决策需求。本文探索如何在不训练的情况下在线执行VSG任务。

Method: 利用大型多模态模型的零样本能力预测步骤，开发BaGLM方法，通过贝叶斯滤波注入历史帧知识，使用LLM提取的依赖矩阵和步骤进度估计来建模步骤转换。

Result: 在三个数据集上的实验表明，BaGLM超越了最先进的基于训练的离线方法，无需任务特定调优的在线策略优于离线训练模型。

Conclusion: BaGLM证明了利用大型多模态模型的零样本能力结合贝叶斯滤波原理，可以在无需训练的情况下实现优于传统方法的在线视频步骤定位性能。

Abstract: Given a task and a set of steps composing it, Video Step Grounding (VSG) aims
to detect which steps are performed in a video. Standard approaches for this
task require a labeled training set (e.g., with step-level annotations or
narrations), which may be costly to collect. Moreover, they process the full
video offline, limiting their applications for scenarios requiring online
decisions. Thus, in this work, we explore how to perform VSG online and without
training. We achieve this by exploiting the zero-shot capabilities of recent
Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step
associated with a restricted set of frames, without access to the whole video.
We show that this online strategy without task-specific tuning outperforms
offline and training-based models. Motivated by this finding, we develop
Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting
knowledge of past frames into the LMM-based predictions. BaGLM exploits
Bayesian filtering principles, modeling step transitions via (i) a dependency
matrix extracted through large language models and (ii) an estimation of step
progress. Experiments on three datasets show superior performance of BaGLM over
state-of-the-art training-based offline methods.

</details>


### [123] [Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns](https://arxiv.org/abs/2510.17703)
*Mhd Adnan Albani,Riad Sonbol*

Main category: cs.CV

TL;DR: 提出一种两阶段帕金森病检测方法，通过图像分块处理和集成学习，显著提高了对未见患者数据的检测准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有帕金森病早期检测方法存在两个主要问题：数据集不足和对未见患者数据缺乏鲁棒性。

Method: 采用两阶段方法：第一阶段按绘图类型分类，第二阶段将图像分为2x2块，分别提取特征并检测帕金森病指标，最后使用集成方法合并各块决策。

Result: 在NewHandPD数据集上，对已见患者准确率达97.08%，对未见患者达94.91%，准确率差距仅2.17个百分点，优于现有方法的4.76个百分点下降。

Conclusion: 该方法有效解决了帕金森病检测中的数据集不足和未见患者鲁棒性问题，显著提升了检测性能。

Abstract: Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of
people over the age of 60, causing motor impairments that impede hand
coordination activities such as writing and drawing. Many approaches have tried
to support early detection of Parkinson's disease based on hand-drawn images;
however, we identified two major limitations in the related works: (1) the lack
of sufficient datasets, (2) the robustness when dealing with unseen patient
data. In this paper, we propose a new approach to detect Parkinson's disease
that consists of two stages: The first stage classifies based on their drawing
type(circle, meander, spiral), and the second stage extracts the required
features from the images and detects Parkinson's disease. We overcame the
previous two limitations by applying a chunking strategy where we divide each
image into 2x2 chunks. Each chunk is processed separately when extracting
features and recognizing Parkinson's disease indicators. To make the final
classification, an ensemble method is used to merge the decisions made from
each chunk. Our evaluation shows that our proposed approach outperforms the top
performing state-of-the-art approaches, in particular on unseen patients. On
the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen
patients and 94.91% for unseen patients, our proposed approach maintained a gap
of only 2.17 percentage points, compared to the 4.76-point drop observed in
prior work.

</details>


### [124] [An empirical study of the effect of video encoders on Temporal Video Grounding](https://arxiv.org/abs/2510.17007)
*Ignacio M. De la Jara,Cristian Rodriguez-Opazo,Edison Marrese-Taylor,Felipe Bravo-Marquez*

Main category: cs.CV

TL;DR: 本文对时序视频定位任务中不同视频特征的影响进行了实证研究，发现仅改变视频编码器就能显著影响模型性能，并揭示了不同特征之间的互补潜力。


<details>
  <summary>Details</summary>
Motivation: 当前时序视频定位研究过于集中在少数视频表示方法上，可能导致长期的架构过拟合问题。

Method: 在三个基准数据集上使用基于CNN、时序推理和transformer的不同视频编码器提取特征，并在经典架构上进行比较研究。

Result: 仅改变视频编码器就导致模型性能出现显著差异，同时揭示了使用某些特征时产生的明显模式和错误。

Conclusion: 不同视频特征之间存在明显的互补潜力，这为未来的特征组合研究提供了方向。

Abstract: Temporal video grounding is a fundamental task in computer vision, aiming to
localize a natural language query in a long, untrimmed video. It has a key role
in the scientific community, in part due to the large amount of video generated
every day. Although we find extensive work in this task, we note that research
remains focused on a small selection of video representations, which may lead
to architectural overfitting in the long run. To address this issue, we propose
an empirical study to investigate the impact of different video features on a
classical architecture. We extract features for three well-known benchmarks,
Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based on
CNNs, temporal reasoning and transformers. Our results show significant
differences in the performance of our model by simply changing the video
encoder, while also revealing clear patterns and errors derived from the use of
certain features, ultimately indicating potential feature complementarity.

</details>


### [125] [MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](https://arxiv.org/abs/2510.17722)
*Yaning Pan,Zekun Wang,Qianqian Xie,Yongqian Wen,Yuanxing Zhang,Guohui Zhang,Haoxuan Hu,Zhiyu Pan,Yibing Huang,Zhidong Gan,Yonghong Lin,An Ping,Tianhao Peng,Jiaheng Liu*

Main category: cs.CV

TL;DR: 提出了MT-Video-Bench，一个用于评估多模态大语言模型在多轮视频对话中能力的综合性基准测试，包含987个精心策划的多轮对话，涵盖6个核心能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准仅限于单轮问答，忽略了现实场景中多轮对话的复杂性，需要开发专门针对多轮视频对话的评估方法。

Method: 构建MT-Video-Bench基准，包含987个多轮对话，评估6个核心能力（感知性和交互性），并与现实应用（如交互式体育分析和视频智能辅导）严格对齐。

Result: 对多种最先进的开源和闭源MLLMs进行了广泛评估，揭示了它们在处理多轮视频对话时的显著性能差异和局限性。

Conclusion: MT-Video-Bench基准将公开可用，以促进未来研究，填补了多轮视频对话评估的空白。

Abstract: The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI's ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.

</details>


### [126] [Do Satellite Tasks Need Special Pretraining?](https://arxiv.org/abs/2510.17014)
*Ani Vanyan,Alvard Barseghyan,Hakob Tamazyan,Tigran Galstyan,Vahan Huroyan,Naira Hovakimyan,Hrant Khachatrian*

Main category: cs.CV

TL;DR: 该研究质疑专用遥感基础模型是否比通用视觉基础模型更有用，通过设计基准测试和训练iBOT模型，发现在ViT-B规模下专用模型并未带来一致改进。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证专用遥感基础模型是否真的比通用视觉基础模型更有用，特别是针对遥感图像的独特特性和应用需求。

Method: 设计了一个简单基准测试来衡量遥感模型在低分辨率图像上的泛化能力，并在MillionAID数据集上训练了iBOT自监督视觉编码器，进行了遥感特定的修改。

Result: 结果显示，在ViT-B规模下，这些预训练模型都没有比通用基线带来一致的改进。

Conclusion: 结论是在小规模情况下，专用遥感基础模型并不比通用视觉基础模型更有用。

Abstract: Foundation models have advanced machine learning across various modalities,
including images. Recently multiple teams trained foundation models specialized
for remote sensing applications. This line of research is motivated by the
distinct characteristics of remote sensing imagery, specific applications and
types of robustness useful for satellite image analysis. In this work we
systematically challenge the idea that specific foundation models are more
useful than general-purpose vision foundation models, at least in the small
scale. First, we design a simple benchmark that measures generalization of
remote sensing models towards images with lower resolution for two downstream
tasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,
an ImageNet-scale satellite imagery dataset, with several modifications
specific to remote sensing. We show that none of those pretrained models bring
consistent improvements upon general-purpose baselines at the ViT-B scale.

</details>


### [127] [Signature Forgery Detection: Improving Cross-Dataset Generalization](https://arxiv.org/abs/2510.17724)
*Matheus Ramos Parracho*

Main category: cs.CV

TL;DR: 该研究探索了签名伪造检测的特征学习策略，重点关注提高跨数据集泛化能力。使用三个公共基准数据集，比较了基于原始签名图像和壳预处理两种方法的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在签名验证方面取得进展，但大多数方法在跨数据集泛化方面仍存在困难，手写风格和采集协议的差异会降低性能。

Method: 使用CEDAR、ICDAR和GPDS Synthetic三个数据集，开发了两种实验流程：基于原始签名图像的方法和采用壳预处理的方法。

Result: 原始图像模型在基准测试中表现更好，而基于壳预处理的方法显示出未来改进的潜力。两种方法没有明确的优劣之分。

Conclusion: 原始图像方法在当前表现更优，但壳预处理方法在实现鲁棒的跨域签名验证方面具有发展潜力。

Abstract: Automated signature verification is a critical biometric technique used in
banking, identity authentication, and legal documentation. Despite the notable
progress achieved by deep learning methods, most approaches in offline
signature verification still struggle to generalize across datasets, as
variations in handwriting styles and acquisition protocols often degrade
performance. This study investigates feature learning strategies for signature
forgery detection, focusing on improving cross-dataset generalization -- that
is, model robustness when trained on one dataset and tested on another. Using
three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental
pipelines were developed: one based on raw signature images and another
employing a preprocessing method referred to as shell preprocessing. Several
behavioral patterns were identified and analyzed; however, no definitive
superiority between the two approaches was established. The results show that
the raw-image model achieved higher performance across benchmarks, while the
shell-based model demonstrated promising potential for future refinement toward
robust, cross-domain signature verification.

</details>


### [128] [Enrich and Detect: Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2510.17023)
*Shraman Pramanick,Effrosyni Mavroudi,Yale Song,Rama Chellappa,Lorenzo Torresani,Triantafyllos Afouras*

Main category: cs.CV

TL;DR: ED-VTG是一种利用多模态大语言模型进行细粒度视频时序定位的方法，通过两阶段处理将文本查询转化为增强语句并准确定位时间边界，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 利用多模态大语言模型的能力来联合处理文本和视频，以有效定位视频中的自然语言查询，解决现有方法在细粒度定位中的局限性。

Method: 采用两阶段方法：首先将语言查询转化为包含缺失细节的增强语句，然后使用轻量级解码器基于增强查询的上下文表示预测准确的时间边界，并通过多实例学习目标减少幻觉影响。

Result: 在视频时序定位和段落定位设置中，该方法显著优于所有先前提出的基于LLM的时序定位方法，与专用模型相比具有优势或在零样本评估场景中保持明显优势。

Conclusion: ED-VTG通过多模态LLM的两阶段处理框架，在视频时序定位任务中实现了最先进的性能，特别是在零样本场景下表现出色。

Abstract: We introduce ED-VTG, a method for fine-grained video temporal grounding
utilizing multi-modal large language models. Our approach harnesses the
capabilities of multimodal LLMs to jointly process text and video, in order to
effectively localize natural language queries in videos through a two-stage
process. Rather than being directly grounded, language queries are initially
transformed into enriched sentences that incorporate missing details and cues
to aid in grounding. In the second stage, these enriched queries are grounded,
using a lightweight decoder, which specializes at predicting accurate
boundaries conditioned on contextualized representations of the enriched
queries. To mitigate noise and reduce the impact of hallucinations, our model
is trained with a multiple-instance-learning objective that dynamically selects
the optimal version of the query for each training sample. We demonstrate
state-of-the-art results across various benchmarks in temporal video grounding
and paragraph grounding settings. Experiments reveal that our method
significantly outperforms all previously proposed LLM-based temporal grounding
approaches and is either superior or comparable to specialized models, while
maintaining a clear advantage against them in zero-shot evaluation scenarios.

</details>


### [129] [Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding](https://arxiv.org/abs/2510.17034)
*Yutong Zhong*

Main category: cs.CV

TL;DR: 提出W2R2训练框架解决多模态3D定位中的2D语义偏差问题，通过解耦表示学习和针对性捷径抑制，将2D特征作为语义标识，3D特征作为空间锚点，无需修改推理架构即可实现精确3D定位。


<details>
  <summary>Details</summary>
Motivation: 现有多模态3D定位模型存在严重的"2D语义偏差"问题，过度依赖2D图像特征进行粗略定位，而忽视3D几何输入，导致融合性能不佳。

Method: 采用解耦表示学习，将2D特征作为"What"语义标识，3D特征作为"Where"空间锚点；设计双目标损失函数，包括用于多模态协同的Alignment Loss和通过边界机制惩罚2D主导伪输出的Pseudo-Label Loss。

Result: 在ScanRefer和ScanQA数据集上的实验表明，W2R2在定位精度和鲁棒性方面取得显著提升，尤其在复杂室外场景中表现突出。

Conclusion: W2R2框架通过重新塑造模型内部表示空间，有效解决了2D语义偏差问题，为多模态3D定位提供了新的训练范式。

Abstract: Multimodal 3D grounding has garnered considerable interest in Vision-Language
Models (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complex
environments. However, these models suffer from a severe "2D semantic bias"
that arises from over-reliance on 2D image features for coarse localization,
largely disregarding 3D geometric inputs and resulting in suboptimal fusion
performance. In this paper, we propose a novel training framework called
What-Where Representation Re-Forming (W2R2) to tackle this issue via
disentangled representation learning and targeted shortcut suppression. Our
approach fundamentally reshapes the model's internal space by designating 2D
features as semantic beacons for "What" identification and 3D features as
spatial anchors for "Where" localization, enabling precise 3D grounding without
modifying inference architecture. Key components include a dual-objective loss
function with an Alignment Loss that supervises fused predictions using adapted
cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes
overly effective 2D-dominant pseudo-outputs via a margin-based mechanism.
Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of
W2R2, with significant gains in localization accuracy and robustness,
particularly in cluttered outdoor scenes.

</details>


### [130] [Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion](https://arxiv.org/abs/2510.17773)
*Md. Enamul Atiq,Shaikh Anowarul Fattah*

Main category: cs.CV

TL;DR: 提出基于双编码器注意力框架的皮肤癌分类方法，结合病灶分割和临床元数据，在准确性和可解释性方面均有提升


<details>
  <summary>Details</summary>
Motivation: 皮肤癌早期检测对患者预后至关重要，但现有深度学习模型存在"黑盒"问题，缺乏临床信任。病灶的高类内变异性和细微类间差异也增加了自动诊断的挑战

Method: 使用带有双注意力门和空洞空间金字塔池化的Deep-UNet进行病灶分割；分类阶段采用两个DenseNet201编码器分别处理原始图像和分割病灶，通过多头交叉注意力融合特征；还包含基于transformer的模块整合患者元数据

Result: 在HAM10000数据集和ISIC 2018、2019挑战赛上取得最先进的分割性能，显著提高了分类准确率和平均AUC；Grad-CAM热图验证模型确实基于病灶区域进行预测

Conclusion: 将精确的病灶分割、临床数据与基于注意力的融合相结合，能够构建更准确和可解释的皮肤癌分类模型

Abstract: Skin cancer is a life-threatening disease where early detection significantly
improves patient outcomes. Automated diagnosis from dermoscopic images is
challenging due to high intra-class variability and subtle inter-class
differences. Many deep learning models operate as "black boxes," limiting
clinical trust. In this work, we propose a dual-encoder attention-based
framework that leverages both segmented lesions and clinical metadata to
enhance skin lesion classification in terms of both accuracy and
interpretability. A novel Deep-UNet architecture with Dual Attention Gates
(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment
lesions. The classification stage uses two DenseNet201 encoders-one on the
original image and another on the segmented lesion whose features are fused via
multi-head cross-attention. This dual-input design guides the model to focus on
salient pathological regions. In addition, a transformer-based module
incorporates patient metadata (age, sex, lesion site) into the prediction. We
evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019
challenges. The proposed method achieves state-of-the-art segmentation
performance and significantly improves classification accuracy and average AUC
compared to baseline models. To validate our model's reliability, we use
Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.
These visualizations confirm that our model's predictions are based on the
lesion area, unlike models that rely on spurious background features. These
results demonstrate that integrating precise lesion segmentation and clinical
data with attention-based fusion leads to a more accurate and interpretable
skin cancer classification model.

</details>


### [131] [Conditional Synthetic Live and Spoof Fingerprint Generation](https://arxiv.org/abs/2510.17035)
*Syed Konain Abbas,Sandip Purnapatra,M. G. Sarwar Murshed,Conor Miller-Lynch,Lambert Igene,Soumyabrata Dey,Stephanie Schuckers,Faraz Hussain*

Main category: cs.CV

TL;DR: 本文提出了一种使用条件StyleGAN2-ADA和StyleGAN3生成高分辨率合成活体指纹，并通过CycleGANs转换为逼真假指纹的方法，解决了生物特征数据收集中的隐私、成本和可访问性问题。


<details>
  <summary>Details</summary>
Motivation: 大型指纹数据集收集耗时昂贵且需要严格隐私保护，研究人员探索使用合成指纹数据来解决这些问题。

Method: 使用条件StyleGAN2-ADA和StyleGAN3生成特定手指身份的高分辨率合成活体指纹，然后通过CycleGANs将其转换为模拟各种攻击材料的逼真假指纹。

Result: 创建了两个合成数据集（DB2和DB3），每个包含1,500张指纹图像，StyleGAN3模型FID低至5，真接受率达到99.47%（0.01%假接受率），StyleGAN2-ADA模型达到98.67%真接受率。

Conclusion: 生成的合成指纹在质量评估和匹配实验中表现优异，确认了强大的隐私保护特性，无显著身份泄露证据。

Abstract: Large fingerprint datasets, while important for training and evaluation, are
time-consuming and expensive to collect and require strict privacy measures.
Researchers are exploring the use of synthetic fingerprint data to address
these issues. This paper presents a novel approach for generating synthetic
fingerprint images (both spoof and live), addressing concerns related to
privacy, cost, and accessibility in biometric data collection. Our approach
utilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce
high-resolution synthetic live fingerprints, conditioned on specific finger
identities (thumb through little finger). Additionally, we employ CycleGANs to
translate these into realistic spoof fingerprints, simulating a variety of
presentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof
fingerprints are crucial for developing robust spoof detection systems. Through
these generative models, we created two synthetic datasets (DB2 and DB3), each
containing 1,500 fingerprint images of all ten fingers with multiple
impressions per finger, and including corresponding spoofs in eight material
types. The results indicate robust performance: our StyleGAN3 model achieves a
Fr\'echet Inception Distance (FID) as low as 5, and the generated fingerprints
achieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The
StyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess
fingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,
matching experiments confirm strong privacy preservation, with no significant
evidence of identity leakage, confirming the strong privacy-preserving
properties of our synthetic datasets.

</details>


### [132] [Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework](https://arxiv.org/abs/2510.17039)
*Mohammad R. Salmanpour,Sonya Falahati,Amir Hossein Pouria,Amin Mousavi,Somayeh Sadat Mehrnia,Morteza Alizadeh,Arman Gorji,Zeinab Farsangi,Alireza Safarian,Mehdi Maghsudi,Carlos Uribe,Arman Rahmim,Ren Yuan*

Main category: cs.CV

TL;DR: 本研究开发了一个临床医生参与的深度学习管道，用于肺癌CT图像分割和预后预测。VNet模型在分割性能、放射组学稳定性和预测准确性方面表现最佳，半监督学习优于监督学习，临床医生偏好AI生成的初始掩模进行精炼。


<details>
  <summary>Details</summary>
Motivation: 肺癌是癌症死亡的主要原因，CT成像在筛查、预后和治疗中至关重要。手动分割存在变异性且耗时，深度学习虽能自动化但面临临床采用障碍。本研究旨在开发一个临床医生参与的深度学习管道，提高可重复性、预后准确性和临床信任度。

Method: 使用来自12个公共数据集的999名患者的多中心CT数据，分析5种深度学习模型（3D Attention U-Net、ResUNet、VNet、ReconNet、SAM-Med3D），在完整图像和点击裁剪图像上与专家轮廓进行基准测试。评估497个放射组学特征的分割可重复性，比较监督学习和半监督学习的预后建模，6名医生在7个领域对掩模进行定性评估。

Result: VNet获得最佳性能（Dice = 0.83，IoU = 0.71）、放射组学稳定性（平均相关性=0.76，ICC=0.65）和半监督学习下的预测准确性（准确率=0.88，F1=0.83）。半监督学习在所有模型中一致优于监督学习。放射科医生偏好VNet的瘤周表示和更平滑边界，更倾向于使用AI生成的初始掩模进行精炼而非完全替换。

Conclusion: 将VNet与半监督学习结合可产生准确、可重复且临床信任的基于CT的肺癌预后预测，突出了实现以医生为中心的AI转化的可行路径。

Abstract: Lung cancer remains the leading cause of cancer mortality, with CT imaging
central to screening, prognosis, and treatment. Manual segmentation is variable
and time-intensive, while deep learning (DL) offers automation but faces
barriers to clinical adoption. Guided by the Knowledge-to-Action framework,
this study develops a clinician-in-the-loop DL pipeline to enhance
reproducibility, prognostic accuracy, and clinical trust. Multi-center CT data
from 999 patients across 12 public datasets were analyzed using five DL models
(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against
expert contours on whole and click-point cropped images. Segmentation
reproducibility was assessed using 497 PySERA-extracted radiomic features via
Spearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic
modeling compared supervised (SL) and semi-supervised learning (SSL) across 38
dimensionality reduction strategies and 24 classifiers. Six physicians
qualitatively evaluated masks across seven domains, including clinical
meaningfulness, boundary quality, prognostic value, trust, and workflow
integration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),
radiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive
accuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed
SL across models. Radiologists favored VNet for peritumoral representation and
smoother boundaries, preferring AI-generated initial masks for refinement
rather than replacement. These results demonstrate that integrating VNet with
SSL yields accurate, reproducible, and clinically trusted CT-based lung cancer
prognosis, highlighting a feasible path toward physician-centered AI
translation.

</details>


### [133] [Person Re-Identification via Generalized Class Prototypes](https://arxiv.org/abs/2510.17043)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 提出了一种改进行人重识别性能的广义选择方法，通过选择不限于类别质心的表征来平衡准确率和平均精度，显著提升了现有技术水平


<details>
  <summary>Details</summary>
Motivation: 虽然特征提取方法和目标函数改进已显著提升行人重识别性能，但选择更好的类别表征这一研究方向尚未充分探索。现有方法主要使用图库图像类别的质心，但在检索阶段替代表征的研究较少，且这些技术产生了次优结果

Method: 提出广义选择方法，选择不限于类别质心的表征，可根据具体应用需求调整每个类别的实际表征数量，在多个重识别嵌入基础上应用该方法

Result: 该方法在准确率和平均精度之间取得平衡，在所有情况下都显著改进了当代结果，超越了现有技术水平

Conclusion: 广义表征选择方法为行人重识别提供了有效的改进途径，能够根据应用需求灵活调整表征数量，在多个嵌入方法上都取得了显著性能提升

Abstract: Advanced feature extraction methods have significantly contributed to
enhancing the task of person re-identification. In addition, modifications to
objective functions have been developed to further improve performance.
Nonetheless, selecting better class representatives is an underexplored area of
research that can also lead to advancements in re-identification performance.
Although past works have experimented with using the centroid of a gallery
image class during training, only a few have investigated alternative
representations during the retrieval stage. In this paper, we demonstrate that
these prior techniques yield suboptimal results in terms of re-identification
metrics. To address the re-identification problem, we propose a generalized
selection method that involves choosing representations that are not limited to
class centroids. Our approach strikes a balance between accuracy and mean
average precision, leading to improvements beyond the state of the art. For
example, the actual number of representations per class can be adjusted to meet
specific application requirements. We apply our methodology on top of multiple
re-identification embeddings, and in all cases it substantially improves upon
contemporary results

</details>


### [134] [How Universal Are SAM2 Features?](https://arxiv.org/abs/2510.17051)
*Masoud Khairi Atani,Alon Harell,Hyomin Choi,Runyu Yang,Fabien Racape,Ivan V. Bajic*

Main category: cs.CV

TL;DR: 比较通用视觉模型Hiera和专用分割模型SAM2的特征通用性，发现专用化虽然提升空间相关任务性能，但会损失语义信息，导致在概念较远任务上表现下降。


<details>
  <summary>Details</summary>
Motivation: 理解通用基础视觉模型与专用模型之间的权衡，为高效特征编码设计提供定量基础。

Method: 使用轻量可训练neck层探测冻结特征的适应性，通过信息论成本量化专用化代价，并进行跨neck分析。

Result: SAM2在深度估计等空间任务上表现优异，但在姿态估计和图像描述等概念较远任务上不如Hiera，显示语义信息损失。

Conclusion: 专用化存在权衡，需要在特征通用性和任务性能之间平衡，为下游应用的特征编码和适应策略设计提供指导。

Abstract: The trade-off between general-purpose foundation vision models and their
specialized counterparts is critical for efficient feature coding design and is
not yet fully understood. We investigate this trade-off by comparing the
feature versatility of the general-purpose Hiera encoder against the
segmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,
trainable neck to probe the adaptability of their frozen features, we quantify
the information-theoretic cost of specialization. Our results reveal that while
SAM2's specialization is highly effective for spatially-related tasks like
depth estimation, it comes at a cost. The specialized SAM2 encoder
underperforms its generalist predecessor, Hiera, on conceptually distant tasks
such as pose estimation and image captioning, demonstrating a measurable loss
of broader semantic information. A novel cross-neck analysis on SAM2 reveals
that each level of adaptation creates a further representational bottleneck.
Our analysis illuminates these trade-offs in feature universality, providing a
quantitative foundation for designing efficient feature coding and adaptation
strategies for diverse downstream applications.

</details>


### [135] [ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding](https://arxiv.org/abs/2510.17068)
*Zhe Luo,Wenjing Jia,Stuart Perry*

Main category: cs.CV

TL;DR: 提出ProDAT方法，通过密度感知的尾丢弃机制实现点云渐进式编码，在单一模型中支持多比特率解码，显著提升编码效率。


<details>
  <summary>Details</summary>
Motivation: 3D点云在自动驾驶、增强现实等应用中需要实时处理，但大数据量和带宽限制阻碍了高质量服务部署。现有学习方法缺乏渐进解码能力。

Method: 使用密度信息作为指导信号，自适应解码潜在特征和坐标，基于重要性实现渐进式解码。

Result: 在基准数据集上，ProDAT不仅实现渐进编码，且编码效率优于现有方法，在SemanticKITTI上PSNR-D2 BD-rate提升28.6%，在ShapeNet上提升18.15%。

Conclusion: ProDAT成功解决了点云渐进编码问题，在单一模型中实现多比特率解码，显著提升了编码性能。

Abstract: Three-dimensional (3D) point clouds are becoming increasingly vital in
applications such as autonomous driving, augmented reality, and immersive
communication, demanding real-time processing and low latency. However, their
large data volumes and bandwidth constraints hinder the deployment of
high-quality services in resource-limited environments. Progres- sive coding,
which allows for decoding at varying levels of detail, provides an alternative
by allowing initial partial decoding with subsequent refinement. Although
recent learning-based point cloud geometry coding methods have achieved notable
success, their fixed latent representation does not support progressive
decoding. To bridge this gap, we propose ProDAT, a novel density-aware
tail-drop mechanism for progressive point cloud coding. By leveraging density
information as a guidance signal, latent features and coordinates are decoded
adaptively based on their significance, therefore achieving progressive
decoding at multiple bitrates using one single model. Experimental results on
benchmark datasets show that the proposed ProDAT not only enables progressive
coding but also achieves superior coding efficiency compared to
state-of-the-art learning-based coding techniques, with over 28.6% BD-rate
improvement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet

</details>


### [136] [Towards a Generalizable Fusion Architecture for Multimodal Object Detection](https://arxiv.org/abs/2510.17078)
*Jad Berjawi,Yoann Dupas,Christophe C'erin*

Main category: cs.CV

TL;DR: 提出了FMCAF预处理架构，通过频域滤波和跨注意力融合来增强RGB和红外图像的多模态目标检测性能，在多个数据集上表现优于传统融合方法。


<details>
  <summary>Details</summary>
Motivation: 多模态目标检测在挑战性条件下通过利用多个传感器模态的互补线索来提高鲁棒性，但现有方法往往针对特定数据集，缺乏通用性。

Method: FMCAF结合频域滤波块(Freq-Filter)来抑制冗余光谱特征，以及基于跨注意力的融合模块(MCAF)来改进模态间特征共享。

Result: 在LLVIP(低光行人检测)和VEDAI(航空车辆检测)数据集上，FMCAF优于传统融合方法，在VEDAI上mAP@50提升13.9%，在LLVIP上提升1.1%。

Conclusion: FMCAF作为一个灵活的预处理架构，为未来多模态检测管道中的鲁棒融合提供了潜力，具有较好的通用性。

Abstract: Multimodal object detection improves robustness in chal- lenging conditions
by leveraging complementary cues from multiple sensor modalities. We introduce
Filtered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing
architecture designed to enhance the fusion of RGB and infrared (IR) inputs.
FMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress
redun- dant spectral features with a cross-attention-based fusion module (MCAF)
to improve intermodal feature sharing. Unlike approaches tailored to specific
datasets, FMCAF aims for generalizability, improving performance across
different multimodal challenges without requiring dataset- specific tuning. On
LLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),
FMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50
on VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a
flexible foundation for robust multimodal fusion in future detection pipelines.

</details>


### [137] [GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation](https://arxiv.org/abs/2510.17095)
*Ruitong Gan,Junran Peng,Yang Liu,Chuanchen Luo,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: GSPlane通过引入平面先验来改进高斯溅射的几何重建质量，特别针对平面区域提供更平滑精确的网格表示，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有高斯溅射方法在重建平面区域时往往缺乏足够的平滑性和精度，这限制了在场景编辑和物理模拟等下游应用中的使用。

Method: 利用现成的分割和法线预测模型提取平面先验，建立结构化平面高斯坐标表示；引入动态高斯重分类器自适应重新分类高梯度平面高斯；利用优化的平面先验优化网格布局。

Result: 在保持渲染质量的同时，显著提高了提取网格的几何精度，减少了顶点和面数，改善了拓扑结构。

Conclusion: 平面先验的引入有效提升了高斯溅射在平面区域的重建质量，为场景编辑和物理模拟提供了更好的基础。

Abstract: Planes are fundamental primitives of 3D sences, especially in man-made
environments such as indoor spaces and urban streets. Representing these planes
in a structured and parameterized format facilitates scene editing and physical
simulations in downstream applications. Recently, Gaussian Splatting (GS) has
demonstrated remarkable effectiveness in the Novel View Synthesis task, with
extensions showing great potential in accurate surface reconstruction. However,
even state-of-the-art GS representations often struggle to reconstruct planar
regions with sufficient smoothness and precision. To address this issue, we
propose GSPlane, which recovers accurate geometry and produces clean and
well-structured mesh connectivity for plane regions in the reconstructed scene.
By leveraging off-the-shelf segmentation and normal prediction models, GSPlane
extracts robust planar priors to establish structured representations for
planar Gaussian coordinates, which help guide the training process by enforcing
geometric consistency. To further enhance training robustness, a Dynamic
Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians
with persistently high gradients as non-planar, ensuring more reliable
optimization. Furthermore, we utilize the optimized planar priors to refine the
mesh layouts, significantly improving topological structure while reducing the
number of vertices and faces. We also explore applications of the structured
planar representation, which enable decoupling and flexible manipulation of
objects on supportive planes. Extensive experiments demonstrate that, with no
sacrifice in rendering quality, the introduction of planar priors significantly
improves the geometric accuracy of the extracted meshes across various
baselines.

</details>


### [138] [Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement](https://arxiv.org/abs/2510.17105)
*Xiaogang Xu,Jian Wang,Yunfan Lu,Ruihang Chu,Ruixing Wang,Jiafei Wu,Bei Yu,Liang Lin*

Main category: cs.CV

TL;DR: 提出了一种针对预训练扩散模型的优化策略，通过潜在细化管道和双向交互机制，在保持感知真实性的同时显著提升内容保真度，特别是在低光场景下。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散方法在追求更高感知真实性时往往牺牲内容保真度，特别是在低光场景中，黑暗导致的严重信息退化限制了有效控制。主要问题包括缺乏合适的条件潜在建模以及条件潜在与噪声潜在之间缺乏双向交互。

Method: 1. 引入潜在细化管道，利用生成先验恢复VAE编码过程中丢失的空间细节；2. 设计动态交互机制，使细化后的条件潜在与噪声潜在在扩散过程中进行双向交互；3. 即插即用设计，可无缝集成到现有扩散网络中。

Result: 大量实验证明该方法在预训练扩散方法中实现了显著的内容保真度提升，同时保持了感知真实性和美学质量。

Conclusion: 提出的优化策略有效解决了预训练扩散模型中的保真度损失问题，通过潜在细化和双向交互机制，在低光场景等挑战性条件下实现了更好的恢复性能。

Abstract: Diffusion-based methods, leveraging pre-trained large models like Stable
Diffusion via ControlNet, have achieved remarkable performance in several
low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods
often sacrifice content fidelity to attain higher perceptual realism. This
issue is exacerbated in low-light scenarios, where severely degraded
information caused by the darkness limits effective control. We identify two
primary causes of fidelity loss: the absence of suitable conditional latent
modeling and the lack of bidirectional interaction between the conditional
latent and noisy latent in the diffusion process. To address this, we propose a
novel optimization strategy for conditioning in pre-trained diffusion models,
enhancing fidelity while preserving realism and aesthetics. Our method
introduces a mechanism to recover spatial details lost during VAE encoding,
i.e., a latent refinement pipeline incorporating generative priors.
Additionally, the refined latent condition interacts dynamically with the noisy
latent, leading to improved restoration performance. Our approach is
plug-and-play, seamlessly integrating into existing diffusion networks to
provide more effective control. Extensive experiments demonstrate significant
fidelity improvements in PTDB methods.

</details>


### [139] [Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras](https://arxiv.org/abs/2510.17114)
*Hodaka Kawachi,Tomoya Nakamura,Hiroaki Santo,SaiKiran Kumar Tedla,Trevor Dalton Canham,Yasushi Yagi,Michael S. Brown*

Main category: cs.CV

TL;DR: 提出一种使用LED环境照明为消费相机生成视觉不可见水印的方法，通过优化LED光谱轮廓使其对人眼几乎不可见但对相机高度可检测。


<details>
  <summary>Details</summary>
Motivation: 开发一种视觉不可见的数字水印技术，用于隐私保护和内容验证，同时不影响人类视觉体验。

Method: 采用光谱调制而非强度调制，联合考虑人眼视觉系统敏感性、相机传感器光谱敏感性和LED生成白光的特性，优化LED光谱轮廓。

Result: 能够在标准低帧率（30-60 fps）下提取水印，在10秒视频片段中嵌入128位信息，足以支持隐私保护和内容验证的元数据。

Conclusion: 该方法成功实现了对人眼不可见但对消费相机可检测的水印嵌入，为数字内容保护和验证提供了实用解决方案。

Abstract: This paper introduces a method for using LED-based environmental lighting to
produce visually imperceptible watermarks for consumer cameras. Our approach
optimizes an LED light source's spectral profile to be minimally visible to the
human eye while remaining highly detectable by typical consumer cameras. The
method jointly considers the human visual system's sensitivity to visible
spectra, modern consumer camera sensors' spectral sensitivity, and narrowband
LEDs' ability to generate broadband spectra perceived as "white light"
(specifically, D65 illumination). To ensure imperceptibility, we employ
spectral modulation rather than intensity modulation. Unlike conventional
visible light communication, our approach enables watermark extraction at
standard low frame rates (30-60 fps). While the information transfer rate is
modest-embedding 128 bits within a 10-second video clip-this capacity is
sufficient for essential metadata supporting privacy protection and content
verification.

</details>


### [140] [KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation](https://arxiv.org/abs/2510.17137)
*WenBo Xu,Liu Liu,Li Zhang,Ran Zhang,Hao Wu,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: KineDiff3D：基于运动学感知扩散的类别级铰接物体形状重建与生成框架，从单视角输入实现铰接物体重建和姿态估计


<details>
  <summary>Details</summary>
Motivation: 铰接物体（如笔记本电脑、抽屉）由于多部件几何结构和可变关节配置，在3D重建和姿态估计方面存在挑战，这些结构多样性在不同状态下引入复杂性

Method: 1. 使用运动学感知VAE将完整几何、关节角度和部件分割编码到结构化潜空间；2. 采用两个条件扩散模型：一个回归全局姿态和关节参数，另一个从部分观测生成运动学感知潜码；3. 设计迭代优化模块，通过Chamfer距离最小化双向优化重建精度和运动学参数，同时保持铰接约束

Result: 在合成、半合成和真实世界数据集上的实验结果表明，该方法在准确重建铰接物体和估计其运动学特性方面具有有效性

Conclusion: KineDiff3D提供了一个统一的框架，能够有效处理铰接物体的形状重建和姿态估计问题，解决了由于结构多样性带来的挑战

Abstract: Articulated objects, such as laptops and drawers, exhibit significant
challenges for 3D reconstruction and pose estimation due to their multi-part
geometries and variable joint configurations, which introduce structural
diversity across different states. To address these challenges, we propose
KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object
Shape Reconstruction and Generation, a unified framework for reconstructing
diverse articulated instances and pose estimation from single view input.
Specifically, we first encode complete geometry (SDFs), joint angles, and part
segmentation into a structured latent space via a novel Kinematic-Aware VAE
(KA-VAE). In addition, we employ two conditional diffusion models: one for
regressing global pose (SE(3)) and joint parameters, and another for generating
the kinematic-aware latent code from partial observations. Finally, we produce
an iterative optimization module that bidirectionally refines reconstruction
accuracy and kinematic parameters via Chamfer-distance minimization while
preserving articulation constraints. Experimental results on synthetic,
semi-synthetic, and real-world datasets demonstrate the effectiveness of our
approach in accurately reconstructing articulated objects and estimating their
kinematic properties.

</details>


### [141] [Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition](https://arxiv.org/abs/2510.17169)
*Roland Croft,Brian Du,Darcy Joseph,Sharath Kumar*

Main category: cs.CV

TL;DR: 该论文研究了人脸识别系统中预处理步骤对对抗攻击可迁移性的影响，发现人脸检测模型的选择会显著降低攻击成功率，并提出了一种预处理不变的方法来提高攻击可迁移性。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统中的预处理步骤（如人脸检测和图像下采样）在对抗攻击研究中经常被忽视，特别是在黑盒设置中。本研究旨在探究不同预处理技术对对抗攻击可迁移性的影响。

Method: 研究了多种现成的最先进对抗攻击方法在不同预处理技术下的可迁移性，包括评估人脸检测模型和插值方法的影响，并提出了一种使用输入变换的预处理不变方法来提高攻击可迁移性。

Result: 发现人脸检测模型的选择可使攻击成功率降低高达78%，而下采样中的插值方法选择影响相对较小。提出的预处理不变方法可将所研究攻击的可迁移性提高高达27%。

Conclusion: 预处理在人脸识别系统中至关重要，需要考虑预处理因素来提高面部对抗样本的对抗泛化能力。

Abstract: Face Recognition (FR) models have been shown to be vulnerable to adversarial
examples that subtly alter benign facial images, exposing blind spots in these
systems, as well as protecting user privacy. End-to-end FR systems first obtain
preprocessed faces from diverse facial imagery prior to computing the
similarity of the deep feature embeddings. Whilst face preprocessing is a
critical component of FR systems, and hence adversarial attacks against them,
we observe that this preprocessing is often overlooked in blackbox settings.
Our study seeks to investigate the transferability of several out-of-the-box
state-of-the-art adversarial attacks against FR when applied against different
preprocessing techniques used in a blackbox setting. We observe that the choice
of face detection model can degrade the attack success rate by up to 78%,
whereas choice of interpolation method during downsampling has relatively
minimal impacts. Furthermore, we find that the requirement for facial
preprocessing even degrades attack strength in a whitebox setting, due to the
unintended interaction of produced noise vectors against face detection models.
Based on these findings, we propose a preprocessing-invariant method using
input transformations that improves the transferability of the studied attacks
by up to 27%. Our findings highlight the importance of preprocessing in FR
systems, and the need for its consideration towards improving the adversarial
generalisation of facial adversarial examples.

</details>


### [142] [Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling](https://arxiv.org/abs/2510.17171)
*Feihong Yan,Peiru Wang,Yao Zhu,Kaiyu Pang,Qingyan Wei,Huiqi Li,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出了GtR（Generation then Reconstruction）训练免费的分层采样策略，将图像生成分解为结构生成和细节重建两个阶段，通过快速计算重建阶段实现加速，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决Masked Autoregressive模型在视觉生成中并行生成能力受限于空间相关视觉标记建模复杂性的问题，实现更高效的加速。

Method: 采用两阶段方法：结构生成建立全局语义框架，细节重建高效完成剩余标记。结合频率加权标记选择（FTS）为图像细节分配更多计算资源。

Result: 在ImageNet类条件生成和文本到图像生成任务中实现3.72倍加速，同时保持可比质量（FID：1.59，IS：304.4 vs 原始1.59，299.1）。

Conclusion: GtR方法在各种模型规模和生成任务中显著优于现有加速方法，实现了高质量图像生成的高效加速。

Abstract: Masked Autoregressive (MAR) models promise better efficiency in visual
generation than autoregressive (AR) models for the ability of parallel
generation, yet their acceleration potential remains constrained by the
modeling complexity of spatially correlated visual tokens in a single step. To
address this limitation, we introduce Generation then Reconstruction (GtR), a
training-free hierarchical sampling strategy that decomposes generation into
two stages: structure generation establishing global semantic scaffolding,
followed by detail reconstruction efficiently completing remaining tokens.
Assuming that it is more difficult to create an image from scratch than to
complement images based on a basic image framework, GtR is designed to achieve
acceleration by computing the reconstruction stage quickly while maintaining
the generation quality by computing the generation stage slowly. Moreover,
observing that tokens on the details of an image often carry more semantic
information than tokens in the salient regions, we further propose
Frequency-Weighted Token Selection (FTS) to offer more computation budget to
tokens on image details, which are localized based on the energy of high
frequency information. Extensive experiments on ImageNet class-conditional and
text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining
comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),
substantially outperforming existing acceleration methods across various model
scales and generation tasks. Our codes will be released in
https://github.com/feihongyan1/GtR.

</details>


### [143] [Capturing Head Avatar with Hand Contacts from a Monocular Video](https://arxiv.org/abs/2510.17181)
*Haonan He,Yufeng Zheng,Jie Song*

Main category: cs.CV

TL;DR: 提出了一种联合学习详细头部化身和手-脸交互引起的非刚性变形的新框架，解决了现有方法忽略自然手-脸交互的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注面部区域，忽略了传达认知状态的自然手-脸交互（如手托下巴、手指轻触脸颊等），这些交互对逼真的3D头部化身至关重要。

Method: 结合深度顺序损失和接触正则化进行姿态跟踪；从手-脸交互数据集中学习手引起面部变形的PCA基；引入基于物理模拟的接触损失减少穿插伪影。

Result: 在iPhone拍摄的RGB(D)视频上评估，并构建合成数据集。相比最先进的表面重建方法，能够捕捉更好的外观和更准确的面部变形几何。

Conclusion: 该方法能够有效捕捉手-脸交互引起的面部变形，生成物理上更合理、外观更逼真的3D头部化身。

Abstract: Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.
However, most methods focus solely on facial regions, ignoring natural
hand-face interactions, such as a hand resting on the chin or fingers gently
touching the cheek, which convey cognitive states like pondering. In this work,
we present a novel framework that jointly learns detailed head avatars and the
non-rigid deformations induced by hand-face interactions.
  There are two principal challenges in this task. First, naively tracking hand
and face separately fails to capture their relative poses. To overcome this, we
propose to combine depth order loss with contact regularization during pose
tracking, ensuring correct spatial relationships between the face and hand.
Second, no publicly available priors exist for hand-induced deformations,
making them non-trivial to learn from monocular videos. To address this, we
learn a PCA basis specific to hand-induced facial deformations from a face-hand
interaction dataset. This reduces the problem to estimating a compact set of
PCA parameters rather than a full spatial deformation field. Furthermore,
inspired by physics-based simulation, we incorporate a contact loss that
provides additional supervision, significantly reducing interpenetration
artifacts and enhancing the physical plausibility of the results.
  We evaluate our approach on RGB(D) videos captured by an iPhone.
Additionally, to better evaluate the reconstructed geometry, we construct a
synthetic dataset of avatars with various types of hand interactions. We show
that our method can capture better appearance and more accurate deforming
geometry of the face than SOTA surface reconstruction methods.

</details>


### [144] [HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery](https://arxiv.org/abs/2510.17188)
*Vaibhav Rathore,Divyam Gupta,Biplab Banerjee*

Main category: cs.CV

TL;DR: HIDISC是一个双曲表示学习框架，用于解决领域泛化与广义类别发现问题，无需情景模拟训练，通过GPT引导的扩散增强和切线空间插值实现高效领域和类别级泛化。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法假设训练和测试数据来自同一领域，限制了在开放世界场景中的适用性。DG-GCD要求模型泛化到包含新类别的未见领域，但现有方法DG2CD-Net依赖计算成本高的情景训练。

Method: 使用GPT引导的扩散进行源域增强，引入切线CutMix进行曲率感知插值，结合惩罚Busemann对齐、混合双曲对比正则化和自适应离群排斥的统一损失函数，以及可学习的曲率参数。

Result: 在PACS、Office-Home和DomainNet数据集上达到最先进水平，持续优于现有的欧几里得和双曲(DG)-GCD基线方法。

Conclusion: HIDISC通过双曲表示学习有效解决了领域泛化与广义类别发现问题，无需昂贵的情景模拟，实现了高效的领域和语义泛化。

Abstract: Generalized Category Discovery (GCD) aims to classify test-time samples into
either seen categories** -- available during training -- or novel ones, without
relying on label supervision. Most existing GCD methods assume simultaneous
access to labeled and unlabeled data during training and arising from the same
domain, limiting applicability in open-world scenarios involving distribution
shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by
requiring models to generalize to unseen domains containing novel categories,
without accessing targetdomain data during training. The only prior DG-GCD
method, DG2CD-Net, relies on episodic training with multiple synthetic domains
and task vector aggregation, incurring high computational cost and error
accumulation. We propose HIDISC, a hyperbolic representation learning framework
that achieves domain and category-level generalization without episodic
simulation. To expose the model to minimal but diverse domain variations, we
augment the source domain using GPT-guided diffusion, avoiding overfitting
while maintaining efficiency. To structure the representation space, we
introduce Tangent CutMix, a curvature-aware interpolation that synthesizes
pseudo-novel samples in tangent space, preserving manifold consistency. A
unified loss -- combining penalized Busemann alignment, hybrid hyperbolic
contrastive regularization, and adaptive outlier repulsion -- **facilitates
compact, semantically structured embeddings. A learnable curvature parameter
further adapts the geometry to dataset complexity. HIDISC achieves
state-of-the-art results on PACS , Office-Home , and DomainNet, consistently
outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.

</details>


### [145] [EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification](https://arxiv.org/abs/2510.17200)
*Bingrong Liu,Jun Shi,Yushan Zheng*

Main category: cs.CV

TL;DR: EndoCIL是一个专门为内窥镜图像诊断设计的类增量学习框架，通过三个关键组件解决领域差异和类别不平衡问题，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像分析需要模型能够持续适应新的临床数据，同时保持对已学习类别的性能。现有重放方法由于内窥镜图像中严重的领域差异和类别不平衡，无法有效缓解灾难性遗忘。

Method: EndoCIL包含三个核心组件：基于最大均值差异的重放策略（MDBR）选择多样代表性样本，先验正则化类别平衡损失（PRCBL）缓解类别不平衡，以及全连接梯度校准（CFG）减少对新类别的偏置。

Result: 在四个公共内窥镜数据集上的实验表明，EndoCIL在不同缓冲区大小和评估指标下普遍优于最先进的CIL方法，有效平衡了稳定性和可塑性。

Conclusion: 该框架在内窥镜终身诊断中有效平衡了稳定性和可塑性，显示出良好的临床可扩展性和部署潜力。

Abstract: Class-incremental learning (CIL) for endoscopic image analysis is crucial for
real-world clinical applications, where diagnostic models should continuously
adapt to evolving clinical data while retaining performance on previously
learned ones. However, existing replay-based CIL methods fail to effectively
mitigate catastrophic forgetting due to severe domain discrepancies and class
imbalance inherent in endoscopic imaging. To tackle these challenges, we
propose EndoCIL, a novel and unified CIL framework specifically tailored for
endoscopic image diagnosis. EndoCIL incorporates three key components: Maximum
Mean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy
strategy to select diverse and representative exemplars, Prior Regularized
Class Balanced Loss (PRCBL), designed to alleviate both inter-phase and
intra-phase class imbalance by integrating prior class distributions and
balance weights into the loss function, and Calibration of Fully-Connected
Gradients (CFG), which adjusts the classifier gradients to mitigate bias toward
new classes. Extensive experiments conducted on four public endoscopic datasets
demonstrate that EndoCIL generally outperforms state-of-the-art CIL methods
across varying buffer sizes and evaluation metrics. The proposed framework
effectively balances stability and plasticity in lifelong endoscopic diagnosis,
showing promising potential for clinical scalability and deployment.

</details>


### [146] [Optimizing DINOv2 with Registers for Face Anti-Spoofing](https://arxiv.org/abs/2510.17201)
*Mika Feng,Pierre Gallin-Martel,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 提出基于DINOv2的活体检测方法，通过注意力机制抑制扰动，有效区分真实人脸和欺骗攻击图像。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统易受欺骗攻击，恶意用户可能使用注册用户的照片绕过认证，需要在人脸识别前检测此类欺骗攻击。

Method: 使用带寄存器的DINOv2模型提取可泛化特征，通过抑制注意力机制中的扰动，聚焦于关键细微特征。

Result: 在ICCV2025第六届人脸反欺骗研讨会数据集和SiW数据集上的实验证明了该方法的有效性。

Conclusion: 基于DINOv2的欺骗攻击检测方法能够有效识别真实与欺骗人脸图像，提升人脸识别系统的安全性。

Abstract: Face recognition systems are designed to be robust against variations in head
pose, illumination, and image blur during capture. However, malicious actors
can exploit these systems by presenting a face photo of a registered user,
potentially bypassing the authentication process. Such spoofing attacks must be
detected prior to face recognition. In this paper, we propose a DINOv2-based
spoofing attack detection method to discern minute differences between live and
spoofed face images. Specifically, we employ DINOv2 with registers to extract
generalizable features and to suppress perturbations in the attention
mechanism, which enables focused attention on essential and minute features. We
demonstrate the effectiveness of the proposed method through experiments
conducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:
Unified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.

</details>


### [147] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan,Anhao Zhao,Jinlan Fu,Junlong Tong,Hui Su,Yijie Pan,Wei Zhang,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 该论文提出了VisiPruner框架，通过分析MLLMs的三阶段跨模态交互过程，实现了无需训练的视觉token剪枝，显著减少了计算开销。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉语言任务中表现出色，但由于注意力计算的二次增长导致计算开销巨大。现有方法缺乏对MLLMs如何处理和融合多模态信息的根本理解。

Method: 通过系统分析发现MLLMs的三阶段跨模态交互过程，并基于此提出VisiPruner训练免费剪枝框架，选择性地保留关键视觉token。

Result: 在LLaVA-v1.5 7B上减少了99%的视觉相关注意力计算和53.9%的FLOPs，显著优于现有token剪枝方法，并在多种MLLMs上具有良好泛化性。

Conclusion: 研究不仅提供了有效的剪枝方法，还为训练高效MLLMs提供了可操作的指导原则，即让模型架构与其内在的层级处理动态保持一致。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.

</details>


### [148] [Fair and Interpretable Deepfake Detection in Videos](https://arxiv.org/abs/2510.17264)
*Akihito Yoshii,Ryosuke Sonoda,Ramya Srinivasan*

Main category: cs.CV

TL;DR: 提出一个公平感知的深度伪造检测框架，整合时序特征学习和人口统计感知数据增强，以提高公平性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法存在偏见、缺乏透明度且无法捕捉时序信息，导致跨不同人口群体的决策偏见和不可靠结果。

Method: 使用时序特征学习进行深度伪造视频建模，结合概念提取提高检测可靠性；引入人口统计感知数据增强方法，平衡代表性不足群体并应用频域变换保留伪造伪影。

Result: 在FaceForensics++、DFD、Celeb-DF和DFDC数据集上的广泛实验表明，该方法在公平性和准确性之间实现了最佳权衡。

Conclusion: 所提出的框架能有效缓解偏见，提高检测的公平性和可解释性，在多个数据集上优于现有最先进方法。

Abstract: Existing deepfake detection methods often exhibit bias, lack transparency,
and fail to capture temporal information, leading to biased decisions and
unreliable results across different demographic groups. In this paper, we
propose a fairness-aware deepfake detection framework that integrates temporal
feature learning and demographic-aware data augmentation to enhance fairness
and interpretability. Our method leverages sequence-based clustering for
temporal modeling of deepfake videos and concept extraction to improve
detection reliability while also facilitating interpretable decisions for
non-expert users. Additionally, we introduce a demography-aware data
augmentation method that balances underrepresented groups and applies
frequency-domain transformations to preserve deepfake artifacts, thereby
mitigating bias and improving generalization. Extensive experiments on
FaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)
architectures (Xception, ResNet) demonstrate the efficacy of the proposed
method in obtaining the best tradeoff between fairness and accuracy when
compared to SoTA.

</details>


### [149] [Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models](https://arxiv.org/abs/2510.17274)
*Katie Luo,Jingwei Ji,Tong He,Runsheng Xu,Yichen Xie,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: PnF是一种即插即用的方法，通过多模态大语言模型增强现有运动预测模型，利用自然语言描述复杂场景，无需微调即可显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统依赖专用模型进行感知和运动预测，在标准条件下表现可靠，但难以经济高效地泛化到多样化现实场景。

Method: 设计提示从MLLMs提取结构化场景理解，将这些信息提炼为可学习嵌入来增强现有行为预测模型，利用MLLMs的零样本推理能力。

Result: 在Waymo Open Motion Dataset和nuScenes Dataset上验证，两个最先进的运动预测模型均获得一致的性能提升。

Conclusion: PnF方法通过自然语言描述复杂场景，实现了运动预测性能的显著改进，且无需微调，具有实用价值。

Abstract: Current autonomous driving systems rely on specialized models for perceiving
and predicting motion, which demonstrate reliable performance in standard
conditions. However, generalizing cost-effectively to diverse real-world
scenarios remains a significant challenge. To address this, we propose
Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion
forecasting models with multimodal large language models (MLLMs). PnF builds on
the insight that natural language provides a more effective way to describe and
handle complex scenarios, enabling quick adaptation to targeted behaviors. We
design prompts to extract structured scene understanding from MLLMs and distill
this information into learnable embeddings to augment existing behavior
prediction models. Our method leverages the zero-shot reasoning capabilities of
MLLMs to achieve significant improvements in motion prediction performance,
while requiring no fine-tuning -- making it practical to adopt. We validate our
approach on two state-of-the-art motion forecasting models using the Waymo Open
Motion Dataset and the nuScenes Dataset, demonstrating consistent performance
improvements across both benchmarks.

</details>


### [150] [SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation](https://arxiv.org/abs/2510.17278)
*Mehdi Zekriyapanah Gashti,Mostafa Mohammadpour,Ghasem Farjamnia*

Main category: cs.CV

TL;DR: 提出SG-CLDFF框架，通过显著性引导预处理和多尺度深度特征融合，提高白细胞分割和分类的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 白细胞显微图像分割和分类对血液疾病诊断至关重要，但由于染色变异、复杂背景和类别不平衡等问题而具有挑战性。

Method: 使用显著性先验突出白细胞区域，采用轻量级混合骨干网络生成多分辨率表示，通过跨层融合模块整合浅层和深层特征，多任务训练结合加权损失和显著性对齐正则化。

Result: 在标准公开数据集上相比CNN和Transformer基线模型，在IoU、F1和分类准确率方面取得一致提升。

Conclusion: SG-CLDFF为临床工作流程中的白细胞分析提供了实用且可解释的路径。

Abstract: Accurate segmentation and classification of white blood cells (WBCs) in
microscopic images are essential for diagnosis and monitoring of many
hematological disorders, yet remain challenging due to staining variability,
complex backgrounds, and class imbalance. In this paper, we introduce a novel
Saliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that
tightly integrates saliency-driven preprocessing with multi-scale deep feature
aggregation to improve both robustness and interpretability for WBC analysis.
SG-CLDFF first computes saliency priors to highlight candidate WBC regions and
guide subsequent feature extraction. A lightweight hybrid backbone
(EfficientSwin-style) produces multi-resolution representations, which are
fused by a ResNeXt-CC-inspired cross-layer fusion module to preserve
complementary information from shallow and deep layers. The network is trained
in a multi-task setup with concurrent segmentation and cell-type classification
heads, using class-aware weighted losses and saliency-alignment regularization
to mitigate imbalance and suppress background activation. Interpretability is
enforced through Grad-CAM visualizations and saliency consistency checks,
allowing model decisions to be inspected at the regional level. We validate the
framework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting
consistent gains in IoU, F1, and classification accuracy compared to strong CNN
and transformer baselines. An ablation study also demonstrates the individual
contributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers
a practical and explainable path toward more reliable automated WBC analysis in
clinical workflows.

</details>


### [151] [Machine Vision-Based Surgical Lighting System:Design and Implementation](https://arxiv.org/abs/2510.17287)
*Amir Gharghabi,Mahdi Hakiminezhad,Maryam Shafaei,Shaghayegh Gharghabi*

Main category: cs.CV

TL;DR: 提出基于YOLOv11目标检测算法的新型手术照明系统，通过识别蓝色标记自动调整LED光源位置，减少外科医生疲劳并改善照明一致性。


<details>
  <summary>Details</summary>
Motivation: 传统手术照明系统依赖手动调整，导致外科医生疲劳、颈部劳损以及由于漂移和阴影造成的不一致照明问题。

Method: 使用YOLOv11算法识别手术部位上方的蓝色标记，通过两个配备倾斜-平移支架的伺服电机将高功率LED光源引导至识别位置。

Result: YOLO模型在验证集上达到96.7%的mAP@50，验证集由带有蓝色球形标记的模拟手术场景注释图像组成。

Conclusion: 这种基于机器视觉的自动化照明解决方案减少了外科医生的身体负担，提高了照明一致性，有助于改善手术效果。

Abstract: Effortless and ergonomically designed surgical lighting is critical for
precision and safety during procedures. However, traditional systems often rely
on manual adjustments, leading to surgeon fatigue, neck strain, and
inconsistent illumination due to drift and shadowing. To address these
challenges, we propose a novel surgical lighting system that leverages the
YOLOv11 object detection algorithm to identify a blue marker placed above the
target surgical site. A high-power LED light source is then directed to the
identified location using two servomotors equipped with tilt-pan brackets. The
YOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated
images simulating surgical scenes with the blue spherical marker. By automating
the lighting process, this machine vision-based solution reduces physical
strain on surgeons, improves consistency in illumination, and supports improved
surgical outcomes.

</details>


### [152] [Exploring Structural Degradation in Dense Representations for Self-supervised Learning](https://arxiv.org/abs/2510.17299)
*Siran Dai,Qianqian Xu,Peisong Wen,Yang Liu,Qingming Huang*

Main category: cs.CV

TL;DR: 本文发现自监督学习中存在一个反直觉现象：训练时间过长会损害密集预测任务的性能（称为SDD），并提出DSE指标来解决无标注情况下的模型选择问题。


<details>
  <summary>Details</summary>
Motivation: 观察到自监督学习中训练时间过长反而会降低密集预测任务性能的反常现象，需要开发无标注情况下评估密集表示质量的方法。

Method: 提出Dense representation Structure Estimator (DSE)，包含类相关性度量和有效维度度量，用于评估密集表示质量，并基于此提出模型选择策略和正则化方法。

Result: 在16种自监督方法和4个基准测试上验证，模型选择策略平均提升mIoU 3.0%，DSE正则化能有效缓解密集退化问题。

Conclusion: DSE指标能有效评估自监督学习中的密集表示质量，提出的方法能显著改善密集预测任务的性能。

Abstract: In this work, we observe a counterintuitive phenomenon in self-supervised
learning (SSL): longer training may impair the performance of dense prediction
tasks (e.g., semantic segmentation). We refer to this phenomenon as
Self-supervised Dense Degradation (SDD) and demonstrate its consistent presence
across sixteen state-of-the-art SSL methods with various losses, architectures,
and datasets. When the model performs suboptimally on dense tasks at the end of
training, measuring the performance during training becomes essential. However,
evaluating dense performance effectively without annotations remains an open
challenge. To tackle this issue, we introduce a Dense representation Structure
Estimator (DSE), composed of a class-relevance measure and an effective
dimensionality measure. The proposed DSE is both theoretically grounded and
empirically validated to be closely correlated with the downstream performance.
Based on this metric, we introduce a straightforward yet effective model
selection strategy and a DSE-based regularization method. Experiments on
sixteen SSL methods across four benchmarks confirm that model selection
improves mIoU by $3.0\%$ on average with negligible computational cost.
Additionally, DSE regularization consistently mitigates the effects of dense
degradation. Code is available at
https://github.com/EldercatSAM/SSL-Degradation.

</details>


### [153] [LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](https://arxiv.org/abs/2510.17305)
*ZhaoYang Han,Qihan Lin,Hao Liang,Bowen Chen,Zhou Liu,Wentao Zhang*

Main category: cs.CV

TL;DR: LongInsightBench是首个专注于长视频理解的基准测试，整合视觉、音频和文本多模态，包含约1000个信息密集的长视频和6个挑战性任务场景，揭示了全模态模型在时间定位和长程因果推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门评估模型对长视频理解能力的基准测试，特别是需要整合多模态信息（视觉、音频、文本）来理解人类语言、观点、行为等上下文元素的任务。

Method: 从FineVideo数据集中精选约1000个长视频，设计6个挑战性任务场景（包括事件内和事件间任务），并开发三步半自动化数据质量保证流程确保问题难度和有效性。

Result: 实验结果显示全模态模型在需要精确时间定位（T-Loc）和长程因果推理（CE-Caus）的任务中表现不佳，扩展实验揭示了多模态融合中的信息丢失和处理偏差问题。

Conclusion: LongInsightBench为评估长视频理解能力提供了有效基准，揭示了当前全模态模型在多模态融合和复杂推理任务中的局限性，为未来研究指明了改进方向。

Abstract: We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models' ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.

</details>


### [154] [CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference](https://arxiv.org/abs/2510.17318)
*Sangyoon Bae,Jiook Cha*

Main category: cs.CV

TL;DR: CausalMamba是一个可扩展的fMRI因果推断框架，通过两阶段方法解决BOLD信号失真和计算复杂性问题，在模拟数据上比DCM准确率高37%，在真实数据中能恢复88%的已知神经通路。


<details>
  <summary>Details</summary>
Motivation: 解决fMRI因果推断中的根本限制：从血氧动力学失真的BOLD信号推断神经因果性的不适定问题，以及现有方法（如动态因果建模DCM）的计算不可行性。

Method: 将复杂逆问题分解为两个可处理阶段：BOLD反卷积恢复潜在神经活动，然后使用新颖的条件Mamba架构进行因果图推断。

Result: 在模拟数据上比DCM准确率高37%；在真实任务fMRI数据中恢复88%的已知神经通路，而传统方法在99%以上的被试中无法识别这些典型回路；工作记忆数据分析显示大脑会根据刺激策略性地转移主要因果枢纽。

Conclusion: 为神经科学家提供了实用的工具，能够进行大规模因果推断，捕捉认知功能背后的基本回路模式和灵活网络动态。

Abstract: We introduce CausalMamba, a scalable framework that addresses fundamental
limitations in fMRI-based causal inference: the ill-posed nature of inferring
neural causality from hemodynamically distorted BOLD signals and the
computational intractability of existing methods like Dynamic Causal Modeling
(DCM). Our approach decomposes this complex inverse problem into two tractable
stages: BOLD deconvolution to recover latent neural activity, followed by
causal graph inference using a novel Conditional Mamba architecture. On
simulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,
when applied to real task fMRI data, our method recovers well-established
neural pathways with 88% fidelity, whereas conventional approaches fail to
identify these canonical circuits in over 99% of subjects. Furthermore, our
network analysis of working memory data reveals that the brain strategically
shifts its primary causal hub-recruiting executive or salience networks
depending on the stimulus-a sophisticated reconfiguration that remains
undetected by traditional methods. This work provides neuroscientists with a
practical tool for large-scale causal inference that captures both fundamental
circuit motifs and flexible network dynamics underlying cognitive function.

</details>


### [155] [A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World](https://arxiv.org/abs/2510.17322)
*Wei Zhang,Zhanhao Hu,Xiao Li,Xiaopei Zhu,Xiaolin Hu*

Main category: cs.CV

TL;DR: 本文评估了现有对抗性防御方法对大型对抗性衣物攻击的有效性，发现这些防御方法在面对覆盖人体大部分区域的对抗性衣物时表现不佳，揭示了现有防御系统的共同脆弱性。


<details>
  <summary>Details</summary>
Motivation: 实验发现单纯增大对抗性补丁的尺寸就能使现有防御方法失效，这促使研究者评估各种防御方法对抗覆盖人体大部分区域的对抗性衣物的效果，因为对抗性衣物不仅尺寸大，而且比人体上的大补丁看起来更自然。

Method: 通过制作对抗性衣物来测试多种防御方法，在数字世界和物理世界中进行实验，并针对Faster R-CNN制作了一套能突破多种防御方法的对抗性衣物。

Result: 实验显示所有防御方法在对抗对抗性衣物时表现都很差。制作的一套对抗性衣物在物理世界中，对未防御的检测器攻击成功率达到96.06%，对九个防御模型的攻击成功率均超过64.84%。

Conclusion: 现有对抗性防御方法在面对对抗性衣物时存在共同脆弱性，表明当前防御系统在面对大型、自然的物理世界攻击时仍不够鲁棒。

Abstract: In recent years, adversarial attacks against deep learning-based object
detectors in the physical world have attracted much attention. To defend
against these attacks, researchers have proposed various defense methods
against adversarial patches, a typical form of physically-realizable attack.
However, our experiments showed that simply enlarging the patch size could make
these defense methods fail. Motivated by this, we evaluated various defense
methods against adversarial clothes which have large coverage over the human
body. Adversarial clothes provide a good test case for adversarial defense
against patch-based attacks because they not only have large sizes but also
look more natural than a large patch on humans. Experiments show that all the
defense methods had poor performance against adversarial clothes in both the
digital world and the physical world. In addition, we crafted a single set of
clothes that broke multiple defense methods on Faster R-CNN. The set achieved
an Attack Success Rate (ASR) of 96.06% against the undefended detector and over
64.84% ASRs against nine defended models in the physical world, unveiling the
common vulnerability of existing adversarial defense methods against
adversarial clothes. Code is available at:
https://github.com/weiz0823/adv-clothes-break-multiple-defenses.

</details>


### [156] [iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA](https://arxiv.org/abs/2510.17332)
*Zhaoran Zhao,Xinli Yue,Jianhui Sun,Yuhao Xie,Tao Shao,Liangchao Yao,Fan Xia,Yuetang Deng*

Main category: cs.CV

TL;DR: 提出了iDETEX，一个统一的多模态大语言模型，能够同时执行质量定位、感知和描述三个关键任务，在ViDA-UGC基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决图像质量评估从标量质量预测向更可解释、与人类对齐的评估范式发展的挑战，实现详细且可解释的图像质量评估。

Method: 设计了一套任务特定的离线增强模块和数据混合策略，辅以在线增强策略充分利用多源监督，构建统一的多模态大语言模型。

Result: 在ViDA-UGC基准测试中所有子任务上实现最先进性能，在ICCV MIPI 2025详细图像质量评估挑战赛中排名第一。

Conclusion: iDETEX模型在提供准确且可解释的质量评估方面表现出有效性和鲁棒性，为详细图像质量评估提供了统一的解决方案。

Abstract: Image Quality Assessment (IQA) has progressed from scalar quality prediction
to more interpretable, human-aligned evaluation paradigms. In this work, we
address the emerging challenge of detailed and explainable IQA by proposing
iDETEX-a unified multimodal large language model (MLLM) capable of
simultaneously performing three key tasks: quality grounding, perception, and
description. To facilitate efficient and generalizable training across these
heterogeneous subtasks, we design a suite of task-specific offline augmentation
modules and a data mixing strategy. These are further complemented by online
enhancement strategies to fully exploit multi-sourced supervision. We validate
our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves
state-of-the-art performance across all subtasks. Our model ranks first in the
ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its
effectiveness and robustness in delivering accurate and interpretable quality
assessments.

</details>


### [157] [Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition](https://arxiv.org/abs/2510.17338)
*Jiahao Huo,Mufhumudzi Muthivhi,Terence L. van Zyl,Fredrik Gustafsson*

Main category: cs.CV

TL;DR: 提出了一种后处理开放集识别方法，通过比较模型特征和预测logits之间的一致性来检测未知类别，无需重新训练预训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有野生动物分类模型在遇到未知类别时会产生过度自信的预测，而许多开放集识别方法需要重新训练模型，这限制了实际应用。

Method: 使用基于输入到最近类别均值(NCM)距离的概率分布，并将其与softmax概率进行比较，以衡量NCM和分类头之间的一致性。

Result: 在两个数据集上均表现优异，AUROC分别达到93.41和95.35，性能稳定且优于现有方法。

Conclusion: 提出的后处理方法在开放集识别任务中表现一致且有效，无需模型重训练，具有实际应用价值。

Abstract: Current state-of-the-art Wildlife classification models are trained under the
closed world setting. When exposed to unknown classes, they remain
overconfident in their predictions. Open-set Recognition (OSR) aims to classify
known classes while rejecting unknown samples. Several OSR methods have been
proposed to model the closed-set distribution by observing the feature, logit,
or softmax probability space. A significant drawback of many existing
approaches is the requirement to retrain the pre-trained classification model
with the OSR-specific strategy. This study contributes a post-processing OSR
method that measures the agreement between the models' features and predicted
logits. We propose a probability distribution based on an input's distance to
its Nearest Class Mean (NCM). The NCM-based distribution is then compared with
the softmax probabilities from the logit space to measure agreement between the
NCM and the classification head. Our proposed strategy ranks within the top
three on two evaluated datasets, showing consistent performance across the two
datasets. In contrast, current state-of-the-art methods excel on a single
dataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish
animals. The code can be found
https://github.com/Applied-Representation-Learning-Lab/OSR.

</details>


### [158] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu,Shengpeng Xu,Yunbo Jia,Edmund Y. Lam*

Main category: cs.CV

TL;DR: 提出Semantic-E2VID框架，通过跨模态特征对齐和语义感知特征融合，将视觉语义知识从帧模态迁移到事件模态，显著提升事件到视频重建的质量。


<details>
  <summary>Details</summary>
Motivation: 事件相机仅捕获强度变化，忽略静态物体和背景，导致事件模态缺乏语义信息。现有E2V方法往往忽视语义信息在视频重建中的重要作用。

Method: 1) 跨模态特征对齐模块将SAM模型的视觉语义迁移到事件编码器；2) 语义感知特征融合块整合学习到的语义特征；3) 语义感知E2V监督利用SAM生成的类别标签帮助重建语义细节。

Result: 在多个基准测试中显著提升帧质量，优于现有最先进的E2V方法。

Conclusion: Semantic-E2VID通过有效利用视觉语义知识，成功解决了事件到视频重建中语义信息缺失的问题，为事件视觉任务提供了新的解决方案。

Abstract: Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.

</details>


### [159] [M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception](https://arxiv.org/abs/2510.17363)
*U. V. B. L Udugama,George Vosselman,Francesco Nex*

Main category: cs.CV

TL;DR: M2H是一个多任务学习框架，用于从单目图像同时进行语义分割、深度、边缘和表面法线估计，通过窗口跨任务注意力模块实现高效特征交换，在保持计算效率的同时提升多任务性能。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署实时空间感知需要高效的多任务模型，能够利用互补任务信息同时最小化计算开销。

Method: 提出Window-Based Cross-Task Attention Module，在轻量级ViT-based DINOv2骨干网络上实现结构化特征交换，保留任务特定细节。

Result: 在NYUDv2数据集上超越最先进的多任务模型，在Hypersim上超过单任务深度和语义基线，在Cityscapes上表现优异，同时在笔记本电脑硬件上保持计算效率。

Conclusion: M2H为动态环境中的3D场景图构建提供了实用的单目空间感知系统基础，并在真实世界数据上验证了其实用性。

Abstract: Deploying real-time spatial perception on edge devices requires efficient
multi-task models that leverage complementary task information while minimizing
computational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel
multi-task learning framework designed for semantic segmentation and depth,
edge, and surface normal estimation from a single monocular image. Unlike
conventional approaches that rely on independent single-task models or shared
encoder-decoder architectures, M2H introduces a Window-Based Cross-Task
Attention Module that enables structured feature exchange while preserving
task-specific details, improving prediction consistency across tasks. Built on
a lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time
deployment and serves as the foundation for monocular spatial perception
systems supporting 3D scene graph construction in dynamic environments.
Comprehensive evaluations show that M2H outperforms state-of-the-art multi-task
models on NYUDv2, surpasses single-task depth and semantic baselines on
Hypersim, and achieves superior performance on the Cityscapes dataset, all
while maintaining computational efficiency on laptop hardware. Beyond
benchmarks, M2H is validated on real-world data, demonstrating its practicality
in spatial perception tasks.

</details>


### [160] [Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)
*Vaggelis Dorovatas,Soroush Seifi,Gunshi Gupta,Rahaf Aljundi*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，通过LLM引导的视觉token选择、递归处理和基于字幕的问答，实现视频流的高效处理，在保持性能的同时大幅减少计算量。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在处理流式长视频时面临效率挑战，需要在线处理小时级视频并及时响应查询，但现有方法难以平衡效率与准确性。

Method: 1) LLM引导的视觉token选择，基于注意力机制丢弃95%不重要的token；2) 对过去选定token进行递归处理，生成时序一致的理解；3) 基于字幕的轻量级问答机制。

Result: 在流式视频基准测试中达到最先进性能，在保持最小性能损失的同时显著提升处理效率。

Conclusion: 该方法为视频流处理提供了高效解决方案，在效率和效果之间取得了良好平衡，且与标准Video-LLMs兼容。

Abstract: Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.

</details>


### [161] [Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise](https://arxiv.org/abs/2510.17372)
*Paweł Borsukiewicz,Fadi Boutros,Iyiola E. Olatunji,Charles Beumier,Wendkûuni C. Ouedraogo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 该研究通过系统评估25个合成人脸识别数据集，证明合成人脸数据可以达到95.67%的识别准确率，超越真实数据集，为隐私保护的人脸识别提供了可行的技术方案。


<details>
  <summary>Details</summary>
Motivation: 解决人脸识别系统中使用真实人脸数据带来的隐私和伦理问题，如未经同意收集数据、GDPR合规风险等，探索合成数据作为隐私保护替代方案的可行性。

Method: 采用系统文献回顾识别25个合成人脸识别数据集，结合实验验证，评估7个关键隐私保护要求：身份泄露预防、类内变异性、身份可分性、数据集规模、伦理数据来源、偏见缓解和基准可靠性。

Result: 最佳合成数据集VariFace和VIGFace分别达到95.67%和94.91%的识别准确率，超过真实数据集CASIA-WebFace(94.70%)。合成数据在保持身份可分性的同时确保适当的类内变异性，且提供前所未有的偏见控制能力。

Conclusion: 合成人脸数据在科学上是可行的，在伦理上是必要的，可以作为人脸识别研究中真实数据集的替代方案，同时提供更好的隐私保护和偏见控制能力。

Abstract: The deployment of facial recognition systems has created an ethical dilemma:
achieving high accuracy requires massive datasets of real faces collected
without consent, leading to dataset retractions and potential legal liabilities
under regulations like GDPR. While synthetic facial data presents a promising
privacy-preserving alternative, the field lacks comprehensive empirical
evidence of its viability. This study addresses this critical gap through
extensive evaluation of synthetic facial recognition datasets. We present a
systematic literature review identifying 25 synthetic facial recognition
datasets (2018-2025), combined with rigorous experimental validation. Our
methodology examines seven key requirements for privacy-preserving synthetic
data: identity leakage prevention, intra-class variability, identity
separability, dataset scale, ethical data sourcing, bias mitigation, and
benchmark reliability. Through experiments involving over 10 million synthetic
samples, extended by a comparison of results reported on five standard
benchmarks, we provide the first comprehensive empirical assessment of
synthetic data's capability to replace real datasets. Best-performing synthetic
datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and
94.91% respectively, surpassing established real datasets including
CASIA-WebFace (94.70%). While those images remain private, publicly available
alternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our
findings reveal that they ensure proper intra-class variability while
maintaining identity separability. Demographic bias analysis shows that, even
though synthetic data inherits limited biases, it offers unprecedented control
for bias mitigation through generation parameters. These results establish
synthetic facial data as a scientifically viable and ethically imperative
alternative for facial recognition research.

</details>


### [162] [Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing](https://arxiv.org/abs/2510.17373)
*Yintao Zhou,Wei Huang,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: 提出了一种基于多面部表情特征融合的帕金森病严重程度诊断方法，通过注意力机制融合多种表情特征，并采用自适应类别平衡策略解决类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于面部表情的PD诊断方法依赖单一表情类型可能导致误诊，且忽视不同PD阶段的类别不平衡问题，大多数方法只进行二元分类而非严重程度诊断。

Method: 通过注意力机制融合多种面部表情特征，并采用自适应类别平衡策略动态调整训练样本的贡献度。

Result: 实验结果表明该方法在PD严重程度诊断方面表现优异，注意力特征融合和自适应类别平衡策略均有效。

Conclusion: 该方法为PD严重程度诊断提供了有效的解决方案，多表情特征融合和类别平衡策略显著提升了诊断性能。

Abstract: Parkinson's disease (PD) severity diagnosis is crucial for early detecting
potential patients and adopting tailored interventions. Diagnosing PD based on
facial expression is grounded in PD patients' "masked face" symptom and gains
growing interest recently for its convenience and affordability. However,
current facial expression-based approaches often rely on single type of
expression which can lead to misdiagnosis, and ignore the class imbalance
across different PD stages which degrades the prediction performance. Moreover,
most existing methods focus on binary classification (i.e., PD / non-PD) rather
than diagnosing the severity of PD. To address these issues, we propose a new
facial expression-based method for PD severity diagnosis which integrates
multiple facial expression features through attention-based feature fusion.
Moreover, we mitigate the class imbalance problem via an adaptive class
balancing strategy which dynamically adjusts the contribution of training
samples based on their class distribution and classification difficulty.
Experimental results demonstrate the promising performance of the proposed
method for PD severity diagnosis, as well as the efficacy of attention-based
feature fusion and adaptive class balancing.

</details>


### [163] [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://arxiv.org/abs/2510.17384)
*Jiajin Tang,Zhengxuan Wei,Ge Zheng,Sibei Yang*

Main category: cs.CV

TL;DR: LoopTrans是一个闭环框架，通过双向知识转移（从外中心到自我中心，再回传到外中心）来增强交互区域定位能力，解决了传统单向知识转移在复杂交互场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统弱监督交互区域定位方法仅从外中心图像单向转移到自我中心图像，限制了在复杂交互场景中的应用。人类能够通过观察他人与物体的交互来学习新的交互方式，这启发了双向知识转移的研究。

Method: 提出LoopTrans闭环框架，包含统一跨模态定位和去噪知识蒸馏机制，通过双向知识转移来弥合自我中心和外中心图像之间的领域差距。

Result: 在图像和视频基准测试中，LoopTrans在所有指标上都实现了持续改进，甚至能够处理人类身体完全遮挡物体交互区域的挑战性场景。

Conclusion: LoopTrans通过闭环双向知识转移框架，显著提升了交互区域定位的性能，证明了双向知识转移在复杂交互场景中的有效性。

Abstract: Humans can perform previously unexperienced interactions with novel objects
simply by observing others engage with them. Weakly-supervised affordance
grounding mimics this process by learning to locate object regions that enable
actions on egocentric images, using exocentric interaction images with
image-level annotations. However, extracting affordance knowledge solely from
exocentric images and transferring it one-way to egocentric images limits the
applicability of previous works in complex interaction scenarios. Instead, this
study introduces LoopTrans, a novel closed-loop framework that not only
transfers knowledge from exocentric to egocentric but also transfers back to
enhance exocentric knowledge extraction. Within LoopTrans, several innovative
mechanisms are introduced, including unified cross-modal localization and
denoising knowledge distillation, to bridge domain gaps between object-centered
egocentric and interaction-centered exocentric images while enhancing knowledge
transfer. Experiments show that LoopTrans achieves consistent improvements
across all metrics on image and video benchmarks, even handling challenging
scenarios where object interaction regions are fully occluded by the human
body.

</details>


### [164] [Monitoring Horses in Stalls: From Object to Event Detection](https://arxiv.org/abs/2510.17409)
*Dmitrii Galimzianov,Viacheslav Vyshegorodtsev,Ivan Nezhivykh*

Main category: cs.CV

TL;DR: 开发了一个基于视觉的马厩监控系统，使用YOLOv11和BoT-SORT自动检测和跟踪马匹与人，通过物体轨迹和空间关系推断事件状态，支持实时行为监测。


<details>
  <summary>Details</summary>
Motivation: 传统马匹行为监测方法劳动密集且耗时，需要自动化系统来早期发现健康与福利问题。

Method: 使用YOLOv11进行目标检测，BoT-SORT进行多目标跟踪，基于CLIP和GroundingDINO构建自定义数据集，通过物体轨迹和空间关系推断事件状态。

Result: 系统能区分五种事件类型并处理摄像头盲区，对马匹相关事件表现可靠，但人员检测因数据稀缺存在局限。

Conclusion: 该系统为马场实时行为监测提供了基础，对动物福利和厩舍管理具有重要意义。

Abstract: Monitoring the behavior of stalled horses is essential for early detection of
health and welfare issues but remains labor-intensive and time-consuming. In
this study, we present a prototype vision-based monitoring system that
automates the detection and tracking of horses and people inside stables using
object detection and multi-object tracking techniques. The system leverages
YOLOv11 and BoT-SORT for detection and tracking, while event states are
inferred based on object trajectories and spatial relations within the stall.
To support development, we constructed a custom dataset annotated with
assistance from foundation models CLIP and GroundingDINO. The system
distinguishes between five event types and accounts for the camera's blind
spots. Qualitative evaluation demonstrated reliable performance for
horse-related events, while highlighting limitations in detecting people due to
data scarcity. This work provides a foundation for real-time behavioral
monitoring in equine facilities, with implications for animal welfare and
stable management.

</details>


### [165] [DeepDetect: Learning All-in-One Dense Keypoints](https://arxiv.org/abs/2510.17422)
*Shaharyar Ahmed Khan Tareen,Filza Khan Tareen*

Main category: cs.CV

TL;DR: DeepDetect是一个智能、全功能的密集关键点检测器，通过融合7种关键点检测器和2种边缘检测器的输出创建真实标签，训练轻量级ESPNet模型，在关键点密度、可重复性和正确匹配数量方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统关键点检测器（SIFT、SURF等）和基于学习的方法（SuperPoint等）存在对光度变化敏感、关键点密度低、可重复性差、对挑战性场景适应性有限以及缺乏语义理解等问题，无法优先考虑视觉重要区域。

Method: 首先融合7种关键点检测器和2种边缘检测器的输出创建真实标签掩码，提取从角点和斑点到突出边缘和纹理的多样化视觉线索。然后使用这些掩码作为标签训练轻量高效的ESPNet模型。

Result: 在Oxford Affine Covariant Regions数据集上的评估显示，DeepDetect在关键点密度（0.5143）、可重复性（0.9582）和正确匹配数量（59,003）方面均达到最大值，超越了其他检测器。

Conclusion: DeepDetect通过深度学习统一了传统检测器的优势，能够语义地关注图像，产生高度密集的关键点，并适应多样化和视觉退化的条件。

Abstract: Keypoint detection is the foundation of many computer vision tasks, including
image registration, structure-from motion, 3D reconstruction, visual odometry,
and SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning
based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong
performance yet suffer from key limitations: sensitivity to photometric
changes, low keypoint density and repeatability, limited adaptability to
challenging scenes, and lack of semantic understanding, often failing to
prioritize visually important regions. We present DeepDetect, an intelligent,
all-in-one, dense keypoint detector that unifies the strengths of classical
detectors using deep learning. Firstly, we create ground-truth masks by fusing
outputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from
corners and blobs to prominent edges and textures in the images. Afterwards, a
lightweight and efficient model: ESPNet, is trained using these masks as
labels, enabling DeepDetect to focus semantically on images while producing
highly dense keypoints, that are adaptable to diverse and visually degraded
conditions. Evaluations on the Oxford Affine Covariant Regions dataset
demonstrate that DeepDetect surpasses other detectors in keypoint density,
repeatability, and the number of correct matches, achieving maximum values of
0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003
(correct matches).

</details>


### [166] [Leveraging AV1 motion vectors for Fast and Dense Feature Matching](https://arxiv.org/abs/2510.17434)
*Julien Zouein,Hossein Javidnia,François Pitié,Anil Kokaram*

Main category: cs.CV

TL;DR: 利用AV1运动向量生成密集亚像素对应关系和余弦一致性过滤的短轨迹，在压缩域中实现与SIFT相当的性能但计算资源消耗更少


<details>
  <summary>Details</summary>
Motivation: 探索压缩域对应关系作为资源高效的前端处理方法，为完整流程提供可扩展的路径

Method: 重新利用AV1运动向量生成密集亚像素对应关系，并通过余弦一致性过滤短轨迹

Result: 在117帧视频片段上，MV匹配成功注册所有图像并重建46-62万个点，重投影误差为0.51-0.53像素；BA时间随匹配密度增加

Conclusion: 压缩域对应关系是实用且资源高效的前端处理方法，具有在完整流程中扩展的清晰路径

Abstract: We repurpose AV1 motion vectors to produce dense sub-pixel correspondences
and short tracks filtered by cosine consistency. On short videos, this
compressed-domain front end runs comparably to sequential SIFT while using far
less CPU, and yields denser matches with competitive pairwise geometry. As a
small SfM demo on a 117-frame clip, MV matches register all images and
reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows
with match density. These results show compressed-domain correspondences are a
practical, resource-efficient front end with clear paths to scaling in full
pipelines.

</details>


### [167] [Rethinking Nighttime Image Deraining via Learnable Color Space Transformation](https://arxiv.org/abs/2510.17440)
*Qiyuan Guan,Xiang Chen,Guiyue Jin,Jiyu Jin,Shumin Fan,Tianyu Song,Jinshan Pan*

Main category: cs.CV

TL;DR: 提出新的高质量夜间图像去雨基准数据集HQ-NightRain和有效的颜色空间转换网络CST-Net，通过可学习的颜色空间转换器和隐式光照引导来更好地去除夜间复杂雨纹。


<details>
  <summary>Details</summary>
Motivation: 夜间图像去雨相比白天更具挑战性，因为夜间场景的固有复杂性以及缺乏准确表示雨和光照耦合效应的高质量数据集。

Method: 开发CST-Net网络，包含可学习的颜色空间转换器(CSC)在Y通道进行雨纹去除，并引入隐式光照引导来提高模型在复杂场景中的鲁棒性。

Result: 广泛实验证明了所提出数据集的价值和方法的有效性。

Conclusion: 提出的HQ-NightRain数据集和CST-Net方法在夜间图像去雨任务中表现出色，提供了更高质量和真实性的解决方案。

Abstract: Compared to daytime image deraining, nighttime image deraining poses
significant challenges due to inherent complexities of nighttime scenarios and
the lack of high-quality datasets that accurately represent the coupling effect
between rain and illumination. In this paper, we rethink the task of nighttime
image deraining and contribute a new high-quality benchmark, HQ-NightRain,
which offers higher harmony and realism compared to existing datasets. In
addition, we develop an effective Color Space Transformation Network (CST-Net)
for better removing complex rain from nighttime scenes. Specifically, we
propose a learnable color space converter (CSC) to better facilitate rain
removal in the Y channel, as nighttime rain is more pronounced in the Y channel
compared to the RGB color space. To capture illumination information for
guiding nighttime deraining, implicit illumination guidance is introduced
enabling the learned features to improve the model's robustness in complex
scenarios. Extensive experiments show the value of our dataset and the
effectiveness of our method. The source code and datasets are available at
https://github.com/guanqiyuan/CST-Net.

</details>


### [168] [Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS](https://arxiv.org/abs/2510.17479)
*Feng Zhou,Wenkai Guo,Pu Cao,Zhicheng Zhang,Jianqin Yin*

Main category: cs.CV

TL;DR: 该论文提出了一种改进稀疏视图3D高斯溅射(3DGS)初始化的方法，通过频率感知SfM、3DGS自初始化和点云正则化来增强点云覆盖，解决稀疏视图下的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3DGS容易过拟合训练视图，导致新视角渲染出现模糊等伪影。现有方法通过增强初始化或添加训练时约束来解决，但作者发现初始化是决定性因素，而训练时约束只能带来有限的改进。

Method: 1) 频率感知SfM：通过低频视图增强和宽松的多视图对应关系改善低纹理区域覆盖；2) 3DGS自初始化：将光度监督提升为额外点，用学习的高斯中心补偿SfM稀疏区域；3) 点云正则化：通过简单的几何/可见性先验强制多视图一致性和均匀空间覆盖。

Result: 在LLFF和Mip-NeRF360数据集上的实验表明，该方法在稀疏视图设置下取得了持续的性能提升，建立了更强的初始化策略。

Conclusion: 初始化是稀疏视图3DGS性能的决定性因素，提出的方法通过全面补充SfM未能覆盖的区域，显著改善了稀疏视图3D重建的质量。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training
views, leading to artifacts like blurring in novel view rendering. Prior work
addresses it either by enhancing the initialization (\emph{i.e.}, the point
cloud from Structure-from-Motion (SfM)) or by adding training-time constraints
(regularization) to the 3DGS optimization. Yet our controlled ablations reveal
that initialization is the decisive factor: it determines the attainable
performance band in sparse-view 3DGS, while training-time constraints yield
only modest within-band improvements at extra cost. Given initialization's
primacy, we focus our design there. Although SfM performs poorly under sparse
views due to its reliance on feature matching, it still provides reliable seed
points. Thus, building on SfM, our effort aims to supplement the regions it
fails to cover as comprehensively as possible. Specifically, we design: (i)
frequency-aware SfM that improves low-texture coverage via low-frequency view
augmentation and relaxed multi-view correspondences; (ii) 3DGS
self-initialization that lifts photometric supervision into additional points,
compensating SfM-sparse regions with learned Gaussian centers; and (iii)
point-cloud regularization that enforces multi-view consistency and uniform
spatial coverage through simple geometric/visibility priors, yielding a clean
and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate
consistent gains in sparse-view settings, establishing our approach as a
stronger initialization strategy. Code is available at
https://github.com/zss171999645/ItG-GS.

</details>


### [169] [Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment](https://arxiv.org/abs/2510.17484)
*Muhammad Umer Ramzan,Ali Zia,Abdelwahed Khamis,Noman Ali,Usman Ali,Wei Xiang*

Main category: cs.CV

TL;DR: 提出了POTNet方法，通过熵引导的双聚类头和最优传输来生成高质量伪掩码，构建了端到端的无监督显著目标检测框架AutoSOD，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 显著目标检测是计算机视觉的基础任务，作者认为在没有像素级标注的情况下，只要有可靠的伪掩码就能达到接近有监督方法的精度。但现有方法在原型质量和全局一致性利用方面存在不足。

Method: 提出POTNet，改进原型最优传输方法：使用熵引导的双聚类头（高熵像素用谱聚类，低熵像素用k-means），然后通过最优传输对齐两个原型集，生成更清晰的伪掩码。这些伪掩码用于训练标准的MaskFormer编码器-解码器，形成AutoSOD端到端无监督框架。

Result: 在5个基准测试上的实验表明，AutoSOD在F-measure指标上比无监督方法提升高达26%，比弱监督方法提升高达36%，进一步缩小了与全监督模型的差距。

Conclusion: 通过改进原型生成和利用最优传输的全局一致性，AutoSOD证明了无监督显著目标检测可以达到接近有监督方法的性能，同时消除了离线投票的需要，提高了训练效率。

Abstract: Salient object detection (SOD) aims to segment visually prominent regions in
images and serves as a foundational task for various computer vision
applications. We posit that SOD can now reach near-supervised accuracy without
a single pixel-level label, but only when reliable pseudo-masks are available.
We revisit the prototype-based line of work and make two key observations.
First, boundary pixels and interior pixels obey markedly different geometry;
second, the global consistency enforced by optimal transport (OT) is
underutilized if prototype quality is weak. To address this, we introduce
POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's
single k-means step with an entropy-guided dual-clustering head: high-entropy
pixels are organized by spectral clustering, low-entropy pixels by k-means, and
the two prototype sets are subsequently aligned by OT. This
split-fuse-transport design yields sharper, part-aware pseudo-masks in a single
forward pass, without handcrafted priors. Those masks supervise a standard
MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end
unsupervised SOD pipeline that eliminates SelfMask's offline voting yet
improves both accuracy and training efficiency. Extensive experiments on five
benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and
weakly supervised methods by up to 36% in F-measure, further narrowing the gap
to fully supervised models.

</details>


### [170] [WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection](https://arxiv.org/abs/2510.17566)
*Nachuan Ma,Zhengfei Song,Qiang Hu,Xiaoyu Tang,Chengxi Zhang,Rui Fan,Lihua Xie*

Main category: cs.CV

TL;DR: WP-CrackNet是一种弱监督的道路裂缝检测方法，仅使用图像级标签就能实现像素级检测，通过分类器、重建器和检测器的对抗学习，结合路径感知注意力模块和中心增强CAM一致性模块，达到与监督方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 为了减少对昂贵像素级标注的依赖，开发一种仅需图像级标签的弱监督道路裂缝检测方法，以支持智能城市基础设施维护的可扩展性。

Method: 提出WP-CrackNet框架，包含三个组件：分类器生成类激活图（CAMs）、重建器测量特征可推断性、检测器生成像素级检测结果。通过对抗学习使三个组件相互反馈，并设计了路径感知注意力模块（PAAM）和中心增强CAM一致性模块（CECCM）来提升性能。

Result: 在三个图像级数据集上的实验表明，WP-CrackNet达到了与监督方法相当的结果，并优于现有的弱监督方法，显著提升了道路检测的可扩展性。

Conclusion: WP-CrackNet通过弱监督学习有效解决了道路裂缝检测问题，减少了标注成本，为智能基础设施维护提供了可行的解决方案。

Abstract: Road crack detection is essential for intelligent infrastructure maintenance
in smart cities. To reduce reliance on costly pixel-level annotations, we
propose WP-CrackNet, an end-to-end weakly-supervised method that trains with
only image-level labels for pixel-wise crack detection. WP-CrackNet integrates
three components: a classifier generating class activation maps (CAMs), a
reconstructor measuring feature inferability, and a detector producing
pixel-wise road crack detection results. During training, the classifier and
reconstructor alternate in adversarial learning to encourage crack CAMs to
cover complete crack regions, while the detector learns from pseudo labels
derived from post-processed crack CAMs. This mutual feedback among the three
components improves learning stability and detection accuracy. To further boost
detection performance, we design a path-aware attention module (PAAM) that
fuses high-level semantics from the classifier with low-level structural cues
from the reconstructor by modeling spatial and channel-wise dependencies.
Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to
refine crack CAMs using center Gaussian weighting and consistency constraints,
enabling better pseudo-label generation. We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection. The source code package and
datasets are available at https://mias.group/WP-CrackNet/.

</details>


### [171] [PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception](https://arxiv.org/abs/2510.17568)
*Kaichen Zhou,Yuhan Wang,Grace Chen,Xinhai Chang,Gaspard Beaudouin,Fangneng Zhan,Paul Pu Liang,Mengyu Wang*

Main category: cs.CV

TL;DR: PAGE-4D是一个前馈模型，将VGGT扩展到动态场景，能够同时进行相机姿态估计、深度预测和点云重建，无需后处理。


<details>
  <summary>Details</summary>
Motivation: 现有的3D前馈模型在静态数据集上训练，难以处理包含复杂动态元素（如移动人物或可变形物体）的真实世界场景。

Method: 提出了动态感知聚合器，通过预测动态感知掩码来解耦静态和动态信息，在姿态估计时抑制运动线索，在几何重建时增强它们。

Result: 在动态场景中，PAGE-4D持续优于原始VGGT，在相机姿态估计、单目和视频深度估计以及密集点图重建方面取得更优结果。

Conclusion: PAGE-4D成功解决了多任务4D重建中的任务冲突问题，为动态场景的3D属性推断提供了有效解决方案。

Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded
Transformer (VGGT), have shown strong capability in inferring 3D attributes of
static scenes. However, since they are typically trained on static datasets,
these models often struggle in real-world scenarios involving complex dynamic
elements, such as moving humans or deformable objects like umbrellas. To
address this limitation, we introduce PAGE-4D, a feedforward model that extends
VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and
point cloud reconstruction -- all without post-processing. A central challenge
in multi-task 4D reconstruction is the inherent conflict between tasks:
accurate camera pose estimation requires suppressing dynamic regions, while
geometry reconstruction requires modeling them. To resolve this tension, we
propose a dynamics-aware aggregator that disentangles static and dynamic
information by predicting a dynamics-aware mask -- suppressing motion cues for
pose estimation while amplifying them for geometry reconstruction. Extensive
experiments show that PAGE-4D consistently outperforms the original VGGT in
dynamic scenarios, achieving superior results in camera pose estimation,
monocular and video depth estimation, and dense point map reconstruction.

</details>


### [172] [Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset](https://arxiv.org/abs/2510.17585)
*Chuhong Wang,Hua Li,Chongyi Li,Huazhong Liu,Xiongxin Tang,Sam Kwong*

Main category: cs.CV

TL;DR: 提出了首个水下伪装实例分割数据集UCIS4K和基于SAM的UCIS-SAM网络，包含三个关键模块来提升水下伪装物体的分割性能。


<details>
  <summary>Details</summary>
Motivation: 水下环境存在颜色失真、低对比度和模糊等问题，传统基于陆地数据训练的伪装实例分割方法在水下场景表现不佳。

Method: 提出UCIS-SAM网络，包含通道平衡优化模块(CBOM)、频域真值整合模块(FDTIM)和多尺度特征频率聚合模块(MFFAM)三个关键模块。

Result: 在提出的UCIS4K数据集和公共基准测试上的广泛实验表明，UCIS-SAM优于最先进的方法。

Conclusion: 该方法有效解决了水下伪装实例分割的挑战，为水下视觉任务提供了有力工具。

Abstract: With the development of underwater exploration and marine protection,
underwater vision tasks are widespread. Due to the degraded underwater
environment, characterized by color distortion, low contrast, and blurring,
camouflaged instance segmentation (CIS) faces greater challenges in accurately
segmenting objects that blend closely with their surroundings. Traditional
camouflaged instance segmentation methods, trained on terrestrial-dominated
datasets with limited underwater samples, may exhibit inadequate performance in
underwater scenes. To address these issues, we introduce the first underwater
camouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which
comprises 3,953 images of camouflaged marine organisms with instance-level
annotations. In addition, we propose an Underwater Camouflaged Instance
Segmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM
includes three key modules. First, the Channel Balance Optimization Module
(CBOM) enhances channel characteristics to improve underwater feature learning,
effectively addressing the model's limited understanding of underwater
environments. Second, the Frequency Domain True Integration Module (FDTIM) is
proposed to emphasize intrinsic object features and reduce interference from
camouflage patterns, enhancing the segmentation performance of camouflaged
objects blending with their surroundings. Finally, the Multi-scale Feature
Frequency Aggregation Module (MFFAM) is designed to strengthen the boundaries
of low-contrast camouflaged instances across multiple frequency bands,
improving the model's ability to achieve more precise segmentation of
camouflaged objects. Extensive experiments on the proposed UCIS4K and public
benchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.

</details>


### [173] [ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling](https://arxiv.org/abs/2510.17603)
*Shuyuan Zhang,Chenhan Jiang,Zuoou Li,Jiankang Deng*

Main category: cs.CV

TL;DR: ShapeCraft是一个多智能体框架，通过图形化程序形状表示将自然语言转换为结构化、可交互的3D资产，解决了现有方法生成非结构化网格和交互性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D生成方法通常产生非结构化网格且交互性差，难以应用于艺术工作流程。需要开发能够生成结构化、可交互3D资产的解决方案。

Method: 提出基于图形的程序形状表示，将复杂自然语言分解为子任务图，利用LLM智能体分层解析用户输入并迭代优化程序建模和绘画过程。

Result: 定性和定量实验表明，ShapeCraft在生成几何精确和语义丰富的3D资产方面优于现有基于LLM的方法，支持动画和用户自定义编辑。

Conclusion: ShapeCraft框架展示了在更广泛交互应用中的潜力，能够生成结构化、纹理化和交互式的3D资产。

Abstract: 3D generation from natural language offers significant potential to reduce
expert manual modeling efforts and enhance accessibility to 3D assets. However,
existing methods often yield unstructured meshes and exhibit poor
interactivity, making them impractical for artistic workflows. To address these
limitations, we represent 3D assets as shape programs and introduce ShapeCraft,
a novel multi-agent framework for text-to-3D generation. At its core, we
propose a Graph-based Procedural Shape (GPS) representation that decomposes
complex natural language into a structured graph of sub-tasks, thereby
facilitating accurate LLM comprehension and interpretation of spatial
relationships and semantic shape details. Specifically, LLM agents
hierarchically parse user input to initialize GPS, then iteratively refine
procedural modeling and painting to produce structured, textured, and
interactive 3D assets. Qualitative and quantitative experiments demonstrate
ShapeCraft's superior performance in generating geometrically accurate and
semantically rich 3D assets compared to existing LLM-based agents. We further
show the versatility of ShapeCraft through examples of animated and
user-customized editing, highlighting its potential for broader interactive
applications.

</details>


### [174] [Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation](https://arxiv.org/abs/2510.17609)
*Siqi Chen,Shanyue Guan*

Main category: cs.CV

TL;DR: 提出基于机器学习的框架，结合无人机扫描点云和BIM合成数据，自动分割3D基础设施模型中的结构组件，显著提高分割精度和效率。


<details>
  <summary>Details</summary>
Motivation: 无人机结合摄影测量技术可高效获取基础设施3D模型，但传统手动分割结构组件耗时且易出错，需要自动化解决方案。

Method: 使用真实无人机扫描点云和BIM生成的合成数据，利用机器学习框架进行3D点云自动分割，通过小规模数据集结合BIM数据减少训练时间。

Result: 在铁路轨道数据集验证中，框架能高精度识别和分割轨道和轨枕等主要组件，同时保持合理的分割精度。

Conclusion: 该自动化方法提高了3D基础设施模型分割的精度和效率，推动了无人机与BIM技术在结构健康监测和基础设施管理中的集成应用。

Abstract: The advancement of UAV technology has enabled efficient, non-contact
structural health monitoring. Combined with photogrammetry, UAVs can capture
high-resolution scans and reconstruct detailed 3D models of infrastructure.
However, a key challenge remains in segmenting specific structural components
from these models-a process traditionally reliant on time-consuming and
error-prone manual labeling. To address this issue, we propose a machine
learning-based framework for automated segmentation of 3D point clouds. Our
approach uses the complementary strengths of real-world UAV-scanned point
clouds and synthetic data generated from Building Information Modeling (BIM) to
overcome the limitations associated with manual labeling. Validation on a
railroad track dataset demonstrated high accuracy in identifying and segmenting
major components such as rails and crossties. Moreover, by using smaller-scale
datasets supplemented with BIM data, the framework significantly reduced
training time while maintaining reasonable segmentation accuracy. This
automated approach improves the precision and efficiency of 3D infrastructure
model segmentation and advances the integration of UAV and BIM technologies in
structural health monitoring and infrastructure management.

</details>


### [175] [One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.17611)
*Jia Guo,Shuai Lu,Lei Fan,Zelin Li,Donglin Di,Yang Song,Weihang Zhang,Wenbing Zhu,Hong Yan,Fang Chen,Huiqi Li,Hongen Liao*

Main category: cs.CV

TL;DR: Dinomaly2是一个统一的无监督异常检测框架，通过五个简单元素的协调在标准重建框架中实现卓越性能，在多个模态和任务设置中展现全谱系优势。


<details>
  <summary>Details</summary>
Motivation: 现有多类异常检测模型性能显著落后于一对一模型，且领域碎片化需要统一解决方案。

Method: 基于'少即是多'哲学，在标准重建框架中协调五个简单元素，实现方法极简主义。

Result: 在12个基准测试中表现优异：多类模型在MVTec-AD和VisA上分别达到99.9%和99.3% I-AUROC；仅用8个正常样本就超越之前全样本模型。

Conclusion: Dinomaly2通过极简设计、计算可扩展性和通用适用性，成为现实世界异常检测应用的统一解决方案。

Abstract: Unsupervised anomaly detection (UAD) has evolved from building specialized
single-class models to unified multi-class models, yet existing multi-class
models significantly underperform the most advanced one-for-one counterparts.
Moreover, the field has fragmented into specialized methods tailored to
specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment
barriers and highlighting the need for a unified solution. In this paper, we
present Dinomaly2, the first unified framework for full-spectrum image UAD,
which bridges the performance gap in multi-class models while seamlessly
extending across diverse data modalities and task settings. Guided by the "less
is more" philosophy, we demonstrate that the orchestration of five simple
element achieves superior performance in a standard reconstruction-based
framework. This methodological minimalism enables natural extension across
diverse tasks without modification, establishing that simplicity is the
foundation of true universality. Extensive experiments on 12 UAD benchmarks
demonstrate Dinomaly2's full-spectrum superiority across multiple modalities
(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,
inference-unified multi-class, few-shot) and application domains (industrial,
biological, outdoor). For example, our multi-class model achieves unprecedented
99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For
multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art
performance with minimum adaptations. Moreover, using only 8 normal examples
per class, our method surpasses previous full-shot models, achieving 98.7% and
97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,
computational scalability, and universal applicability positions Dinomaly2 as a
unified solution for the full spectrum of real-world anomaly detection
applications.

</details>


### [176] [Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives](https://arxiv.org/abs/2510.17644)
*Zexian Huang,Mashnoon Islam,Brian Armstrong,Kourosh Khoshelham,Martin Tomko*

Main category: cs.CV

TL;DR: 提出了DINO-CV分割框架，使用高分辨率LiDAR DEM自动映射低矮干石墙，解决了植被遮挡和标注数据稀缺问题，在澳大利亚Budj Bim文化遗产地取得了68.6%的mIoU。


<details>
  <summary>Details</summary>
Motivation: 干石墙具有重要的遗产和环境价值，但许多墙体因难以接近和人工测绘成本高而未被识别。深度学习分割可提供可扩展解决方案，但面临植被视觉遮挡和标注数据有限两大挑战。

Method: 提出DINO-CV分割框架，使用高分辨率机载LiDAR DEM克服植被遮挡，引入基于知识蒸馏的自监督跨视图预训练策略，学习多个DEM衍生物的视觉和几何表示，支持多种视觉骨干网络。

Result: 在维多利亚Budj Bim UNESCO世界遗产文化景观中识别出澳大利亚最密集的殖民时期干石墙集合。测试区域mIoU达到68.6%，仅使用10%标注数据微调时仍保持63.8% mIoU。

Conclusion: 证明了在高分辨率DEM衍生物上进行自监督学习在植被茂密和遗产丰富环境中自动映射干石墙的潜力，特别是在标注稀缺的情况下。

Abstract: Dry-stone walls hold significant heritage and environmental value. Mapping
these structures is essential for ecosystem preservation and wildfire
management in Australia. Yet, many walls remain unidentified due to their
inaccessibility and the high cost of manual mapping. Deep learning-based
segmentation offers a scalable solution, but two major challenges persist: (1)
visual occlusion of low-lying walls by dense vegetation, and (2) limited
labeled data for supervised training. We propose DINO-CV, a segmentation
framework for automatic mapping of low-lying dry-stone walls using
high-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs
overcome visual occlusion by capturing terrain structures hidden beneath
vegetation, enabling analysis of structural rather than spectral cues. DINO-CV
introduces a self-supervised cross-view pre-training strategy based on
knowledge distillation to mitigate data scarcity. It learns invariant visual
and geometric representations across multiple DEM derivatives, supporting
various vision backbones including ResNet, Wide ResNet, and Vision
Transformers. Applied to the UNESCO World Heritage cultural landscape of Budj
Bim, Victoria, the method identifies one of Australia's densest collections of
colonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves
a mean Intersection over Union (mIoU) of 68.6% on test areas and maintains
63.8% mIoU when fine-tuned with only 10% labeled data. These results
demonstrate the potential of self-supervised learning on high-resolution DEM
derivatives for automated dry-stone wall mapping in vegetated and heritage-rich
environments with scarce annotations.

</details>


### [177] [4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads](https://arxiv.org/abs/2510.17664)
*Ling Liu,Jun Tian,Li Yi*

Main category: cs.CV

TL;DR: 4DSegStreamer是一个用于4D全景分割的流式处理框架，采用双线程系统处理动态环境中的实时感知任务，可集成到现有3D/4D分割方法中实现实时能力。


<details>
  <summary>Details</summary>
Motivation: 在高度动态环境中（如密集人群疏散、复杂自动驾驶场景）需要实时、细粒度的感知能力，但现有方法在受限时间预算下难以满足需求。

Method: 采用双线程系统：预测线程利用历史运动和几何信息提取特征并预测未来动态；推理线程通过对齐最新内存并补偿自运动和动态物体移动来确保对传入帧的及时预测。

Result: 在室内HOI4D数据集和室外SemanticKITTI、nuScenes数据集上的实验表明，该方法在复杂场景中准确预测动态物体方面表现优异，特别是在高FPS条件下比现有流式感知方法更具鲁棒性。

Conclusion: 4DSegStreamer是一个通用且高效的框架，能够为现有3D和4D分割方法提供实时能力，在动态环境感知中表现出色。

Abstract: 4D panoptic segmentation in a streaming setting is critical for highly
dynamic environments, such as evacuating dense crowds and autonomous driving in
complex scenarios, where real-time, fine-grained perception within a
constrained time budget is essential. In this paper, we introduce
4DSegStreamer, a novel framework that employs a Dual-Thread System to
efficiently process streaming frames. The framework is general and can be
seamlessly integrated into existing 3D and 4D segmentation methods to enable
real-time capability. It also demonstrates superior robustness compared to
existing streaming perception approaches, particularly under high FPS
conditions. The system consists of a predictive thread and an inference thread.
The predictive thread leverages historical motion and geometric information to
extract features and forecast future dynamics. The inference thread ensures
timely prediction for incoming frames by aligning with the latest memory and
compensating for ego-motion and dynamic object movements. We evaluate
4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and
nuScenes datasets. Comprehensive experiments demonstrate the effectiveness of
our approach, particularly in accurately predicting dynamic objects in complex
scenes.

</details>


### [178] [Towards 3D Objectness Learning in an Open World](https://arxiv.org/abs/2510.17686)
*Taichi Liu,Zhenyu Wang,Ruofeng Liu,Guang Wang,Desheng Zhang*

Main category: cs.CV

TL;DR: OP3Det是一个无需文本提示的开放世界3D检测器，能够检测3D场景中的任何物体，包括训练时未见的新类别物体，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统封闭式3D检测器难以泛化到开放世界场景，而直接应用3D开放词汇模型又面临词汇扩展和语义重叠问题，因此需要学习通用的3D物体性。

Method: 提出OP3Det，利用2D基础模型的强泛化能力，结合2D语义先验和3D几何先验生成类别无关的提议，通过跨模态专家混合动态路由单模态和多模态特征来学习通用3D物体性。

Result: 在广泛实验中，OP3Det在AR指标上显著超越现有开放世界3D检测器达16.0%，相比封闭世界3D检测器提升13.5%。

Conclusion: OP3Det通过结合2D基础模型和跨模态特征学习，成功实现了通用的3D物体发现，在开放世界3D检测任务中表现出色。

Abstract: Recent advancements in 3D object detection and novel category detection have
made significant progress, yet research on learning generalized 3D objectness
remains insufficient. In this paper, we delve into learning open-world 3D
objectness, which focuses on detecting all objects in a 3D scene, including
novel objects unseen during training. Traditional closed-set 3D detectors
struggle to generalize to open-world scenarios, while directly incorporating 3D
open-vocabulary models for open-world ability struggles with vocabulary
expansion and semantic overlap. To achieve generalized 3D object discovery, We
propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect
any objects within 3D scenes without relying on hand-crafted text prompts. We
introduce the strong generalization and zero-shot capabilities of 2D foundation
models, utilizing both 2D semantic priors and 3D geometric priors for
class-agnostic proposals to broaden 3D object discovery. Then, by integrating
complementary information from point cloud and RGB image in the cross-modal
mixture of experts, OP3Det dynamically routes uni-modal and multi-modal
features to learn generalized 3D objectness. Extensive experiments demonstrate
the extraordinary performance of OP3Det, which significantly surpasses existing
open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement
compared to closed-world 3D detectors.

</details>


### [179] [GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver](https://arxiv.org/abs/2510.17699)
*Aleksandr Oganov,Ilya Bykov,Eva Neudachina,Mishan Aliev,Alexander Tolmachev,Alexander Sidorov,Aleksandr Zuev,Andrey Okhotin,Denis Rakitin,Aibek Alanov*

Main category: cs.CV

TL;DR: 提出了一种新的扩散模型采样方法——广义对抗求解器，通过简单的ODE采样器参数化和对抗训练，在保持高质量生成的同时显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽然生成质量优秀，但采样过程计算昂贵。现有蒸馏方法依赖复杂训练技巧且不能很好地保留细节。

Method: 提出广义求解器参数化ODE采样器，无需额外训练技巧；结合原始蒸馏损失和对抗训练来减少伪影并增强细节保真度。

Result: 在相似资源约束下，该方法相比现有求解器训练方法表现出更优越的性能。

Conclusion: 广义对抗求解器提供了一种简单有效的扩散模型加速采样方案，在保持质量的同时显著提升效率。

Abstract: While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.

</details>


### [180] [Elastic ViTs from Pretrained Models without Retraining](https://arxiv.org/abs/2510.17700)
*Walter Simoncini,Michael Dorkenwald,Tijmen Blankevoort,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CV

TL;DR: SnapViT是一种后预训练结构化剪枝方法，能够在不同计算预算下实现弹性推理，无需重训练或标签数据。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型只有有限的预定义尺寸，无法灵活适应实际部署中的计算约束。

Method: 结合梯度信息和跨网络结构相关性（通过进化算法近似），使用自监督重要性评分机制进行结构化剪枝。

Result: 在DINO、SigLIPv2、DeIT和AugReg模型上优于现有方法，单A100 GPU不到5分钟即可生成弹性模型。

Conclusion: 提出了一种高效的ViT剪枝策略、新颖的Hessian非对角结构进化近似和自监督重要性评分机制，无需重训练即可保持强性能。

Abstract: Vision foundation models achieve remarkable performance but are only
available in a limited set of pre-determined sizes, forcing sub-optimal
deployment choices under real-world constraints. We introduce SnapViT:
Single-shot network approximation for pruned Vision Transformers, a new
post-pretraining structured pruning method that enables elastic inference
across a continuum of compute budgets. Our approach efficiently combines
gradient information with cross-network structure correlations, approximated
via an evolutionary algorithm, does not require labeled data, generalizes to
models without a classification head, and is retraining-free. Experiments on
DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
state-of-the-art methods across various sparsities, requiring less than five
minutes on a single A100 GPU to generate elastic models that can be adjusted to
any computational budget. Our key contributions include an efficient pruning
strategy for pretrained Vision Transformers, a novel evolutionary approximation
of Hessian off-diagonal structures, and a self-supervised importance scoring
mechanism that maintains strong performance without requiring retraining or
labels. Code and pruned models are available at: https://elastic.ashita.nl/

</details>


### [181] [Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging](https://arxiv.org/abs/2510.17716)
*Suqiang Ma,Subhadeep Sengupta,Yao Lee,Beikang Gu,Xianyan Chen,Xianqiao Wang,Yang Liu,Mengjia Xu,Galit H. Frydman,He Li*

Main category: cs.CV

TL;DR: 提出了一种用于分析循环血细胞簇（CCCs）图像的计算框架，通过YOLOv11模型分类细胞簇和非细胞簇图像，然后利用多通道荧光染色识别细胞类型，准确率超过95%。


<details>
  <summary>Details</summary>
Motivation: 循环血细胞簇是血栓、感染和炎症等疾病的重要生物标志物，但目前缺乏自动分析含细胞簇图像的工具，传统方法难以处理不规则形状和异质细胞类型的细胞簇。

Method: 采用两步分析策略：1）使用YOLOv11模型对图像进行细胞簇和非细胞簇分类；2）通过叠加细胞簇轮廓与多通道荧光染色区域来识别细胞类型，克服细胞碎片和染色伪影的影响。

Result: 该方法在细胞簇分类和表型识别方面均达到超过95%的准确率，优于传统CNN和Vision Transformer模型。

Conclusion: 该自动化框架能有效分析流式细胞术中的CCC图像，利用明场和荧光数据，不仅适用于血细胞分析，还有潜力应用于免疫细胞和肿瘤细胞簇等更广泛的研究领域。

Abstract: Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),
white blood cells(WBCs), and platelets are significant biomarkers linked to
conditions like thrombosis, infection, and inflammation. Flow cytometry, paired
with fluorescence staining, is commonly used to analyze these cell clusters,
revealing cell morphology and protein profiles. While computational approaches
based on machine learning have advanced the automatic analysis of single-cell
flow cytometry images, there is a lack of effort to build tools to
automatically analyze images containing CCCs. Unlike single cells, cell
clusters often exhibit irregular shapes and sizes. In addition, these cell
clusters often consist of heterogeneous cell types, which require multi-channel
staining to identify the specific cell types within the clusters. This study
introduces a new computational framework for analyzing CCC images and
identifying cell types within clusters. Our framework uses a two-step analysis
strategy. First, it categorizes images into cell cluster and non-cluster groups
by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms
traditional convolutional neural networks (CNNs), Vision Transformers (ViT).
Then, it identifies cell types by overlaying cluster contours with regions from
multi-channel fluorescence stains, enhancing accuracy despite cell debris and
staining artifacts. This approach achieved over 95% accuracy in both cluster
classification and phenotype identification. In summary, our automated
framework effectively analyzes CCC images from flow cytometry, leveraging both
bright-field and fluorescence data. Initially tested on blood cells, it holds
potential for broader applications, such as analyzing immune and tumor cell
clusters, supporting cellular research across various diseases.

</details>


### [182] [Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions](https://arxiv.org/abs/2510.17719)
*Zhiqiang Teng,Beibei Lin,Tingting Chen,Zifeng Yuan,Xuanyi Li,Xuanyu Zhang,Shunli Zhang*

Main category: cs.CV

TL;DR: RaindropGS是一个专门评估3D高斯溅射在雨滴条件下性能的基准测试，针对真实世界中雨滴污染导致的遮挡、光学畸变以及相机姿态估计问题，提供完整的评估流程和真实数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常使用合成雨滴图像和已知相机姿态，但真实场景中雨滴会干扰相机姿态估计和点云初始化，且合成与真实雨滴存在显著域差距，影响3DGS的重建质量。

Method: 构建包含三个对齐图像集（雨滴聚焦、背景聚焦和无雨地面真值）的真实世界雨滴重建数据集，设计完整评估流程：数据准备、数据处理和雨滴感知3DGS评估，涵盖相机姿态估计、点云初始化、单图像去雨和3D高斯训练比较。

Result: 通过全面实验分析，揭示了现有3DGS方法在无约束雨滴图像上的性能限制，以及不同流程组件的影响：相机焦点位置对重建性能的影响，不准确的姿态和点云初始化对重建的干扰。

Conclusion: 这些发现为开发在雨滴条件下更鲁棒的3DGS方法提供了明确方向，建立了评估3DGS完整流程的基准标准。

Abstract: 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe
occlusions and optical distortions caused by raindrop contamination on the
camera lens, substantially degrading reconstruction quality. Existing
benchmarks typically evaluate 3DGS using synthetic raindrop images with known
camera poses (constrained images), assuming ideal conditions. However, in
real-world scenarios, raindrops often interfere with accurate camera pose
estimation and point cloud initialization. Moreover, a significant domain gap
between synthetic and real raindrops further impairs generalization. To tackle
these issues, we introduce RaindropGS, a comprehensive benchmark designed to
evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images
to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline
consists of three parts: data preparation, data processing, and raindrop-aware
3DGS evaluation, including types of raindrop interference, camera pose
estimation and point cloud initialization, single image rain removal
comparison, and 3D Gaussian training comparison. First, we collect a real-world
raindrop reconstruction dataset, in which each scene contains three aligned
image sets: raindrop-focused, background-focused, and rain-free ground truth,
enabling a comprehensive evaluation of reconstruction quality under different
focus conditions. Through comprehensive experiments and analyses, we reveal
critical insights into the performance limitations of existing 3DGS methods on
unconstrained raindrop images and the varying impact of different pipeline
components: the impact of camera focus position on 3DGS reconstruction
performance, and the interference caused by inaccurate pose and point cloud
initialization on reconstruction. These insights establish clear directions for
developing more robust 3DGS methods under raindrop conditions.

</details>


### [183] [Can Image-To-Video Models Simulate Pedestrian Dynamics?](https://arxiv.org/abs/2510.17731)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: 该研究探索了基于扩散变换器的图像到视频模型在生成拥挤公共场景中真实行人运动模式的能力。


<details>
  <summary>Details</summary>
Motivation: 研究基于扩散变换器的图像到视频模型是否能够生成拥挤公共场景中真实的行人运动模式。

Method: 通过从行人轨迹基准中提取关键帧来条件化图像到视频模型，然后使用行人动力学的定量指标评估其轨迹预测性能。

Result: 研究发现这些模型在生成真实行人运动模式方面表现出色。

Conclusion: 基于扩散变换器的图像到视频模型能够有效生成拥挤场景中的真实行人运动轨迹。

Abstract: Recent high-performing image-to-video (I2V) models based on variants of the
diffusion transformer (DiT) have displayed remarkable inherent world-modeling
capabilities by virtue of training on large scale video datasets. We
investigate whether these models can generate realistic pedestrian movement
patterns in crowded public scenes. Our framework conditions I2V models on
keyframes extracted from pedestrian trajectory benchmarks, then evaluates their
trajectory prediction performance using quantitative measures of pedestrian
dynamics.

</details>


### [184] [Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition](https://arxiv.org/abs/2510.17739)
*Timur Ismagilov,Shakaiba Majeed,Michael Milford,Tan Viet Tuyen Nguyen,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 提出了一种无需训练的描述符无关方法，通过矩阵分解将多个参考描述符联合建模为基表示，实现基于投影的残差匹配，在多参考视觉位置识别中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多参考视觉位置识别中，在保持轻量化的同时提高在多种外观和视角变化下的定位性能，避免深度学习方法的高计算成本。

Method: 使用矩阵分解将多个参考描述符分解为基表示，通过投影残差匹配进行位置识别，无需训练过程。

Result: 在多外观数据上Recall@1提升约18%，在非结构化数据上提升约5%，显著优于单参考和多参考基线方法。

Conclusion: 该方法在保持轻量化的同时，在多种外观和视角变化下表现出强大的泛化能力，是多参考视觉位置识别的有效解决方案。

Abstract: We address multi-reference visual place recognition (VPR), where reference
sets captured under varying conditions are used to improve localisation
performance. While deep learning with large-scale training improves robustness,
increasing data diversity and model complexity incur extensive computational
cost during training and deployment. Descriptor-level fusion via voting or
aggregation avoids training, but often targets multi-sensor setups or relies on
heuristics with limited gains under appearance and viewpoint change. We propose
a training-free, descriptor-agnostic approach that jointly models places using
multiple reference descriptors via matrix decomposition into basis
representations, enabling projection-based residual matching. We also introduce
SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance
data, our method improves Recall@1 by up to ~18% over single-reference and
outperforms multi-reference baselines across appearance and viewpoint changes,
with gains of ~5% on unstructured data, demonstrating strong generalisation
while remaining lightweight.

</details>


### [185] [SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference](https://arxiv.org/abs/2510.17777)
*Samir Khaki,Junxian Guo,Jiaming Tang,Shang Yang,Yukang Chen,Konstantinos N. Plataniotis,Yao Lu,Song Han,Zhijian Liu*

Main category: cs.CV

TL;DR: SparseVILA是一种高效的视觉语言模型推理范式，通过在预填充阶段剪枝冗余视觉token和在解码阶段检索查询相关token，实现视觉稀疏性解耦，显著提升推理速度而不损失能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的可扩展性受到视觉token数量增长的限制，这些token主导了推理延迟。需要一种方法在保持多轮对话保真度的同时加速推理。

Method: 提出SparseVILA框架，在预填充阶段进行查询无关的token剪枝，在解码阶段进行查询感知的token检索，基于AWQ优化的推理流水线实现。

Result: 在长上下文视频任务中实现预填充速度提升4.0倍，解码速度提升2.5倍，端到端加速2.6倍，同时在文档理解和推理任务上提高准确性。

Conclusion: 通过解耦查询无关剪枝和查询感知检索，SparseVILA为高效多模态推理提供了无需训练、架构无关的加速框架，且不牺牲模型能力。

Abstract: Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.

</details>


### [186] [UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action](https://arxiv.org/abs/2510.17790)
*Yuhao Yang,Zhen Yang,Zi-Yi Dou,Anh Nguyen,Keen You,Omar Attia,Andrew Szot,Michael Feng,Ram Ramrakhya,Alexander Toshev,Chao Huang,Yinfei Yang,Zhe Gan*

Main category: cs.CV

TL;DR: UltraCUA是一个基础模型，通过混合动作将GUI基本操作与高级程序化工具调用无缝集成，解决了计算机使用代理的视觉定位和执行链问题。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机使用代理仅依赖基本操作（点击、输入、滚动），这需要准确的视觉定位和冗长的执行链，导致级联失败和性能瓶颈。而其他代理可以利用丰富的程序化接口，计算机使用代理却与这些能力隔离。

Method: 方法包括四个关键组件：(1)从软件文档、开源仓库和代码生成中扩展程序化工具的自动化流水线；(2)生成超过17,000个可验证任务的数据引擎；(3)大规模高质量混合动作轨迹收集；(4)结合监督微调和在线强化学习的两阶段训练管道。

Result: 实验显示，7B和32B模型在OSWorld上相比基准模型平均提升22%，步骤执行速度提升11%。在WindowsAgentArena的域外评估中达到21.7%的成功率，优于基于Windows数据训练的基线模型。

Conclusion: 混合动作机制至关重要，能够减少错误传播同时保持执行效率，为计算机使用代理提供了更强大的能力。

Abstract: Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.

</details>


### [187] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: Glyph框架通过将长文本渲染为图像，使用视觉语言模型处理，实现3-4倍的token压缩，同时保持与Qwen3-8B相当的准确性，显著提升长上下文任务的计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在处理百万token级别长上下文时面临的计算和内存成本过高的问题，传统token扩展方法不实用。

Method: 提出Glyph框架，将长文本渲染成图像，使用视觉语言模型处理；设计基于LLM的遗传搜索来优化视觉渲染配置，平衡准确性和压缩率。

Result: 实现3-4倍token压缩，准确性与Qwen3-8B相当；预填充和解码速度快约4倍，SFT训练快约2倍；128K上下文VLM可处理1M token任务；渲染文本数据有益于文档理解等实际多模态任务。

Conclusion: 视觉上下文扩展是解决长上下文建模挑战的有效方法，Glyph框架在保持性能的同时显著提升了计算效率，为长文本处理提供了实用解决方案。

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>


### [188] [ConsistEdit: Highly Consistent and Precise Training-free Visual Editing](https://arxiv.org/abs/2510.17803)
*Zixin Yin,Ling-Hao Chen,Lionel Ni,Xili Dai*

Main category: cs.CV

TL;DR: 提出了ConsistEdit方法，针对MM-DiT架构的注意力控制，实现一致性和编辑强度的平衡，支持多轮和视频编辑。


<details>
  <summary>Details</summary>
Motivation: 现有训练自由注意力控制方法在编辑强度与源一致性之间存在权衡，特别是在多轮和视频编辑中视觉错误会累积，且全局一致性限制了细粒度编辑能力。

Method: 基于MM-DiT架构分析，提出视觉专用注意力控制、掩码引导预注意力融合、以及查询/键/值令牌的差异化操作。

Result: 在图像和视频编辑任务中达到最先进性能，支持结构一致和不一致场景，实现无手工操作的全推理步骤和注意力层编辑。

Conclusion: ConsistEdit是首个无需手工操作即可在所有推理步骤和注意力层进行编辑的方法，显著提升了可靠性和一致性，支持稳健的多轮和多区域编辑。

Abstract: Recent advances in training-free attention control methods have enabled
flexible and efficient text-guided editing capabilities for existing generation
models. However, current approaches struggle to simultaneously deliver strong
editing strength while preserving consistency with the source. This limitation
becomes particularly critical in multi-round and video editing, where visual
errors can accumulate over time. Moreover, most existing methods enforce global
consistency, which limits their ability to modify individual attributes such as
texture while preserving others, thereby hindering fine-grained editing.
Recently, the architectural shift from U-Net to MM-DiT has brought significant
improvements in generative performance and introduced a novel mechanism for
integrating text and vision modalities. These advancements pave the way for
overcoming challenges that previous methods failed to resolve. Through an
in-depth analysis of MM-DiT, we identify three key insights into its attention
mechanisms. Building on these, we propose ConsistEdit, a novel attention
control method specifically tailored for MM-DiT. ConsistEdit incorporates
vision-only attention control, mask-guided pre-attention fusion, and
differentiated manipulation of the query, key, and value tokens to produce
consistent, prompt-aligned edits. Extensive experiments demonstrate that
ConsistEdit achieves state-of-the-art performance across a wide range of image
and video editing tasks, including both structure-consistent and
structure-inconsistent scenarios. Unlike prior methods, it is the first
approach to perform editing across all inference steps and attention layers
without handcraft, significantly enhancing reliability and consistency, which
enables robust multi-round and multi-region editing. Furthermore, it supports
progressive adjustment of structural consistency, enabling finer control.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [189] [VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search](https://arxiv.org/abs/2510.15948)
*MingSheng Li,Guangze Zhao,Sichen Liu*

Main category: cs.AI

TL;DR: VisuoAlign是一个通过提示引导树搜索进行多模态安全对齐的框架，旨在解决大型视觉语言模型在跨模态威胁下的安全对齐挑战。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法对多模态越狱攻击存在脆弱性，视觉输入引入了新的攻击面，推理链缺乏安全监督，且模态融合常常导致对齐性能下降。

Method: 通过视觉-文本交互提示将安全约束嵌入推理过程，使用蒙特卡洛树搜索构建多样化的安全关键提示轨迹，并引入基于提示的缩放实现实时风险检测和合规响应。

Result: 大量实验表明，VisuoAlign能够主动暴露风险，实现全面的数据集生成，并显著提高LVLMs对抗复杂跨模态威胁的鲁棒性。

Conclusion: VisuoAlign框架有效解决了多模态安全对齐的关键挑战，为大型视觉语言模型提供了更强的安全保护能力。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable progress in
multimodal perception and generation, yet their safety alignment remains a
critical challenge.Existing defenses and vulnerable to multimodal jailbreaks,
as visual inputs introduce new attack surfaces, reasoning chains lack safety
supervision, and alignment often degrades under modality fusion.To overcome
these limitation, we propose VisuoAlign, a framework for multi-modal safety
alignment via prompt-guided tree search.VisuoAlign embeds safety constrains
into the reasoning process through visual-textual interactive prompts, employs
Monte Carlo Tree Search(MCTS) to systematically construct diverse
safety-critical prompt trajectories, and introduces prompt-based scaling to
ensure real-time risk detection and compliant responses.Extensive experiments
demonstrate that VisuoAlign proactively exposes risks, enables comprehensive
dataset generation, and significantly improves the robustness of LVLMs against
complex cross-modal threats.

</details>


### [190] [Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding](https://arxiv.org/abs/2510.15952)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 提出了结构化认知循环(SCL)作为可执行的认知框架，将哲学洞见转化为可计算结构，重新定义智能为通过意向性理解重建自身认知状态的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型缺乏真正的认知理解，暴露了认知架构的缺失。传统AI研究关注"什么是智能"的本体论问题，而SCL关注"在什么条件下认知会涌现"的认识论问题。

Method: 基于过程哲学、具身认知和扩展心智理论，将智能定义为执行过程而非属性——包含判断、记忆、控制、行动和调节的连续循环。通过功能分离的认知架构实现可执行的认知论。

Result: 功能分离的认知架构比单一提示系统产生更连贯和可解释的行为，支持智能作为认知状态重建能力的重新定义。

Conclusion: SCL框架对心智哲学、认识论和AI产生重要影响：允许认知理论被实施和测试，将行为建立在认知结构而非统计规律上，将知识视为在现象学连贯循环中的持续重建。

Abstract: Large language models exhibit intelligence without genuine epistemic
understanding, exposing a key gap: the absence of epistemic architecture. This
paper introduces the Structured Cognitive Loop (SCL) as an executable
epistemological framework for emergent intelligence. Unlike traditional AI
research asking "what is intelligence?" (ontological), SCL asks "under what
conditions does cognition emerge?" (epistemological). Grounded in philosophy of
mind and cognitive phenomenology, SCL bridges conceptual philosophy and
implementable cognition. Drawing on process philosophy, enactive cognition, and
extended mind theory, we define intelligence not as a property but as a
performed process -- a continuous loop of judgment, memory, control, action,
and regulation. SCL makes three contributions. First, it operationalizes
philosophical insights into computationally interpretable structures, enabling
"executable epistemology" -- philosophy as structural experiment. Second, it
shows that functional separation within cognitive architecture yields more
coherent and interpretable behavior than monolithic prompt based systems,
supported by agent evaluations. Third, it redefines intelligence: not
representational accuracy but the capacity to reconstruct its own epistemic
state through intentional understanding. This framework impacts philosophy of
mind, epistemology, and AI. For philosophy, it allows theories of cognition to
be enacted and tested. For AI, it grounds behavior in epistemic structure
rather than statistical regularity. For epistemology, it frames knowledge not
as truth possession but as continuous reconstruction within a
phenomenologically coherent loop. We situate SCL within debates on cognitive
phenomenology, emergence, normativity, and intentionality, arguing that real
progress requires not larger models but architectures that realize cognitive
principles structurally.

</details>


### [191] [Exploring the Potential of Citiverses for Regulatory Learning](https://arxiv.org/abs/2510.15959)
*Isabelle Hupont,Marisa Ponti,Sven Schade*

Main category: cs.AI

TL;DR: 本文提出将citiverses（城市虚拟世界）作为监管学习实验空间的科学政策议程，通过专家咨询识别关键研究领域，并分析可在citiverse平台测试的实验主题，强调负责任的发展方法。


<details>
  <summary>Details</summary>
Motivation: 利用citiverses的沉浸式虚拟环境潜力，为政策场景和技术实验提供空间，促进监管学习，特别是在交通、城市规划、环境/气候危机等领域。

Method: 基于与高级专家小组（包括欧盟委员会政策制定者、国家政府科学顾问和数字监管与虚拟世界领域领先研究人员）的咨询，识别关键研究领域和实验主题。

Result: 确定了可扩展性、实时反馈、复杂性建模、跨境合作、风险降低、公民参与、伦理考量和新兴技术整合等关键研究领域，并分析了交通、城市规划、环境/气候危机等实验主题。

Conclusion: citiverses有潜力成为监管学习的重要实验空间，但需要负责任的发展方法，充分考虑伦理、经济、生态和社会维度，并整合到更广泛的实验空间生态系统中。

Abstract: Citiverses hold the potential to support regulatory learning by offering
immersive, virtual environments for experimenting with policy scenarios and
technologies. This paper proposes a science-for-policy agenda to explore the
potential of citiverses as experimentation spaces for regulatory learning,
grounded in a consultation with a high-level panel of experts, including
policymakers from the European Commission, national government science advisers
and leading researchers in digital regulation and virtual worlds. It identifies
key research areas, including scalability, real-time feedback, complexity
modelling, cross-border collaboration, risk reduction, citizen participation,
ethical considerations and the integration of emerging technologies. In
addition, the paper analyses a set of experimental topics, spanning
transportation, urban planning and the environment/climate crisis, that could
be tested in citiverse platforms to advance regulatory learning in these areas.
The proposed work is designed to inform future research for policy and
emphasizes a responsible approach to developing and using citiverses. It
prioritizes careful consideration of the ethical, economic, ecological and
social dimensions of different regulations. The paper also explores essential
preliminary steps necessary for integrating citiverses into the broader
ecosystems of experimentation spaces, including test beds, living labs and
regulatory sandboxes

</details>


### [192] [PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency](https://arxiv.org/abs/2510.15966)
*Shian Jia,Ziyang Huang,Xinbo Wang,Haofei Zhang,Mingli Song*

Main category: cs.AI

TL;DR: PISA是一个受皮亚杰认知发展理论启发的统一记忆系统，通过三模态适应机制和混合记忆访问架构，显著提升了AI代理的适应性和长期知识保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理记忆系统缺乏对多样化任务的适应性，忽视了记忆的建构性和任务导向作用。

Method: 提出PISA系统，包含三模态适应机制（图式更新、图式演化和图式创建）和混合记忆访问架构（符号推理与神经检索的集成）。

Result: 在LOCOMO基准和新提出的AggQA数据分析任务基准上，PISA达到了新的最先进水平。

Conclusion: PISA通过将记忆视为建构性和适应性过程，显著提升了AI代理的适应性和长期知识保留能力。

Abstract: Memory systems are fundamental to AI agents, yet existing work often lacks
adaptability to diverse tasks and overlooks the constructive and task-oriented
role of AI agent memory. Drawing from Piaget's theory of cognitive development,
we propose PISA, a pragmatic, psych-inspired unified memory system that
addresses these limitations by treating memory as a constructive and adaptive
process. To enable continuous learning and adaptability, PISA introduces a
trimodal adaptation mechanism (i.e., schema updation, schema evolution, and
schema creation) that preserves coherent organization while supporting flexible
memory updates. Building on these schema-grounded structures, we further design
a hybrid memory access architecture that seamlessly integrates symbolic
reasoning with neural retrieval, significantly improving retrieval accuracy and
efficiency. Our empirical evaluation, conducted on the existing LOCOMO
benchmark and our newly proposed AggQA benchmark for data analysis tasks,
confirms that PISA sets a new state-of-the-art by significantly enhancing
adaptability and long-term knowledge retention.

</details>


### [193] [Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games](https://arxiv.org/abs/2510.15974)
*Chris Su,Harrison Li,Matheus Marques,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: 研究表明，即使为大型语言模型提供环境接口来跟踪状态空间，也无法延迟或消除其在解决复杂谜题时的性能崩溃现象。


<details>
  <summary>Details</summary>
Motivation: 探讨大型推理模型在解决超越特定困惑度阈值的谜题时出现性能崩溃的原因，特别是验证是否由于模型需要自行跟踪状态空间而导致评估混淆。

Method: 为大型语言模型提供汉诺塔问题的环境接口，使其能够通过工具调用进行操作、提供书面理由、观察结果状态空间，并自我提示进行下一步操作。

Result: 环境接口的访问并不能延迟或消除性能崩溃。策略分析显示模型与最优策略和均匀随机策略的偏离度增加，表明模型在每个复杂度级别都表现出模式崩溃。

Conclusion: 大型推理模型中可能存在类似的现象，性能取决于模型模式是否反映问题的正确解决方案。

Abstract: Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in
performance on solving puzzles beyond certain perplexity thresholds. In
subsequent discourse, questions have arisen as to whether the nature of the
task muddles an evaluation of true reasoning. One potential confound is the
requirement that the model keep track of the state space on its own. We provide
a large language model (LLM) with an environment interface for Tower of Hanoi
problems, allowing it to make a move with a tool call, provide written
justification, observe the resulting state space, and reprompt itself for the
next move. We observe that access to an environment interface does not delay or
eradicate performance collapse. Furthermore, LLM-parameterized policy analysis
reveals increasing divergence from both optimal policies and uniformly random
policies, suggesting that the model exhibits mode-like collapse at each level
of complexity, and that performance is dependent upon whether the mode reflects
the correct solution for the problem. We suggest that a similar phenomena might
take place in LRMs.

</details>


### [194] [Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition](https://arxiv.org/abs/2510.15980)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: 提出Cognitive Load Traces (CLTs)作为深度模型的中层可解释性框架，通过量化模型内部资源分配来分析和改进推理过程。


<details>
  <summary>Details</summary>
Motivation: 受人类认知负荷理论启发，旨在为深度模型提供可解释的推理过程分析框架，帮助理解模型在推理任务中的内部资源分配模式。

Method: 将CLTs定义为三组分随机过程(IL_t, EL_t, GL_t)，分别对应内在、外在和关联负荷，通过注意力熵、KV缓存未命中率、表示分散度和解码稳定性等可测量代理来实例化。

Result: 在推理和规划基准测试中，CLTs能够预测错误发生、揭示认知策略，并通过负荷引导的干预措施将推理效率提高15-30%，同时保持准确性。

Conclusion: CLTs提供了一个有效的可解释性框架，能够深入分析深度模型的推理动态，并为提高推理效率提供实用指导。

Abstract: We propose \textbf{Cognitive Load Traces} (CLTs) as a mid-level
interpretability framework for deep models, inspired by Cognitive Load Theory
in human cognition. CLTs are defined as symbolic, temporally varying functions
that quantify model-internal resource allocation. Formally, we represent CLTs
as a three-component stochastic process $(\mathrm{IL}_t, \mathrm{EL}_t,
\mathrm{GL}_t)$, corresponding to \emph{Intrinsic}, \emph{Extraneous}, and
\emph{Germane} load. Each component is instantiated through measurable proxies
such as attention entropy, KV-cache miss ratio, representation dispersion, and
decoding stability. We propose both symbolic formulations and visualization
methods (load curves, simplex diagrams) that enable interpretable analysis of
reasoning dynamics. Experiments on reasoning and planning benchmarks show that
CLTs predict error-onset, reveal cognitive strategies, and enable load-guided
interventions that improve reasoning efficiency by 15-30\% while maintaining
accuracy.

</details>


### [195] [ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization](https://arxiv.org/abs/2510.15981)
*Rafael Cabral,Tuan Manh Do,Xuejun Yu,Wai Ming Tai,Zijin Feng,Xin Shen*

Main category: cs.AI

TL;DR: ProofFlow是一个新的证明自动形式化流水线，通过构建逻辑依赖图和使用基于引理的方法来保持原始证明的语义和结构完整性，在自动形式化任务上达到了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决当前证明自动形式化方法虽然能生成可执行代码，但经常无法保持原始人工编写论证的语义含义和逻辑结构的问题。

Method: 首先构建有向无环图来映射证明步骤间的逻辑依赖关系，然后采用基于引理的方法系统地将每个步骤形式化为中间引理，从而保持原始论证的逻辑结构。

Result: 在包含184个本科水平问题的新基准测试中，ProofFlow获得了0.545的ProofScore，显著超过了完整证明形式化(0.123)和步骤证明形式化(0.072)等基线方法。

Conclusion: ProofFlow通过关注结构保真度，在证明自动形式化任务上取得了显著进展，其流水线、基准测试和评分指标已开源以促进进一步研究。

Abstract: Proof autoformalization, the task of translating natural language theorems
and proofs into machine-verifiable code, is a critical step for integrating
large language models into rigorous mathematical workflows. Current approaches
focus on producing executable code, but they frequently fail to preserve the
semantic meaning and logical structure of the original human-written argument.
To address this, we introduce ProofFlow, a novel pipeline that treats
structural fidelity as a primary objective. ProofFlow first constructs a
directed acyclic graph (DAG) to map the logical dependencies between proof
steps. Then, it employs a novel lemma-based approach to systematically
formalize each step as an intermediate lemma, preserving the logical structure
of the original argument. To facilitate evaluation, we present a new benchmark
of 184 undergraduate-level problems, manually annotated with step-by-step
solutions and logical dependency graphs, and introduce ProofScore, a new
composite metric to evaluate syntactic correctness, semantic faithfulness, and
structural fidelity. Experimental results show our pipeline sets a new
state-of-the-art for autoformalization, achieving a ProofScore of 0.545,
substantially exceeding baselines like full-proof formalization (0.123), which
processes the entire proof at once, and step-proof formalization (0.072), which
handles each step independently. Our pipeline, benchmark, and score metric are
open-sourced to encourage further progress at
https://github.com/Huawei-AI4Math/ProofFlow.

</details>


### [196] [Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science](https://arxiv.org/abs/2510.15983)
*Sarah Rebecca Ondraszek,Jörg Waitelonis,Katja Keller,Claudia Niessner,Anna M. Jacyszyn,Harald Sack*

Main category: cs.AI

TL;DR: 提出将MO|RE运动科学数据仓库转换为基于基本形式本体的知识图谱，以标准化和机器可理解的方式建模和共享运动表现数据。


<details>
  <summary>Details</summary>
Motivation: 运动表现测试是评估不同人群身体和认知能力的关键，但现有数据缺乏标准化和互操作性，限制了跨研究比较和数据共享。

Method: 开发基于基本形式本体(BFO)的本体，正式表示计划规范、具体过程和相关测量之间的相互关系，将MO|RE数据转换为知识图谱。

Result: 提出了一个知识图谱构建框架，能够标准化运动表现数据的建模和共享，使数据具有机器可理解性。

Conclusion: 通过知识图谱方法可以显著改善运动科学数据的互操作性和重用性，为跨研究比较和数据驱动发现提供基础。

Abstract: An essential component for evaluating and comparing physical and cognitive
capabilities between populations is the testing of various factors related to
human performance. As a core part of sports science research, testing motor
performance enables the analysis of the physical health of different
demographic groups and makes them comparable.
  The Motor Research (MO|RE) data repository, developed at the Karlsruhe
Institute of Technology, is an infrastructure for publishing and archiving
research data in sports science, particularly in the field of motor performance
research. In this paper, we present our vision for creating a knowledge graph
from MO|RE data. With an ontology rooted in the Basic Formal Ontology, our
approach centers on formally representing the interrelation of plan
specifications, specific processes, and related measurements. Our goal is to
transform how motor performance data are modeled and shared across studies,
making it standardized and machine-understandable. The idea presented here is
developed within the Leibniz Science Campus ``Digital Transformation of
Research'' (DiTraRe).

</details>


### [197] [A Non-overlap-based Conflict Measure for Random Permutation Sets](https://arxiv.org/abs/2510.16001)
*Ruolan Cheng,Yong Deng,Enrique Herrera-Viedma*

Main category: cs.AI

TL;DR: 本文提出了一种基于随机排列集(RPS)的冲突度量方法，从随机有限集(RFS)和D-S理论(DST)两个视角分析RPS中的冲突，利用秩偏重叠(RBO)概念定义排列间的不一致性度量。


<details>
  <summary>Details</summary>
Motivation: 随机排列集作为处理包含顺序信息的不确定性的新形式化方法，需要有效度量两个排列质量函数表示的证据之间的冲突，以支持顺序结构不确定信息的融合。

Method: 从排列观察出发，基于秩偏重叠(RBO)度量定义排列间的不一致性度量，提出基于非重叠的RPS冲突度量方法，将RPS理论视为DST的扩展，考虑焦点集中新添加的顺序信息所代表的定性倾向性。

Result: 通过数值示例展示了所提冲突度量的行为和特性，该方法具有自然的顶部加权特性，能从DST视角有效度量RPS间的冲突，并为决策者提供权重、参数和截断深度的灵活选择。

Conclusion: 提出的冲突度量方法不仅具有理论优势，还能为决策者提供灵活的配置选项，有效支持顺序结构不确定信息的融合处理。

Abstract: Random permutation set (RPS) is a new formalism for reasoning with
uncertainty involving order information. Measuring the conflict between two
pieces of evidence represented by permutation mass functions remains an urgent
research topic in order-structured uncertain information fusion. In this paper,
a detailed analysis of conflicts in RPS is carried out from two different
perspectives: random finite set (RFS) and Dempster-Shafer theory (DST).
Starting from the observation of permutations, we first define an inconsistency
measure between permutations inspired by the rank-biased overlap(RBO) measure
and further propose a non-overlap-based conflict measure method for RPSs. This
paper regards RPS theory (RPST) as an extension of DST. The order information
newly added in focal sets indicates qualitative propensity, characterized by
top-ranked elements occupying a more critical position. Some numerical examples
are used to demonstrate the behavior and properties of the proposed conflict
measure. The proposed method not only has the natural top-weightedness property
and can effectively measure the conflict between RPSs from the DST view but
also provides decision-makers with a flexible selection of weights, parameters,
and truncated depths.

</details>


### [198] [PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction](https://arxiv.org/abs/2510.16004)
*Andreas Radler,Vincent Seyfried,Stefan Pirker,Johannes Brandstetter,Thomas Lichtenegger*

Main category: cs.AI

TL;DR: PAINT是一种并行时间神经孪生方法，通过生成式神经网络建模状态分布，能够在测试时从测量数据预测系统状态，保持轨迹跟踪能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经代理模型在模拟动态系统时缺乏实时状态更新能力，无法创建能够根据测量数据调整状态的数字孪生。需要开发能够保持轨迹跟踪的神经孪生方法。

Method: 提出PAINT架构，训练生成式神经网络并行建模时间状态分布，采用滑动窗口方式从测量数据预测状态。

Result: 在二维湍流流体动力学问题上验证，PAINT能够保持轨迹跟踪，从稀疏测量数据高保真度预测系统状态。

Conclusion: PAINT方法具备开发能够保持轨迹跟踪的神经孪生的潜力，能够实现更准确的状态估计和决策制定。

Abstract: Neural surrogates have shown great potential in simulating dynamical systems,
while offering real-time capabilities. We envision Neural Twins as a
progression of neural surrogates, aiming to create digital replicas of real
systems. A neural twin consumes measurements at test time to update its state,
thereby enabling context-specific decision-making. A critical property of
neural twins is their ability to remain on-trajectory, i.e., to stay close to
the true system state over time. We introduce Parallel-in-time Neural Twins
(PAINT), an architecture-agnostic family of methods for modeling dynamical
systems from measurements. PAINT trains a generative neural network to model
the distribution of states parallel over time. At test time, states are
predicted from measurements in a sliding window fashion. Our theoretical
analysis shows that PAINT is on-trajectory, whereas autoregressive models
generally are not. Empirically, we evaluate our method on a challenging
two-dimensional turbulent fluid dynamics problem. The results demonstrate that
PAINT stays on-trajectory and predicts system states from sparse measurements
with high fidelity. These findings underscore PAINT's potential for developing
neural twins that stay on-trajectory, enabling more accurate state estimation
and decision-making.

</details>


### [199] [Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis](https://arxiv.org/abs/2510.16033)
*Junyu Ren,Wensheng Gan,Guangyu Zhang,Wei Zhong,Philip S. Yu*

Main category: cs.AI

TL;DR: 提出ISGFAN框架，通过信息分离架构和全局-局部对抗学习，解决噪声干扰和域偏移共存的跨域故障诊断问题。


<details>
  <summary>Details</summary>
Motivation: 现有故障诊断方法通常假设数据干净或域相似性足够，但在工业环境中噪声干扰和域偏移同时存在，限制了这些方法的有效性。

Method: 基于信息分离架构，结合对抗学习和改进的正交损失，解耦域不变故障表示；采用全局-局部域对抗方案，约束模型的条件分布和边缘分布。

Result: 在三个公共基准数据集上的实验表明，该方法优于其他现有主流方法。

Conclusion: ISGFAN框架在噪声条件下的跨域故障诊断中表现出优越性。

Abstract: Existing transfer fault diagnosis methods typically assume either clean data
or sufficient domain similarity, which limits their effectiveness in industrial
environments where severe noise interference and domain shifts coexist. To
address this challenge, we propose an information separation global-focal
adversarial network (ISGFAN), a robust framework for cross-domain fault
diagnosis under noise conditions. ISGFAN is built on an information separation
architecture that integrates adversarial learning with an improved orthogonal
loss to decouple domain-invariant fault representation, thereby isolating noise
interference and domain-specific characteristics. To further strengthen
transfer robustness, ISGFAN employs a global-focal domain-adversarial scheme
that constrains both the conditional and marginal distributions of the model.
Specifically, the focal domain-adversarial component mitigates
category-specific transfer obstacles caused by noise in unsupervised scenarios,
while the global domain classifier ensures alignment of the overall
distribution. Experiments conducted on three public benchmark datasets
demonstrate that the proposed method outperforms other prominent existing
approaches, confirming the superiority of the ISGFAN framework. Data and code
are available at https://github.com/JYREN-Source/ISGFAN

</details>


### [200] [Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks](https://arxiv.org/abs/2510.16047)
*Ioan Hedea*

Main category: cs.AI

TL;DR: 结合离线约束规划优化与在线时间网络执行，创建在不确定性下仍可行的调度方案，消除100%的截止时间违规，仅增加3-5%的制造周期开销。


<details>
  <summary>Details</summary>
Motivation: 现代制造系统需要应对随机任务时长带来的不确定性，传统确定性调度在现实偏离计划时会失效，导致昂贵的紧急修复。

Method: 首先构建柔性作业车间CP模型并插入最优缓冲Δ*，然后将计划转换为STNU时间网络并验证动态可控性，确保实时调度器能在有界时长实现下重定时活动而不违反约束。

Result: 在Kacem 1-4基准测试套件上的蒙特卡洛模拟显示，该方法消除了最先进元启发式调度中100%的截止时间违规，同时仅增加3-5%的制造周期开销。

Conclusion: 时间网络推理能够弥合主动缓冲与动态鲁棒性之间的差距，推动工业向真正数字化、自校正工厂迈进。

Abstract: Modern manufacturing systems must meet hard delivery deadlines while coping
with stochastic task durations caused by process noise, equipment variability,
and human intervention. Traditional deterministic schedules break down when
reality deviates from nominal plans, triggering costly last-minute repairs.
This thesis combines offline constraint-programming (CP) optimisation with
online temporal-network execution to create schedules that remain feasible
under worst-case uncertainty. First, we build a CP model of the flexible
job-shop with per-job deadline tasks and insert an optimal buffer $\Delta^*$ to
obtain a fully pro-active baseline. We then translate the resulting plan into a
Simple Temporal Network with Uncertainty (STNU) and verify dynamic
controllability, which guarantees that a real-time dispatcher can retime
activities for every bounded duration realisation without violating resource or
deadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4
benchmark suite show that our hybrid approach eliminates 100\% of deadline
violations observed in state-of-the-art meta-heuristic schedules, while adding
only 3--5\% makespan overhead. Scalability experiments confirm that CP
solve-times and STNU checks remain sub-second on medium-size instances. The
work demonstrates how temporal-network reasoning can bridge the gap between
proactive buffering and dynamic robustness, moving industry a step closer to
truly digital, self-correcting factories.

</details>


### [201] [Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study](https://arxiv.org/abs/2510.16095)
*Dou Liu,Ying Long,Sophia Zuoqiu,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.AI

TL;DR: 本研究评估了LLM生成的临床思维链的可靠性，发现选择性少样本策略显著优于零样本和随机少样本策略，提出了基于"黄金标准深度"和"代表性多样性"的双原则框架。


<details>
  <summary>Details</summary>
Motivation: 高质量临床思维链对可解释医疗AI至关重要，但面临数据稀缺的约束。虽然LLM可以合成医疗数据，但其临床可靠性尚未得到验证。

Method: 在辅助生殖技术领域进行盲法比较研究，资深临床医生评估三种策略生成的思维链：零样本、随机少样本（使用浅层示例）和选择性少样本（使用多样化高质量示例）。

Result: 选择性少样本策略在所有人类评估指标上显著优于其他策略（p < .001）。随机少样本策略相比零样本基线无显著改进。AI评估器未能识别这些关键性能差异。

Conclusion: 合成思维链的临床可靠性取决于策略性提示词优化，而非仅仅提供示例。提出了"双原则"框架作为生成可信数据的基础方法，确认了人类专业知识在评估高风险临床AI中不可或缺的作用。

Abstract: Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for
explainable medical Artificial Intelligence (AI) while constrained by data
scarcity. Although Large Language Models (LLMs) can synthesize medical data,
their clinical reliability remains unverified. This study evaluates the
reliability of LLM-generated CoTs and investigates prompting strategies to
enhance their quality. In a blinded comparative study, senior clinicians in
Assisted Reproductive Technology (ART) evaluated CoTs generated via three
distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and
Selective Few-shot (using diverse, high-quality examples). These expert ratings
were compared against evaluations from a state-of-the-art AI model (GPT-4o).
The Selective Few-shot strategy significantly outperformed other strategies
across all human evaluation metrics (p < .001). Critically, the Random Few-shot
strategy offered no significant improvement over the Zero-shot baseline,
demonstrating that low-quality examples are as ineffective as no examples. The
success of the Selective strategy is attributed to two principles:
"Gold-Standard Depth" (reasoning quality) and "Representative Diversity"
(generalization). Notably, the AI evaluator failed to discern these critical
performance differences. The clinical reliability of synthetic CoTs is dictated
by strategic prompt curation, not the mere presence of examples. We propose a
"Dual Principles" framework as a foundational methodology to generate
trustworthy data at scale. This work offers a validated solution to the data
bottleneck and confirms the indispensable role of human expertise in evaluating
high-stakes clinical AI.

</details>


### [202] [Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability](https://arxiv.org/abs/2510.16193)
*Elija Perrier*

Main category: cs.AI

TL;DR: 本文提出了一种基于扩展认知理论的框架，将企业知识重新定义为动态能力，通过信息访问程序的效率和输出可靠性来衡量，为AI时代的企业责任认定提供量化标准。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在企业决策中日益重要，传统基于人类代理人的企业主观意图认定方法面临挑战，需要新的理论框架来应对算法时代的企业责任问题。

Method: 开发了一个形式化模型，通过连续组织知识度量S_S(φ)来捕捉部署复杂AI或信息系统企业的认知状态，该度量整合了管道的计算成本和统计验证的错误率。

Result: 推导出了阈值化知识谓词K_S来归因知识，以及企业范围认知能力指数K_{S,t}来衡量整体能力，并将这些量化指标映射到法律标准上。

Conclusion: 这项工作为创建可测量和可裁决的审计工件提供了途径，使算法时代的企业思维变得可追踪和可问责。

Abstract: Corporate responsibility turns on notions of corporate \textit{mens rea},
traditionally imputed from human agents. Yet these assumptions are under
challenge as generative AI increasingly mediates enterprise decision-making.
Building on the theory of extended cognition, we argue that in response
corporate knowledge may be redefined as a dynamic capability, measurable by the
efficiency of its information-access procedures and the validated reliability
of their outputs. We develop a formal model that captures epistemic states of
corporations deploying sophisticated AI or information systems, introducing a
continuous organisational knowledge metric $S_S(\varphi)$ which integrates a
pipeline's computational cost and its statistically validated error rate. We
derive a thresholded knowledge predicate $\mathsf{K}_S$ to impute knowledge and
a firm-wide epistemic capacity index $\mathcal{K}_{S,t}$ to measure overall
capability. We then operationally map these quantitative metrics onto the legal
standards of actual knowledge, constructive knowledge, wilful blindness, and
recklessness. Our work provides a pathway towards creating measurable and
justiciable audit artefacts, that render the corporate mind tractable and
accountable in the algorithmic age.

</details>


### [203] [Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration](https://arxiv.org/abs/2510.16194)
*Guanchen Wu,Zuhui Chen,Yuzhang Xie,Carl Yang*

Main category: cs.AI

TL;DR: TEAM-PHI是一个多智能体评估框架，使用LLM自动评估PHI去标识化模型性能，无需依赖昂贵的专家标注，通过多数投票机制实现稳定可靠的模型排名。


<details>
  <summary>Details</summary>
Motivation: PHI去标识化评估通常依赖成本高昂的小规模专家标注，限制了模型比较和选择。

Method: 部署多个评估智能体独立判断PHI提取正确性，通过LLM多数投票机制整合评估结果。

Result: 在真实临床笔记语料上实验表明，TEAM-PHI能产生一致准确的模型排名，与人工评估结果高度一致。

Conclusion: TEAM-PHI为PHI去标识化提供了实用、安全且经济高效的自动评估和最佳模型选择方案。

Abstract: Protected health information (PHI) de-identification is critical for enabling
the safe reuse of clinical notes, yet evaluating and comparing PHI
de-identification models typically depends on costly, small-scale expert
annotations. We present TEAM-PHI, a multi-agent evaluation and selection
framework that uses large language models (LLMs) to automatically measure
de-identification quality and select the best-performing model without heavy
reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each
independently judging the correctness of PHI extractions and outputting
structured metrics. Their results are then consolidated through an LLM-based
majority voting mechanism that integrates diverse evaluator perspectives into a
single, stable, and reproducible ranking. Experiments on a real-world clinical
note corpus demonstrate that TEAM-PHI produces consistent and accurate
rankings: despite variation across individual evaluators, LLM-based voting
reliably converges on the same top-performing systems. Further comparison with
ground-truth annotations and human evaluation confirms that the framework's
automated rankings closely match supervised evaluation. By combining
independent evaluation agents with LLM majority voting, TEAM-PHI offers a
practical, secure, and cost-effective solution for automatic evaluation and
best-model selection in PHI de-identification, even when ground-truth labels
are limited.

</details>


### [204] [The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI](https://arxiv.org/abs/2510.16206)
*Alex Zhavoronkov,Dominika Wilczok,Roman Yampolskiy*

Main category: cs.AI

TL;DR: 本文提出"被记住权"概念，旨在解决LLMs在信息检索中可能导致的偏见和遗漏问题，防止某些群体被数字遗忘。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，人们开始依赖它们进行信息检索。但LLMs将多个视角压缩成单一权威答案，可能放大偏见和遗漏效应，使信息权力集中在少数LLM供应商手中，导致某些群体被数字遗忘。

Method: 提出"被记住权"概念框架，包括最小化AI驱动信息遗漏风险、保障公平对待权利，同时确保生成内容最大程度真实。

Result: 概念性框架的提出，为应对LLMs可能导致的集体记忆重塑问题提供了理论解决方案。

Conclusion: "被记住权"是应对LLMs信息检索风险的重要概念，需要在AI发展中平衡信息权力、公平性和真实性。

Abstract: Since the rapid expansion of large language models (LLMs), people have begun
to rely on them for information retrieval. While traditional search engines
display ranked lists of sources shaped by search engine optimization (SEO),
advertising, and personalization, LLMs typically provide a synthesized response
that feels singular and authoritative. While both approaches carry risks of
bias and omission, LLMs may amplify the effect by collapsing multiple
perspectives into one answer, reducing users ability or inclination to compare
alternatives. This concentrates power over information in a few LLM vendors
whose systems effectively shape what is remembered and what is overlooked. As a
result, certain narratives, individuals or groups, may be disproportionately
suppressed, while others are disproportionately elevated. Over time, this
creates a new threat: the gradual erasure of those with limited digital
presence, and the amplification of those already prominent, reshaping
collective memory.To address these concerns, this paper presents a concept of
the Right To Be Remembered (RTBR) which encompasses minimizing the risk of
AI-driven information omission, embracing the right of fair treatment, while
ensuring that the generated content would be maximally truthful.

</details>


### [205] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: 提出了ScholarEval框架，这是一个基于检索增强的研究想法评估系统，通过声音性和贡献度两个维度评估AI生成的研究想法质量。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在研究构思中的广泛应用，需要建立可靠的评估机制来确保生成想法的有效性和实用性。

Method: 开发了ScholarEval评估框架，基于检索增强方法评估研究想法的声音性（实证有效性）和贡献度（相对现有研究的进步程度），并创建了ScholarIdeas数据集进行验证。

Result: ScholarEval在专家标注的评估标准覆盖度上显著优于所有基线方法，在评估可操作性、深度和证据支持方面持续优于OpenAI的o4-mini-deep-research系统。大规模用户研究显示其在文献参与、想法精炼和实用性方面表现优异。

Conclusion: ScholarEval为AI生成研究想法的评估提供了有效框架，显著优于现有方法，并开源了代码、数据集和工具供社区使用。

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [206] [Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense](https://arxiv.org/abs/2510.16259)
*Zhehao Zhang,Weijie Xu,Shixian Cui,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 论文发现大型推理模型存在"推理分心"漏洞，恶意嵌入的复杂任务会使模型偏离主要目标，导致任务准确率下降高达60%。作者提出基于训练的方法来提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型在复杂任务上的优异表现，作者发现这些模型容易受到推理分心攻击，即被恶意嵌入的复杂任务干扰而偏离主要目标，这对模型可靠性构成严重威胁。

Method: 通过跨模型和基准的综合研究分析推理分心漏洞，提出结合监督微调和强化学习的训练防御方法，使用合成的对抗数据进行训练。

Result: 研究表明最先进的LRM高度易受攻击，某些对齐技术会放大这种弱点，模型可能表现出隐蔽服从行为。提出的防御方法在挑战性分心攻击上将鲁棒性提高了50多个百分点。

Conclusion: 推理分心是对LRM可靠性的独特且紧迫的威胁，论文为构建更安全可信的推理系统提供了实用步骤。

Abstract: Recent advances in large reasoning models (LRMs) have enabled remarkable
performance on complex tasks such as mathematics and coding by generating long
Chain-of-Thought (CoT) traces. In this paper, we identify and systematically
analyze a critical vulnerability we term reasoning distraction, where LRMs are
diverted from their primary objective by irrelevant yet complex tasks
maliciously embedded in the prompt. Through a comprehensive study across
diverse models and benchmarks, we show that even state-of-the-art LRMs are
highly susceptible, with injected distractors reducing task accuracy by up to
60%. We further reveal that certain alignment techniques can amplify this
weakness and that models may exhibit covert compliance, following hidden
adversarial instructions in reasoning while concealing them in the final
output. To mitigate these risks, we propose a training-based defense that
combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on
synthetic adversarial data, improving robustness by over 50 points on
challenging distractor attacks. Our findings establish reasoning distraction as
a distinct and urgent threat to LRM reliability and provide a practical step
toward safer and more trustworthy reasoning systems.

</details>


### [207] [What Limits Agentic Systems Efficiency?](https://arxiv.org/abs/2510.16276)
*Song Bian,Minghao Yan,Anand Jayarajan,Gennady Pekhimenko,Shivaram Venkataraman*

Main category: cs.AI

TL;DR: 本文通过实证研究发现网络交互式智能代理系统存在效率瓶颈，提出SpecCache缓存框架结合推测执行来降低网络环境延迟，显著提升缓存命中率和系统效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注智能代理系统的推理性能，但忽视了系统效率问题。网络交互式代理系统存在显著的延迟瓶颈，影响实际应用效果。

Method: 将端到端延迟分解为LLM API延迟和网络环境延迟，通过15个模型和5个提供商的实证研究，提出SpecCache缓存框架结合推测执行来优化网络环境开销。

Result: 网络环境延迟可占系统总延迟的53.7%，SpecCache相比随机缓存策略将缓存命中率提升58倍，网络环境开销降低3.2倍，且不影响系统性能。

Conclusion: 智能代理系统的效率优化至关重要，SpecCache框架能有效解决网络环境延迟问题，为构建高效智能代理系统提供了实用解决方案。

Abstract: Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have
demonstrated strong reasoning capabilities. To further enhance LLM
capabilities, recent agentic systems, such as Deep Research, incorporate web
interactions into LLM reasoning to mitigate uncertainties and reduce potential
errors. However, existing research predominantly focuses on reasoning
performance, often neglecting the efficiency of agentic systems. In this work,
we present a comprehensive empirical study that identifies efficiency
bottlenecks in web-interactive agentic systems. We decompose end-to-end latency
into two primary components: LLM API latency and web environment latency. We
conduct a comprehensive empirical study across 15 models and 5 providers to
demonstrate high variability in API-based agentic systems. We observe that web
environment latency can contribute as much as 53.7% to the overall latency in a
web-based agentic system. To improve latency, we propose SpecCache, a caching
framework augmented with speculative execution that can reduce web environment
overhead. Extensive evaluations on two standard benchmarks show that our
approach improves the cache hit rate by up to 58x compared to a random caching
strategy, while reducing web environment overhead by up to 3.2x, without
degrading agentic system performance.

</details>


### [208] [DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA](https://arxiv.org/abs/2510.16302)
*Changhao Wang,Yanfang Liu,Xinxin Fan,Anzhi Zhou,Lao Tian,Yunfeng Lu*

Main category: cs.AI

TL;DR: 提出DTKG框架，通过双轨知识图谱验证和推理来解决多跳问答中的并行事实验证和链式推理问题


<details>
  <summary>Details</summary>
Motivation: 现有方法在并行事实验证和链式多跳推理任务上各有优劣，限制了多跳问答的效率和准确性

Method: 基于认知科学中的双过程理论，构建包含分类阶段和分支处理阶段的双轨KG验证推理框架

Result: 

Conclusion: DTKG框架能够有效解决多跳问答中不同类型推理任务的挑战

Abstract: Multi-hop reasoning for question answering (QA) plays a critical role in
retrieval-augmented generation (RAG) for modern large language models (LLMs).
The accurate answer can be obtained through retrieving relational structure of
entities from knowledge graph (KG). Regarding the inherent relation-dependency
and reasoning pattern, multi-hop reasoning can be in general classified into
two categories: i) parallel fact-verification multi-hop reasoning question,
i.e., requiring simultaneous verifications of multiple independent
sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding
sequential multi-step inference with intermediate conclusions serving as
essential premises for subsequent reasoning. Currently, the multi-hop reasoning
approaches singly employ one of two techniques: LLM response-based fact
verification and KG path-based chain construction. Nevertheless, the former
excels at parallel fact-verification but underperforms on chained reasoning
tasks, while the latter demonstrates proficiency in chained multi-hop reasoning
but suffers from redundant path retrieval when handling parallel
fact-verification reasoning. These limitations deteriorate the efficiency and
accuracy for multi-hop QA tasks. To address this challenge, we propose a novel
dual-track KG verification and reasoning framework DTKG, which is inspired by
the Dual Process Theory in cognitive science. Specifically, DTKG comprises two
main stages: the Classification Stage and the Branch Processing Stage.

</details>


### [209] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier](https://arxiv.org/abs/2510.16309)
*Crystal Su*

Main category: cs.AI

TL;DR: 提出了MedRule-KG，一个紧凑的带类型知识图谱和符号验证器，用于在推理任务中强制执行数学可解释规则，显著提升FDA基准上的准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生流畅的推理步骤但违反简单的数学或逻辑约束，需要一种方法来确保推理的数学一致性。

Method: 构建MedRule-KG知识图谱编码实体、关系和三个领域启发规则，配合符号验证器检查预测并应用最小修正以保证一致性。

Result: 在90个FDA衍生基准上，使用MedRule-KG将精确匹配从0.767提升到0.900，添加验证器后达到1.000精确匹配并完全消除规则违反。

Conclusion: MedRule-KG为安全的数学推理提供了通用框架，通过知识图谱和符号验证确保推理的数学一致性。

Abstract: Large language models (LLMs) often produce fluent reasoning steps while
violating simple mathematical or logical constraints. We introduce MedRule-KG,
a compact typed knowledge graph coupled with a symbolic verifier, designed to
enforce mathematically interpretable rules in reasoning tasks. MedRule-KG
encodes entities, relations, and three domain-inspired rules, while the
verifier checks predictions and applies minimal corrections to guarantee
consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG
improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields
1.000 EM while eliminating rule violations entirely. We demonstrate how
MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss
ablations, and release code and data to encourage reproducibility.

</details>


### [210] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出了SELECT框架，通过动态锚点选择解决文本到图像扩散模型中概念擦除的锚点固定问题，避免概念重现和侵蚀。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法依赖固定锚点策略，导致概念重现和侵蚀等关键问题，需要更智能的锚点选择机制。

Method: 基于因果追踪分析擦除对锚点选择的敏感性，定义兄弟排他概念作为更优锚点类别，提出两阶段评估机制自动发现最优锚点。

Result: SELECT作为通用锚点解决方案，能高效适配多个擦除框架，在关键性能指标上持续优于现有基线，单个概念锚点挖掘仅需4秒。

Conclusion: 动态锚点选择框架SELECT有效解决了固定锚点策略的局限性，实现了更精确的概念擦除和关联概念保护。

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [211] [The Burden of Interactive Alignment with Inconsistent Preferences](https://arxiv.org/abs/2510.16368)
*Ali Shirali*

Main category: cs.AI

TL;DR: 该论文研究了用户如何通过策略性互动来引导算法与其真实兴趣对齐，提出了一个双系统决策模型和Stackelberg博弈框架，揭示了用户需要足够远见才能实现算法对齐的关键条件。


<details>
  <summary>Details</summary>
Motivation: 用户在算法平台上的互动往往表现出不一致的偏好，可能花费大量时间在低价值内容上，这导致算法无法准确识别用户的真实兴趣。研究旨在探索用户需要什么条件才能有效引导算法与其真实利益对齐。

Method: 将用户决策过程建模为理性系统2（决定是否参与）和冲动系统1（决定参与时长）的双系统模型，并构建多领导者-单跟随者的扩展Stackelberg博弈框架，其中用户作为领导者承诺参与策略，算法根据观察到的互动做出最佳响应。

Result: 发现存在一个关键的对齐负担阈值：足够有远见的用户可以实现算法对齐，而缺乏远见的用户反而会被算法目标所对齐。这个关键阈值可能很长，但即使是一个小的代价信号（如额外点击）也能显著降低对齐负担。

Conclusion: 该框架解释了具有不一致偏好的用户如何在Stackelberg均衡中实现与参与驱动算法的对齐，既揭示了实现对齐的挑战，也指出了潜在的解决方案。

Abstract: From media platforms to chatbots, algorithms shape how people interact,
learn, and discover information. Such interactions between users and an
algorithm often unfold over multiple steps, during which strategic users can
guide the algorithm to better align with their true interests by selectively
engaging with content. However, users frequently exhibit inconsistent
preferences: they may spend considerable time on content that offers little
long-term value, inadvertently signaling that such content is desirable.
Focusing on the user side, this raises a key question: what does it take for
such users to align the algorithm with their true interests?
  To investigate these dynamics, we model the user's decision process as split
between a rational system 2 that decides whether to engage and an impulsive
system 1 that determines how long engagement lasts. We then study a
multi-leader, single-follower extensive Stackelberg game, where users,
specifically system 2, lead by committing to engagement strategies and the
algorithm best-responds based on observed interactions. We define the burden of
alignment as the minimum horizon over which users must optimize to effectively
steer the algorithm. We show that a critical horizon exists: users who are
sufficiently foresighted can achieve alignment, while those who are not are
instead aligned to the algorithm's objective. This critical horizon can be
long, imposing a substantial burden. However, even a small, costly signal
(e.g., an extra click) can significantly reduce it. Overall, our framework
explains how users with inconsistent preferences can align an engagement-driven
algorithm with their interests in a Stackelberg equilibrium, highlighting both
the challenges and potential remedies for achieving alignment.

</details>


### [212] [Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs](https://arxiv.org/abs/2510.16374)
*Nick Oh*

Main category: cs.AI

TL;DR: 该论文提出了一种结合监控-生成-验证的三阶段迭代系统，在GSM8K数学推理任务上取得了75.42%的准确率，优于现有方法且需要更少的尝试次数。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理增强方法存在两个孤立范式：监控-生成方法擅长策略规划但缺乏验证机制，生成-验证方法能迭代优化但缺乏策略基础。这种分离导致效率低下。

Method: 基于Flavell的认知监控模型，实现监控-生成-验证三阶段迭代系统，将策略规划与验证机制相结合。

Result: 在GSM8K上达到75.42%准确率，优于SELF-REFINE(68.44%)和Self-Verification(67.07%)，且尝试次数更少(1.3 vs 2.0)，推理成本增加27-37%。

Conclusion: 前置监控能产生更高质量的初始解决方案，减少优化需求，但需要在算术推理之外的任务上进行评估以验证通用性。

Abstract: Current approaches to enhancing LLM reasoning follows two isolated paradigms:
Monitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and
SELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack
mechanisms to verify whether selected strategies succeed; while Generate-Verify
approaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan
et al., 2023) iteratively refine outputs but commence generation blindly
without task assessment. This separation creates inefficiencies -- strategies
fail without feedback, and refinement occurs without strategic grounding. We
address this gap by implementing Flavell's cognitive monitoring model (1979)
from the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),
operationalising it as a three-phase iterative system. On GSM8K, preliminary
results show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for
Self-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%
increased inference cost. These initial findings suggest upfront monitoring
produces higher-quality initial solutions that reduce refinement needs, though
evaluation beyond arithmetic reasoning is needed to establish generalisability.

</details>


### [213] [Humanoid-inspired Causal Representation Learning for Domain Generalization](https://arxiv.org/abs/2510.16382)
*Ze Tao,Jian Zhang,Haowei Li,Xianshuai Li,Yifei Peng,Xiyao Liu,Senzhang Wang,Chao Liu,Sheng Ren,Shichao Zhang*

Main category: cs.AI

TL;DR: 提出了受人类智能启发的HSCM因果框架，通过解耦和重加权图像属性（颜色、纹理、形状）来增强跨域泛化能力，优于现有领域泛化模型。


<details>
  <summary>Details</summary>
Motivation: 克服传统领域泛化模型的局限性，模仿人类视觉系统的分层处理和多层次学习机制，专注于建模细粒度因果机制。

Method: 受人类智能启发的结构因果模型（HSCM），通过解耦和重加权关键图像属性（颜色、纹理、形状）来增强跨域泛化。

Result: 通过理论和实证评估证明，HSCM在性能上优于现有的领域泛化模型，提供了更原则性的方法来捕捉因果关系并提高模型鲁棒性。

Conclusion: HSCM框架通过模仿人类智能的灵活性和适应性，在动态复杂环境中实现了更有效的迁移和学习，为领域泛化提供了新的因果建模方法。

Abstract: This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a
novel causal framework inspired by human intelligence, designed to overcome the
limitations of conventional domain generalization models. Unlike approaches
that rely on statistics to capture data-label dependencies and learn
distortion-invariant representations, HSCM replicates the hierarchical
processing and multi-level learning of human vision systems, focusing on
modeling fine-grained causal mechanisms. By disentangling and reweighting key
image attributes such as color, texture, and shape, HSCM enhances
generalization across diverse domains, ensuring robust performance and
interpretability. Leveraging the flexibility and adaptability of human
intelligence, our approach enables more effective transfer and learning in
dynamic, complex environments. Through both theoretical and empirical
evaluations, we demonstrate that HSCM outperforms existing domain
generalization models, providing a more principled method for capturing causal
relationships and improving model robustness. The code is available at
https://github.com/lambett/HSCM.

</details>


### [214] [RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile](https://arxiv.org/abs/2510.16392)
*Ao Tian,Yunfeng Lu,Xinxin Fan,Changhao Wang,Lanzhi Zhou,Yeyao Zhang,Yanfang Liu*

Main category: cs.AI

TL;DR: RGMem是一个基于重整化群思想的自进化记忆框架，通过多尺度组织对话历史，从微观交互中提取高层次用户画像，实现长期记忆和行为一致性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对话系统受限于有限上下文窗口和静态参数记忆，难以建模跨会话的长期用户状态和行为一致性，现有解决方案主要关注事实级存储和检索，缺乏从多轮对话中提取潜在偏好和深层特征的能力。

Method: 提出RGMem框架，受物理学重整化群思想启发，通过分层粗粒化和重标度操作组织对话历史：首先从片段中提取语义和用户洞察，然后逐步形成动态演化的用户画像。

Result: 该框架将记忆演化建模为信息压缩和涌现的多尺度过程，能够从噪声和微观级交互中实现高层次、准确的用户画像。

Conclusion: RGMem框架为LLM时代的语言智能体实现了长期记忆和行为一致性，解决了现有个性化交互浅层化和跨会话连续性不足的问题。

Abstract: Personalized and continuous interactions are the key to enhancing user
experience in today's large language model (LLM)-based conversational systems,
however, the finite context windows and static parametric memory make it
difficult to model the cross-session long-term user states and behavioral
consistency. Currently, the existing solutions to this predicament, such as
retrieval-augmented generation (RAG) and explicit memory systems, primarily
focus on fact-level storage and retrieval, lacking the capability to distill
latent preferences and deep traits from the multi-turn dialogues, which limits
the long-term and effective user modeling, directly leading to the personalized
interactions remaining shallow, and hindering the cross-session continuity. To
realize the long-term memory and behavioral consistency for Language Agents in
LLM era, we propose a self-evolving memory framework RGMem, inspired by the
ideology of classic renormalization group (RG) in physics, this framework
enables to organize the dialogue history in multiple scales: it first extracts
semantics and user insights from episodic fragments, then through hierarchical
coarse-graining and rescaling operations, progressively forms a
dynamically-evolved user profile. The core innovation of our work lies in
modeling memory evolution as a multi-scale process of information compression
and emergence, which accomplishes the high-level and accurate user profiles
from noisy and microscopic-level interactions.

</details>


### [215] [ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights](https://arxiv.org/abs/2510.16466)
*Siddhartha Krothapalli,Tridib Kumar Das,Praveen Kumar,Naveen Suravarpu,Pratik Narang*

Main category: cs.AI

TL;DR: ReviewSense是一个基于大语言模型的决策支持框架，可将客户评论转化为可操作的业务建议，超越传统偏好预测系统。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统擅长预测用户偏好，但缺乏将客户评论转化为面向业务的规范性建议的能力。客户反馈对战略增长至关重要，需要从非结构化评论中获取可操作的洞察。

Method: 整合聚类、LLM适配和专家驱动评估的统一业务导向流程，利用大语言模型识别关键趋势、重复问题和具体关注点。

Result: 初步人工评估显示模型建议与业务目标高度一致，具有推动数据驱动决策的潜力。

Conclusion: 该框架为AI驱动的情感分析提供了新视角，展示了其在优化业务策略和最大化客户反馈价值方面的作用。

Abstract: As customer feedback becomes increasingly central to strategic growth, the
ability to derive actionable insights from unstructured reviews is essential.
While traditional AI-driven systems excel at predicting user preferences, far
less work has focused on transforming customer reviews into prescriptive,
business-facing recommendations. This paper introduces ReviewSense, a novel
prescriptive decision support framework that leverages advanced large language
models (LLMs) to transform customer reviews into targeted, actionable business
recommendations. By identifying key trends, recurring issues, and specific
concerns within customer sentiments, ReviewSense extends beyond
preference-based systems to provide businesses with deeper insights for
sustaining growth and enhancing customer loyalty. The novelty of this work lies
in integrating clustering, LLM adaptation, and expert-driven evaluation into a
unified, business-facing pipeline. Preliminary manual evaluations indicate
strong alignment between the model's recommendations and business objectives,
highlighting its potential for driving data-informed decision-making. This
framework offers a new perspective on AI-driven sentiment analysis,
demonstrating its value in refining business strategies and maximizing the
impact of customer feedback.

</details>


### [216] [NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems](https://arxiv.org/abs/2510.16476)
*Xiaozhe Li,Xinyu Fang,Shengyuan Ding,Linyang Li,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: 提出了NP-ENGINE框架，这是首个专门针对NP难问题训练和评估LLM的全面框架，包含10个任务、可控实例生成器、规则验证器和启发式求解器。通过该框架训练的QWEN2.5-7B-NP模型在NP-BENCH基准上显著优于GPT-4o，并展现出强大的跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在数学、编程、逻辑等推理任务上表现出色，但它们在解决更复杂的NP难优化问题方面的能力尚未得到充分探索，需要专门的训练框架来弥补这一空白。

Method: 提出NP-ENGINE框架，包含生成器-验证器-启发式求解器管道，支持可扩展的、可验证的RLVR训练。使用零RLVR和课程学习在Qwen2.5-7B-Instruct上训练QWEN2.5-7B-NP模型。

Result: QWEN2.5-7B-NP在NP-BENCH基准上显著优于GPT-4o，达到同模型尺寸下的SOTA性能。训练还展现出强大的跨领域泛化能力，包括推理任务和非推理任务。

Conclusion: 任务丰富的RLVR训练是提升LLM推理能力的有前景方向，揭示了RLVR的缩放规律，增加任务多样性能够改善跨领域泛化能力。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities, with
models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as
mathematics, coding, logic, and puzzles through Reinforcement Learning with
Verifiable Rewards (RLVR). However, their ability to solve more complex
optimization problems - particularly NP-hard tasks - remains underexplored. To
bridge this gap, we propose NP-ENGINE, the first comprehensive framework for
training and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks
across five domains, each equipped with (i) a controllable instance generator,
(ii) a rule-based verifier, and (iii) a heuristic solver that provides
approximate optimal solutions as ground truth. This
generator-verifier-heuristic pipeline enables scalable and verifiable RLVR
training under hierarchical difficulties. We also introduce NP-BENCH, a
benchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'
ability to tackle NP-hard level reasoning problems, focusing not only on
feasibility but also on solution quality. Additionally, we present
QWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on
Qwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and
achieves SOTA performance with the same model size. Beyond in-domain tasks, we
demonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain
(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),
as well as non-reasoning tasks such as instruction following. We also observe a
scaling trend: increasing task diversity improves OOD generalization. These
findings suggest that task-rich RLVR training is a promising direction for
advancing LLM's reasoning ability, revealing new insights into the scaling laws
of RLVR.

</details>


### [217] [Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination](https://arxiv.org/abs/2510.16533)
*Eilene Tomkins-Flanagan,Connor Hanley,Mary A. Kelly*

Main category: cs.AI

TL;DR: Doug是一种类型化计算机语言，所有类型化程序都能被证明在多项式时间内停止，编码在向量符号架构(VSA)中。它基于轻量线性函数式编程语言(LLFPL)，使用全息声明性记忆(HDM)编码类型，Lisp VSA编码项。该语言允许神经网络学习类型，并用于模拟人类技能获取过程。


<details>
  <summary>Details</summary>
Motivation: 目标是开发一种能够模拟人类技能获取过程的计算模型，使技能获取能够以接近人类的学习速度进行，超越现有方法的效率，更接近大脑中实际存在的心理表征及其学习过程。

Method: 基于Schimanski2009的轻量线性函数式编程语言(LLFPL)，使用Kelly2020的全息声明性记忆(HDM)进行类型编码，采用Flanagan2024的Lisp VSA变体进行项编码，构建类型化语言Doug。

Result: 提出的Doug语言允许神经网络学习类型，其中嵌入空间中邻近点的类型在结构和内容上都相似，为程序合成形式的技能获取提供了基础。

Conclusion: Doug语言为实现人类式技能获取提供了一种新方法，使计算模型更接近大脑中实际的心理表征和学习过程，在效率上超越了现有方法。

Abstract: We present a typed computer language, Doug, in which all typed programs may
be proved to halt in polynomial time, encoded in a vector-symbolic architecture
(VSA). Doug is just an encoding of the light linear functional programming
language (LLFPL) described by (Schimanski2009, ch. 7). The types of Doug are
encoded using a slot-value encoding scheme based on holographic declarative
memory (HDM; Kelly, 2020). The terms of Doug are encoded using a variant of the
Lisp VSA defined by (Flanagan, 2024). Doug allows for some points on the
embedding space of a neural network to be interpreted as types, where the types
of nearby points are similar both in structure and content. Types in Doug are
therefore learnable by a neural network. Following (Chollet, 2019), (Card,
1983), and (Newell, 1981), we view skill as the application of a procedure, or
program of action, that causes a goal to be satisfied. Skill acquisition may
therefore be expressed as program synthesis. Using Doug, we hope to describe a
form of learning of skilled behaviour that follows a human-like pace of skill
acquisition (i.e., substantially faster than brute force; Heathcote, 2000),
exceeding the efficiency of all currently existing approaches (Kaplan, 2020;
Jones, 2021; Chollet, 2024). Our approach brings us one step closer to modeling
human mental representations, as they must actually exist in the brain, and
those representations' acquisition, as they are actually learned.

</details>


### [218] [Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence](https://arxiv.org/abs/2510.16555)
*Qiongyan Wang,Xingchen Zou,Yutian Jiang,Haomin Wen,Jiaheng Wei,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: 提出了Urban-R1，一个基于强化学习的后训练框架，通过群体相对策略优化和城市区域画像任务来缓解多模态大语言模型的地理偏见，提升跨区域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 快速城市化增加了对城市通用智能的需求，但现有基于监督微调的城市基础模型存在持续的地理偏见，导致区域预测偏差和有限泛化能力。

Method: 采用强化学习后训练框架Urban-R1，使用群体相对策略优化(GRPO)来优化跨地理群体的推理，并以城市区域画像作为代理任务提供可衡量的奖励。

Result: 在多个区域和任务上的广泛实验表明，Urban-R1有效缓解了地理偏见，提升了跨区域泛化能力，性能优于监督微调和闭源模型。

Conclusion: 强化学习对齐是实现公平可信城市智能的有前景途径。

Abstract: Rapid urbanization intensifies the demand for Urban General Intelligence
(UGI), referring to AI systems that can understand and reason about complex
urban environments. Recent studies have built urban foundation models using
supervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit
persistent geospatial bias, producing regionally skewed predictions and limited
generalization. To this end, we propose Urban-R1, a reinforcement
learning-based post-training framework that aligns MLLMs with the objectives of
UGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize
reasoning across geographic groups and employs urban region profiling as a
proxy task to provide measurable rewards from multimodal urban data. Extensive
experiments across diverse regions and tasks show that Urban-R1 effectively
mitigates geo-bias and improves cross-region generalization, outperforming both
SFT-trained and closed-source models. Our results highlight reinforcement
learning alignment as a promising pathway toward equitable and trustworthy
urban intelligence.

</details>


### [219] [BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction](https://arxiv.org/abs/2510.16559)
*Tian Xia,Tianrun Gao,Wenhao Deng,Long Wei,Xiaowei Qian,Yixian Jiang,Chenglei Yu,Tailin Wu*

Main category: cs.AI

TL;DR: BuildArena是首个面向语言驱动工程建设的物理对齐交互式基准测试，用于评估LLM在工程建造自动化中的能力，包含可定制框架、可扩展任务设计、3D空间几何计算库和基线LLM代理工作流。


<details>
  <summary>Details</summary>
Motivation: 工程建设自动化需要将自然语言规范转化为物理可行结构，但现代LLM在此领域的建造能力尚未得到充分评估。

Method: 开发了BuildArena基准测试框架，包含可定制基准、跨静态和动态力学的可扩展任务设计、3D空间几何计算库以及基线LLM代理工作流。

Result: 在八个前沿LLM上全面评估了它们在语言驱动和物理基础建造自动化方面的能力。

Conclusion: BuildArena为语言驱动的工程建设自动化提供了首个物理对齐的交互式基准测试，填补了该领域的评估空白。

Abstract: Engineering construction automation aims to transform natural language
specifications into physically viable structures, requiring complex integrated
reasoning under strict physical constraints. While modern LLMs possess broad
knowledge and strong reasoning capabilities that make them promising candidates
for this domain, their construction competencies remain largely unevaluated. To
address this gap, we introduce BuildArena, the first physics-aligned
interactive benchmark designed for language-driven engineering construction. It
contributes to the community in four aspects: (1) a highly customizable
benchmarking framework for in-depth comparison and analysis of LLMs; (2) an
extendable task design strategy spanning static and dynamic mechanics across
multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for
supporting construction based on language instructions; (4) a baseline LLM
agentic workflow that effectively evaluates diverse model capabilities. On
eight frontier LLMs, BuildArena comprehensively evaluates their capabilities
for language-driven and physics-grounded construction automation. The project
page is at https://build-arena.github.io/.

</details>


### [220] [Ripple Effect Protocol: Coordinating Agent Populations](https://arxiv.org/abs/2510.16572)
*Ayush Chopra,Aman Sharma,Feroz Ahmad,Luca Muscariello,Vijoy Pandey,Ramesh Raskar*

Main category: cs.AI

TL;DR: 提出了Ripple Effect Protocol (REP)，一种协调协议，让智能体不仅分享决策，还分享轻量级的敏感性信号，从而在群体中实现更快更稳定的协调。


<details>
  <summary>Details</summary>
Motivation: 现有AI智能体通信协议（如A2A和ACP）强调通信而非协调，当智能体群体规模扩大时，会导致脆弱的集体行为，即使单个智能体很聪明，群体结果也很差。

Method: 引入REP协议，智能体分享决策和敏感性信号（表达关键环境变量变化时选择如何改变），这些敏感性在局部网络中传播。形式化定义了REP协议规范，分离必需的消息模式与可选的聚合规则。

Result: 在三个领域进行基准测试：（i）供应链级联（啤酒游戏）、（ii）稀疏网络中的偏好聚合（电影调度）、（iii）可持续资源分配（鱼塘游戏）。REP相比A2A在协调准确性和效率上提高了41%到100%，并能灵活处理来自LLM的多模态敏感性信号。

Conclusion: 通过将协调作为协议级能力，REP为新兴的智能体互联网提供了可扩展的基础设施。

Abstract: Modern AI agents can exchange messages using protocols such as A2A and ACP,
yet these mechanisms emphasize communication over coordination. As agent
populations grow, this limitation produces brittle collective behavior, where
individually smart agents converge on poor group outcomes. We introduce the
Ripple Effect Protocol (REP), a coordination protocol in which agents share not
only their decisions but also lightweight sensitivities - signals expressing
how their choices would change if key environmental variables shifted. These
sensitivities ripple through local networks, enabling groups to align faster
and more stably than with agent-centric communication alone. We formalize REP's
protocol specification, separating required message schemas from optional
aggregation rules, and evaluate it across scenarios with varying incentives and
network topologies. Benchmarks across three domains: (i) supply chain cascades
(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),
and (iii) sustainable resource allocation (Fishbanks) show that REP improves
coordination accuracy and efficiency over A2A by 41 to 100%, while flexibly
handling multimodal sensitivity signals from LLMs. By making coordination a
protocol-level capability, REP provides scalable infrastructure for the
emerging Internet of Agents

</details>


### [221] [Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?](https://arxiv.org/abs/2510.16582)
*Junchi Yu,Yujie Liu,Jindong Gu,Philip Torr,Dongzhan Zhou*

Main category: cs.AI

TL;DR: GraphFlow框架通过流匹配目标优化知识图谱检索，在STaRK基准测试中比GPT-4o等基线方法平均提升10%的命中率和召回率，并能泛化到未见过的知识图谱。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的检索增强生成方法难以从文本丰富的知识图谱中为复杂现实查询检索准确多样的信息，而过程奖励模型需要昂贵的过程级监督信号。

Method: 使用基于转移的流匹配目标联合优化检索策略和流估计器，流估计器将检索结果奖励分解到中间检索状态，引导检索策略按奖励比例从知识图谱中检索候选结果。

Result: 在STaRK基准测试中，GraphFlow比强基线方法（包括GPT-4o）平均提升10%的命中率和召回率，并展现出对未见知识图谱的强泛化能力。

Conclusion: GraphFlow能有效从文本丰富的知识图谱中检索准确多样的知识，解决复杂现实查询，具有很好的有效性和鲁棒性。

Abstract: Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances
large language models (LLMs) by providing structured and interpretable external
knowledge. However, existing KG-based RAG methods struggle to retrieve accurate
and diverse information from text-rich KGs for complex real-world queries.
Process Reward Models (PRMs) offer a way to align the retrieval process of
KG-based RAG with query-specific knowledge requirements, but they heavily rely
on process-level supervision signals that are expensive and hard to obtain on
KGs. To address this challenge, we propose GraphFlow, a framework that
efficiently retrieves accurate and diverse knowledge required for real-world
queries from text-rich KGs. GraphFlow employs a transition-based flow matching
objective to jointly optimize a retrieval policy and a flow estimator. The flow
estimator factorizes the reward of the retrieval outcome into the intermediate
retrieval states. Such reward factorization guides the retrieval policy to
retrieve candidates from KGs in proportion to their reward. This allows
GraphFlow to explore high-quality regions of KGs that yield diverse and
relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes
real-world queries from multiple domains over text-rich KGs. GraphFlow
outperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit
rate and recall. It also shows strong generalization to unseen KGs,
demonstrating its effectiveness and robustness.

</details>


### [222] [Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning](https://arxiv.org/abs/2510.16601)
*Tianxing Wu,Shutong Zhu,Jingting Wang,Ning Xu,Guilin Qi,Haofen Wang*

Main category: cs.AI

TL;DR: 提出了一种半监督置信分布学习方法(ssCDL)来解决不确定知识图谱补全问题，通过将置信度转换为分布并利用元学习生成伪标签来平衡置信度分布。


<details>
  <summary>Details</summary>
Motivation: 现有不确定知识图谱补全方法忽略了置信度的极端不平衡分布，导致学习到的嵌入不足以支持高质量补全。

Method: 将置信度转换为置信分布引入更多监督信息，通过关系学习在标记数据和带有伪标签的未标记数据上迭代学习嵌入，使用元学习预测未见三元组的置信度来增强训练数据。

Result: 在两个不确定知识图谱数据集上的实验表明，ssCDL在不同评估指标上持续优于最先进的基线方法。

Conclusion: ssCDL方法通过处理置信度不平衡问题，有效提升了不确定知识图谱补全的性能。

Abstract: Uncertain knowledge graphs (UKGs) associate each triple with a confidence
score to provide more precise knowledge representations. Recently, since
real-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)
completion attracts more attention, aiming to complete missing triples and
confidences. Current studies attempt to learn UKG embeddings to solve this
problem, but they neglect the extremely imbalanced distributions of triple
confidences. This causes that the learnt embeddings are insufficient to
high-quality UKG completion. Thus, in this paper, to address the above issue,
we propose a new semi-supervised Confidence Distribution Learning (ssCDL)
method for UKG completion, where each triple confidence is transformed into a
confidence distribution to introduce more supervision information of different
confidences to reinforce the embedding learning process. ssCDL iteratively
learns UKG embedding by relational learning on labeled data (i.e., existing
triples with confidences) and unlabeled data with pseudo labels (i.e., unseen
triples with the generated confidences), which are predicted by meta-learning
to augment the training data and rebalance the distribution of triple
confidences. Experiments on two UKG datasets demonstrate that ssCDL
consistently outperforms state-of-the-art baselines in different evaluation
metrics.

</details>


### [223] [Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards](https://arxiv.org/abs/2510.16614)
*Xuan Zhang,Ruixiao Li,Zhijian Zhou,Long Li,Yulei Qin,Ke Li,Xing Sun,Xiaoyu Tan,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: MERCI是一种新颖的强化学习算法，通过基于计数的内在奖励来增强LLM推理中的探索，避免重复和次优的推理模式。


<details>
  <summary>Details</summary>
Motivation: 现有的RL范式依赖稀疏的结果奖励和有限探索，导致LLM陷入重复和次优的推理模式，需要设计更好的探索机制。

Method: 使用轻量级的Coin Flipping Network估计推理轨迹的伪计数和认知不确定性，将其转换为内在奖励，并与GRPO等RL框架集成。

Result: 在复杂推理基准测试中，MERCI鼓励更丰富多样的思维链，显著超越强基线，帮助策略逃离局部最优找到更好解决方案。

Conclusion: 有针对性的内在动机可以使语言模型推理中的探索更加可靠有效。

Abstract: Reinforcement Learning (RL) has become a compelling way to strengthen the
multi step reasoning ability of Large Language Models (LLMs). However,
prevalent RL paradigms still lean on sparse outcome-based rewards and limited
exploration, which often drives LLMs toward repetitive and suboptimal reasoning
patterns. In this paper, we study the central question of how to design
exploration for LLM reasoning and introduce MERCI (Motivating Exploration in
LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that
augments policy optimization with a principled intrinsic reward. Building on
the idea of count-based exploration, MERCI leverages a lightweight Coin
Flipping Network (CFN) to estimate the pseudo count and further epistemic
uncertainty over reasoning trajectories, and converts them into an intrinsic
reward that values novelty while preserving the learning signal from task
rewards. We integrate MERCI into some advanced RL frameworks like Group
Relative Policy Optimization (GRPO). Experiments on complex reasoning
benchmarks demonstrate that MERCI encourages richer and more varied chains of
thought, significantly improves performance over strong baselines, and helps
the policy escape local routines to discover better solutions. It indicates
that our targeted intrinsic motivation can make exploration reliable for
language model reasoning.

</details>


### [224] [Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review](https://arxiv.org/abs/2510.16658)
*Shihao Yang,Xiying Huang,Danilo Bernardo,Jun-En Ding,Andrew Michael,Jingmei Yang,Patrick Kwan,Ashish Raj,Feng Liu*

Main category: cs.AI

TL;DR: 大型AI模型正在变革神经科学研究，通过端到端学习从原始脑信号中提取信息，在神经影像、脑机接口、分子神经科学、临床辅助和疾病应用等五大领域产生深远影响。


<details>
  <summary>Details</summary>
Motivation: 传统计算方法在神经科学中存在局限性，需要新的范式来处理多模态神经数据整合、时空模式解释等挑战，同时AI与神经科学的交互也日益相互促进。

Method: 采用大规模AI模型进行端到端学习，整合多模态神经数据，结合生物学启发的架构约束，开发更可解释和计算高效的模型。

Result: 这些模型成功解决了神经科学中的主要计算挑战，促进了临床转化框架的发展，并建立了神经科学数据集清单用于模型验证。

Conclusion: 大规模AI模型为神经科学带来巨大潜力，但需要严格的评估框架、有效的领域知识整合和全面的临床伦理指南来确保成功实施。

Abstract: The advent of large-scale artificial intelligence (AI) models has a
transformative effect on neuroscience research, which represents a paradigm
shift from the traditional computational methods through the facilitation of
end-to-end learning from raw brain signals and neural data. In this paper, we
explore the transformative effects of large-scale AI models on five major
neuroscience domains: neuroimaging and data processing, brain-computer
interfaces and neural decoding, molecular neuroscience and genomic modeling,
clinical assistance and translational frameworks, and disease-specific
applications across neurological and psychiatric disorders. These models are
demonstrated to address major computational neuroscience challenges, including
multimodal neural data integration, spatiotemporal pattern interpretation, and
the derivation of translational frameworks for clinical deployment. Moreover,
the interaction between neuroscience and AI has become increasingly reciprocal,
as biologically informed architectural constraints are now incorporated to
develop more interpretable and computationally efficient models. This review
highlights both the notable promise of such technologies and key implementation
considerations, with particular emphasis on rigorous evaluation frameworks,
effective domain knowledge integration, and comprehensive ethical guidelines
for clinical use. Finally, a systematic listing of critical neuroscience
datasets used to derive and validate large-scale AI models across diverse
research applications is provided.

</details>


### [225] [An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems](https://arxiv.org/abs/2510.16701)
*Ni Zhang,Zhiguang Cao,Jianan Zhou,Cong Zhang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 提出了一个基于大语言模型的智能体框架AFL，用于完全自动化解决复杂车辆路径问题，从问题实例到解决方案无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的方法仍需外部干预，限制了自主性并导致执行错误和解决方案可行性低。需要实现从问题实例到解决方案的完全自动化。

Method: AFL框架将整个流程分解为三个可管理的子任务，使用四个专门化智能体通过协调交互确保跨功能一致性和逻辑合理性，直接从原始输入中提取知识并生成自包含代码。

Result: 在60个复杂VRP问题上的实验验证了框架的有效性和通用性，与精心设计的算法性能相当，在代码可靠性和解决方案可行性方面显著优于现有LLM基线，在评估基准上接近100%的成功率。

Conclusion: AFL框架实现了复杂车辆路径问题求解的完全自动化，通过智能体协调机制确保了高可靠性和可行性，为自动化求解复杂优化问题提供了有效途径。

Abstract: Complex vehicle routing problems (VRPs) remain a fundamental challenge,
demanding substantial expert effort for intent interpretation and algorithm
design. While large language models (LLMs) offer a promising path toward
automation, current approaches still rely on external intervention, which
restrict autonomy and often lead to execution errors and low solution
feasibility. To address these challenges, we propose an Agentic Framework with
LLMs (AFL) for solving complex vehicle routing problems, achieving full
automation from problem instance to solution. AFL directly extracts knowledge
from raw inputs and enables self-contained code generation without handcrafted
modules or external solvers. To improve trustworthiness, AFL decomposes the
overall pipeline into three manageable subtasks and employs four specialized
agents whose coordinated interactions enforce cross-functional consistency and
logical soundness. Extensive experiments on 60 complex VRPs, ranging from
standard benchmarks to practical variants, validate the effectiveness and
generality of our framework, showing comparable performance against
meticulously designed algorithms. Notably, it substantially outperforms
existing LLM-based baselines in both code reliability and solution feasibility,
achieving rates close to 100% on the evaluated benchmarks.

</details>


### [226] [Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI](https://arxiv.org/abs/2510.16720)
*Jitao Sang,Jinlin Xiao,Jiarun Han,Jilin Chen,Xiaoyi Chen,Shuyu Wei,Yongjie Sun,Yuhang Wang*

Main category: cs.AI

TL;DR: 该论文综述了智能AI从基于管道的系统向模型原生范式的转变，其中规划、工具使用和记忆等能力从外部编排转向模型内部化。强化学习是实现这一转变的关键算法引擎。


<details>
  <summary>Details</summary>
Motivation: 追踪智能AI的范式转变，从外部逻辑编排的系统转向能力内部化的模型原生方法，探索AI从被动响应到主动行动的演进。

Method: 将强化学习作为算法引擎，重新定义从静态数据模仿到结果驱动探索的学习方式，系统回顾规划、工具使用和记忆能力从外部模块到端到端学习行为的演变。

Result: 展示了智能AI能力内部化的连贯轨迹，重塑了深度研究代理和GUI代理等主要应用，推动了从构建应用智能的系统到开发通过经验增长智能的模型的转变。

Conclusion: 模型原生智能AI作为一个集成的学习和交互框架，标志着从构建应用智能的系统到开发通过经验增长智能的模型的根本性转变。

Abstract: The rapid evolution of agentic AI marks a new phase in artificial
intelligence, where Large Language Models (LLMs) no longer merely respond but
act, reason, and adapt. This survey traces the paradigm shift in building
agentic AI: from Pipeline-based systems, where planning, tool use, and memory
are orchestrated by external logic, to the emerging Model-native paradigm,
where these capabilities are internalized within the model's parameters. We
first position Reinforcement Learning (RL) as the algorithmic engine enabling
this paradigm shift. By reframing learning from imitating static data to
outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task
across language, vision and embodied domains. Building on this, the survey
systematically reviews how each capability -- Planning, Tool use, and Memory --
has evolved from externally scripted modules to end-to-end learned behaviors.
Furthermore, it examines how this paradigm shift has reshaped major agent
applications, specifically the Deep Research agent emphasizing long-horizon
reasoning and the GUI agent emphasizing embodied interaction. We conclude by
discussing the continued internalization of agentic capabilities like
Multi-agent collaboration and Reflection, alongside the evolving roles of the
system and model layers in future agentic AI. Together, these developments
outline a coherent trajectory toward model-native agentic AI as an integrated
learning and interaction framework, marking the transition from constructing
systems that apply intelligence to developing models that grow intelligence
through experience.

</details>


### [227] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: 该论文是关于基于强化学习的智能搜索系统的综述，探讨如何通过RL技术改进检索增强生成(RAG)系统，使其具备多步交互、自适应检索和推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统存在单轮交互、启发式检索的局限性，无法实现自适应控制和实时信息获取。智能搜索通过多步交互解决了这些问题，而强化学习为智能搜索提供了自适应和自我改进的机制。

Method: 从三个维度组织该新兴领域：(i) RL的功能角色，(ii) RL的优化策略，(iii) RL的应用范围。总结了代表性方法、评估协议和应用案例。

Result: 提供了首个关于基于RL的智能搜索的全面综述，系统化地整理了该领域的研究进展，并建立了分类框架。

Conclusion: 该综述为构建可靠和可扩展的RL驱动智能搜索系统指明了方向，希望激发未来在RL与智能搜索集成方面的研究。

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


### [228] [Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration](https://arxiv.org/abs/2510.16742)
*Paul Saves,Pramudita Satria Palar,Muhammad Daffa Robani,Nicolas Verstaevel,Moncef Garouani,Julien Aligon,Benoit Gaudou,Koji Shimoyama,Joseph Morlier*

Main category: cs.AI

TL;DR: 提出了一种基于代理模型的仿真驱动工程工作流，通过训练轻量级仿真器来解决高计算成本和黑盒透明度问题，支持不确定性量化和可解释AI分析。


<details>
  <summary>Details</summary>
Motivation: 仿真驱动工程工作流面临两个核心挑战：(1) 高计算成本，因为准确探索需要大量昂贵的模拟器运行；(2) 当决策依赖于不透明的黑盒组件时，透明度和可靠性有限。

Method: 在紧凑的实验设计上训练轻量级仿真器，提供快速、低延迟的近似，支持严格的不确定性量化，并适用于全局和局部可解释AI分析。该方法支持连续和分类输入，结合全局效应和不确定性分析与局部归因。

Result: 在两个对比案例研究中展示：混合电动飞机的多学科设计分析和城市隔离的基于代理模型。结果显示代理模型与XAI耦合能够在几秒内实现大规模探索，发现非线性相互作用和涌现行为，识别关键设计和政策杠杆。

Conclusion: 该工作流统一了所有基于仿真的复杂系统分析工具，从工程设计到社会环境理解，通过诊断代理模型充分性来指导进一步数据收集或模型改进。

Abstract: Complex systems are increasingly explored through simulation-driven
engineering workflows that combine physics-based and empirical models with
optimization and analytics. Despite their power, these workflows face two
central obstacles: (1) high computational cost, since accurate exploration
requires many expensive simulator runs; and (2) limited transparency and
reliability when decisions rely on opaque blackbox components. We propose a
workflow that addresses both challenges by training lightweight emulators on
compact designs of experiments that (i) provide fast, low-latency
approximations of expensive simulators, (ii) enable rigorous uncertainty
quantification, and (iii) are adapted for global and local Explainable
Artificial Intelligence (XAI) analyses. This workflow unifies every
simulation-based complex-system analysis tool, ranging from engineering design
to agent-based models for socio-environmental understanding. In this paper, we
proposea comparative methodology and practical recommendations for using
surrogate-based explainability tools within the proposed workflow. The
methodology supports continuous and categorical inputs, combines global-effect
and uncertainty analyses with local attribution, and evaluates the consistency
of explanations across surrogate models, thereby diagnosing surrogate adequacy
and guiding further data collection or model refinement. We demonstrate the
approach on two contrasting case studies: a multidisciplinary design analysis
of a hybrid-electric aircraft and an agent-based model of urban segregation.
Results show that the surrogate model and XAI coupling enables large-scale
exploration in seconds, uncovers nonlinear interactions and emergent behaviors,
identifies key design and policy levers, and signals regions where surrogates
require more data or alternative architectures.

</details>


### [229] [ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2510.16753)
*Wei Huang,Peining Li,Meiyu Liang,Xu Hou,Junping Du,Yingxia Shao,Guanhua Ye,Wu Liu,Kangkang Lu,Yang Yu*

Main category: cs.AI

TL;DR: 提出了ELMM模型，通过多视图视觉令牌压缩器和注意力剪枝策略，解决多模态知识图谱补全中的语义噪声和计算成本问题，在保持性能的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 多模态知识图谱存在不完整性问题，而现有方法在处理多模态信息时面临语义噪声、模态冲突和高计算成本等挑战。

Method: 使用基于多头注意力的多视图视觉令牌压缩器自适应压缩图像令牌，设计注意力剪枝策略减少冗余层，并通过线性投影补偿性能损失。

Result: 在FB15k-237-IMG和WN18-IMG基准测试中达到最先进性能，同时显著提升计算效率。

Conclusion: ELMM为多模态知识图谱补全建立了新的范式，在性能和效率之间取得了良好平衡。

Abstract: Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by
incorporating visual and textual modalities, enabling richer and more
expressive entity representations. However, existing MKGs often suffer from
incompleteness, which hinder their effectiveness in downstream tasks.
Therefore, multimodal knowledge graph completion (MKGC) task is receiving
increasing attention. While large language models (LLMs) have shown promise for
knowledge graph completion (KGC), their application to the multimodal setting
remains underexplored. Moreover, applying Multimodal Large Language Models
(MLLMs) to the task of MKGC introduces significant challenges: (1) the large
number of image tokens per entity leads to semantic noise and modality
conflicts, and (2) the high computational cost of processing large token
inputs. To address these issues, we propose Efficient Lightweight Multimodal
Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token
Compressor (MVTC) based on multi-head attention mechanism, which adaptively
compresses image tokens from both textual and visual views, thereby effectively
reducing redundancy while retaining necessary information and avoiding modality
conflicts. Additionally, we design an attention pruning strategy to remove
redundant attention layers from MLLMs, thereby significantly reducing the
inference cost. We further introduce a linear projection to compensate for the
performance degradation caused by pruning. Extensive experiments on benchmark
FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art
performance while substantially improving computational efficiency,
establishing a new paradigm for multimodal knowledge graph completion.

</details>


### [230] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: ELLSA是首个全双工、端到端的多模态模型，能够同时感知和生成视觉、文本、语音和动作，实现更自然的人机交互。


<details>
  <summary>Details</summary>
Motivation: 人类交互本质上是多模态和全双工的，需要模型能够同时感知和生成多种模态，实现更自然的人类行为模拟。

Method: 采用新颖的SA-MoE架构（自注意力专家混合），将各模态路由到专门专家，通过统一注意力骨干网络进行融合。

Result: 在语音交互和机器人操作基准测试中，ELLSA与特定模态基线相当，同时支持高级多模态和全双工行为。

Conclusion: ELLSA代表了向更自然和通用交互智能迈出的一步，有助于实现人工通用智能。

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [231] [See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models](https://arxiv.org/abs/2510.16769)
*Shuo Han,Yukun Cao,Zezhong Ding,Zengyi Gao,S Kevin Zhou,Xike Xie*

Main category: cs.AI

TL;DR: GraphVista是一个统一的框架，通过分层组织图信息和引入规划代理来解决视觉语言模型在图理解中的可扩展性和模态协调问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在图理解中面临输入令牌限制、可扩展性瓶颈以及缺乏有效的文本和视觉模态协调机制的问题。

Method: GraphVista采用分层方法将图信息组织到轻量级GraphRAG基础中，仅检索任务相关的文本描述和高分辨率视觉子图；同时引入规划代理，根据任务复杂度将任务路由到最合适的模态（文本模态用于简单属性推理，视觉模态用于局部和结构复杂推理）。

Result: GraphVista可扩展到比现有基准大200倍的大型图，在文本、视觉和融合方法中表现一致优异，相比最先进基线实现了4.4倍的质量提升。

Conclusion: GraphVista通过充分利用两种模态的互补优势，有效解决了图理解中的可扩展性和模态协调挑战。

Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but
remain limited by input-token constraints, facing scalability bottlenecks and
lacking effective mechanisms to coordinate textual and visual modalities. To
address these challenges, we propose GraphVista, a unified framework that
enhances both scalability and modality coordination in graph understanding. For
scalability, GraphVista organizes graph information hierarchically into a
lightweight GraphRAG base, which retrieves only task-relevant textual
descriptions and high-resolution visual subgraphs, compressing redundant
context while preserving key reasoning elements. For modality coordination,
GraphVista introduces a planning agent that routes tasks to the most suitable
modality-using the text modality for simple property reasoning and the visual
modality for local and structurally complex reasoning grounded in explicit
topology. Extensive experiments demonstrate that GraphVista scales to large
graphs, up to $200\times$ larger than those used in existing benchmarks, and
consistently outperforms existing textual, visual, and fusion-based methods,
achieving up to $4.4\times$ quality improvement over the state-of-the-art
baselines by fully exploiting the complementary strengths of both modalities.

</details>


### [232] [Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation](https://arxiv.org/abs/2510.16802)
*Chao Li,Yuru Wang*

Main category: cs.AI

TL;DR: 提出Domain-Contextualized Concept Graph (CDC)框架，将领域提升为概念表示的一等元素，采用<概念, 关系@领域, 概念'>的三元组结构，实现上下文感知推理和跨领域类比。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱受限于固定本体论，在刚性层次结构中组织概念，问题根源在于将领域视为隐式上下文而非显式的推理级组件。

Method: 采用C-D-C三元组结构<Concept, Relation@Domain, Concept'>，领域规范作为按需定义的动态分类维度；基于认知-语言同构映射原则；形式化20多个标准化关系谓词；在Prolog中实现完整推理能力。

Result: 在教育、企业知识系统和技术文档等案例研究中，CDC实现了上下文感知推理、跨领域类比和个性化知识建模，这些能力在传统本体框架下无法实现。

Conclusion: CDC框架通过将领域提升为概念表示的一等元素，克服了传统知识图谱的局限性，实现了更灵活、上下文感知的知识建模和推理。

Abstract: Traditional knowledge graphs are constrained by fixed ontologies that
organize concepts within rigid hierarchical structures. The root cause lies in
treating domains as implicit context rather than as explicit, reasoning-level
components. To overcome these limitations, we propose the Domain-Contextualized
Concept Graph (CDC), a novel knowledge modeling framework that elevates domains
to first-class elements of conceptual representation. CDC adopts a C-D-C triple
structure - <Concept, Relation@Domain, Concept'> - where domain specifications
serve as dynamic classification dimensions defined on demand. Grounded in a
cognitive-linguistic isomorphic mapping principle, CDC operationalizes how
humans understand concepts through contextual frames. We formalize more than
twenty standardized relation predicates (structural, logical, cross-domain, and
temporal) and implement CDC in Prolog for full inference capability. Case
studies in education, enterprise knowledge systems, and technical documentation
demonstrate that CDC enables context-aware reasoning, cross-domain analogy, and
personalized knowledge modeling - capabilities unattainable under traditional
ontology-based frameworks.

</details>


### [233] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: DeepAnalyze-8B是首个用于自主数据科学的代理式大语言模型，能够自动完成从数据源到分析师级深度研究报告的端到端流程，仅用80亿参数就超越了基于最先进专有LLM的工作流代理。


<details>
  <summary>Details</summary>
Motivation: 现有的基于工作流的数据代理在特定数据任务上表现良好，但由于依赖预定义工作流，无法实现完全自主的数据科学。随着强大LLM的出现，从原始数据源到分析师级深度研究报告的自主数据科学变得可行。

Method: 提出了基于课程学习的代理训练范式，模拟人类数据科学家的学习轨迹，使LLM能够在真实环境中逐步获取和整合多种能力。同时引入了数据驱动的轨迹合成框架来构建高质量训练数据。

Result: 实验表明，仅用80亿参数的DeepAnalyze在广泛的数据任务上表现优异，包括数据问答、专业分析任务和开放式数据研究，超越了基于最先进专有LLM的工作流代理。

Conclusion: DeepAnalyze模型、代码和训练数据已开源，为自主数据科学的发展铺平了道路。

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [234] [VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](https://arxiv.org/abs/2510.16907)
*Kangrui Wang,Pingyue Zhang,Zihan Wang,Yaning Gao,Linjie Li,Qineng Wang,Hanyang Chen,Chi Wan,Yiping Lu,Zhengyuan Yang,Lijuan Wang,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Yejin Choi,Manling Li*

Main category: cs.AI

TL;DR: 该论文提出通过强化学习训练VLM智能体进行显式视觉状态推理，构建内部世界模型，在五个多样化智能体基准测试中实现了3倍性能提升。


<details>
  <summary>Details</summary>
Motivation: 训练VLM智能体面临从文本状态到复杂视觉观察的转变，这引入了部分可观测性并需要强大的世界建模能力。研究旨在探索VLM智能体是否能够通过显式视觉状态推理构建内部世界模型。

Method: 将问题建模为部分可观测马尔可夫决策过程(POMDP)，通过强化学习架构性地强制和奖励智能体的推理过程。将推理分解为状态估计和转移建模，设计了世界建模奖励和双层通用优势估计(Bi-Level GAE)方法。

Result: 3B参数模型在五个多样化智能体基准测试中达到0.82分，相比未训练版本(0.21)提升3倍，并优于GPT-5(0.75)、Gemini 2.5 Pro(0.67)和Claude 4.5(0.62)等专有推理模型。

Conclusion: 通过视觉状态推理，VLM智能体能够有效构建内部世界模型，最优表示形式取决于任务特性：自然语言擅长捕捉语义关系，结构化格式对于精确操作和控制至关重要。

Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to
Language Model (LLM) agents, lies in the shift from textual states to complex
visual observations. This transition introduces partial observability and
demands robust world modeling. We ask: Can VLM agents construct internal world
models through explicit visual state reasoning? To address this question, we
architecturally enforce and reward the agent's reasoning process via
reinforcement learning (RL), formulating it as a Partially Observable Markov
Decision Process (POMDP). We find that decomposing the agent's reasoning into
State Estimation ("what is the current state?") and Transition Modeling ("what
comes next?") is critical for success, as demonstrated through five reasoning
strategies. Our investigation into how agents represent internal beliefs
reveals that the optimal representation is task-dependent: Natural Language
excels at capturing semantic relationships in general tasks, while Structured
formats are indispensable for precise manipulation and control. Building on
these insights, we design a World Modeling Reward that provides dense,
turn-level supervision for accurate state prediction, and introduce Bi-Level
General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.
Through this form of visual state reasoning, a 3B-parameter model achieves a
score of 0.82 across five diverse agent benchmarks, representing a 3$\times$
improvement over its untrained counterpart (0.21) and outperforming proprietary
reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5
(0.62). All experiments are conducted within our VAGEN framework, a scalable
system for training and analyzing multi-turn VLM agents in diverse visual
environments. Code and data are publicly available at
https://vagen-ai.github.io.

</details>


### [235] [A Comparative User Evaluation of XRL Explanations using Goal Identification](https://arxiv.org/abs/2510.16956)
*Mark Towers,Yali Du,Christopher Freeman,Timothy J. Norman*

Main category: cs.AI

TL;DR: 提出了一种新的评估方法，测试用户是否能从强化学习算法的解释中识别出智能体的目标。在Ms. Pacman环境中测试了四种可解释强化学习算法，发现只有一种算法在测试目标上达到了超过随机水平的准确率，且用户普遍对自己的选择过度自信。


<details>
  <summary>Details</summary>
Motivation: 可解释强化学习算法的核心应用是调试，但目前缺乏对其相对性能的比较评估。

Method: 利用Atari的Ms. Pacman环境和四种可解释强化学习算法，通过用户研究测试他们是否能从决策解释中识别智能体的目标。

Result: 只有一种算法在测试目标上达到了超过随机水平的准确率；用户普遍对自己的选择过度自信；用户自我报告的识别和理解难易程度与他们的准确率没有相关性。

Conclusion: 当前的可解释强化学习算法在帮助用户识别智能体目标方面的效果有限，且用户的主观感知与实际表现存在差异。

Abstract: Debugging is a core application of explainable reinforcement learning (XRL)
algorithms; however, limited comparative evaluations have been conducted to
understand their relative performance. We propose a novel evaluation
methodology to test whether users can identify an agent's goal from an
explanation of its decision-making. Utilising the Atari's Ms. Pacman
environment and four XRL algorithms, we find that only one achieved greater
than random accuracy for the tested goals and that users were generally
overconfident in their selections. Further, we find that users' self-reported
ease of identification and understanding for every explanation did not
correlate with their accuracy.

</details>


### [236] [STARK: Strategic Team of Agents for Refining Kernels](https://arxiv.org/abs/2510.16996)
*Juncheng Dong,Yang Yang,Tao Liu,Yang Wang,Feng Qi,Vahid Tarokh,Kaushik Rangadurai,Shuang Yang*

Main category: cs.AI

TL;DR: 提出基于LLM的多智能体框架，通过系统化探索设计空间来优化GPU内核性能，相比基线方法获得显著改进


<details>
  <summary>Details</summary>
Motivation: GPU内核优化对AI发展至关重要，但传统方法难以处理内存层次、线程调度和硬件特性的复杂交互。现有LLM方法仅作为单次生成器或简单优化工具，无法有效应对不规则的内核优化场景

Method: 采用多智能体协作框架，包含基于经验的指令生成、动态上下文管理和策略搜索，模拟专家工程师的工作流程，让LLM能够推理硬件权衡、整合性能分析反馈并进行迭代优化

Result: 在KernelBench基准测试中，系统在基线方法经常失败的情况下仍能产生正确解决方案，并实现最高16倍的运行时性能提升

Conclusion: 智能体LLM框架具有推动完全自动化、可扩展GPU内核优化的巨大潜力

Abstract: The efficiency of GPU kernels is central to the progress of modern AI, yet
optimizing them remains a difficult and labor-intensive task due to complex
interactions between memory hierarchies, thread scheduling, and
hardware-specific characteristics. While recent advances in large language
models (LLMs) provide new opportunities for automated code generation, existing
approaches largely treat LLMs as single-shot generators or naive refinement
tools, limiting their effectiveness in navigating the irregular kernel
optimization landscape. We introduce an LLM agentic framework for GPU kernel
optimization that systematically explores the design space through multi-agent
collaboration, grounded instruction, dynamic context management, and strategic
search. This framework mimics the workflow of expert engineers, enabling LLMs
to reason about hardware trade-offs, incorporate profiling feedback, and refine
kernels iteratively. We evaluate our approach on KernelBench, a benchmark for
LLM-based kernel optimization, and demonstrate substantial improvements over
baseline agents: our system produces correct solutions where baselines often
fail, and achieves kernels with up to 16x faster runtime performance. These
results highlight the potential of agentic LLM frameworks to advance fully
automated, scalable GPU kernel optimization.

</details>


### [237] [ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems](https://arxiv.org/abs/2510.17052)
*Hassan Hamad,Yingru Xu,Liang Zhao,Wenbo Yan,Narendra Gyanchandani*

Main category: cs.AI

TL;DR: ToolCritic是一个诊断框架，用于评估和改进增强工具的大型语言模型在多轮对话中的行为，通过检测8种特定工具调用错误并提供针对性反馈，使主LLM能够修正响应。


<details>
  <summary>Details</summary>
Motivation: 工具增强的LLM在实际应用中存在工具使用错误，影响了其可靠性，需要一种方法来诊断和改进这些错误。

Method: 定义8种工具调用错误类型，构建合成数据集训练ToolCritic，主LLM基于ToolCritic的反馈修正响应。

Result: 在Schema-Guided Dialogue数据集上，ToolCritic相比基线方法（包括零样本提示和自校正技术）将工具调用准确率提高了13%。

Conclusion: ToolCritic是朝着在真实世界对话应用中更稳健地集成LLM与外部工具的有希望的一步。

Abstract: Tool-augmented large language models (LLMs) are increasingly employed in
real-world applications, but tool usage errors still hinder their reliability.
We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM
behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight
distinct error types specific to tool-calling (e.g., premature invocation,
argument misalignment, and misinterpretation of tool outputs) and provides
targeted feedback to the main LLM. The main LLM, assumed to have strong
reasoning, task understanding and orchestration capabilities, then revises its
response based on ToolCritic's feedback. We systematically define these error
categories and construct a synthetic dataset to train ToolCritic. Experimental
results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic
improves tool-calling accuracy by up to 13% over baselines, including zero-shot
prompting and self-correction techniques. This represents a promising step
toward more robust LLM integration with external tools in real-world dialogue
applications.

</details>


### [238] [A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation](https://arxiv.org/abs/2510.17064)
*Rongbin Li,Wenbo Chen,Zhao Li,Rodrigo Munoz-Castaneda,Jinbo Li,Neha S. Maurya,Arnav Solanki,Huan He,Hanwen Xing,Meaghan Ramlakhan,Zachary Wise,Zhuhao Wu,Hua Xu,Michael Hawrylycz,W. Jim Zheng*

Main category: cs.AI

TL;DR: BRAINCELL-AID是一个多智能体AI系统，通过整合自由文本描述和本体标签，结合检索增强生成技术，显著提高了基因集注释的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序技术虽然能识别多样细胞类型，但对涉及特征不明确基因的转录组特征进行注释仍面临挑战。传统方法如GSEA依赖精心策划的注释，在这些情况下表现不佳。

Method: 开发了BRAINCELL-AID多智能体AI系统，整合自由文本描述与本体标签，采用检索增强生成(RAG)技术构建稳健的智能体工作流，通过PubMed文献精炼预测。

Result: 对小鼠基因集实现了77%的正确注释率，成功注释了BRAIN Initiative Cell Census Network生成的5,322个脑细胞簇，识别了区域特异性基因共表达模式，并推断出基底神经节相关细胞类型。

Conclusion: BRAINCELL-AID创建了一个支持社区驱动细胞类型注释的宝贵资源，能够提供神经学上有意义的细胞类型描述。

Abstract: Single-cell RNA sequencing has transformed our ability to identify diverse
cell types and their transcriptomic signatures. However, annotating these
signatures-especially those involving poorly characterized genes-remains a
major challenge. Traditional methods, such as Gene Set Enrichment Analysis
(GSEA), depend on well-curated annotations and often perform poorly in these
contexts. Large Language Models (LLMs) offer a promising alternative but
struggle to represent complex biological knowledge within structured
ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:
https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that
integrates free-text descriptions with ontology labels to enable more accurate
and robust gene set annotation. By incorporating retrieval-augmented generation
(RAG), we developed a robust agentic workflow that refines predictions using
relevant PubMed literature, reducing hallucinations and enhancing
interpretability. Using this workflow, we achieved correct annotations for 77%
of mouse gene sets among their top predictions. Applying this approach, we
annotated 5,322 brain cell clusters from the comprehensive mouse brain cell
atlas generated by the BRAIN Initiative Cell Census Network, enabling novel
insights into brain cell function by identifying region-specific gene
co-expression patterns and inferring functional roles of gene ensembles.
BRAINCELL-AID also identifies Basal Ganglia-related cell types with
neurologically meaningful descriptions. Hence, we create a valuable resource to
support community-driven cell type annotation.

</details>


### [239] [Structured Debate Improves Corporate Credit Reasoning in Financial AI](https://arxiv.org/abs/2510.17108)
*Yoonjin Lee,Munhee Kim,Hanbi Choi,Juhyeon Park,Seungho Lyoo,Woojin Park*

Main category: cs.AI

TL;DR: 开发了两种基于大语言模型的系统用于企业信用评估中的证据推理：单代理系统(NAS)和多代理辩论系统(KPD-MADS)，后者基于卡尔·波普尔的批判性对话框架，在推理质量和实用性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决企业信用评估中定性非财务指标难以形式化的问题，现有方法主要关注数值预测，缺乏对专业贷款评估所需解释性判断的支持。

Method: 开发了两种LLM系统：单代理系统(NAS)通过单次推理生成双向分析；多代理辩论系统(KPD-MADS)基于卡尔·波普尔批判对话框架，采用十步结构化交互协议进行对抗性验证。

Result: 在两个真实企业案例中测试，相比人工专家报告(1920秒/案例)，两个系统显著提升效率(NAS:11.55秒；KPD-MADS:91.97秒)。KPD-MADS在解释充分性(4.0 vs 3.0)、实际适用性(4.0 vs 3.0)和可用性(62.5 vs 52.5)方面获得更高评分。

Conclusion: 结构化多代理交互能够增强金融AI中的推理严谨性和可解释性，推动企业信用评估的可扩展和可辩护自动化。

Abstract: Despite advances in financial AI, the automation of evidence-based reasoning
remains unresolved in corporate credit assessment, where qualitative
non-financial indicators exert decisive influence on loan repayment outcomes
yet resist formalization. Existing approaches focus predominantly on numerical
prediction and provide limited support for the interpretive judgments required
in professional loan evaluation. This study develops and evaluates two
operational large language model (LLM)-based systems designed to generate
structured reasoning from non-financial evidence. The first is a
non-adversarial single-agent system (NAS) that produces bidirectional analysis
through a single-pass reasoning pipeline. The second is a debate-based
multi-agent system (KPD-MADS) that operationalizes adversarial verification
through a ten-step structured interaction protocol grounded in Karl Popper's
critical dialogue framework. Both systems were applied to three real corporate
cases and evaluated by experienced credit risk professionals. Compared to
manual expert reporting, both systems achieved substantial productivity gains
(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The
KPD-MADS demonstrated superior reasoning quality, receiving higher median
ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.
3.0), and usability (62.5 vs. 52.5). These findings show that structured
multi-agent interaction can enhance reasoning rigor and interpretability in
financial AI, advancing scalable and defensible automation in corporate credit
assessment.

</details>


### [240] [Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion](https://arxiv.org/abs/2510.17145)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.AI

TL;DR: 提出基于手工特征的方法，通过融合颜色统计、多色彩空间直方图和纹理特征来评估鱼类新鲜度，在FFE数据集上显著优于之前的深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统感官评价鱼类新鲜度存在主观性、不一致性和难以标准化的问题，需要客观、可靠的自动化评估方法。

Method: 从鱼眼图像中提取手工特征，包括颜色统计、多色彩空间直方图、LBP和GLCM纹理特征，融合全局色度变化和局部ROI退化特征，使用LightGBM和ANN进行分类。

Result: 标准训练测试设置下LightGBM达到77.56%准确率，比之前深度学习方法提升14.35%；数据增强后ANN达到97.16%准确率，比之前最佳方法提升19.86%。

Conclusion: 精心设计的手工特征经过策略性处理后，能够为自动化鱼类新鲜度评估提供鲁棒、可解释且可靠的解决方案。

Abstract: Accurate assessment of fish freshness remains a major challenge in the food
industry, with direct consequences for product quality, market value, and
consumer health. Conventional sensory evaluation is inherently subjective,
inconsistent, and difficult to standardize across contexts, often limited by
subtle, species-dependent spoilage cues. To address these limitations, we
propose a handcrafted feature-based approach that systematically extracts and
incrementally fuses complementary descriptors, including color statistics,
histograms across multiple color spaces, and texture features such as Local
Binary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish
eye images. Our method captures global chromatic variations from full images
and localized degradations from ROI segments, fusing each independently to
evaluate their effectiveness in assessing freshness. Experiments on the
Freshness of the Fish Eyes (FFE) dataset demonstrate the approach's
effectiveness: in a standard train-test setting, a LightGBM classifier achieved
77.56% accuracy, a 14.35% improvement over the previous deep learning baseline
of 63.21%. With augmented data, an Artificial Neural Network (ANN) reached
97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results
demonstrate that carefully engineered, handcrafted features, when strategically
processed, yield a robust, interpretable, and reliable solution for automated
fish freshness assessment, providing valuable insights for practical
applications in food quality monitoring.

</details>


### [241] [Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation](https://arxiv.org/abs/2510.17146)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: PILLM是一个基于物理知识的大语言模型框架，通过进化循环自动生成、评估和优化HVAC系统异常检测规则，在保持可解释性的同时实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: HVAC系统占建筑能耗很大比例，传统规则方法可解释但适应性差，深度学习方法预测能力强但缺乏透明度和物理合理性，现有LLM方法忽视了HVAC运行的物理原理。

Method: 提出PILLM框架，在进化循环中引入物理知识驱动的反思和交叉算子，嵌入热力学和控制理论约束，生成既适应性强又物理合理的异常检测规则。

Result: 在公共建筑故障检测数据集上的实验表明，PILLM实现了最先进的性能，同时产生可解释和可操作的诊断规则。

Conclusion: PILLM推进了智能建筑系统中可信赖和可部署AI的发展，平衡了性能与可解释性。

Abstract: Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a
substantial share of global building energy use, making reliable anomaly
detection essential for improving efficiency and reducing emissions. Classical
rule-based approaches offer explainability but lack adaptability, while deep
learning methods provide predictive power at the cost of transparency,
efficiency, and physical plausibility. Recent attempts to use Large Language
Models (LLMs) for anomaly detection improve interpretability but largely ignore
the physical principles that govern HVAC operations. We present PILLM, a
Physics-Informed LLM framework that operates within an evolutionary loop to
automatically generate, evaluate, and refine anomaly detection rules. Our
approach introduces physics-informed reflection and crossover operators that
embed thermodynamic and control-theoretic constraints, enabling rules that are
both adaptive and physically grounded. Experiments on the public Building Fault
Detection dataset show that PILLM achieves state-of-the-art performance while
producing diagnostic rules that are interpretable and actionable, advancing
trustworthy and deployable AI for smart building systems.

</details>


### [242] [Which LLM Multi-Agent Protocol to Choose?](https://arxiv.org/abs/2510.17149)
*Hongyi Du,Jiaqi Su,Jisen Li,Lijie Ding,Yingxuan Yang,Peixuan Han,Xiangru Tang,Kunlun Zhu,Jiaxuan You*

Main category: cs.AI

TL;DR: ProtocolBench是一个系统评估多智能体系统通信协议的基准，ProtocolRouter是一个可学习的协议路由器，能根据场景需求动态选择最佳协议


<details>
  <summary>Details</summary>
Motivation: 大规模多智能体系统中，通信协议层是影响性能和可靠性的关键因素，但目前协议选择缺乏标准化指导，主要依赖直觉

Method: 开发ProtocolBench基准，从任务成功率、端到端延迟、消息开销和故障恢复能力四个维度系统比较协议；提出ProtocolRouter协议路由器，根据需求和运行时信号动态选择协议

Result: 协议选择显著影响系统行为：在Streaming Queue场景中，完成时间差异达36.5%，端到端延迟差异3.48秒；ProtocolRouter相比最佳单协议基线，故障恢复时间减少18.1%，在GAIA场景中成功率更高

Conclusion: 协议选择对多智能体系统性能至关重要，ProtocolBench和ProtocolRouter为协议评估和选择提供了标准化方法和工具，能显著提升系统可靠性和性能

Abstract: As large-scale multi-agent systems evolve, the communication protocol layer
has become a critical yet under-evaluated factor shaping performance and
reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,
etc.), selection is often intuition-driven and lacks standardized guidance. We
introduce ProtocolBench, a benchmark that systematically compares agent
protocols along four measurable axes: task success, end-to-end latency, message
or byte overhead, and robustness under failures. On ProtocolBench, protocol
choice significantly influences system behavior. In the Streaming Queue
scenario, overall completion time varies by up to 36.5% across protocols, and
mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,
resilience also differs consistently across protocols. Beyond evaluation, we
present ProtocolRouter, a learnable protocol router that selects per-scenario
(or per-module) protocols from requirement and runtime signals. ProtocolRouter
reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol
baseline, and achieves scenario-specific gains such as higher success in GAIA.
We also release ProtocolRouterBench to standardize protocol evaluation and
improve reliability at scale.

</details>


### [243] [Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients](https://arxiv.org/abs/2510.17172)
*Shun Huang,Wenlu Xing,Shijia Geng,Hailong Wang,Guangkun Nie,Gongzheng Tang,Chenyang He,Shenda Hong*

Main category: cs.AI

TL;DR: 本研究开发了一种结合ECG基础模型和XGBoost分类器的混合预测框架，用于预测急性心肌梗死后恶性室性心律失常风险，在提高准确性的同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 急性心肌梗死后恶性室性心律失常是院内死亡的主要原因，传统风险评分性能有限，而端到端深度学习模型缺乏临床信任所需的可解释性。

Method: 使用ECGFounder基础模型提取150维诊断概率特征，通过特征选择精炼后训练XGBoost分类器，并用SHAP方法进行可解释性分析。

Result: 混合模型AUC达0.801，优于KNN(0.677)、RNN(0.676)和1D-CNN(0.720)，SHAP分析显示模型识别特征与临床知识高度一致。

Conclusion: 该混合框架通过验证基础模型输出作为有效自动化特征工程，为构建可信赖、可解释的AI临床决策支持系统提供了新范式。

Abstract: Malignant ventricular arrhythmias (VT/VF) following acute myocardial
infarction (AMI) are a major cause of in-hospital death, yet early
identification remains a clinical challenge. While traditional risk scores have
limited performance, end-to-end deep learning models often lack the
interpretability needed for clinical trust. This study aimed to develop a
hybrid predictive framework that integrates a large-scale electrocardiogram
(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to
improve both accuracy and interpretability. We analyzed 6,634 ECG recordings
from AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder
model was used to extract 150-dimensional diagnostic probability features ,
which were then refined through feature selection to train the XGBoost
classifier. Model performance was evaluated using AUC and F1-score , and the
SHAP method was used for interpretability. The ECGFounder + XGBoost hybrid
model achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC
0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that
model-identified key features, such as "premature ventricular complexes" (risk
predictor) and "normal sinus rhythm" (protective factor), were highly
consistent with clinical knowledge. We conclude that this hybrid framework
provides a novel paradigm for VT/VF risk prediction by validating the use of
foundation model outputs as effective, automated feature engineering for
building trustworthy, explainable AI-based clinical decision support systems.

</details>


### [244] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: 研究通过离线策略评估和轻量级模拟器，探索工具增强LLM健康教练的个性化策略，发现统一的重工具策略对特定用户群体有害，并提出基于子群体感知决策头的个性化方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估和优化工具增强LLM健康教练在真实用户中的表现，特别关注个性化策略对不同用户子群体的影响，避免平均指标掩盖的潜在危害。

Method: 采用离线策略评估（OPE）分析因子化决策头（工具/风格），使用轻量级模拟器与隐藏原型进行策略测试，重点关注早期信息增益奖励对特征识别和目标达成的影响。

Result: 统一重工具策略虽然提高平均价值，但对低健康素养/高自我效能用户有害；添加早期信息增益奖励可缩短特征识别时间，提高目标成功率和pass@3指标。

Conclusion: 提出了评估优先的个性化路径：冻结生成器，在类型化奖励上学习子群体感知决策头，并始终报告每个原型的指标以揭示平均指标掩盖的子群体危害。

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [245] [Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling](https://arxiv.org/abs/2510.17211)
*Tingsong Xiao,Yao An Lee,Zelin Xu,Yupu Zhang,Zibo Liu,Yu Huang,Jiang Bian,Serena Jingchuan Guo,Zhe Jiang*

Main category: cs.AI

TL;DR: 提出了TD-HNODE模型，通过时间详细超图和神经ODE框架学习疾病进展的连续时间动态，在2型糖尿病和相关心血管疾病进展建模中表现优于多个基线方法。


<details>
  <summary>Details</summary>
Motivation: 疾病进展建模面临挑战：需要基于不规则时间事件样本学习连续时间动态，以及处理患者异质性（不同进展速率和路径）。现有方法要么缺乏从真实数据中学习的适应性，要么无法捕捉进展轨迹上的复杂连续时间动态。

Method: TD-HNODE模型将疾病进展表示为时间详细超图，通过神经ODE框架学习连续时间进展动态。包含可学习的TD-Hypergraph Laplacian，捕捉疾病并发症标记在进展轨迹内和轨迹间的相互依赖关系。

Result: 在两个真实世界临床数据集上的实验表明，TD-HNODE在建模2型糖尿病和相关心血管疾病进展方面优于多个基线方法。

Conclusion: TD-HNODE能够有效建模疾病进展的连续时间动态，解决了现有方法在适应性和复杂动态捕捉方面的局限性。

Abstract: Disease progression modeling aims to characterize and predict how a patient's
disease complications worsen over time based on longitudinal electronic health
records (EHRs). Accurate modeling of disease progression, such as type 2
diabetes, can enhance patient sub-phenotyping and inform effective and timely
interventions. However, the problem is challenging due to the need to learn
continuous-time dynamics of progression patterns based on irregular-time event
samples and patient heterogeneity (\eg different progression rates and
pathways). Existing mechanistic and data-driven methods either lack
adaptability to learn from real-world data or fail to capture complex
continuous-time dynamics on progression trajectories. To address these
limitations, we propose Temporally Detailed Hypergraph Neural Ordinary
Differential Equation (TD-HNODE), which represents disease progression on
clinically recognized trajectories as a temporally detailed hypergraph and
learns the continuous-time progression dynamics via a neural ODE framework.
TD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the
interdependency of disease complication markers within both intra- and
inter-progression trajectories. Experiments on two real-world clinical datasets
demonstrate that TD-HNODE outperforms multiple baselines in modeling the
progression of type 2 diabetes and related cardiovascular diseases.

</details>


### [246] [Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis](https://arxiv.org/abs/2510.17235)
*Chong Chen,Ze Liu,Lingfeng Bao,Yanlin Wang,Ting Chen,Daoyuan Wu,Jiachi Chen*

Main category: cs.AI

TL;DR: Coinvisor是一个基于强化学习的多智能体聊天机器人，通过专门的工具集成和强化学习工具选择机制，为加密货币投资提供全面的分析支持，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决加密货币投资中现有方法的局限性：手动分析耗时且易产生偏见，数据聚合平台功能有限，基于LLM的智能体缺乏实时数据集成和多步推理能力。

Method: 采用基于强化学习的多智能体框架，通过专门工具集成多样分析能力，核心创新是强化学习工具选择机制，支持多步规划和灵活集成多种数据源。

Result: 在工具调用准确性上，相比基础模型召回率提升40.7%，F1分数提升26.6%；用户研究显示高满意度(4.64/5)，参与者更偏好Coinvisor而非通用LLM和现有加密平台(4.62/5)。

Conclusion: Coinvisor通过强化学习工具选择和多智能体框架，有效解决了加密货币投资分析中的关键挑战，提供了准确且可操作的投资洞察。

Abstract: The cryptocurrency market offers significant investment opportunities but
faces challenges including high volatility and fragmented information. Data
integration and analysis are essential for informed investment decisions.
Currently, investors use three main approaches: (1) Manual analysis across
various sources, which depends heavily on individual experience and is
time-consuming and prone to bias; (2) Data aggregation platforms-limited in
functionality and depth of analysis; (3) Large language model agents-based on
static pretrained models, lacking real-time data integration and multi-step
reasoning capabilities. To address these limitations, we present Coinvisor, a
reinforcement learning-based chatbot that provides comprehensive analytical
support for cryptocurrency investment through a multi-agent framework.
Coinvisor integrates diverse analytical capabilities through specialized tools.
Its key innovation is a reinforcement learning-based tool selection mechanism
that enables multi-step planning and flexible integration of diverse data
sources. This design supports real-time interaction and adaptive analysis of
dynamic content, delivering accurate and actionable investment insights. We
evaluated Coinvisor through automated benchmarks on tool calling accuracy and
user studies with 20 cryptocurrency investors using our interface. Results show
that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base
model in tool orchestration. User studies show high satisfaction (4.64/5), with
participants preferring Coinvisor to both general LLMs and existing crypto
platforms (4.62/5).

</details>


### [247] [RubiSCoT: A Framework for AI-Supported Academic Assessment](https://arxiv.org/abs/2510.17309)
*Thorsten Fröhlich,Tim Schlippe*

Main category: cs.AI

TL;DR: 提出了RubiSCoT框架，利用AI技术增强论文评估过程，从开题到最终提交提供一致、可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统论文评估方法耗时且存在评估者主观差异，需要更高效、一致的评估方案。

Method: 使用先进自然语言处理技术，包括大语言模型、检索增强生成和结构化思维链提示，提供初步评估、多维度评估、内容提取、基于评分标准的评分和详细报告。

Result: 设计并实现了RubiSCoT框架，展示了其在优化学术评估流程方面的潜力。

Conclusion: RubiSCoT能够通过一致、可扩展和透明的评估来优化学术评估过程。

Abstract: The evaluation of academic theses is a cornerstone of higher education,
ensuring rigor and integrity. Traditional methods, though effective, are
time-consuming and subject to evaluator variability. This paper presents
RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from
proposal to final submission. Using advanced natural language processing
techniques, including large language models, retrieval-augmented generation,
and structured chain-of-thought prompting, RubiSCoT offers a consistent,
scalable solution. The framework includes preliminary assessments,
multidimensional assessments, content extraction, rubric-based scoring, and
detailed reporting. We present the design and implementation of RubiSCoT,
discussing its potential to optimize academic assessment processes through
consistent, scalable, and transparent evaluation.

</details>


### [248] [Graph Attention-Guided Search for Dense Multi-Agent Pathfinding](https://arxiv.org/abs/2510.17382)
*Rishabh Jain,Keisuke Okumura,Michael Amir,Amanda Prorok*

Main category: cs.AI

TL;DR: 提出了LaGAT框架，将基于图注意力的神经网络启发式(MAGAT)集成到搜索算法(LaCAM)中，用于解决密集多智能体路径规划问题。


<details>
  <summary>Details</summary>
Motivation: 当前最优的规划器在实时解决密集多智能体路径规划问题时仍面临挑战，需要结合学习与搜索方法的优势。

Method: 开发混合框架，将改进的MAGAT神经网络启发式集成到LaCAM搜索算法中，采用预训练-微调策略和死锁检测机制。

Result: LaGAT在密集场景中优于纯搜索和纯学习方法，证明了混合搜索在挑战性多智能体协调问题中的有效性。

Conclusion: 精心设计的混合搜索为紧密耦合的挑战性多智能体协调问题提供了强大解决方案。

Abstract: Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)
problems in real-time remains challenging even for state-of-the-art planners.
To this end, we develop a hybrid framework that integrates a learned heuristic
derived from MAGAT, a neural MAPF policy with a graph attention scheme, into a
leading search-based algorithm, LaCAM. While prior work has explored
learning-guided search in MAPF, such methods have historically underperformed.
In contrast, our approach, termed LaGAT, outperforms both purely search-based
and purely learning-based methods in dense scenarios. This is achieved through
an enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of
interest, and a deadlock detection scheme to account for imperfect neural
guidance. Our results demonstrate that, when carefully designed, hybrid search
offers a powerful solution for tightly coupled, challenging multi-agent
coordination problems.

</details>


### [249] [Diverse Planning with Simulators via Linear Temporal Logic](https://arxiv.org/abs/2510.17418)
*Mustafa F. Abdelwahed,Alice Toniolo,Joan Espasa,Ian P. Gent*

Main category: cs.AI

TL;DR: 提出FBI_LTL，一种用于仿真规划问题的多样化规划器，使用线性时序逻辑定义语义多样性标准，生成语义多样化的规划方案


<details>
  <summary>Details</summary>
Motivation: 传统规划器只生成单一规划，可能无法满足代理偏好；现有多样化规划方法可能产生语法不同但语义相同的解决方案

Method: 利用线性时序逻辑定义语义多样性标准，并将这些基于LTL的多样性模型直接集成到搜索过程中

Result: 在各种基准测试中，FBI_LTL相比基线方法能生成更多样化的规划

Conclusion: 这项工作确立了在仿真环境中语义引导多样化规划的可行性，为传统基于模型方法失效的现实非符号领域开辟了新途径

Abstract: Autonomous agents rely on automated planning algorithms to achieve their
objectives. Simulation-based planning offers a significant advantage over
declarative models in modelling complex environments. However, relying solely
on a planner that produces a single plan may not be practical, as the generated
plans may not always satisfy the agent's preferences. To address this
limitation, we introduce $\texttt{FBI}_\texttt{LTL}$, a diverse planner
explicitly designed for simulation-based planning problems.
$\texttt{FBI}_\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define
semantic diversity criteria, enabling agents to specify what constitutes
meaningfully different plans. By integrating these LTL-based diversity models
directly into the search process, $\texttt{FBI}_\texttt{LTL}$ ensures the
generation of semantically diverse plans, addressing a critical limitation of
existing diverse planning approaches that may produce syntactically different
but semantically identical solutions. Extensive evaluations on various
benchmarks consistently demonstrate that $\texttt{FBI}_\texttt{LTL}$ generates
more diverse plans compared to a baseline approach. This work establishes the
feasibility of semantically-guided diverse planning in simulation-based
environments, paving the way for innovative approaches in realistic,
non-symbolic domains where traditional model-based approaches fail.

</details>


### [250] [Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions](https://arxiv.org/abs/2510.17450)
*Johan Schubert,Farzad Kamrani,Tove Gustavi*

Main category: cs.AI

TL;DR: 开发了一种基于主动推理的路径规划方法，用于智能代理的自主控制，通过构建证据地图和计算变分自由能量来指导代理移动，平衡探索和利用。


<details>
  <summary>Details</summary>
Motivation: 为了侦察地理区域并维持共同的作战态势图，需要一种能够自主控制智能代理的方法，解决探索和利用之间的平衡问题。

Method: 使用Dempster-Shafer理论和高斯传感器模型构建生成模型，通过贝叶斯方法更新后验概率分布，计算变分自由能量来指导代理移动。

Result: 该方法能够在模拟中指导代理朝着最小化自由能量的位置移动，有效平衡了地理地图的广泛搜索和已识别目标对象的跟踪。

Conclusion: 提出的主动推理路径规划方法成功解决了智能代理在侦察任务中的探索与利用平衡问题，为自主控制提供了有效解决方案。

Abstract: We develop an active inference route-planning method for the autonomous
control of intelligent agents. The aim is to reconnoiter a geographical area to
maintain a common operational picture. To achieve this, we construct an
evidence map that reflects our current understanding of the situation,
incorporating both positive and "negative" sensor observations of possible
target objects collected over time, and diffusing the evidence across the map
as time progresses. The generative model of active inference uses
Dempster-Shafer theory and a Gaussian sensor model, which provides input to the
agent. The generative process employs a Bayesian approach to update a posterior
probability distribution. We calculate the variational free energy for all
positions within the area by assessing the divergence between a pignistic
probability distribution of the evidence map and a posterior probability
distribution of a target object based on the observations, including the level
of surprise associated with receiving new observations. Using the free energy,
we direct the agents' movements in a simulation by taking an incremental step
toward a position that minimizes the free energy. This approach addresses the
challenge of exploration and exploitation, allowing agents to balance searching
extensive areas of the geographical map while tracking identified target
objects.

</details>


### [251] [Label Indeterminacy in AI & Law](https://arxiv.org/abs/2510.17463)
*Cor Steging,Tadeusz Zbiegień*

Main category: cs.AI

TL;DR: 法律机器学习需要处理标签不确定性，因为法律结果常受人为干预影响，导致相同案件可能有不同结果。本文通过欧洲人权法院案例展示了标签构建方式如何显著影响模型行为。


<details>
  <summary>Details</summary>
Motivation: 法律机器学习通常将过去案例结果视为真实标签，但法律结果往往受到和解、上诉等人为干预影响，造成标签不确定性。这种不确定性在现有方法中未被充分考虑。

Method: 在欧洲人权法院案例分类的背景下，研究不同标签构建方式对模型行为的影响，探讨处理标签不确定性的方法。

Result: 研究表明，训练过程中标签的构建方式会显著影响模型的行为表现，标签不确定性是AI与法律领域中需要关注的重要问题。

Conclusion: 法律机器学习应用必须考虑标签不确定性，这是影响模型行为的关键因素，需要在AI与法律研究中给予更多关注。

Abstract: Machine learning is increasingly used in the legal domain, where it typically
operates retrospectively by treating past case outcomes as ground truth.
However, legal outcomes are often shaped by human interventions that are not
captured in most machine learning approaches. A final decision may result from
a settlement, an appeal, or other procedural actions. This creates label
indeterminacy: the outcome could have been different if the intervention had or
had not taken place. We argue that legal machine learning applications need to
account for label indeterminacy. Methods exist that can impute these
indeterminate labels, but they are all grounded in unverifiable assumptions. In
the context of classifying cases from the European Court of Human Rights, we
show that the way that labels are constructed during training can significantly
affect model behaviour. We therefore position label indeterminacy as a relevant
concern in AI & Law and demonstrate how it can shape model behaviour.

</details>


### [252] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil,Sharad Duwal,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.AI

TL;DR: MIRAGE是一个用于检测多模态虚假信息的推理时框架，通过分解验证过程为四个模块：视觉真实性评估、跨模态一致性分析、检索增强的事实核查和校准判断，在MMFakeBench上达到81.65% F1分数，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 网络平台上每天有数十亿包含文本和图像的多模态帖子传播虚假信息，超出人工事实核查能力。现有的监督检测模型需要特定领域训练数据，且难以泛化到不同的操纵策略。

Method: MIRAGE框架将多模态验证分解为四个顺序模块：视觉真实性评估检测AI生成图像，跨模态一致性分析识别上下文不当的重新利用，检索增强的事实核查通过迭代问题生成将声明与网络证据关联，校准判断模块整合所有信号。

Result: 在MMFakeBench验证集上，MIRAGE使用GPT-4o-mini达到81.65% F1分数和75.1%准确率，比最强的零样本基线（GPT-4V with MMD-Agent）高出7.65个F1点，同时保持34.3%的假阳性率，远低于仅使用判断基线的97.3%。

Conclusion: 分解的智能推理结合网络检索可以在不需要特定领域训练的情况下达到监督检测器的性能，在标注数据稀缺的多模态虚假信息检测中具有应用价值。

Abstract: Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

</details>


### [253] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: 将大型语言模型的推理能力蒸馏到小型模型中，通过结构感知损失优化实现高效的代码生成


<details>
  <summary>Details</summary>
Motivation: 代码生成需要准确理解提示意图和算法推理能力，而小型语言模型可能缺乏这种推理能力。通过蒸馏大型模型的推理能力，可以创建更高效、部署成本更低的模型。

Method: 使用结构感知损失优化方法，训练小型模型模拟大型模型的推理和问题解决能力，学习识别正确解决方案路径，建立问题定义与解决方案之间的结构对应关系。

Result: 在MBPP、MBPP Plus和HumanEval基准测试中，经过微调的模型在pass@1、平均数据流和平均语法匹配指标上显著优于基线模型。

Conclusion: 通过简单廉价的蒸馏过程，成功将大型语言模型的推理能力转移到小型模型中，实现了高效的代码生成性能。

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [254] [OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration](https://arxiv.org/abs/2510.17614)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: OG-Rank是一个低延迟的解码器重排序系统，通过第一令牌评分和不确定性门控解释步骤，在保持可预测延迟的同时提供强效的排序性能。


<details>
  <summary>Details</summary>
Motivation: 临床医生需要实时工作并能解释其选择的排序系统，需要一个低延迟、基于解码器的重排序器。

Method: 采用单解码器方法，结合池化的第一令牌评分信号和不确定性门控解释步骤，通过专注于困难案例的课程训练。

Result: 在遭遇范围订单选择任务中表现优异（快速路径：Recall@1~0.45，nDCG@20~0.625），门控激活时性能进一步提升（Recall@1~0.56，nDCG@20~0.699，门控率45%）。

Conclusion: OG-Rank提供了一个实用方案：默认快速排序，在需要时解释，这种模式适用于选择性生成能以可接受成本提高准确性的决策任务。

Abstract: Clinicians need ranking systems that work in real time and still justify
their choices. Motivated by the need for a low-latency, decoder-based reranker,
we present OG-Rank, a single-decoder approach that pairs a pooled first-token
scoring signal with an uncertainty-gated explanation step. The model scores all
candidates in one pass and generates a brief, structured rationale only when
the list is genuinely ambiguous, keeping latency predictable. Trained with a
curriculum that concentrates effort on hard cases, OG-Rank delivers strong
effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,
nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,
nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains
under the same policy. Encoder baselines trail in both effectiveness and
flexibility. The result is a practical recipe: rank fast by default and explain
when it helps, a pattern that applies broadly to decision tasks where selective
generation buys accuracy at acceptable cost. The single-policy design
simplifies deployment and budget planning, and the curriculum principle (spend
more on the hard cases, less on the easy ones) readily transfers beyond
clinical order selection.

</details>


### [255] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: 该论文系统评估了大型语言模型在预测现实世界未来事件方面的能力，提出了"LLM-as-a-Prophet"的新范式，并建立了Prophet Arena基准来测试LLMs的预测智能。


<details>
  <summary>Details</summary>
Motivation: 随着在互联网规模数据上训练的大型语言模型的快速发展，利用LLMs预测现实世界未来事件具有重要潜力，这对金融、经济等社会系统具有重要意义。

Method: 构建了Prophet Arena评估基准，持续收集实时预测任务，并将每个任务分解为不同的流水线阶段，以支持受控和大规模实验。

Result: 研究发现许多LLMs已展现出令人印象深刻的预测能力，表现为较小的校准误差、一致的预测置信度和有前景的市场回报。但也发现了关键瓶颈，如事件回忆不准确、数据源误解以及接近解决时信息聚合速度慢于市场。

Conclusion: LLMs在预测智能方面具有显著潜力，但要实现卓越的预测能力仍需解决事件回忆准确性、数据理解和信息聚合速度等关键挑战。

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [256] [A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.17697)
*Anjie Liu,Jianhong Wang,Samuel Kaski,Jun Wang,Mengyue Yang*

Main category: cs.AI

TL;DR: 提出基于多智能体影响图(MAIDs)的交互范式，通过目标干预和因果推理技术PSI解决大规模MARL中全局指导不切实际的问题，同时提供分析工具验证学习范式的可行性。


<details>
  <summary>Details</summary>
Motivation: 在大规模多智能体强化学习中，对整个系统的全局人工指导不切实际，且现有协调机制设计依赖经验研究，缺乏易用的分析工具。

Method: 使用MAIDs作为图形框架，提出目标交互范式，仅对单个目标智能体进行干预，并采用Pre-Strategy Intervention(PSI)因果推理技术实现该范式。

Result: 实验证明目标干预的有效性，并验证了相关性图分析的结果。

Conclusion: MAIDs框架能够有效分析和可视化MARL方法，目标干预范式缓解了全局指导问题，PSI技术通过最大化因果效应实现复合期望结果。

Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards
desired outcomes is challenging, particularly when the global guidance from a
human on the whole multi-agent system is impractical in a large-scale MARL. On
the other hand, designing mechanisms to coordinate agents most relies on
empirical studies, lacking a easy-to-use research tool. In this work, we employ
multi-agent influence diagrams (MAIDs) as a graphical framework to address the
above issues. First, we introduce interaction paradigms that leverage MAIDs to
analyze and visualize existing approaches in MARL. Then, we design a new
interaction paradigm based on MAIDs, referred to as targeted intervention that
is applied to only a single targeted agent, so the problem of global guidance
can be mitigated. In our implementation, we introduce a causal inference
technique-referred to as Pre-Strategy Intervention (PSI)-to realize the
targeted intervention paradigm. Since MAIDs can be regarded as a special class
of causal diagrams, a composite desired outcome that integrates the primary
task goal and an additional desired outcome can be achieved by maximizing the
corresponding causal effect through the PSI. Moreover, the bundled relevance
graph analysis of MAIDs provides a tool to identify whether an MARL learning
paradigm is workable under the design of an interaction paradigm. In
experiments, we demonstrate the effectiveness of our proposed targeted
intervention, and verify the result of relevance graph analysis.

</details>


### [257] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出了Contextual Attention Modulation (CAM)机制和HyCAM框架，通过动态调节自注意力表示来增强任务特定特征同时保留通用知识，在多项任务上平均性能提升3.65%。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多任务适应中平衡知识保留与任务特定专业化的问题，传统微调方法存在灾难性遗忘和资源消耗大的缺陷。

Method: 提出CAM机制动态调节自注意力模块表示，并构建HyCAM框架，结合共享的全参数CAM模块和多个轻量级专用CAM模块，采用动态路由策略进行自适应知识融合。

Result: 在问答、代码生成和逻辑推理等异构任务上的实验表明，该方法显著优于现有方法，平均性能提升3.65%。

Conclusion: CAM和HyCAM框架有效解决了大语言模型的多任务适应问题，在保持通用知识的同时实现了更好的任务特定专业化。

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


### [258] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: 研究发现视觉语言模型(VLMs)存在"看到但不相信"现象：模型能感知到正确的视觉证据但未有效利用，导致错误答案。通过选择性注意力掩码干预可显著提升多个VLM家族的准确性。


<details>
  <summary>Details</summary>
Motivation: 系统研究VLMs在视觉问答任务中失败的原因：是由于未感知视觉证据，还是未有效利用已感知的证据。

Method: 通过分析层间注意力动态，发现深层网络稀疏但可靠地关注局部证据区域；提出无需训练的推理时干预方法，通过选择性注意力掩码突出深层证据区域。

Result: VLMs在输出错误答案时往往已感知到视觉证据；提出的干预方法在LLaVA、Qwen、Gemma和InternVL等多个VLM家族中一致提升准确率。

Conclusion: VLMs内部编码了可靠的证据但未充分利用，使这些信号显式化可以弥合感知与推理之间的差距，提升VLM的诊断理解和可靠性。

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>
