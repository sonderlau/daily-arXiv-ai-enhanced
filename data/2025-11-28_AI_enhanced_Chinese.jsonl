{"id": "2511.21674", "categories": ["cs.NE", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2511.21674", "abs": "https://arxiv.org/abs/2511.21674", "authors": ["Agnes Korcsak-Gorzo", "Jes\u00fas A. Espinoza Valverde", "Jonas Stapmanns", "Hans Ekkehard Plesser", "David Dahmen", "Matthias Bolten", "Sacha J. van Albada", "Markus Diesmann"], "title": "Event-driven eligibility propagation in large sparse networks: efficiency shaped by biological realism", "comment": null, "summary": "Despite remarkable technological advances, AI systems may still benefit from biological principles, such as recurrent connectivity and energy-efficient mechanisms. Drawing inspiration from the brain, we present a biologically plausible extension of the eligibility propagation (e-prop) learning rule for recurrent spiking networks. By translating the time-driven update scheme into an event-driven one, we integrate the learning rule into a simulation platform for large-scale spiking neural networks and demonstrate its applicability to tasks such as neuromorphic MNIST. We extend the model with prominent biological features such as continuous dynamics and weight updates, strict locality, and sparse connectivity. Our results show that biologically grounded constraints can inform the design of computationally efficient AI algorithms, offering scalability to millions of neurons without compromising learning performance. This work bridges machine learning and computational neuroscience, paving the way for sustainable, biologically inspired AI systems while advancing our understanding of brain-like learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u751f\u7269\u542f\u53d1\u7684\u6269\u5c55\u8d44\u683c\u4f20\u64ad\u5b66\u4e60\u89c4\u5219\uff0c\u7528\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff0c\u5c06\u65f6\u95f4\u9a71\u52a8\u66f4\u65b0\u8f6c\u6362\u4e3a\u4e8b\u4ef6\u9a71\u52a8\uff0c\u5e76\u6574\u5408\u5230\u5927\u89c4\u6a21\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u5e73\u53f0\u4e2d\u3002", "motivation": "\u5c3d\u7ba1\u6280\u672f\u8fdb\u6b65\u663e\u8457\uff0cAI\u7cfb\u7edf\u4ecd\u53ef\u4ece\u751f\u7269\u539f\u7406\u4e2d\u53d7\u76ca\uff0c\u5982\u5faa\u73af\u8fde\u63a5\u548c\u80fd\u91cf\u9ad8\u6548\u673a\u5236\u3002\u53d7\u5927\u8111\u542f\u53d1\uff0c\u5f00\u53d1\u66f4\u751f\u7269\u5408\u7406\u7684\u7b97\u6cd5\u3002", "method": "\u6269\u5c55e-prop\u5b66\u4e60\u89c4\u5219\uff0c\u5c06\u5176\u4ece\u65f6\u95f4\u9a71\u52a8\u8f6c\u6362\u4e3a\u4e8b\u4ef6\u9a71\u52a8\u66f4\u65b0\uff0c\u96c6\u6210\u5230\u5927\u89c4\u6a21\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u5e73\u53f0\uff0c\u5e76\u52a0\u5165\u8fde\u7eed\u52a8\u6001\u3001\u4e25\u683c\u5c40\u90e8\u6027\u3001\u7a00\u758f\u8fde\u63a5\u7b49\u751f\u7269\u7279\u5f81\u3002", "result": "\u5728\u795e\u7ecf\u5f62\u6001MNIST\u7b49\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9002\u7528\u6027\uff0c\u80fd\u591f\u6269\u5c55\u5230\u6570\u767e\u4e07\u795e\u7ecf\u5143\u800c\u4e0d\u5f71\u54cd\u5b66\u4e60\u6027\u80fd\u3002", "conclusion": "\u751f\u7269\u57fa\u7840\u7ea6\u675f\u53ef\u4ee5\u4e3a\u8ba1\u7b97\u9ad8\u6548\u7684AI\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4fe1\u606f\uff0c\u8fd9\u9879\u5de5\u4f5c\u8fde\u63a5\u4e86\u673a\u5668\u5b66\u4e60\u548c\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\uff0c\u4e3a\u53ef\u6301\u7eed\u7684\u751f\u7269\u542f\u53d1AI\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2511.20721", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2511.20721", "abs": "https://arxiv.org/abs/2511.20721", "authors": ["Guillaume Letellier", "Siddharth Srivastava", "Fr\u00e9d\u00e9ric Jurie", "Gaurav Sharma"], "title": "Foundry: Distilling 3D Foundation Models for the Edge", "comment": null, "summary": "Foundation models pre-trained with self-supervised learning (SSL) on large-scale datasets have become powerful general-purpose feature extractors. However, their immense size and computational cost make them prohibitive for deployment on edge devices such as robots and AR/VR headsets. Existing compression techniques like standard knowledge distillation create efficient 'specialist' models but sacrifice the crucial, downstream-agnostic generality that makes foundation models so valuable.  In this paper, we introduce Foundation Model Distillation (FMD), a new paradigm for compressing large SSL models into compact, efficient, and faithful proxies that retain their general-purpose representational power. We present Foundry, the first implementation of FMD for 3D point clouds. Our approach, Foundry, trains a student to learn a compressed set of SuperTokens that reconstruct the teacher's token-level representations, capturing a compact basis of its latent space. A single distilled model maintains strong transferability across diverse downstream tasks-classification, part segmentation, and few-shot scenarios-approaching full foundation-model performance while using significantly fewer tokens and FLOPs, making such models more practical for deployment on resourceconstrained hardware.", "AI": {"tldr": "\u63d0\u51faFoundation Model Distillation (FMD)\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u538b\u7f29\u5927\u578bSSL\u6a21\u578b\u521b\u5efa\u7d27\u51d1\u9ad8\u6548\u7684\u4ee3\u7406\u6a21\u578b\uff0c\u4fdd\u7559\u901a\u7528\u8868\u5f81\u80fd\u529b\u3002\u5b9e\u73b0Foundry\u7cfb\u7edf\u7528\u4e8e3D\u70b9\u4e91\uff0c\u8bad\u7ec3\u5b66\u751f\u5b66\u4e60\u538b\u7f29\u7684SuperTokens\u6765\u91cd\u5efa\u6559\u5e08\u6a21\u578b\u7684\u8868\u5f81\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u867d\u7136\u5f3a\u5927\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002\u73b0\u6709\u538b\u7f29\u6280\u672f\u4f1a\u727a\u7272\u6a21\u578b\u7684\u901a\u7528\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u6301\u901a\u7528\u8868\u5f81\u80fd\u529b\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Foundation Model Distillation (FMD)\u8303\u5f0f\uff0c\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u5b66\u4e60\u538b\u7f29\u7684SuperTokens\u6765\u91cd\u5efa\u6559\u5e08\u6a21\u578b\u7684token\u7ea7\u8868\u5f81\uff0c\u6355\u6349\u6f5c\u5728\u7a7a\u95f4\u7684\u7d27\u51d1\u57fa\u5411\u91cf\u3002", "result": "\u5355\u4e2a\u84b8\u998f\u6a21\u578b\u5728\u5206\u7c7b\u3001\u90e8\u5206\u5206\u5272\u548c\u5c11\u6837\u672c\u573a\u666f\u7b49\u591a\u6837\u5316\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4fdd\u6301\u5f3a\u8fc1\u79fb\u6027\uff0c\u63a5\u8fd1\u5b8c\u6574\u57fa\u7840\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11token\u6570\u91cf\u548cFLOPs\u3002", "conclusion": "FMD\u80fd\u591f\u521b\u5efa\u7d27\u51d1\u9ad8\u6548\u7684\u4ee3\u7406\u6a21\u578b\uff0c\u4fdd\u7559\u57fa\u7840\u6a21\u578b\u7684\u901a\u7528\u8868\u5f81\u80fd\u529b\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u4e0a\u90e8\u7f72\u3002"}}
{"id": "2511.20803", "categories": ["physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.20803", "abs": "https://arxiv.org/abs/2511.20803", "authors": ["R. Louw", "W. A. van Wijngaarden", "W. Happer"], "title": "Sunlight, the Bond Albedo, CO2, and Earth's Temperature", "comment": null, "summary": "The main determinants of Earth's absolute surface Temperature, T, are the solar constant, S, the Bond albedo, A, and the effective emissivity for thermal radiation, e. In this note we assume that the value of the effective emissivity, e = e(C), is determined by the atmospheric concentration C of CO2. We show that the solar constant is most important, the albedo is second, and the CO2 concentration is a distant third.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u5f71\u54cd\u5730\u7403\u8868\u9762\u6e29\u5ea6\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u53d1\u73b0\u592a\u9633\u5e38\u6570\u6700\u91cd\u8981\uff0c\u53cd\u7167\u7387\u6b21\u4e4b\uff0cCO2\u6d53\u5ea6\u5f71\u54cd\u6700\u5c0f\u3002", "motivation": "\u786e\u5b9a\u5f71\u54cd\u5730\u7403\u7edd\u5bf9\u8868\u9762\u6e29\u5ea6\u7684\u5173\u952e\u56e0\u7d20\u53ca\u5176\u76f8\u5bf9\u91cd\u8981\u6027\u3002", "method": "\u5047\u8bbe\u6709\u6548\u53d1\u5c04\u7387\u7531\u5927\u6c14CO2\u6d53\u5ea6\u51b3\u5b9a\uff0c\u5206\u6790\u592a\u9633\u5e38\u6570\u3001\u53cd\u7167\u7387\u548cCO2\u6d53\u5ea6\u5bf9\u6e29\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u592a\u9633\u5e38\u6570\u5bf9\u6e29\u5ea6\u5f71\u54cd\u6700\u5927\uff0c\u53cd\u7167\u7387\u7b2c\u4e8c\u91cd\u8981\uff0cCO2\u6d53\u5ea6\u5f71\u54cd\u76f8\u5bf9\u8f83\u5c0f\u3002", "conclusion": "\u5728\u5f71\u54cd\u5730\u7403\u8868\u9762\u6e29\u5ea6\u7684\u56e0\u7d20\u4e2d\uff0c\u592a\u9633\u5e38\u6570\u662f\u6700\u4e3b\u8981\u7684\u51b3\u5b9a\u56e0\u7d20\uff0c\u5176\u6b21\u662f\u53cd\u7167\u7387\uff0cCO2\u6d53\u5ea6\u7684\u5f71\u54cd\u76f8\u5bf9\u8f83\u5f31\u3002"}}
{"id": "2511.20679", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20679", "abs": "https://arxiv.org/abs/2511.20679", "authors": ["Melika Ayoughi", "Pascal Mettes", "Paul Groth"], "title": "Minimizing Hyperbolic Embedding Distortion with LLM-Guided Hierarchy Restructuring", "comment": null, "summary": "Hyperbolic geometry is an effective geometry for embedding hierarchical data structures. Hyperbolic learning has therefore become increasingly prominent in machine learning applications where data is hierarchically organized or governed by hierarchical semantics, ranging from recommendation systems to computer vision. The quality of hyperbolic embeddings is tightly coupled to the structure of the input hierarchy, which is often derived from knowledge graphs or ontologies. Recent work has uncovered that for an optimal hyperbolic embedding, a high branching factor and single inheritance are key, while embedding algorithms are robust to imbalance and hierarchy size. To assist knowledge engineers in reorganizing hierarchical knowledge, this paper investigates whether Large Language Models (LLMs) have the ability to automatically restructure hierarchies to meet these criteria. We propose a prompt-based approach to transform existing hierarchies using LLMs, guided by known desiderata for hyperbolic embeddings. Experiments on 16 diverse hierarchies show that LLM-restructured hierarchies consistently yield higher-quality hyperbolic embeddings across several standard embedding quality metrics. Moreover, we show how LLM-guided hierarchy restructuring enables explainable reorganizations, providing justifications to knowledge engineers.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u91cd\u6784\u5c42\u6b21\u7ed3\u6784\u4ee5\u4f18\u5316\u53cc\u66f2\u5d4c\u5165\u8d28\u91cf\uff0c\u5b9e\u9a8c\u8868\u660eLLM\u91cd\u6784\u7684\u5c42\u6b21\u7ed3\u6784\u80fd\u663e\u8457\u63d0\u5347\u5d4c\u5165\u8d28\u91cf\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u91cd\u7ec4\u7406\u7531\u3002", "motivation": "\u53cc\u66f2\u5d4c\u5165\u7684\u8d28\u91cf\u4e0e\u8f93\u5165\u5c42\u6b21\u7ed3\u6784\u7d27\u5bc6\u76f8\u5173\uff0c\u800c\u73b0\u6709\u5c42\u6b21\u7ed3\u6784\u5f80\u5f80\u4e0d\u7b26\u5408\u53cc\u66f2\u5d4c\u5165\u7684\u6700\u4f73\u6761\u4ef6\uff08\u9ad8\u5206\u652f\u56e0\u5b50\u548c\u5355\u7ee7\u627f\uff09\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22LLM\u80fd\u5426\u81ea\u52a8\u91cd\u6784\u5c42\u6b21\u7ed3\u6784\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u6807\u51c6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528LLM\u5728\u5df2\u77e5\u53cc\u66f2\u5d4c\u5165\u671f\u671b\u6807\u51c6\u7684\u6307\u5bfc\u4e0b\u8f6c\u6362\u73b0\u6709\u5c42\u6b21\u7ed3\u6784\u3002\u572816\u4e2a\u4e0d\u540c\u5c42\u6b21\u7ed3\u6784\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLM\u91cd\u6784\u7684\u5c42\u6b21\u7ed3\u6784\u5728\u591a\u4e2a\u6807\u51c6\u5d4c\u5165\u8d28\u91cf\u6307\u6807\u4e0a\u4e00\u81f4\u4ea7\u751f\u66f4\u9ad8\u8d28\u91cf\u7684\u53cc\u66f2\u5d4c\u5165\uff0c\u5e76\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u91cd\u7ec4\u7406\u7531\u3002", "conclusion": "LLM\u80fd\u591f\u6709\u6548\u81ea\u52a8\u91cd\u6784\u5c42\u6b21\u7ed3\u6784\u4ee5\u4f18\u5316\u53cc\u66f2\u5d4c\u5165\u8d28\u91cf\uff0c\u540c\u65f6\u4e3a\u77e5\u8bc6\u5de5\u7a0b\u5e08\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u91cd\u7ec4\u8fc7\u7a0b\u3002"}}
{"id": "2511.20710", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.20710", "abs": "https://arxiv.org/abs/2511.20710", "authors": ["David Amebley", "Sayanton Dibbo"], "title": "Are Neuro-Inspired Multi-Modal Vision-Language Models Resilient to Membership Inference Privacy Leakage?", "comment": null, "summary": "In the age of agentic AI, the growing deployment of multi-modal models (MMs) has introduced new attack vectors that can leak sensitive training data in MMs, causing privacy leakage. This paper investigates a black-box privacy attack, i.e., membership inference attack (MIA) on multi-modal vision-language models (VLMs). State-of-the-art research analyzes privacy attacks primarily to unimodal AI-ML systems, while recent studies indicate MMs can also be vulnerable to privacy attacks. While researchers have demonstrated that biologically inspired neural network representations can improve unimodal model resilience against adversarial attacks, it remains unexplored whether neuro-inspired MMs are resilient against privacy attacks. In this work, we introduce a systematic neuroscience-inspired topological regularization (tau) framework to analyze MM VLMs resilience against image-text-based inference privacy attacks. We examine this phenomenon using three VLMs: BLIP, PaliGemma 2, and ViT-GPT2, across three benchmark datasets: COCO, CC3M, and NoCaps. Our experiments compare the resilience of baseline and neuro VLMs (with topological regularization), where the tau > 0 configuration defines the NEURO variant of VLM. Our results on the BLIP model using the COCO dataset illustrate that MIA attack success in NEURO VLMs drops by 24% mean ROC-AUC, while achieving similar model utility (similarities between generated and reference captions) in terms of MPNet and ROUGE-2 metrics. This shows neuro VLMs are comparatively more resilient against privacy attacks, while not significantly compromising model utility. Our extensive evaluation with PaliGemma 2 and ViT-GPT2 models, on two additional datasets: CC3M and NoCaps, further validates the consistency of the findings. This work contributes to the growing understanding of privacy risks in MMs and provides evidence on neuro VLMs privacy threat resilience.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u62d3\u6251\u6b63\u5219\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u6210\u5458\u63a8\u65ad\u653b\u51fb\u7684\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6548\u7528\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u8bad\u7ec3\u6570\u636e\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u65e5\u76ca\u7a81\u51fa\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u7cfb\u7edf\u7684\u9690\u79c1\u653b\u51fb\uff0c\u800c\u591a\u6a21\u6001\u6a21\u578b\u7684\u9690\u79c1\u8106\u5f31\u6027\u53ca\u5176\u4e0e\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u6a21\u578b\u7ed3\u6784\u4e4b\u95f4\u7684\u5173\u7cfb\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u7cfb\u7edf\u6027\u7684\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u62d3\u6251\u6b63\u5219\u5316\u6846\u67b6(tau)\uff0c\u5728\u4e09\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(BLIP\u3001PaliGemma 2\u3001ViT-GPT2)\u4e0a\u5e94\u7528\u8be5\u6b63\u5219\u5316\uff0c\u5e76\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6(COCO\u3001CC3M\u3001NoCaps)\u4e0a\u8bc4\u4f30\u5176\u5bf9\u6210\u5458\u63a8\u65ad\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728BLIP\u6a21\u578b\u4e0a\uff0c\u795e\u7ecf\u542f\u53d1\u7684VLM\u53d8\u4f53\u4f7fMIA\u653b\u51fb\u6210\u529f\u7387\u5e73\u5747ROC-AUC\u4e0b\u964d24%\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u4f3c\u7684\u6a21\u578b\u6548\u7528(MPNet\u548cROUGE-2\u6307\u6807)\u3002\u5728\u5176\u4ed6\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u53d1\u73b0\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u795e\u7ecf\u542f\u53d1\u7684\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4fdd\u6301\u6a21\u578b\u6548\u7528\u7684\u540c\u65f6\uff0c\u5bf9\u9690\u79c1\u653b\u51fb\u5177\u6709\u66f4\u5f3a\u7684\u62b5\u5fa1\u80fd\u529b\uff0c\u4e3a\u7406\u89e3\u591a\u6a21\u6001\u6a21\u578b\u7684\u9690\u79c1\u98ce\u9669\u63d0\u4f9b\u4e86\u91cd\u8981\u8bc1\u636e\u3002"}}
{"id": "2511.20897", "categories": ["physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.20897", "abs": "https://arxiv.org/abs/2511.20897", "authors": ["Alan Gadian"], "title": "The early history of Marine Cloud Brightening (MCB); the legacy of John Latham and Stephen Salter", "comment": "11 pages", "summary": "This paper discusses the initial development of Marine Cloud Brightening (MCB) as a theoretical idea, from its inception as a cloud microphysics process in circe 1990 to the full-blown concept by 2015. It primarily focuses on the work of founders John Latham and Stephen Salter and their contributions. Recently the concept has been developed further, e.g. in the UK ARIA project, as a prospective method to ameliorate the Earths rapid warming.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u6d77\u6d0b\u4e91\u589e\u4eae(MCB)\u6982\u5ff5\u4ece1990\u5e74\u4f5c\u4e3a\u4e91\u5fae\u7269\u7406\u8fc7\u7a0b\u7684\u7406\u8bba\u6784\u60f3\uff0c\u52302015\u5e74\u53d1\u5c55\u6210\u719f\u7684\u8fc7\u7a0b\uff0c\u91cd\u70b9\u5173\u6ce8\u521b\u59cb\u4ebaJohn Latham\u548cStephen Salter\u7684\u8d21\u732e\uff0c\u4ee5\u53ca\u8be5\u6280\u672f\u4f5c\u4e3a\u7f13\u89e3\u5168\u7403\u53d8\u6696\u7684\u6f5c\u5728\u65b9\u6cd5\u7684\u6700\u65b0\u53d1\u5c55\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u7f13\u89e3\u5730\u7403\u5feb\u901f\u53d8\u6696\u7684\u5730\u7403\u5de5\u7a0b\u6280\u672f\uff0c\u6d77\u6d0b\u4e91\u589e\u4eae\u88ab\u8ba4\u4e3a\u662f\u901a\u8fc7\u589e\u52a0\u4e91\u5c42\u53cd\u5c04\u7387\u6765\u964d\u4f4e\u5730\u7403\u8868\u9762\u6e29\u5ea6\u7684\u6f5c\u5728\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4e91\u5fae\u7269\u7406\u8fc7\u7a0b\u7684\u7406\u8bba\u7814\u7a76\uff0c\u4ece\u6700\u521d\u7684\u6982\u5ff5\u6784\u60f3\u5230\u5b8c\u6574\u6280\u672f\u65b9\u6848\u7684\u5f00\u53d1\uff0c\u5305\u62ec\u82f1\u56fdARIA\u9879\u76ee\u4e2d\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u6d77\u6d0b\u4e91\u589e\u4eae\u4f5c\u4e3a\u5730\u7403\u5de5\u7a0b\u6280\u672f\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u7f13\u89e3\u5168\u7403\u53d8\u6696\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u6d77\u6d0b\u4e91\u589e\u4eae\u6280\u672f\u4ece\u7406\u8bba\u6982\u5ff5\u53d1\u5c55\u4e3a\u6709\u524d\u666f\u7684\u5730\u7403\u5de5\u7a0b\u65b9\u6cd5\uff0c\u4e3a\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2511.20686", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20686", "abs": "https://arxiv.org/abs/2511.20686", "authors": ["Chae-Gyun Lim", "Seung-Ho Han", "EunYoung Byun", "Jeongyun Han", "Soohyun Cho", "Eojin Joo", "Heehyeon Kim", "Sieun Kim", "Juhoon Lee", "Hyunsoo Lee", "Dongkun Lee", "Jonghwan Hyeon", "Yechan Hwang", "Young-Jun Lee", "Kyeongryul Lee", "Minhyeong An", "Hyunjun Ahn", "Jeongwoo Son", "Junho Park", "Donggyu Yoon", "Taehyung Kim", "Jeemin Kim", "Dasom Choi", "Kwangyoung Lee", "Hyunseung Lim", "Yeohyun Jung", "Jongok Hong", "Sooyohn Nam", "Joonyoung Park", "Sungmin Na", "Yubin Choi", "Jeanne Choi", "Yoojin Hong", "Sueun Jang", "Youngseok Seo", "Somin Park", "Seoungung Jo", "Wonhye Chae", "Yeeun Jo", "Eunyoung Kim", "Joyce Jiyoung Whang", "HwaJung Hong", "Joseph Seering", "Uichin Lee", "Juho Kim", "Sunna Choi", "Seokyeon Ko", "Taeho Kim", "Kyunghoon Kim", "Myungsik Ha", "So Jung Lee", "Jemin Hwang", "JoonHo Kwak", "Ho-Jin Choi"], "title": "AssurAI: Experience with Constructing Korean Socio-cultural Datasets to Discover Potential Risks of Generative AI", "comment": "16 pages, HuggingFace: https://huggingface.co/datasets/TTA01/AssurAI", "summary": "The rapid evolution of generative AI necessitates robust safety evaluations. However, current safety datasets are predominantly English-centric, failing to capture specific risks in non-English, socio-cultural contexts such as Korean, and are often limited to the text modality. To address this gap, we introduce AssurAI, a new quality-controlled Korean multimodal dataset for evaluating the safety of generative AI. First, we define a taxonomy of 35 distinct AI risk factors, adapted from established frameworks by a multidisciplinary expert group to cover both universal harms and relevance to the Korean socio-cultural context. Second, leveraging this taxonomy, we construct and release AssurAI, a large-scale Korean multimodal dataset comprising 11,480 instances across text, image, video, and audio. Third, we apply the rigorous quality control process used to ensure data integrity, featuring a two-phase construction (i.e., expert-led seeding and crowdsourced scaling), triple independent annotation, and an iterative expert red-teaming loop. Our pilot study validates AssurAI's effectiveness in assessing the safety of recent LLMs. We release AssurAI to the public to facilitate the development of safer and more reliable generative AI systems for the Korean community.", "AI": {"tldr": "AssurAI\u662f\u4e00\u4e2a\u9488\u5bf9\u97e9\u8bed\u591a\u6a21\u6001\u751f\u6210AI\u5b89\u5168\u8bc4\u4f30\u7684\u8d28\u91cf\u63a7\u5236\u6570\u636e\u96c6\uff0c\u5305\u542b11,480\u4e2a\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u5b9e\u4f8b\uff0c\u6db5\u76d635\u79cdAI\u98ce\u9669\u56e0\u7d20\uff0c\u4e13\u95e8\u9488\u5bf9\u97e9\u56fd\u793e\u4f1a\u6587\u5316\u80cc\u666f\u8bbe\u8ba1\u3002", "motivation": "\u5f53\u524d\u7684\u5b89\u5168\u6570\u636e\u96c6\u4e3b\u8981\u662f\u82f1\u8bed\u4e2d\u5fc3\uff0c\u65e0\u6cd5\u6355\u6349\u975e\u82f1\u8bed\u793e\u4f1a\u6587\u5316\u80cc\u666f\uff08\u5982\u97e9\u8bed\uff09\u4e2d\u7684\u7279\u5b9a\u98ce\u9669\uff0c\u4e14\u901a\u5e38\u4ec5\u9650\u4e8e\u6587\u672c\u6a21\u6001\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5b9a\u4e4935\u79cdAI\u98ce\u9669\u56e0\u7d20\u7684\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u591a\u5b66\u79d1\u4e13\u5bb6\u7ec4\u9002\u5e94\u73b0\u6709\u6846\u67b6\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u6784\u5efa\uff08\u4e13\u5bb6\u5f15\u5bfc\u64ad\u79cd\u548c\u4f17\u5305\u6269\u5c55\uff09\u3001\u4e09\u91cd\u72ec\u7acb\u6807\u6ce8\u548c\u8fed\u4ee3\u4e13\u5bb6\u7ea2\u961f\u5faa\u73af\u7684\u8d28\u91cf\u63a7\u5236\u6d41\u7a0b\u3002", "result": "\u6784\u5efa\u5e76\u53d1\u5e03\u4e86AssurAI\u6570\u636e\u96c6\uff0c\u5305\u542b11,480\u4e2a\u591a\u6a21\u6001\u5b9e\u4f8b\uff1b\u8bd5\u70b9\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5728\u8bc4\u4f30\u6700\u65b0LLM\u5b89\u5168\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "AssurAI\u6709\u52a9\u4e8e\u4e3a\u97e9\u56fd\u793e\u533a\u5f00\u53d1\u66f4\u5b89\u5168\u53ef\u9760\u7684\u751f\u6210AI\u7cfb\u7edf\uff0c\u8be5\u6570\u636e\u96c6\u5df2\u5411\u516c\u4f17\u53d1\u5e03\u3002"}}
{"id": "2511.20714", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20714", "abs": "https://arxiv.org/abs/2511.20714", "authors": ["Inferix Team", "Tianyu Feng", "Yizeng Han", "Jiahao He", "Yuanyu He", "Xi Lin", "Teng Liu", "Hanfeng Lu", "Jiasheng Tang", "Wei Wang", "Zhiyuan Wang", "Jichao Wu", "Mingyang Yang", "Yinghao Yu", "Zeyu Zhang", "Bohan Zhuang"], "title": "Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation", "comment": null, "summary": "World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.", "AI": {"tldr": "Inferix\u662f\u4e00\u4e2a\u4e13\u4e3a\u4e16\u754c\u6a21\u578b\u8bbe\u8ba1\u7684\u4e0b\u4e00\u4ee3\u63a8\u7406\u5f15\u64ce\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u534a\u81ea\u56de\u5f52\u89e3\u7801\u5b9e\u73b0\u6c89\u6d78\u5f0f\u4e16\u754c\u5408\u6210\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u89c6\u9891\u6d41\u548c\u7cbe\u7ec6\u8bc4\u4f30\u3002", "motivation": "\u4e16\u754c\u6a21\u578b\u662f\u667a\u80fd\u4f53AI\u3001\u5177\u8eabAI\u548c\u6e38\u620f\u9886\u57df\u7684\u6838\u5fc3\u6a21\u62df\u5668\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u957f\u89c6\u9891\u3002\u6269\u5c55\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u89e3\u9501\u89c6\u89c9\u611f\u77e5\u3001\u7406\u89e3\u548c\u63a8\u7406\u7684\u65b0\u5174\u80fd\u529b\uff0c\u8d85\u8d8a\u5f53\u524d\u4ee5LLM\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u3002", "method": "\u91c7\u7528\u534a\u81ea\u56de\u5f52\uff08\u5757\u6269\u6563\uff09\u89e3\u7801\u8303\u5f0f\uff0c\u7ed3\u5408\u6269\u6563\u548c\u81ea\u56de\u5f52\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5728\u6bcf\u4e2a\u5757\u5185\u5e94\u7528\u6269\u6563\u751f\u6210\u89c6\u9891\u4ee4\u724c\uff0c\u540c\u65f6\u4ee5\u524d\u4e00\u4e2a\u5757\u4e3a\u6761\u4ef6\uff0c\u5b9e\u73b0\u66f4\u8fde\u8d2f\u7a33\u5b9a\u7684\u89c6\u9891\u5e8f\u5217\u3002\u901a\u8fc7\u91cd\u65b0\u5f15\u5165LLM\u98ce\u683c\u7684KV\u7f13\u5b58\u7ba1\u7406\uff0c\u514b\u670d\u6807\u51c6\u89c6\u9891\u6269\u6563\u7684\u9650\u5236\u3002", "result": "Inferix\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u4e16\u754c\u6a21\u62df\uff0c\u4e0e\u9ad8\u5e76\u53d1\u7cfb\u7edf\uff08\u5982vLLM\uff09\u548c\u7ecf\u5178\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08\u5982xDiTs\uff09\u4e0d\u540c\uff0c\u652f\u6301\u9ad8\u6548\u3001\u53ef\u53d8\u957f\u5ea6\u548c\u9ad8\u8d28\u91cf\u751f\u6210\u3002", "conclusion": "Inferix\u901a\u8fc7\u4f18\u5316\u7684\u534a\u81ea\u56de\u5f52\u89e3\u7801\u3001\u4ea4\u4e92\u5f0f\u89c6\u9891\u6d41\u548cLV-Bench\u96c6\u6210\uff0c\u4e3a\u4e16\u754c\u6a21\u578b\u63a2\u7d22\u63d0\u4f9b\u4e86\u4e13\u95e8\u7684\u63a8\u7406\u5f15\u64ce\uff0c\u5e0c\u671b\u793e\u533a\u5171\u540c\u63a8\u8fdb\u8fd9\u4e00\u65b9\u5411\u3002"}}
{"id": "2511.20899", "categories": ["physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.20899", "abs": "https://arxiv.org/abs/2511.20899", "authors": ["Zhanxiang Hua", "Christina Karamperidou", "Zilu Meng"], "title": "Extratropical Atmospheric Circulation Response to ENSO in Deep Learning Pacific Pacemaker Experiments", "comment": null, "summary": "Coupled atmosphere-ocean deep learning (DL) climate emulators are a new frontier but are known to exhibit weak ENSO variability, raising questions about their ability to simulate teleconnections. Here, we present the first Pacific pacemaker (PACE) experiments using a coupled DL emulator (DLESyM) to bypass this weak variability and isolate the atmospheric response to observed ENSO forcing. We find that while the emulator realistically captures internal atmospheric variability, it produces a significantly amplified forced teleconnection response to ENSO. This amplified response leads to biases in simulating extremes, notably an overestimation of atmospheric blocking frequency and duration with the underestimation of peak intensity. Our findings underscore that coupled DL climate models require in-depth and physically-grounded validation, analogous to traditional numerical models, to build confidence in their use for physical climate analysis.", "AI": {"tldr": "\u4f7f\u7528\u8026\u5408\u6df1\u5ea6\u5b66\u4e60\u6c14\u5019\u6a21\u62df\u5668\u8fdb\u884c\u592a\u5e73\u6d0b\u5b9a\u901f\u5b9e\u9a8c\uff0c\u53d1\u73b0\u867d\u7136\u80fd\u771f\u5b9e\u6355\u6349\u5927\u6c14\u5185\u90e8\u53d8\u7387\uff0c\u4f46\u5bf9ENSO\u5f3a\u8feb\u7684\u9065\u76f8\u5173\u54cd\u5e94\u663e\u8457\u653e\u5927\uff0c\u5bfc\u81f4\u6781\u7aef\u4e8b\u4ef6\u6a21\u62df\u504f\u5dee\u3002", "motivation": "\u8026\u5408\u5927\u6c14-\u6d77\u6d0b\u6df1\u5ea6\u5b66\u4e60\u6c14\u5019\u6a21\u62df\u5668\u5b58\u5728ENSO\u53d8\u7387\u5f31\u7684\u95ee\u9898\uff0c\u9700\u8981\u9a8c\u8bc1\u5176\u6a21\u62df\u9065\u76f8\u5173\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u8026\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u62df\u5668(DLESyM)\u8fdb\u884c\u592a\u5e73\u6d0b\u5b9a\u901f(PACE)\u5b9e\u9a8c\uff0c\u9694\u79bb\u89c2\u6d4bENSO\u5f3a\u8feb\u4e0b\u7684\u5927\u6c14\u54cd\u5e94\u3002", "result": "\u6a21\u62df\u5668\u80fd\u771f\u5b9e\u6355\u6349\u5927\u6c14\u5185\u90e8\u53d8\u7387\uff0c\u4f46\u5bf9ENSO\u5f3a\u8feb\u7684\u9065\u76f8\u5173\u54cd\u5e94\u663e\u8457\u653e\u5927\uff0c\u5bfc\u81f4\u5927\u6c14\u963b\u585e\u9891\u7387\u548c\u6301\u7eed\u65f6\u95f4\u88ab\u9ad8\u4f30\uff0c\u5cf0\u503c\u5f3a\u5ea6\u88ab\u4f4e\u4f30\u3002", "conclusion": "\u8026\u5408\u6df1\u5ea6\u5b66\u4e60\u6c14\u5019\u6a21\u578b\u9700\u8981\u50cf\u4f20\u7edf\u6570\u503c\u6a21\u578b\u4e00\u6837\u8fdb\u884c\u6df1\u5165\u4e14\u7269\u7406\u57fa\u7840\u624e\u5b9e\u7684\u9a8c\u8bc1\uff0c\u4ee5\u5efa\u7acb\u5176\u5728\u7269\u7406\u6c14\u5019\u5206\u6790\u4e2d\u4f7f\u7528\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2511.20693", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.20693", "abs": "https://arxiv.org/abs/2511.20693", "authors": ["Mingming Zhao", "Xiaokang Wei", "Yuanqi Shao", "Kaiwen Zhou", "Lin Yang", "Siwei Rao", "Junhui Zhan", "Zhitang Chen"], "title": "$A^2Flow:$ Automating Agentic Workflow Generation via Self-Adaptive Abstraction Operators", "comment": "Accepted by AAAI-2026", "summary": "Large language models (LLMs) have shown strong potential in automating the design of agentic workflows. However, existing methods still rely heavily on manually predefined operators, limiting generalization and scalability. To address this issue, we propose $A^2Flow$, a fully automated framework for agentic workflow generation based on self-adaptive abstraction operators. $A^2Flow$ employs a three-stage operator extraction process: 1) Case-based Initial Operator Generation: leveraging expert demonstrations and LLM reasoning to generate case-specific operators; 2) Operator Clustering and Preliminary Abstraction: grouping similar operators across tasks to form preliminary abstractions; and 3) Deep Extraction for Abstract Execution Operators: applying long chain-of-thought prompting and multi-path reasoning to derive compact and generalizable execution operators. These operators serve as reusable building blocks for workflow construction without manual predefinition. Furthermore, we enhance node-level workflow search with an operator memory mechanism, which retains historical outputs to enrich context and improve decision-making. Experiments on general and embodied benchmarks show that $A^2Flow$ achieves a 2.4\\% and 19.3\\% average performance improvement and reduces resource usage by 37\\% over state-of-the-art baselines. Homepage:https://github.com/pandawei-ele/A2FLOW", "AI": {"tldr": "A\u00b2Flow\u662f\u4e00\u4e2a\u57fa\u4e8e\u81ea\u9002\u5e94\u62bd\u8c61\u7b97\u5b50\u7684\u5168\u81ea\u52a8\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u7b97\u5b50\u63d0\u53d6\u8fc7\u7a0b\u81ea\u52a8\u751f\u6210\u53ef\u91cd\u7528\u7684\u6267\u884c\u7b97\u5b50\uff0c\u65e0\u9700\u624b\u52a8\u9884\u5b9a\u4e49\uff0c\u5728\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u624b\u52a8\u9884\u5b9a\u4e49\u7b97\u5b50\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u6cdb\u5316\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9700\u8981\u5f00\u53d1\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u7b97\u5b50\u63d0\u53d6\uff1a1)\u57fa\u4e8e\u6848\u4f8b\u7684\u521d\u59cb\u7b97\u5b50\u751f\u6210\uff1b2)\u7b97\u5b50\u805a\u7c7b\u548c\u521d\u6b65\u62bd\u8c61\uff1b3)\u6df1\u5ea6\u63d0\u53d6\u62bd\u8c61\u6267\u884c\u7b97\u5b50\u3002\u540c\u65f6\u5f15\u5165\u7b97\u5b50\u8bb0\u5fc6\u673a\u5236\u589e\u5f3a\u5de5\u4f5c\u6d41\u641c\u7d22\u3002", "result": "\u5728\u901a\u7528\u548c\u5177\u8eab\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cA\u00b2Flow\u5e73\u5747\u6027\u80fd\u63d0\u53472.4%\u548c19.3%\uff0c\u8d44\u6e90\u4f7f\u7528\u51cf\u5c1137%\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "A\u00b2Flow\u901a\u8fc7\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u7b97\u5b50\u63d0\u53d6\u548c\u5de5\u4f5c\u6d41\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u5316\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20716", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.20716", "abs": "https://arxiv.org/abs/2511.20716", "authors": ["Kun Guo", "Yun Shen", "Xijun Wang", "Chaoqun You", "Yun Rui", "Tony Q. S. Quek"], "title": "Video Object Recognition in Mobile Edge Networks: Local Tracking or Edge Detection?", "comment": null, "summary": "Fast and accurate video object recognition, which relies on frame-by-frame video analytics, remains a challenge for resource-constrained devices such as traffic cameras. Recent advances in mobile edge computing have made it possible to offload computation-intensive object detection to edge servers equipped with high-accuracy neural networks, while lightweight and fast object tracking algorithms run locally on devices. This hybrid approach offers a promising solution but introduces a new challenge: deciding when to perform edge detection versus local tracking. To address this, we formulate two long-term optimization problems for both single-device and multi-device scenarios, taking into account the temporal correlation of consecutive frames and the dynamic conditions of mobile edge networks. Based on the formulation, we propose the LTED-Ada in single-device setting, a deep reinforcement learning-based algorithm that adaptively selects between local tracking and edge detection, according to the frame rate as well as recognition accuracy and delay requirement. In multi-device setting, we further enhance LTED-Ada using federated learning to enable collaborative policy training across devices, thereby improving its generalization to unseen frame rates and performance requirements. Finally, we conduct extensive hardware-in-the-loop experiments using multiple Raspberry Pi 4B devices and a personal computer as the edge server, demonstrating the superiority of LTED-Ada.", "AI": {"tldr": "\u63d0\u51faLTED-Ada\u7b97\u6cd5\uff0c\u5728\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u81ea\u9002\u5e94\u9009\u62e9\u672c\u5730\u8ddf\u8e2a\u6216\u8fb9\u7f18\u68c0\u6d4b\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u89c6\u9891\u5bf9\u8c61\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u8981\u6c42\u3002", "motivation": "\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\uff08\u5982\u4ea4\u901a\u6444\u50cf\u5934\uff09\u8fdb\u884c\u89c6\u9891\u5bf9\u8c61\u8bc6\u522b\u65f6\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u5e73\u8861\u8fb9\u7f18\u68c0\u6d4b\u7684\u9ad8\u7cbe\u5ea6\u548c\u672c\u5730\u8ddf\u8e2a\u7684\u4f4e\u5ef6\u8fdf\u3002", "method": "\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u5355\u8bbe\u5907\u573a\u666f\u63d0\u51faLTED-Ada\u7b97\u6cd5\uff0c\u5728\u591a\u8bbe\u5907\u573a\u666f\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u8fdb\u884c\u534f\u4f5c\u7b56\u7565\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u786c\u4ef6\u5728\u73af\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LTED-Ada\u7684\u4f18\u8d8a\u6027\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684\u5e27\u7387\u548c\u6027\u80fd\u8981\u6c42\u3002", "conclusion": "LTED-Ada\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u52a8\u8fb9\u7f18\u7f51\u7edc\u4e2d\u89c6\u9891\u5bf9\u8c61\u8bc6\u522b\u7684\u81ea\u9002\u5e94\u51b3\u7b56\u95ee\u9898\uff0c\u5728\u5355\u8bbe\u5907\u548c\u591a\u8bbe\u5907\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.20963", "categories": ["physics.ao-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20963", "abs": "https://arxiv.org/abs/2511.20963", "authors": ["Jerry Lin", "Zeyuan Hu", "Tom Beucler", "Katherine Frields", "Hannah Christensen", "Walter Hannah", "Helge Heuer", "Peter Ukkonnen", "Laura A. Mansfield", "Tian Zheng", "Liran Peng", "Ritwik Gupta", "Pierre Gentine", "Yusef Al-Naher", "Mingjiang Duan", "Kyo Hattori", "Weiliang Ji", "Chunhan Li", "Kippei Matsuda", "Naoki Murakami", "Shlomo Ron", "Marec Serlin", "Hongjian Song", "Yuma Tanabe", "Daisuke Yamamoto", "Jianyao Zhou", "Mike Pritchard"], "title": "Crowdsourcing the Frontier: Advancing Hybrid Physics-ML Climate Simulation via $50,000 Kaggle Competition", "comment": "Main text: 29 pages, 10 figures. SI: 47 pages, 37 figures", "summary": "Subgrid machine-learning (ML) parameterizations have the potential to introduce a new generation of climate models that incorporate the effects of higher-resolution physics without incurring the prohibitive computational cost associated with more explicit physics-based simulations. However, important issues, ranging from online instability to inconsistent online performance, have limited their operational use for long-term climate projections. To more rapidly drive progress in solving these issues, domain scientists and machine learning researchers opened up the offline aspect of this problem to the broader machine learning and data science community with the release of ClimSim, a NeurIPS Datasets and Benchmarks publication, and an associated Kaggle competition. This paper reports on the downstream results of the Kaggle competition by coupling emulators inspired by the winning teams' architectures to an interactive climate model (including full cloud microphysics, a regime historically prone to online instability) and systematically evaluating their online performance. Our results demonstrate that online stability in the low-resolution, real-geography setting is reproducible across multiple diverse architectures, which we consider a key milestone. All tested architectures exhibit strikingly similar offline and online biases, though their responses to architecture-agnostic design choices (e.g., expanding the list of input variables) can differ significantly. Multiple Kaggle-inspired architectures achieve state-of-the-art (SOTA) results on certain metrics such as zonal mean bias patterns and global RMSE, indicating that crowdsourcing the essence of the offline problem is one path to improving online performance in hybrid physics-AI climate simulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5c06Kaggle\u7ade\u8d5b\u83b7\u80dc\u56e2\u961f\u7684\u673a\u5668\u5b66\u4e60\u67b6\u6784\u96c6\u6210\u5230\u6c14\u5019\u6a21\u578b\u4e2d\uff0c\u8bc1\u660e\u4e86\u5728\u4f4e\u5206\u8fa8\u7387\u771f\u5b9e\u5730\u7406\u73af\u5883\u4e0b\u5728\u7ebf\u7a33\u5b9a\u6027\u7684\u53ef\u91cd\u73b0\u6027\uff0c\u591a\u4e2a\u67b6\u6784\u5728\u7279\u5b9a\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3\u4e9a\u7f51\u683c\u673a\u5668\u5b66\u4e60\u53c2\u6570\u5316\u5728\u957f\u671f\u6c14\u5019\u9884\u6d4b\u4e2d\u5b58\u5728\u7684\u5728\u7ebf\u4e0d\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u63a8\u52a8\u6df7\u5408\u7269\u7406-AI\u6c14\u5019\u6a21\u62df\u7684\u53d1\u5c55\u3002", "method": "\u5c06Kaggle\u7ade\u8d5b\u83b7\u80dc\u56e2\u961f\u7684\u673a\u5668\u5b66\u4e60\u67b6\u6784\u8026\u5408\u5230\u5305\u542b\u5b8c\u6574\u4e91\u5fae\u7269\u7406\u7684\u4ea4\u4e92\u5f0f\u6c14\u5019\u6a21\u578b\u4e2d\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5176\u5728\u7ebf\u6027\u80fd\u3002", "result": "\u6240\u6709\u6d4b\u8bd5\u67b6\u6784\u90fd\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u79bb\u7ebf\u548c\u5728\u7ebf\u504f\u5dee\uff0c\u4f46\u4e0d\u540c\u67b6\u6784\u5bf9\u8bbe\u8ba1\u9009\u62e9\u7684\u54cd\u5e94\u5dee\u5f02\u663e\u8457\u3002\u591a\u4e2aKaggle\u542f\u53d1\u67b6\u6784\u5728\u7279\u5b9a\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u4f17\u5305\u79bb\u7ebf\u95ee\u9898\u7684\u672c\u8d28\u662f\u6539\u5584\u6df7\u5408\u7269\u7406-AI\u6c14\u5019\u6a21\u62df\u5728\u7ebf\u6027\u80fd\u7684\u4e00\u6761\u53ef\u884c\u8def\u5f84\uff0c\u5728\u7ebf\u7a33\u5b9a\u6027\u5728\u4f4e\u5206\u8fa8\u7387\u771f\u5b9e\u5730\u7406\u73af\u5883\u4e0b\u662f\u53ef\u91cd\u73b0\u7684\u5173\u952e\u91cc\u7a0b\u7891\u3002"}}
{"id": "2511.20694", "categories": ["cs.AI", "astro-ph.SR", "cs.LG", "physics.space-ph"], "pdf": "https://arxiv.org/pdf/2511.20694", "abs": "https://arxiv.org/abs/2511.20694", "authors": ["Kevin Lee", "Russell Spiewak", "James Walsh"], "title": "Reasoning With a Star: A Heliophysics Dataset and Benchmark for Agentic Scientific Reasoning", "comment": "Accepted at NeurIPS 2025 Machine Learning and the Physical Sciences (ML4PS) Workshop. Dataset: https://huggingface.co/datasets/SpaceML/ReasoningWithAStar", "summary": "Scientific reasoning through Large Language Models in heliophysics involves more than just recalling facts: it requires incorporating physical assumptions, maintaining consistent units, and providing clear scientific formats through coordinated approaches. To address these challenges, we present Reasoning With a Star, a newly contributed heliophysics dataset applicable to reasoning; we also provide an initial benchmarking approach. Our data are constructed from National Aeronautics and Space Administration & University Corporation for Atmospheric Research Living With a Star summer school problem sets and compiled into a readily consumable question-and-answer structure with question contexts, reasoning steps, expected answer type, ground-truth targets, format hints, and metadata. A programmatic grader checks the predictions using unit-aware numerical tolerance, symbolic equivalence, and schema validation. We benchmark a single-shot baseline and four multi-agent patterns, finding that decomposing workflows through systems engineering principles outperforms direct prompting on problems requiring deductive reasoning rather than pure inductive recall.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\"Reasoning With a Star\"\u7684\u65e5\u7403\u7269\u7406\u5b66\u63a8\u7406\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6a21\u5f0f\u5728\u9700\u8981\u6f14\u7ece\u63a8\u7406\u7684\u95ee\u9898\u4e0a\u4f18\u4e8e\u76f4\u63a5\u63d0\u793a\u3002", "motivation": "\u89e3\u51b3\u65e5\u7403\u7269\u7406\u5b66\u4e2d\u79d1\u5b66\u63a8\u7406\u7684\u6311\u6218\uff0c\u5305\u62ec\u6574\u5408\u7269\u7406\u5047\u8bbe\u3001\u4fdd\u6301\u5355\u4f4d\u4e00\u81f4\u6027\u3001\u63d0\u4f9b\u6e05\u6670\u79d1\u5b66\u683c\u5f0f\u7b49\u9700\u6c42\u3002", "method": "\u6784\u5efa\u6765\u81eaNASA\u548cUCAR Living With a Star\u6691\u671f\u5b66\u6821\u95ee\u9898\u96c6\u7684\u6570\u636e\u96c6\uff0c\u91c7\u7528\u7a0b\u5e8f\u5316\u8bc4\u5206\u5668\u68c0\u67e5\u9884\u6d4b\u7ed3\u679c\uff0c\u5e76\u6d4b\u8bd5\u5355\u6b21\u63d0\u793a\u57fa\u7ebf\u548c\u56db\u79cd\u591a\u667a\u80fd\u4f53\u6a21\u5f0f\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u5de5\u7a0b\u539f\u5219\u5206\u89e3\u5de5\u4f5c\u6d41\u7a0b\u7684\u591a\u667a\u80fd\u4f53\u6a21\u5f0f\u5728\u9700\u8981\u6f14\u7ece\u63a8\u7406\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u4e8e\u76f4\u63a5\u63d0\u793a\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u63a8\u7406\u65b9\u6cd5\u5728\u590d\u6742\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u6f14\u7ece\u63a8\u7406\u800c\u975e\u7eaf\u5f52\u7eb3\u56de\u5fc6\u7684\u95ee\u9898\u4e0a\u3002"}}
{"id": "2511.20720", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20720", "abs": "https://arxiv.org/abs/2511.20720", "authors": ["Haibo HU", "Lianming Huang", "Nan Guan", "Chun Jason Xue"], "title": "DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving", "comment": null, "summary": "Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.", "AI": {"tldr": "DeeAD\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u65e9\u671f\u9000\u51fa\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30\u4e2d\u95f4\u8f68\u8ff9\u7684\u7269\u7406\u53ef\u884c\u6027\u6765\u52a0\u901f\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u7684\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u89c4\u5212\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe28%\u7684\u5c42\u7a00\u758f\u5ea6\u548c29%\u7684\u5ef6\u8fdf\u964d\u4f4e\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7edf\u4e00\u4e86\u611f\u77e5\u3001\u63a8\u7406\u548c\u8f68\u8ff9\u751f\u6210\uff0c\u4f46\u7531\u4e8e\u6df1\u5ea6transformer\u5806\u6808\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u663e\u8457\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u52a8\u4f5c\u5f15\u5bfc\u7684\u65e9\u671f\u9000\u51fa\u6846\u67b6\uff0c\u5f53\u9884\u6d4b\u8f68\u8ff9\u4e0e\u8f7b\u91cf\u7ea7\u89c4\u5212\u5148\u9a8c\uff08\u5982\u5bfc\u822a\u6216\u4f4e\u7cbe\u5ea6\u89c4\u5212\uff09\u5728\u53ef\u5bb9\u5fcd\u504f\u5dee\u5185\u5bf9\u9f50\u65f6\u7ec8\u6b62\u63a8\u7406\uff0c\u5e76\u5f15\u5165\u591a\u8df3\u63a7\u5236\u5668\u81ea\u9002\u5e94\u8df3\u8fc7\u5197\u4f59\u5c42\u3002", "result": "\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8628%\u7684transformer\u5c42\u7a00\u758f\u5ea6\u548c29%\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u89c4\u5212\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002", "conclusion": "DeeAD\u6846\u67b6\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u96c6\u6210\u5230\u73b0\u6709VLA\u6a21\u578b\u4e2d\uff0c\u6709\u6548\u52a0\u901f\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21444", "categories": ["cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.21444", "abs": "https://arxiv.org/abs/2511.21444", "authors": ["Zhe Jiang", "Jiong Wang", "Xiaoyu Yue", "Zijie Guo", "Wenlong Zhang", "Fenghua Ling", "Wanli Ouyang", "Lei Bai"], "title": "EWE: An Agentic Framework for Extreme Weather Analysis", "comment": null, "summary": "Extreme weather events pose escalating risks to global society, underscoring the urgent need to unravel their underlying physical mechanisms. Yet the prevailing expert-driven, labor-intensive diagnostic paradigm has created a critical analytical bottleneck, stalling scientific progress. While AI for Earth Science has achieved notable advances in prediction, the equally essential challenge of automated diagnostic reasoning remains largely unexplored. We present the Extreme Weather Expert (EWE), the first intelligent agent framework dedicated to this task. EWE emulates expert workflows through knowledge-guided planning, closed-loop reasoning, and a domain-tailored meteorological toolkit. It autonomously produces and interprets multimodal visualizations from raw meteorological data, enabling comprehensive diagnostic analyses. To catalyze progress, we introduce the first benchmark for this emerging field, comprising a curated dataset of 103 high-impact events and a novel step-wise evaluation metric. EWE marks a step toward automated scientific discovery and offers the potential to democratize expertise and intellectual resources, particularly for developing nations vulnerable to extreme weather.", "AI": {"tldr": "EWE\u662f\u9996\u4e2a\u7528\u4e8e\u6781\u7aef\u5929\u6c14\u81ea\u52a8\u8bca\u65ad\u63a8\u7406\u7684\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4e13\u5bb6\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ece\u539f\u59cb\u6c14\u8c61\u6570\u636e\u81ea\u4e3b\u751f\u6210\u548c\u89e3\u91ca\u591a\u6a21\u6001\u53ef\u89c6\u5316\uff0c\u5b9e\u73b0\u5168\u9762\u8bca\u65ad\u5206\u6790\u3002", "motivation": "\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u5bf9\u5168\u7403\u793e\u4f1a\u6784\u6210\u65e5\u76ca\u4e25\u91cd\u7684\u98ce\u9669\uff0c\u4f46\u4f20\u7edf\u4e13\u5bb6\u9a71\u52a8\u7684\u4eba\u5de5\u8bca\u65ad\u8303\u5f0f\u5b58\u5728\u5206\u6790\u74f6\u9888\uff0c\u963b\u788d\u79d1\u5b66\u8fdb\u5c55\u3002\u867d\u7136AI\u5728\u5730\u7403\u79d1\u5b66\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u81ea\u52a8\u8bca\u65ad\u63a8\u7406\u8fd9\u4e00\u540c\u7b49\u91cd\u8981\u7684\u6311\u6218\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "EWE\u901a\u8fc7\u77e5\u8bc6\u5f15\u5bfc\u89c4\u5212\u3001\u95ed\u73af\u63a8\u7406\u548c\u9886\u57df\u5b9a\u5236\u7684\u6c14\u8c61\u5de5\u5177\u5305\u6a21\u62df\u4e13\u5bb6\u5de5\u4f5c\u6d41\u7a0b\uff0c\u80fd\u591f\u4ece\u539f\u59cb\u6c14\u8c61\u6570\u636e\u81ea\u4e3b\u751f\u6210\u548c\u89e3\u91ca\u591a\u6a21\u6001\u53ef\u89c6\u5316\u3002", "result": "\u5f00\u53d1\u4e86\u8be5\u9886\u57df\u9996\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b103\u4e2a\u9ad8\u5f71\u54cd\u4e8b\u4ef6\u7684\u7cbe\u9009\u6570\u636e\u96c6\u548c\u65b0\u7684\u9010\u6b65\u8bc4\u4f30\u6307\u6807\u3002EWE\u5c55\u793a\u4e86\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u7684\u6f5c\u529b\u3002", "conclusion": "EWE\u6807\u5fd7\u7740\u5411\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u6709\u671b\u6c11\u4e3b\u5316\u4e13\u4e1a\u77e5\u8bc6\u548c\u667a\u529b\u8d44\u6e90\uff0c\u7279\u522b\u662f\u5bf9\u6613\u53d7\u6781\u7aef\u5929\u6c14\u5f71\u54cd\u7684\u53d1\u5c55\u4e2d\u56fd\u5bb6\u3002"}}
{"id": "2511.20695", "categories": ["cs.AI", "cs.CY", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2511.20695", "abs": "https://arxiv.org/abs/2511.20695", "authors": ["Yunqi Zhang", "Kuangyu Shi", "Biao Li"], "title": "A Brief History of Digital Twin Technology", "comment": "21 pages, 1 figure, 1 table", "summary": "Emerging from NASA's spacecraft simulations in the 1960s, digital twin technology has advanced through industrial adoption to spark a healthcare transformation. A digital twin is a dynamic, data-driven virtual counterpart of a physical system, continuously updated through real-time data streams and capable of bidirectional interaction. In medicine, digital twin integrates imaging, biosensors, and computational models to generate patient-specific simulations that support diagnosis, treatment planning, and drug development. Representative applications include cardiac digital twin for predicting arrhythmia treatment outcomes, oncology digital twin for tracking tumor progression and optimizing radiotherapy, and pharmacological digital twin for accelerating drug discovery. Despite rapid progress, major challenges, including interoperability, data privacy, and model fidelity, continue to limit widespread clinical integration. Emerging solutions such as explainable AI, federated learning, and harmonized regulatory frameworks offer promising pathways forward. Looking ahead, advances in multi-organ digital twin, genomics integration, and ethical governance will be essential to ensure that digital twin shifts healthcare from reactive treatment to predictive, preventive, and truly personalized medicine.", "AI": {"tldr": "\u6570\u5b57\u5b6a\u751f\u6280\u672f\u4eceNASA\u822a\u5929\u5668\u6a21\u62df\u53d1\u5c55\u800c\u6765\uff0c\u73b0\u6b63\u63a8\u52a8\u533b\u7597\u5065\u5eb7\u8f6c\u578b\uff0c\u901a\u8fc7\u521b\u5efa\u60a3\u8005\u7279\u5f02\u6027\u865a\u62df\u6a21\u578b\u652f\u6301\u8bca\u65ad\u3001\u6cbb\u7597\u89c4\u5212\u548c\u836f\u7269\u5f00\u53d1\uff0c\u4f46\u4ecd\u9762\u4e34\u4e92\u64cd\u4f5c\u6027\u3001\u6570\u636e\u9690\u79c1\u7b49\u6311\u6218\u3002", "motivation": "\u5c06\u6570\u5b57\u5b6a\u751f\u6280\u672f\u4ece\u5de5\u4e1a\u9886\u57df\u5f15\u5165\u533b\u7597\u5065\u5eb7\uff0c\u65e8\u5728\u5b9e\u73b0\u4ece\u88ab\u52a8\u6cbb\u7597\u5411\u9884\u6d4b\u6027\u3001\u9884\u9632\u6027\u548c\u4e2a\u6027\u5316\u533b\u7597\u7684\u8f6c\u53d8\uff0c\u63d0\u9ad8\u533b\u7597\u51b3\u7b56\u7684\u7cbe\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u6574\u5408\u533b\u5b66\u5f71\u50cf\u3001\u751f\u7269\u4f20\u611f\u5668\u548c\u8ba1\u7b97\u6a21\u578b\uff0c\u6784\u5efa\u52a8\u6001\u6570\u636e\u9a71\u52a8\u7684\u60a3\u8005\u865a\u62df\u526f\u672c\uff0c\u901a\u8fc7\u5b9e\u65f6\u6570\u636e\u6d41\u6301\u7eed\u66f4\u65b0\uff0c\u652f\u6301\u53cc\u5411\u4ea4\u4e92\u548c\u6a21\u62df\u5206\u6790\u3002", "result": "\u5df2\u5f00\u53d1\u51fa\u5fc3\u810f\u6570\u5b57\u5b6a\u751f\u9884\u6d4b\u5fc3\u5f8b\u5931\u5e38\u6cbb\u7597\u6548\u679c\u3001\u80bf\u7624\u6570\u5b57\u5b6a\u751f\u8ffd\u8e2a\u80bf\u7624\u8fdb\u5c55\u4f18\u5316\u653e\u7597\u3001\u836f\u7406\u5b66\u6570\u5b57\u5b6a\u751f\u52a0\u901f\u836f\u7269\u53d1\u73b0\u7b49\u4ee3\u8868\u6027\u5e94\u7528\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u6280\u672f\u6709\u671b\u5f7b\u5e95\u6539\u53d8\u533b\u7597\u6a21\u5f0f\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u4e92\u64cd\u4f5c\u6027\u3001\u6570\u636e\u9690\u79c1\u7b49\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u7b49\u6280\u672f\u8fdb\u4e00\u6b65\u53d1\u5c55\u591a\u5668\u5b98\u6570\u5b57\u5b6a\u751f\u548c\u57fa\u56e0\u7ec4\u6574\u5408\u3002"}}
{"id": "2511.20700", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.20700", "abs": "https://arxiv.org/abs/2511.20700", "authors": ["Arnaldo de Carvalho Junior", "Diego Oliveira da Cruz", "Bruno da Silva Alves", "Fernando da Silva Paulo Junior", "Jo\u00e3o Inacio da Silva Filho"], "title": "Paraconsistent-Lib: an intuitive PAL2v algorithm Python Library", "comment": "11 pages, 9 figures, 2 appendix", "summary": "This paper introduces Paraconsistent-Lib, an open-source, easy-to-use Python library for building PAL2v algorithms in reasoning and decision-making systems. Paraconsistent-Lib is designed as a general-purpose library of PAL2v standard calculations, presenting three types of results: paraconsistent analysis in one of the 12 classical lattice PAL2v regions, paraconsistent analysis node (PAN) outputs, and a decision output. With Paraconsistent-Lib, well-known PAL2v algorithms such as Para-analyzer, ParaExtrCTX, PAL2v Filter, paraconsistent analysis network (PANnet), and paraconsistent neural network (PNN) can be written in stand-alone or network form, reducing complexity, code size, and bugs, as two examples presented in this paper. Given its stable state, Paraconsistent-Lib is an active development to respond to user-required features and enhancements received on GitHub.", "AI": {"tldr": "Paraconsistent-Lib\u662f\u4e00\u4e2a\u5f00\u6e90\u7684Python\u5e93\uff0c\u7528\u4e8e\u6784\u5efaPAL2v\u7b97\u6cd5\uff0c\u652f\u630112\u79cd\u7ecf\u5178\u683c\u533a\u57df\u5206\u6790\u3001\u5206\u6790\u8282\u70b9\u8f93\u51fa\u548c\u51b3\u7b56\u8f93\u51fa\uff0c\u53ef\u7b80\u5316\u590d\u6742\u7b97\u6cd5\u7684\u5b9e\u73b0\u3002", "motivation": "\u4e3a\u63a8\u7406\u548c\u51b3\u7b56\u7cfb\u7edf\u63d0\u4f9b\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u7684PAL2v\u7b97\u6cd5\u6784\u5efa\u5de5\u5177\uff0c\u51cf\u5c11\u4ee3\u7801\u590d\u6742\u6027\u548c\u9519\u8bef\u3002", "method": "\u5f00\u53d1\u901a\u7528PAL2v\u6807\u51c6\u8ba1\u7b97\u5e93\uff0c\u652f\u6301\u72ec\u7acb\u6216\u7f51\u7edc\u5f62\u5f0f\u7684\u7b97\u6cd5\u5b9e\u73b0\uff0c\u5305\u62ecPara-analyzer\u3001ParaExtrCTX\u7b49\u7ecf\u5178\u7b97\u6cd5\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u7a33\u5b9a\u72b6\u6001\u7684Paraconsistent-Lib\u5e93\uff0c\u80fd\u591f\u6709\u6548\u7b80\u5316PAL2v\u7b97\u6cd5\u7684\u5f00\u53d1\u8fc7\u7a0b\u3002", "conclusion": "Paraconsistent-Lib\u662f\u4e00\u4e2a\u6d3b\u8dc3\u5f00\u53d1\u7684\u9879\u76ee\uff0c\u80fd\u591f\u54cd\u5e94\u7528\u6237\u9700\u6c42\uff0c\u4e3aPAL2v\u7b97\u6cd5\u63d0\u4f9b\u9ad8\u6548\u5b9e\u73b0\u5e73\u53f0\u3002"}}
{"id": "2511.20722", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20722", "abs": "https://arxiv.org/abs/2511.20722", "authors": ["Minh Thong Doi", "Jan Butora", "Vincent Itier", "J\u00e9r\u00e9mie Boulanger", "Patrick Bas"], "title": "DinoLizer: Learning from the Best for Generative Inpainting Localization", "comment": null, "summary": "We introduce DinoLizer, a DINOv2-based model for localizing manipulated regions in generative inpainting. Our method builds on a DINOv2 model pretrained to detect synthetic images on the B-Free dataset. We add a linear classification head on top of the Vision Transformer's patch embeddings to predict manipulations at a $14\\times 14$ patch resolution. The head is trained to focus on semantically altered regions, treating non-semantic edits as part of the original content. Because the ViT accepts only fixed-size inputs, we use a sliding-window strategy to aggregate predictions over larger images; the resulting heatmaps are post-processed to refine the estimated binary manipulation masks. Empirical results show that DinoLizer surpasses state-of-the-art local manipulation detectors on a range of inpainting datasets derived from different generative models. It remains robust to common post-processing operations such as resizing, noise addition, and JPEG (double) compression. On average, DinoLizer achieves a 12\\% higher Intersection-over-Union (IoU) than the next best model, with even greater gains after post-processing. Our experiments with off-the-shelf DINOv2 demonstrate the strong representational power of Vision Transformers for this task. Finally, extensive ablation studies comparing DINOv2 and its successor, DINOv3, in deepfake localization confirm DinoLizer's superiority. The code will be publicly available upon acceptance of the paper.", "AI": {"tldr": "DinoLizer\u662f\u57fa\u4e8eDINOv2\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u5b9a\u4f4d\u751f\u6210\u5f0f\u4fee\u590d\u56fe\u50cf\u4e2d\u7684\u7be1\u6539\u533a\u57df\u3002\u8be5\u65b9\u6cd5\u5728DINOv2\u6a21\u578b\u4e0a\u6dfb\u52a0\u7ebf\u6027\u5206\u7c7b\u5934\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u7b56\u7565\u5904\u7406\u5927\u56fe\u50cf\uff0c\u5728\u591a\u4e2a\u4fee\u590d\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u51c6\u786e\u5b9a\u4f4d\u751f\u6210\u5f0f\u4fee\u590d\u56fe\u50cf\u4e2d\u7be1\u6539\u533a\u57df\u7684\u68c0\u6d4b\u5668\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u7be1\u6539\u68c0\u6d4b\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u5728\u9884\u8bad\u7ec3\u7684DINOv2\u6a21\u578b\u4e0a\u6dfb\u52a0\u7ebf\u6027\u5206\u7c7b\u5934\uff0c\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u7b56\u7565\u5904\u7406\u5927\u5c3a\u5bf8\u56fe\u50cf\uff0c\u901a\u8fc7\u540e\u5904\u7406\u4f18\u5316\u4e8c\u503c\u7be1\u6539\u63a9\u7801\u3002", "result": "\u5728\u591a\u4e2a\u4fee\u590d\u6570\u636e\u96c6\u4e0a\uff0cDinoLizer\u6bd4\u6b21\u4f18\u6a21\u578b\u5e73\u5747IoU\u63d0\u9ad812%\uff0c\u5bf9\u5e38\u89c1\u540e\u5904\u7406\u64cd\u4f5c\uff08\u5982\u8c03\u6574\u5927\u5c0f\u3001\u566a\u58f0\u6dfb\u52a0\u3001JPEG\u538b\u7f29\uff09\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "DinoLizer\u8bc1\u660e\u4e86Vision Transformers\u5728\u7be1\u6539\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u8868\u793a\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4e0eDINOv3\u7684\u5bf9\u6bd4\u7814\u7a76\u786e\u8ba4\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2511.20701", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20701", "abs": "https://arxiv.org/abs/2511.20701", "authors": ["Nitya Tiwari", "Parv Maheshwari", "Vidisha Agarwal"], "title": "Cross Domain Evaluation of Multimodal Chain-of-Thought Reasoning of different datasets into the Amazon CoT Framework", "comment": null, "summary": "While recent work has extended CoT to multimodal settings, achieving state-of-the-art results on science question answering benchmarks like ScienceQA, the generalizability of these approaches across diverse domains remains underexplored. This work presents a comprehensive analysis of Multimodal Chain-of-Thought (Multimodal-CoT) reasoning, evaluating its effectiveness on the A-OKVQA, OKVQA and ChartQA datasets, which requires broad commonsense and world knowledge beyond scientific reasoning. We implement the two-stage framework proposed by Zhang et al. [3], which separates rationale generation from answer inference and integrates vision features through a gated fusion mechanism with T5-based language models. Through systematic ablation studies, we analyze the contributions of vision features, rationale quality, and architectural choices. Our findings reveal that while vision integration significantly reduces hallucination in rationale generation, the effectiveness of CoT reasoning varies substantially across question types, with commonsense reasoning presenting particular challenges. This work provides practical insights for researchers implementing multimodal reasoning systems and identifies key areas for future improvement in cross-domain generalization.", "AI": {"tldr": "\u672c\u6587\u5bf9\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\u8fdb\u884c\u4e86\u7efc\u5408\u5206\u6790\uff0c\u53d1\u73b0\u5728\u79d1\u5b66\u95ee\u7b54\u4e4b\u5916\u7684\u9886\u57df\uff0c\u5176\u6709\u6548\u6027\u56e0\u95ee\u9898\u7c7b\u578b\u800c\u5f02\uff0c\u5e38\u8bc6\u63a8\u7406\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\u3002", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u65b9\u6cd5\u5728\u4e0d\u540c\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5e7f\u6cdb\u5e38\u8bc6\u548c\u4e16\u754c\u77e5\u8bc6\u7684\u4efb\u52a1\u4e2d\u3002", "method": "\u91c7\u7528Zhang\u7b49\u4eba\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5c06\u7406\u7531\u751f\u6210\u4e0e\u7b54\u6848\u63a8\u7406\u5206\u79bb\uff0c\u901a\u8fc7\u95e8\u63a7\u878d\u5408\u673a\u5236\u6574\u5408\u89c6\u89c9\u7279\u5f81\u4e0e\u57fa\u4e8eT5\u7684\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u89c6\u89c9\u7279\u5f81\u6574\u5408\u663e\u8457\u51cf\u5c11\u4e86\u7406\u7531\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\uff0c\u4f46\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u6709\u6548\u6027\u5728\u4e0d\u540c\u95ee\u9898\u7c7b\u578b\u95f4\u5dee\u5f02\u5f88\u5927\uff0c\u5e38\u8bc6\u63a8\u7406\u8868\u73b0\u6700\u5dee\u3002", "conclusion": "\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u7cfb\u7edf\u7684\u5b9e\u73b0\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5e76\u786e\u5b9a\u4e86\u8de8\u9886\u57df\u6cdb\u5316\u7684\u5173\u952e\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2511.20737", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20737", "abs": "https://arxiv.org/abs/2511.20737", "authors": ["Daeheon Jeong", "Seoyeon Byun", "Kihoon Son", "Dae Hyun Kim", "Juho Kim"], "title": "CANVAS: A Benchmark for Vision-Language Models on Tool-Based User Interface Design", "comment": null, "summary": "User interface (UI) design is an iterative process in which designers progressively refine their work with design software such as Figma or Sketch. Recent advances in vision language models (VLMs) with tool invocation suggest these models can operate design software to edit a UI design through iteration. Understanding and enhancing this capacity is important, as it highlights VLMs' potential to collaborate with designers within conventional software. However, as no existing benchmark evaluates tool-based design performance, the capacity remains unknown. To address this, we introduce CANVAS, a benchmark for VLMs on tool-based user interface design. Our benchmark contains 598 tool-based design tasks paired with ground-truth references sampled from 3.3K mobile UI designs across 30 function-based categories (e.g., onboarding, messaging). In each task, a VLM updates the design step-by-step through context-based tool invocations (e.g., create a rectangle as a button background), linked to design software. Specifically, CANVAS incorporates two task types: (i) design replication evaluates the ability to reproduce a whole UI screen; (ii) design modification evaluates the ability to modify a specific part of an existing screen. Results suggest that leading models exhibit more strategic tool invocations, improving design quality. Furthermore, we identify common error patterns models exhibit, guiding future work in enhancing tool-based design capabilities.", "AI": {"tldr": "CANVAS\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u5177\u8c03\u7528\u57fa\u7840\u4e0a\u8fdb\u884c\u7528\u6237\u754c\u9762\u8bbe\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b598\u4e2a\u8bbe\u8ba1\u4efb\u52a1\uff0c\u6db5\u76d6\u8bbe\u8ba1\u590d\u5236\u548c\u8bbe\u8ba1\u4fee\u6539\u4e24\u79cd\u4efb\u52a1\u7c7b\u578b\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u8bc4\u4f30\u57fa\u4e8e\u5de5\u5177\u7684\u8bbe\u8ba1\u6027\u80fd\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u800c\u7406\u89e3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bbe\u8ba1\u8f6f\u4ef6\u4e2d\u8fed\u4ee3\u7f16\u8f91UI\u8bbe\u8ba1\u7684\u80fd\u529b\u5bf9\u4e8e\u5176\u4e0e\u8bbe\u8ba1\u5e08\u534f\u4f5c\u7684\u6f5c\u529b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6784\u5efa\u5305\u542b598\u4e2a\u5de5\u5177\u8bbe\u8ba1\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4efb\u52a1\u57fa\u4e8e3.3K\u79fb\u52a8UI\u8bbe\u8ba1\uff0c\u6db5\u76d630\u4e2a\u529f\u80fd\u7c7b\u522b\uff0c\u5305\u542b\u8bbe\u8ba1\u590d\u5236\u548c\u8bbe\u8ba1\u4fee\u6539\u4e24\u79cd\u4efb\u52a1\u7c7b\u578b\uff0c\u6a21\u578b\u901a\u8fc7\u4e0a\u4e0b\u6587\u5de5\u5177\u8c03\u7528\u9010\u6b65\u66f4\u65b0\u8bbe\u8ba1\u3002", "result": "\u9886\u5148\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u5177\u7b56\u7565\u6027\u7684\u5de5\u5177\u8c03\u7528\uff0c\u63d0\u9ad8\u4e86\u8bbe\u8ba1\u8d28\u91cf\uff0c\u540c\u65f6\u8bc6\u522b\u4e86\u6a21\u578b\u5e38\u89c1\u7684\u9519\u8bef\u6a21\u5f0f\u3002", "conclusion": "CANVAS\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347\u57fa\u4e8e\u5de5\u5177\u7684\u8bbe\u8ba1\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u6307\u5bfc\u672a\u6765\u5728\u589e\u5f3a\u5de5\u5177\u8bbe\u8ba1\u80fd\u529b\u65b9\u9762\u7684\u5de5\u4f5c\u3002"}}
{"id": "2511.20719", "categories": ["cs.AI", "cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.20719", "abs": "https://arxiv.org/abs/2511.20719", "authors": ["Yifan Fan", "Le Liang", "Peng Liu", "Xiao Li", "Ziyang Guo", "Qiao Lan", "Shi Jin", "Wen Tong"], "title": "Learning Multi-Access Point Coordination in Agentic AI Wi-Fi with Large Language Models", "comment": null, "summary": "Multi-access point coordination (MAPC) is a key technology for enhancing throughput in next-generation Wi-Fi within dense overlapping basic service sets. However, existing MAPC protocols rely on static, protocol-defined rules, which limits their ability to adapt to dynamic network conditions such as varying interference levels and topologies. To address this limitation, we propose a novel Agentic AI Wi-Fi framework where each access point, modeled as an autonomous large language model agent, collaboratively reasons about the network state and negotiates adaptive coordination strategies in real time. This dynamic collaboration is achieved through a cognitive workflow that enables the agents to engage in natural language dialogue, leveraging integrated memory, reflection, and tool use to ground their decisions in past experience and environmental feedback. Comprehensive simulation results demonstrate that our agentic framework successfully learns to adapt to diverse and dynamic network environments, significantly outperforming the state-of-the-art spatial reuse baseline and validating its potential as a robust and intelligent solution for future wireless networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u667a\u80fdWi-Fi\u6846\u67b6\uff0c\u5176\u4e2d\u6bcf\u4e2a\u63a5\u5165\u70b9\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u5b9e\u65f6\u534f\u5546\u81ea\u9002\u5e94\u534f\u8c03\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u96c6\u91cd\u53e0\u57fa\u672c\u670d\u52a1\u96c6\u4e2d\u7684\u541e\u5410\u91cf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u63a5\u5165\u70b9\u534f\u8c03\u534f\u8bae\u4f9d\u8d56\u9759\u6001\u7684\u534f\u8bae\u5b9a\u4e49\u89c4\u5219\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u7f51\u7edc\u6761\u4ef6\uff08\u5982\u53d8\u5316\u7684\u5e72\u6270\u6c34\u5e73\u548c\u62d3\u6251\u7ed3\u6784\uff09\uff0c\u9650\u5236\u4e86\u5728\u5bc6\u96c6Wi-Fi\u73af\u5883\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u3002", "method": "\u5c06\u6bcf\u4e2a\u63a5\u5165\u70b9\u5efa\u6a21\u4e3a\u81ea\u4e3b\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\uff0c\u901a\u8fc7\u8ba4\u77e5\u5de5\u4f5c\u6d41\u7a0b\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\uff0c\u5229\u7528\u96c6\u6210\u8bb0\u5fc6\u3001\u53cd\u601d\u548c\u5de5\u5177\u4f7f\u7528\u529f\u80fd\uff0c\u57fa\u4e8e\u5386\u53f2\u7ecf\u9a8c\u548c\u73af\u5883\u53cd\u9988\u5236\u5b9a\u51b3\u7b56\u3002", "result": "\u7efc\u5408\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u4ee3\u7406\u6846\u67b6\u6210\u529f\u9002\u5e94\u4e86\u591a\u6837\u5316\u548c\u52a8\u6001\u7684\u7f51\u7edc\u73af\u5883\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7a7a\u5206\u590d\u7528\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u4ee3\u7406\u5f0fAI Wi-Fi\u6846\u67b6\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u7a33\u5065\u667a\u80fd\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u81ea\u9002\u5e94\u534f\u8c03\u7b56\u7565\u3002"}}
{"id": "2511.20770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.20770", "abs": "https://arxiv.org/abs/2511.20770", "authors": ["Raghuveer Thirukovalluru", "Xiaochuang Han", "Bhuwan Dhingra", "Emily Dinan", "Maha Elbayad"], "title": "Text-Guided Semantic Image Encoder", "comment": "20 pages, 6 figures", "summary": "Image encoders, a fundamental component of vision-language models (VLMs), are typically pretrained independently before being aligned with a language model. This standard paradigm results in encoders that process images agnostically, without regard to the specific downstream task or text query. To address this limitation, we propose the Text-Guided Semantic Image Encoder (TIE), which generates image representations conditioned on the input text query. VLMs equipped with TIE outperform their conventional counterparts by +1.5 and +1.3 points on average across nine image-to-text benchmarks at the 1B and 3B scales, respectively, with gains reaching up to 6 points on tasks such as DocVQA and InfoVQA. Moreover, TIE-based VLMs attain superior performance while utilizing only half as many image tiles (tokens), resulting in notably improved inference efficiency. TIE also generalizes well with generic queries, indicating that text-conditioned training effectively optimizes the encoder to capture key visual features. Qualitative analysis confirms that TIE consistently attends to query-relevant regions, enhancing both interpretability and query-specific grounding.", "AI": {"tldr": "\u63d0\u51fa\u6587\u672c\u5f15\u5bfc\u8bed\u4e49\u56fe\u50cf\u7f16\u7801\u5668(TIE)\uff0c\u4f7f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u56fe\u50cf\u7f16\u7801\u5668\u80fd\u591f\u6839\u636e\u6587\u672c\u67e5\u8be2\u751f\u6210\u6761\u4ef6\u5316\u7684\u56fe\u50cf\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u4efb\u52a1\u6027\u80fd\u5e76\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u56fe\u50cf\u7f16\u7801\u5668\u901a\u5e38\u72ec\u7acb\u9884\u8bad\u7ec3\uff0c\u4e0e\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u540e\u4ecd\u4ee5\u4efb\u52a1\u65e0\u5173\u65b9\u5f0f\u5904\u7406\u56fe\u50cf\uff0c\u65e0\u6cd5\u6839\u636e\u5177\u4f53\u4e0b\u6e38\u4efb\u52a1\u6216\u6587\u672c\u67e5\u8be2\u8fdb\u884c\u4f18\u5316\u3002", "method": "\u5f00\u53d1\u6587\u672c\u5f15\u5bfc\u8bed\u4e49\u56fe\u50cf\u7f16\u7801\u5668(TIE)\uff0c\u901a\u8fc7\u6587\u672c\u6761\u4ef6\u5316\u8bad\u7ec3\u4f7f\u56fe\u50cf\u7f16\u7801\u5668\u80fd\u591f\u6839\u636e\u8f93\u5165\u6587\u672c\u67e5\u8be2\u751f\u6210\u76f8\u5e94\u7684\u56fe\u50cf\u8868\u793a\u3002", "result": "\u57281B\u548c3B\u89c4\u6a21\u4e0b\uff0cTIE\u589e\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57289\u4e2a\u56fe\u50cf\u5230\u6587\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53471.5\u548c1.3\u4e2a\u767e\u5206\u70b9\uff0c\u5728DocVQA\u548cInfoVQA\u7b49\u4efb\u52a1\u4e0a\u63d0\u5347\u9ad8\u8fbe6\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u4ec5\u4f7f\u7528\u4e00\u534a\u56fe\u50cf\u5757\u5373\u53ef\u8fbe\u5230\u66f4\u597d\u6027\u80fd\u3002", "conclusion": "\u6587\u672c\u6761\u4ef6\u5316\u8bad\u7ec3\u6709\u6548\u4f18\u5316\u7f16\u7801\u5668\u4ee5\u6355\u6349\u5173\u952e\u89c6\u89c9\u7279\u5f81\uff0cTIE\u80fd\u591f\u6301\u7eed\u5173\u6ce8\u67e5\u8be2\u76f8\u5173\u533a\u57df\uff0c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u548c\u67e5\u8be2\u7279\u5b9a\u57fa\u7840\u80fd\u529b\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2511.20766", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20766", "abs": "https://arxiv.org/abs/2511.20766", "authors": ["Karen Ullrich", "Jingtong Su", "Claudia Shi", "Arjun Subramonian", "Amir Bar", "Ivan Evtimov", "Nikolaos Tsilivis", "Randall Balestriero", "Julia Kempe", "Mark Ibrahim"], "title": "OpenApps: Simulating Environment Variations to Measure UI-Agent Reliability", "comment": null, "summary": "Reliability is key to realizing the promise of autonomous UI-Agents, multimodal agents that directly interact with apps in the same manner as humans, as users must be able to trust an agent to complete a given task. Current evaluations rely on fixed environments, often clones of existing apps, which are limited in that they can only shed light on whether or how often an agent can complete a task within a specific environment. When deployed however, agents are likely to encounter variations in app design and content that can affect an agent's ability to complete a task. To address this blind spot of measuring agent reliability across app variations, we develop OpenApps, a light-weight open-source ecosystem with six apps (messenger, calendar, maps, etc.) that are configurable in appearance and content. OpenApps requires just a single CPU to run, enabling easy generation and deployment of thousands of versions of each app. Specifically, we run more than 10,000 independent evaluations to study reliability across seven leading multimodal agents. We find that while standard reliability within a fixed app is relatively stable, reliability can vary drastically when measured across app variations. Task success rates for many agents can fluctuate by more than $50\\%$ across app variations. For example, Kimi-VL-3B's average success across all tasks fluctuates from $63\\%$ to just $4\\%$ across app versions. We also find agent behaviors such as looping or hallucinating actions can differ drastically depending on the environment configuration. These initial findings highlight the importance of measuring reliability along this new dimension of app variations. OpenApps is available at https://facebookresearch.github.io/OpenApps/", "AI": {"tldr": "OpenApps\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\uff0c\u5305\u542b6\u4e2a\u53ef\u914d\u7f6e\u5e94\u7528\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001UI\u4ee3\u7406\u5728\u4e0d\u540c\u5e94\u7528\u53d8\u4f53\u4e2d\u7684\u53ef\u9760\u6027\u3002\u7814\u7a76\u53d1\u73b0\u4ee3\u7406\u5728\u56fa\u5b9a\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u76f8\u5bf9\u7a33\u5b9a\uff0c\u4f46\u5728\u4e0d\u540c\u5e94\u7528\u53d8\u4f53\u95f4\u5dee\u5f02\u5de8\u5927\uff0c\u4efb\u52a1\u6210\u529f\u7387\u6ce2\u52a8\u53ef\u8fbe50%\u4ee5\u4e0a\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u73af\u5883\uff0c\u65e0\u6cd5\u8861\u91cf\u4ee3\u7406\u5728\u4e0d\u540c\u5e94\u7528\u8bbe\u8ba1\u548c\u5185\u5bb9\u53d8\u5316\u4e2d\u7684\u53ef\u9760\u6027\u3002OpenApps\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u76f2\u70b9\uff0c\u8bc4\u4f30\u4ee3\u7406\u5728\u771f\u5b9e\u90e8\u7f72\u4e2d\u53ef\u80fd\u9047\u5230\u7684\u5e94\u7528\u53d8\u4f53\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5f00\u53d1OpenApps\u751f\u6001\u7cfb\u7edf\uff0c\u5305\u542b6\u4e2a\u53ef\u914d\u7f6e\u5e94\u7528\uff08\u6d88\u606f\u3001\u65e5\u5386\u3001\u5730\u56fe\u7b49\uff09\uff0c\u4ec5\u9700\u5355\u4e2aCPU\u5373\u53ef\u8fd0\u884c\u6570\u5343\u4e2a\u5e94\u7528\u7248\u672c\u3002\u8fdb\u884c\u4e86\u8d85\u8fc710,000\u6b21\u72ec\u7acb\u8bc4\u4f30\uff0c\u7814\u7a767\u4e2a\u9886\u5148\u591a\u6a21\u6001\u4ee3\u7406\u5728\u4e0d\u540c\u5e94\u7528\u53d8\u4f53\u4e2d\u7684\u53ef\u9760\u6027\u3002", "result": "\u4ee3\u7406\u5728\u56fa\u5b9a\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u76f8\u5bf9\u7a33\u5b9a\uff0c\u4f46\u5728\u4e0d\u540c\u5e94\u7528\u53d8\u4f53\u95f4\u5dee\u5f02\u5de8\u5927\u3002\u4f8b\u5982Kimi-VL-3B\u7684\u5e73\u5747\u6210\u529f\u7387\u5728\u4e0d\u540c\u5e94\u7528\u7248\u672c\u4e2d\u4ece63%\u964d\u81f34%\u3002\u4ee3\u7406\u884c\u4e3a\uff08\u5982\u5faa\u73af\u6216\u5e7b\u89c9\u64cd\u4f5c\uff09\u4e5f\u968f\u73af\u5883\u914d\u7f6e\u663e\u8457\u53d8\u5316\u3002", "conclusion": "\u5e94\u7528\u53d8\u4f53\u662f\u8861\u91cf\u4ee3\u7406\u53ef\u9760\u6027\u7684\u91cd\u8981\u65b0\u7ef4\u5ea6\u3002OpenApps\u4e3a\u8bc4\u4f30\u4ee3\u7406\u5728\u4e0d\u540c\u5e94\u7528\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5f3a\u8c03\u4e86\u5728\u591a\u6837\u5316\u5e94\u7528\u914d\u7f6e\u4e2d\u6d4b\u8bd5\u4ee3\u7406\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.20784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.20784", "abs": "https://arxiv.org/abs/2511.20784", "authors": ["Sindhuja Penchala", "Gavin Money", "Gabriel Marques", "Samuel Wood", "Jessica Kirschman", "Travis Atkison", "Shahram Rahimi", "Noorbakhsh Amiri Golilarz"], "title": "One Patch is All You Need: Joint Surface Material Reconstruction and Classification from Minimal Visual Cues", "comment": "9 pages,3 figures, 5 tables", "summary": "Understanding material surfaces from sparse visual cues is critical for applications in robotics, simulation, and material perception. However, most existing methods rely on dense or full-scene observations, limiting their effectiveness in constrained or partial view environment. To address this challenge, we introduce SMARC, a unified model for Surface MAterial Reconstruction and Classification from minimal visual input. By giving only a single 10% contiguous patch of the image, SMARC recognizes and reconstructs the full RGB surface while simultaneously classifying the material category. Our architecture combines a Partial Convolutional U-Net with a classification head, enabling both spatial inpainting and semantic understanding under extreme observation sparsity. We compared SMARC against five models including convolutional autoencoders [17], Vision Transformer (ViT) [13], Masked Autoencoder (MAE) [5], Swin Transformer [9], and DETR [2] using Touch and Go dataset [16] of real-world surface textures. SMARC achieves state-of-the-art results with a PSNR of 17.55 dB and a material classification accuracy of 85.10%. Our findings highlight the advantages of partial convolution in spatial reasoning under missing data and establish a strong foundation for minimal-vision surface understanding.", "AI": {"tldr": "SMARC\u662f\u4e00\u4e2a\u4ece\u6700\u5c0f\u89c6\u89c9\u8f93\u5165\u8fdb\u884c\u8868\u9762\u6750\u6599\u91cd\u5efa\u548c\u5206\u7c7b\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u4ec5\u9700\u5355\u4e2a10%\u8fde\u7eed\u56fe\u50cf\u5757\u5373\u53ef\u8bc6\u522b\u548c\u91cd\u5efa\u5b8c\u6574RGB\u8868\u9762\u5e76\u5206\u7c7b\u6750\u6599\u7c7b\u522b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u6216\u5168\u573a\u666f\u89c2\u6d4b\uff0c\u5728\u53d7\u9650\u6216\u90e8\u5206\u89c6\u89d2\u73af\u5883\u4e2d\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u4ece\u7a00\u758f\u89c6\u89c9\u7ebf\u7d22\u7406\u89e3\u6750\u6599\u8868\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u90e8\u5206\u5377\u79efU-Net\u548c\u5206\u7c7b\u5934\u7684\u67b6\u6784\uff0c\u5728\u6781\u7aef\u89c2\u6d4b\u7a00\u758f\u6761\u4ef6\u4e0b\u5b9e\u73b0\u7a7a\u95f4\u4fee\u590d\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u4f7f\u7528Touch and Go\u771f\u5b9e\u4e16\u754c\u8868\u9762\u7eb9\u7406\u6570\u636e\u96c6\u3002", "result": "\u5728\u4e94\u4e2a\u6a21\u578b\u6bd4\u8f83\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1aPSNR 17.55 dB\uff0c\u6750\u6599\u5206\u7c7b\u51c6\u786e\u738785.10%\u3002", "conclusion": "\u90e8\u5206\u5377\u79ef\u5728\u7f3a\u5931\u6570\u636e\u7a7a\u95f4\u63a8\u7406\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4e3a\u6700\u5c0f\u89c6\u89c9\u8868\u9762\u7406\u89e3\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2511.20892", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20892", "abs": "https://arxiv.org/abs/2511.20892", "authors": ["Xuyuan Liu", "Zhengzhang Chen", "Xinshuai Dong", "Yanchi Liu", "Xujiang Zhao", "Shengyu Chen", "Haoyu Wang", "Yujun Yan", "Haifeng Chen"], "title": "Representation Interventions Enable Lifelong Unstructured Knowledge Control", "comment": "18 Page", "summary": "Large language models (LLMs) often produce incorrect or outdated content. Updating their knowledge efficiently and accurately without costly retraining is a major challenge. This problem is especially hard for complex, unstructured knowledge in a lifelong setting, where many edits must coexist without interference. We introduce RILKE (Representation Intervention for Lifelong KnowledgE Control), a robust and scalable method that treats knowledge control as interventions within the model's representation space. Leveraging representation-space expressiveness, we identify two properties enabling RILKE to deliver fine-grained control over complex, unstructured knowledge while maintaining general utility with frozen base weights. During training, RILKE learns paraphrase-robust and edit-localized modules that limit each update to a low-dimensional subspace to minimize cross-edit interference. In inference, a query-adaptive router selects the appropriate module to guide the model's generation. In evaluation on knowledge editing benchmarks with LLaMA and Qwen models, RILKE is scalable to large-scale datasets, demonstrating high edit success, strong paraphrase generalization, and preserving general utility with modest memory overhead. These results show RILKE is an effective and scalable solution for lifelong knowledge control in LLMs.", "AI": {"tldr": "RILKE\u662f\u4e00\u79cd\u5728\u8868\u793a\u7a7a\u95f4\u8fdb\u884c\u5e72\u9884\u7684\u77e5\u8bc6\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u57fa\u7840\u6743\u91cd\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u590d\u6742\u975e\u7ed3\u6784\u5316\u77e5\u8bc6\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u652f\u6301\u5927\u89c4\u6a21\u77e5\u8bc6\u7f16\u8f91\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u9519\u8bef\u6216\u8fc7\u65f6\u5185\u5bb9\u7684\u95ee\u9898\uff0c\u5728\u4e0d\u8fdb\u884c\u6602\u8d35\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u9ad8\u6548\u51c6\u786e\u5730\u66f4\u65b0\u6a21\u578b\u77e5\u8bc6\uff0c\u7279\u522b\u662f\u5728\u7ec8\u8eab\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5904\u7406\u591a\u7f16\u8f91\u5171\u5b58\u4e14\u4e0d\u76f8\u4e92\u5e72\u6270\u7684\u6311\u6218\u3002", "method": "\u5728\u6a21\u578b\u8868\u793a\u7a7a\u95f4\u8fdb\u884c\u5e72\u9884\uff0c\u5b66\u4e60\u6297\u91ca\u4e49\u548c\u7f16\u8f91\u5c40\u90e8\u5316\u7684\u6a21\u5757\uff0c\u5c06\u6bcf\u4e2a\u66f4\u65b0\u9650\u5236\u5728\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u4ee5\u51cf\u5c11\u4ea4\u53c9\u7f16\u8f91\u5e72\u6270\uff1b\u63a8\u7406\u65f6\u4f7f\u7528\u67e5\u8be2\u81ea\u9002\u5e94\u8def\u7531\u9009\u62e9\u9002\u5f53\u6a21\u5757\u6307\u5bfc\u751f\u6210\u3002", "result": "\u5728LLaMA\u548cQwen\u6a21\u578b\u7684\u77e5\u8bc6\u7f16\u8f91\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRILKE\u53ef\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u8868\u73b0\u51fa\u9ad8\u7f16\u8f91\u6210\u529f\u7387\u3001\u5f3a\u91ca\u4e49\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4ee5\u9002\u5ea6\u7684\u5185\u5b58\u5f00\u9500\u4fdd\u6301\u901a\u7528\u6548\u7528\u3002", "conclusion": "RILKE\u662f\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7ec8\u8eab\u77e5\u8bc6\u63a7\u5236\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.20785", "abs": "https://arxiv.org/abs/2511.20785", "authors": ["Zuhao Yang", "Sudong Wang", "Kaichen Zhang", "Keming Wu", "Sicong Leng", "Yifan Zhang", "Chengwei Qin", "Shijian Lu", "Xingxuan Li", "Lidong Bing"], "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "comment": null, "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "AI": {"tldr": "LongVT\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5de5\u5177\u94fe\u601d\u7ef4\u5b9e\u73b0\u957f\u89c6\u9891\u63a8\u7406\uff0c\u5229\u7528LMMs\u7684\u65f6\u5e8f\u5b9a\u4f4d\u80fd\u529b\u4f5c\u4e3a\u539f\u751f\u89c6\u9891\u88c1\u526a\u5de5\u5177\uff0c\u91c7\u7528\u5168\u5c40\u5230\u5c40\u90e8\u7684\u63a8\u7406\u5faa\u73af\u6765\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u56e0\u4e3a\u8bc1\u636e\u7a00\u758f\u4e14\u65f6\u95f4\u5206\u6563\u3002\u53d7\u4eba\u7c7b\u7406\u89e3\u957f\u89c6\u9891\u7684\u65b9\u5f0f\u542f\u53d1\uff08\u5148\u5168\u5c40\u6d4f\u89c8\u518d\u68c0\u67e5\u76f8\u5173\u7247\u6bb5\uff09\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8fdb\u884c\u7cbe\u7ec6\u63a8\u7406\u7684\u6846\u67b6\u3002", "method": "\u5229\u7528LMMs\u7684\u65f6\u5e8f\u5b9a\u4f4d\u80fd\u529b\u4f5c\u4e3a\u89c6\u9891\u88c1\u526a\u5de5\u5177\uff0c\u901a\u8fc7\u5168\u5c40\u5230\u5c40\u90e8\u7684\u63a8\u7406\u5faa\u73af\u9010\u6b65\u7ec6\u5316\u89c6\u9891\u5e27\u91c7\u6837\u3002\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u5de5\u5177\u96c6\u6210\u7684\u51b7\u542f\u52a8\u76d1\u7763\u5fae\u8c03\u3001\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u548c\u667a\u80fd\u4f53\u5f3a\u5316\u5fae\u8c03\u3002", "result": "\u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u957f\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLongVT\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "LongVT\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u5de5\u5177\u94fe\u601d\u7ef4\u548c\u5168\u5c40\u5230\u5c40\u90e8\u7684\u63a8\u7406\u5faa\u73af\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20934", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20934", "abs": "https://arxiv.org/abs/2511.20934", "authors": ["Biagio La Rosa", "Leilani H. Gilpin"], "title": "Guaranteed Optimal Compositional Explanations for Neurons", "comment": "41 pages, 10 figures", "summary": "While neurons are the basic units of deep neural networks, it is still unclear what they learn and if their knowledge is aligned with that of humans. Compositional explanations aim to answer this question by describing the spatial alignment between neuron activations and concepts through logical rules. These logical descriptions are typically computed via a search over all possible concept combinations. Since computing the spatial alignment over the entire state space is computationally infeasible, the literature commonly adopts beam search to restrict the space. However, beam search cannot provide any theoretical guarantees of optimality, and it remains unclear how close current explanations are to the true optimum. In this theoretical paper, we address this gap by introducing the first framework for computing guaranteed optimal compositional explanations. Specifically, we propose: (i) a decomposition that identifies the factors influencing the spatial alignment, (ii) a heuristic to estimate the alignment at any stage of the search, and (iii) the first algorithm that can compute optimal compositional explanations within a feasible time. Using this framework, we analyze the differences between optimal and non-optimal explanations in the most popular settings for compositional explanations, the computer vision domain and Convolutional Neural Networks. In these settings, we demonstrate that 10-40 percent of explanations obtained with beam search are suboptimal when overlapping concepts are involved. Finally, we evaluate a beam-search variant guided by our proposed decomposition and heuristic, showing that it matches or improves runtime over prior methods while offering greater flexibility in hyperparameters and computational resources.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u8ba1\u7b97\u4fdd\u8bc1\u6700\u4f18\u7ec4\u5408\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u7a7a\u95f4\u5bf9\u9f50\u56e0\u7d20\u3001\u8bbe\u8ba1\u542f\u53d1\u5f0f\u4f30\u8ba1\u548c\u9ad8\u6548\u7b97\u6cd5\uff0c\u53d1\u73b0\u5728CNN\u4e2d10-40%\u7684beam search\u89e3\u91ca\u662f\u6b21\u4f18\u7684\u3002", "motivation": "\u73b0\u6709\u7ec4\u5408\u89e3\u91ca\u65b9\u6cd5\u4f7f\u7528beam search\u8ba1\u7b97\u795e\u7ecf\u5143\u6fc0\u6d3b\u4e0e\u6982\u5ff5\u7684\u7a7a\u95f4\u5bf9\u9f50\uff0c\u4f46\u65e0\u6cd5\u63d0\u4f9b\u6700\u4f18\u6027\u4fdd\u8bc1\uff0c\u4e0d\u6e05\u695a\u5f53\u524d\u89e3\u91ca\u4e0e\u771f\u6b63\u6700\u4f18\u89e3\u7684\u63a5\u8fd1\u7a0b\u5ea6\u3002", "method": "\u63d0\u51fa\u4e09\u90e8\u5206\u6846\u67b6\uff1a(i)\u8bc6\u522b\u5f71\u54cd\u7a7a\u95f4\u5bf9\u9f50\u56e0\u7d20\u7684\u5206\u89e3\u65b9\u6cd5\uff1b(ii)\u5728\u641c\u7d22\u4efb\u4f55\u9636\u6bb5\u4f30\u8ba1\u5bf9\u9f50\u7684\u542f\u53d1\u5f0f\uff1b(iii)\u9996\u4e2a\u80fd\u5728\u53ef\u884c\u65f6\u95f4\u5185\u8ba1\u7b97\u6700\u4f18\u7ec4\u5408\u89e3\u91ca\u7684\u7b97\u6cd5\u3002", "result": "\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548cCNN\u4e2d\u6700\u5e38\u7528\u8bbe\u7f6e\u4e0b\u5206\u6790\uff0c\u53d1\u73b0\u5f53\u6d89\u53ca\u91cd\u53e0\u6982\u5ff5\u65f6\uff0c10-40%\u901a\u8fc7beam search\u83b7\u5f97\u7684\u89e3\u91ca\u662f\u6b21\u4f18\u7684\u3002", "conclusion": "\u57fa\u4e8e\u6240\u63d0\u5206\u89e3\u548c\u542f\u53d1\u5f0f\u7684beam search\u53d8\u4f53\u5728\u8fd0\u884c\u65f6\u95f4\u4e0a\u5339\u914d\u6216\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u8d85\u53c2\u6570\u548c\u8ba1\u7b97\u8d44\u6e90\u65b9\u9762\u63d0\u4f9b\u66f4\u5927\u7075\u6d3b\u6027\u3002"}}
{"id": "2511.20795", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20795", "abs": "https://arxiv.org/abs/2511.20795", "authors": ["Souradeep Dutta", "Keshav Bulia", "Neena S Nair"], "title": "Revisiting KRISP: A Lightweight Reproduction and Analysis of Knowledge-Enhanced Vision-Language Models", "comment": "7 pages , 4 figures", "summary": "Facebook AI Research introduced KRISP [4], which integrates structured external knowledge into pipelines for vision-language reasoning. Despite its effectiveness, the original model has been developed for industrial-scale training, is computationally demanding, and is tightly connected to a large backbone. In this work, we reexamine KRISP from a different angle and offer a lightweight reproduction with significantly fewer parameters. Even though our replicated model performs about 75 % of the original, the replication process uncovers a number of design flaws, real-world pitfalls, and implicit problems that were not fully covered in the original paper. We offer insights into the scalability and efficacy of knowledge-enhanced VQA architectures under resource constraints through systematic ablation studies, which include a proof-of-concept on synthetic VQA data and evaluation on the DAQUAR dataset. Our model, configured with a low parameter setup and constrained by the external Knowledge graph domain, prevents AI hallucinations and generates outputs solely within that domain. Minimal parameters allow us to function on edge devices like smartphones and AR-VR, further improving offline visual reasoning.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86KRISP\u77e5\u8bc6\u589e\u5f3a\u89c6\u89c9\u95ee\u7b54\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u53c2\u6570\u66f4\u5c11\u7684\u8f7b\u91cf\u7ea7\u590d\u73b0\u7248\u672c\uff0c\u867d\u7136\u6027\u80fd\u7ea6\u4e3a\u539f\u7248\u768475%\uff0c\u4f46\u63ed\u793a\u4e86\u539f\u6a21\u578b\u7684\u8bbe\u8ba1\u7f3a\u9677\u548c\u5b9e\u9645\u95ee\u9898\uff0c\u5e76\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u9a8c\u8bc1\u4e86\u77e5\u8bc6\u589e\u5f3aVQA\u67b6\u6784\u7684\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u539f\u7248KRISP\u6a21\u578b\u867d\u7136\u6709\u6548\uff0c\u4f46\u9700\u8981\u5de5\u4e1a\u7ea7\u8bad\u7ec3\u89c4\u6a21\u3001\u8ba1\u7b97\u9700\u6c42\u5927\u4e14\u4e0e\u5927\u578b\u9aa8\u5e72\u7f51\u7edc\u7d27\u5bc6\u8026\u5408\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7248\u672c\uff0c\u4f7f\u5176\u80fd\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\uff0c\u5e76\u63ed\u793a\u539f\u6a21\u578b\u672a\u5145\u5206\u8ba8\u8bba\u7684\u8bbe\u8ba1\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6d88\u878d\u7814\u7a76\uff0c\u5305\u62ec\u5728\u5408\u6210VQA\u6570\u636e\u4e0a\u7684\u6982\u5ff5\u9a8c\u8bc1\u548c\u5728DAQUAR\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\uff0c\u91cd\u65b0\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53c2\u6570\u8f83\u5c11\u7684\u8f7b\u91cf\u7ea7KRISP\u6a21\u578b\uff0c\u5e76\u7ea6\u675f\u5728\u5916\u90e8\u77e5\u8bc6\u56fe\u8c31\u9886\u57df\u5185\u4ee5\u9632\u6b62AI\u5e7b\u89c9\u3002", "result": "\u590d\u73b0\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u6027\u80fd\u7ea6\u4e3a\u539f\u7248\u768475%\uff0c\u4f46\u80fd\u591f\u63ed\u793a\u539f\u6a21\u578b\u7684\u8bbe\u8ba1\u7f3a\u9677\u548c\u5b9e\u9645\u95ee\u9898\u3002\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c\uff0c\u53ef\u5728\u667a\u80fd\u624b\u673a\u548cAR-VR\u7b49\u8fb9\u7f18\u8bbe\u5907\u4e0a\u79bb\u7ebf\u8fd0\u884c\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u77e5\u8bc6\u589e\u5f3aVQA\u67b6\u6784\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u5177\u6709\u53ef\u884c\u6027\uff0c\u80fd\u591f\u9632\u6b62AI\u5e7b\u89c9\u5e76\u751f\u6210\u9650\u5b9a\u9886\u57df\u5185\u7684\u8f93\u51fa\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u89c6\u89c9\u63a8\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20937", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20937", "abs": "https://arxiv.org/abs/2511.20937", "authors": ["Qineng Wang", "Wenlong Huang", "Yu Zhou", "Hang Yin", "Tianwei Bao", "Jianwen Lyu", "Weiyu Liu", "Ruohan Zhang", "Jiajun Wu", "Li Fei-Fei", "Manling Li"], "title": "ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction", "comment": "Preprint version", "summary": "Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/.", "AI": {"tldr": "ENACT\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5c55\u73b0\u5177\u8eab\u8ba4\u77e5\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u89c6\u89c9\u95ee\u7b54\u5f62\u5f0f\u6d4b\u8bd5\u6a21\u578b\u4ece\u81ea\u6211\u4e2d\u5fc3\u4ea4\u4e92\u4e2d\u8fdb\u884c\u4e16\u754c\u5efa\u6a21\u7684\u80fd\u529b\uff0c\u5305\u542b\u524d\u5411\u548c\u9006\u5411\u4e16\u754c\u5efa\u6a21\u4e24\u4e2a\u4efb\u52a1\u3002", "motivation": "\u7814\u7a76\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5728\u975e\u5177\u8eab\u8bad\u7ec3\u540e\u4ecd\u8868\u73b0\u51fa\u5177\u8eab\u8ba4\u77e5\u7684\u7279\u5f81\uff0c\u5373\u667a\u80fd\u662f\u5426\u6e90\u4e8e\u611f\u89c9\u8fd0\u52a8\u4ea4\u4e92\u800c\u975e\u88ab\u52a8\u89c2\u5bdf\u3002", "method": "\u5c06\u5177\u8eab\u8ba4\u77e5\u8bc4\u4f30\u6784\u5efa\u4e3a\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u4e16\u754c\u5efa\u6a21\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e24\u4e2a\u5e8f\u5217\u91cd\u6392\u5e8f\u4efb\u52a1\uff1a\u7ed9\u5b9a\u52a8\u4f5c\u91cd\u6392\u89c2\u5bdf\u5e8f\u5217\uff08\u524d\u5411\u4e16\u754c\u5efa\u6a21\uff09\u548c\u7ed9\u5b9a\u89c2\u5bdf\u91cd\u6392\u52a8\u4f5c\u5e8f\u5217\uff08\u9006\u5411\u4e16\u754c\u5efa\u6a21\uff09\u3002", "result": "\u524d\u6cbf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u4e14\u5dee\u8ddd\u968f\u4ea4\u4e92\u65f6\u95f4\u8de8\u5ea6\u589e\u5927\u800c\u6269\u5927\uff1b\u6a21\u578b\u5728\u9006\u5411\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u663e\u793a\u51fa\u4eba\u7c7b\u4e2d\u5fc3\u504f\u89c1\uff08\u5982\u504f\u597d\u53f3\u624b\u52a8\u4f5c\uff09\u3002", "conclusion": "\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u8ba4\u77e5\u80fd\u529b\u4e0a\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u957f\u65f6\u7a0b\u4ea4\u4e92\u548c\u504f\u79bb\u4eba\u7c7b\u89c6\u89c9\u7279\u6027\u7684\u573a\u666f\u4e2d\u8868\u73b0\u8f83\u5dee\u3002"}}
{"id": "2511.20800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.20800", "abs": "https://arxiv.org/abs/2511.20800", "authors": ["Dario Morle", "Reid Zaffino"], "title": "Intriguing Properties of Dynamic Sampling Networks", "comment": null, "summary": "Dynamic sampling mechanisms in deep learning architectures have demonstrated utility across many computer vision models, though the theoretical analysis of these structures has not yet been unified. In this paper we connect the various dynamic sampling methods by developing and analyzing a novel operator which generalizes existing methods, which we term \"warping\". Warping provides a minimal implementation of dynamic sampling which is amenable to analysis, and can be used to reconstruct existing architectures including deformable convolutions, active convolutional units, and spatial transformer networks. Using our formalism, we provide statistical analysis of the operator by modeling the inputs as both IID variables and homogeneous random fields. Extending this analysis, we discover a unique asymmetry between the forward and backward pass of the model training. We demonstrate that these mechanisms represent an entirely different class of orthogonal operators to the traditional translationally invariant operators defined by convolutions. With a combination of theoretical analysis and empirical investigation, we find the conditions necessary to ensure stable training of dynamic sampling networks. In addition, statistical analysis of discretization effects are studied. Finally, we introduce a novel loss landscape visualization which utilizes gradient update information directly, to better understand learning behavior.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"warping\"\u7684\u65b0\u578b\u7b97\u5b50\uff0c\u7edf\u4e00\u5206\u6790\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5404\u79cd\u52a8\u6001\u91c7\u6837\u673a\u5236\uff0c\u63ed\u793a\u4e86\u524d\u5411\u4f20\u64ad\u4e0e\u53cd\u5411\u4f20\u64ad\u4e4b\u95f4\u7684\u4e0d\u5bf9\u79f0\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u786e\u4fdd\u52a8\u6001\u91c7\u6837\u7f51\u7edc\u7a33\u5b9a\u8bad\u7ec3\u7684\u6761\u4ef6\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e2d\u7684\u52a8\u6001\u91c7\u6837\u673a\u5236\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u4e2d\u5df2\u8bc1\u660e\u6709\u6548\uff0c\u4f46\u8fd9\u4e9b\u7ed3\u6784\u7684\u7406\u8bba\u5206\u6790\u5c1a\u672a\u7edf\u4e00\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u53ef\u5206\u6790\u7684\u65b0\u578b\u7b97\u5b50\u6765\u8fde\u63a5\u5404\u79cd\u52a8\u6001\u91c7\u6837\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\"warping\"\u7b97\u5b50\u4f5c\u4e3a\u52a8\u6001\u91c7\u6837\u7684\u6700\u5c0f\u5b9e\u73b0\uff0c\u80fd\u591f\u91cd\u6784\u73b0\u6709\u67b6\u6784\u5982\u53ef\u53d8\u5f62\u5377\u79ef\u3001\u4e3b\u52a8\u5377\u79ef\u5355\u5143\u548c\u7a7a\u95f4\u53d8\u6362\u7f51\u7edc\u3002\u901a\u8fc7\u5c06\u8f93\u5165\u5efa\u6a21\u4e3aIID\u53d8\u91cf\u548c\u9f50\u6b21\u968f\u673a\u573a\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u68af\u5ea6\u66f4\u65b0\u4fe1\u606f\u7684\u635f\u5931\u666f\u89c2\u53ef\u89c6\u5316\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u52a8\u6001\u91c7\u6837\u673a\u5236\u4ee3\u8868\u4e86\u4e00\u7c7b\u4e0e\u4f20\u7edf\u5e73\u79fb\u4e0d\u53d8\u5377\u79ef\u7b97\u5b50\u5b8c\u5168\u4e0d\u540c\u7684\u6b63\u4ea4\u7b97\u5b50\u7c7b\u522b\u3002\u63ed\u793a\u4e86\u524d\u5411\u4f20\u64ad\u4e0e\u53cd\u5411\u4f20\u64ad\u4e4b\u95f4\u7684\u72ec\u7279\u4e0d\u5bf9\u79f0\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u786e\u4fdd\u52a8\u6001\u91c7\u6837\u7f51\u7edc\u7a33\u5b9a\u8bad\u7ec3\u7684\u5fc5\u8981\u6761\u4ef6\u3002", "conclusion": "warping\u7b97\u5b50\u4e3a\u52a8\u6001\u91c7\u6837\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7814\u7a76\u63ed\u793a\u4e86\u8fd9\u4e9b\u673a\u5236\u7684\u672c\u8d28\u7279\u6027\uff0c\u4e3a\u7406\u89e3\u52a8\u6001\u91c7\u6837\u7f51\u7edc\u7684\u5b66\u4e60\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2511.20942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20942", "abs": "https://arxiv.org/abs/2511.20942", "authors": ["Rahul Dass", "Thomas Bowlin", "Zebing Li", "Xiao Jin", "Ashok Goel"], "title": "Improving Procedural Skill Explanations via Constrained Generation: A Symbolic-LLM Hybrid Architecture", "comment": null, "summary": "In procedural skill learning, instructional explanations must convey not just steps, but the causal, goal-directed, and compositional logic behind them. Large language models (LLMs) often produce fluent yet shallow responses that miss this structure. We present Ivy, an AI coaching system that delivers structured, multi-step explanations by combining symbolic Task-Method-Knowledge (TMK) models with a generative interpretation layer-an LLM that constructs explanations while being constrained by TMK structure. TMK encodes causal transitions, goal hierarchies, and problem decompositions, and guides the LLM within explicit structural bounds. We evaluate Ivy against responses against GPT and retrieval-augmented GPT baselines using expert and independent annotations across three inferential dimensions. Results show that symbolic constraints consistently improve the structural quality of explanations for \"how\" and \"why\" questions. This study demonstrates a scalable AI for education approach that strengthens the pedagogical value of AI-generated explanations in intelligent coaching systems.", "AI": {"tldr": "Ivy\u662f\u4e00\u4e2aAI\u6559\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u7b26\u53f7\u5316TMK\u6a21\u578b\u4e0e\u751f\u6210\u89e3\u91ca\u5c42\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u3001\u591a\u6b65\u9aa4\u7684\u89e3\u91ca\uff0c\u4f18\u4e8eGPT\u548c\u68c0\u7d22\u589e\u5f3aGPT\u57fa\u7ebf\u3002", "motivation": "\u5728\u7a0b\u5e8f\u6027\u6280\u80fd\u5b66\u4e60\u4e2d\uff0c\u6559\u5b66\u89e3\u91ca\u9700\u8981\u4f20\u8fbe\u6b65\u9aa4\u80cc\u540e\u7684\u56e0\u679c\u3001\u76ee\u6807\u5bfc\u5411\u548c\u7ec4\u5408\u903b\u8f91\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u4ea7\u751f\u6d41\u7545\u4f46\u6d45\u5c42\u7684\u56de\u7b54\uff0c\u7f3a\u4e4f\u8fd9\u79cd\u7ed3\u6784\u3002", "method": "Ivy\u7cfb\u7edf\u7ed3\u5408\u7b26\u53f7\u5316\u4efb\u52a1-\u65b9\u6cd5-\u77e5\u8bc6(TMK)\u6a21\u578b\u4e0e\u751f\u6210\u89e3\u91ca\u5c42\uff0cTMK\u7f16\u7801\u56e0\u679c\u8f6c\u6362\u3001\u76ee\u6807\u5c42\u6b21\u548c\u95ee\u9898\u5206\u89e3\uff0c\u5728\u660e\u786e\u7684\u7ed3\u6784\u8fb9\u754c\u5185\u6307\u5bfcLLM\u6784\u5efa\u89e3\u91ca\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u7b26\u53f7\u7ea6\u675f\u6301\u7eed\u63d0\u9ad8\u4e86\"\u5982\u4f55\"\u548c\"\u4e3a\u4ec0\u4e48\"\u95ee\u9898\u89e3\u91ca\u7684\u7ed3\u6784\u8d28\u91cf\uff0c\u5728\u4e09\u4e2a\u63a8\u7406\u7ef4\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684AI\u6559\u80b2\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86AI\u751f\u6210\u89e3\u91ca\u5728\u667a\u80fd\u6559\u7ec3\u7cfb\u7edf\u4e2d\u7684\u6559\u5b66\u4ef7\u503c\u3002"}}
{"id": "2511.20804", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20804", "abs": "https://arxiv.org/abs/2511.20804", "authors": ["Kriti Ghosh", "Devjyoti Chakraborty", "Lakshmish Ramaswamy", "Suchendra M. Bhandarkar", "In Kee Kim", "Nancy O'Hare", "Deepak Mishra"], "title": "$\u0394$-NeRF: Incremental Refinement of Neural Radiance Fields through Residual Control and Knowledge Transfer", "comment": null, "summary": "Neural Radiance Fields (NeRFs) have demonstrated remarkable capabilities in 3D reconstruction and novel view synthesis. However, most existing NeRF frameworks require complete retraining when new views are introduced incrementally, limiting their applicability in domains where data arrives sequentially. This limitation is particularly problematic in satellite-based terrain analysis, where regions are repeatedly observed over time. Incremental refinement of NeRFs remains underexplored, and naive approaches suffer from catastrophic forgetting when past data is unavailable. We propose $\u0394$-NeRF, a unique modular residual framework for incremental NeRF refinement. $\u0394$-NeRF introduces several novel techniques including: (1) a residual controller that injects per-layer corrections into a frozen base NeRF, enabling refinement without access to past data; (2) an uncertainty-aware gating mechanism that prevents overcorrection by adaptively combining base and refined predictions; and (3) a view selection strategy that reduces training data by up to 47\\% while maintaining performance. Additionally, we employ knowledge distillation to compress the enhanced model into a compact student network (20\\% of original size). Experiments on satellite imagery demonstrate that $\u0394$-NeRF achieves performance comparable to joint training while reducing training time by 30-42\\%. $\u0394$-NeRF consistently outperforms existing baselines, achieving an improvement of up to 43.5\\% in PSNR over naive fine-tuning and surpassing joint training on some metrics.", "AI": {"tldr": "\u0394-NeRF\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u6b8b\u5dee\u6846\u67b6\uff0c\u7528\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\u7684\u589e\u91cf\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5b8c\u6574\u91cd\u8bad\u7ec3\u548c\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\uff0c\u7279\u522b\u9002\u7528\u4e8e\u536b\u661f\u5730\u5f62\u5206\u6790\u7b49\u6570\u636e\u987a\u5e8f\u5230\u8fbe\u7684\u573a\u666f\u3002", "motivation": "\u73b0\u6709NeRF\u6846\u67b6\u5728\u5f15\u5165\u65b0\u89c6\u56fe\u65f6\u9700\u8981\u5b8c\u6574\u91cd\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5728\u6570\u636e\u987a\u5e8f\u5230\u8fbe\u573a\u666f\uff08\u5982\u536b\u661f\u5730\u5f62\u5206\u6790\uff09\u4e2d\u7684\u5e94\u7528\u3002\u589e\u91cf\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4e14\u8fc7\u53bb\u6570\u636e\u4e0d\u53ef\u7528\u65f6\u96be\u4ee5\u6709\u6548\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u0394-NeRF\u6846\u67b6\uff0c\u5305\u542b\uff1a\u6b8b\u5dee\u63a7\u5236\u5668\u5411\u51bb\u7ed3\u7684\u57fa\u7840NeRF\u6ce8\u5165\u9010\u5c42\u4fee\u6b63\uff1b\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u95e8\u63a7\u673a\u5236\u81ea\u9002\u5e94\u7ed3\u5408\u57fa\u7840\u548c\u4f18\u5316\u9884\u6d4b\uff1b\u89c6\u56fe\u9009\u62e9\u7b56\u7565\u51cf\u5c1147%\u8bad\u7ec3\u6570\u636e\uff1b\u77e5\u8bc6\u84b8\u998f\u5c06\u589e\u5f3a\u6a21\u578b\u538b\u7f29\u81f3\u539f\u5927\u5c0f20%\u3002", "result": "\u5728\u536b\u661f\u56fe\u50cf\u4e0a\uff0c\u0394-NeRF\u6027\u80fd\u4e0e\u8054\u5408\u8bad\u7ec3\u76f8\u5f53\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1130-42%\u3002\u76f8\u6bd4\u6734\u7d20\u5fae\u8c03PSNR\u63d0\u534743.5%\uff0c\u5728\u67d0\u4e9b\u6307\u6807\u4e0a\u8d85\u8d8a\u8054\u5408\u8bad\u7ec3\u3002", "conclusion": "\u0394-NeRF\u4e3aNeRF\u7684\u589e\u91cf\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u7279\u522b\u9002\u7528\u4e8e\u536b\u661f\u5730\u5f62\u5206\u6790\u7b49\u987a\u5e8f\u6570\u636e\u573a\u666f\u3002"}}
{"id": "2511.21005", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.21005", "abs": "https://arxiv.org/abs/2511.21005", "authors": ["Jinpeng Wang", "Chao Li", "Ting Ye", "Mengyuan Zhang", "Wei Liu", "Jian Luan"], "title": "ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.", "AI": {"tldr": "\u63d0\u51fa\u4e86ICPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528LLM\u751f\u6210\u4e0d\u540c\u54cd\u5e94\u7684\u6982\u7387\u6765\u53cd\u6620\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u81ea\u6211\u8bc4\u4f30\uff0c\u7ed3\u5408\u504f\u597d\u4f18\u52bf\u5206\u6570\u548c\u53ef\u9a8c\u8bc1\u5956\u52b1\u6765\u6307\u5bfc\u63a2\u7d22\u8fc7\u7a0b\uff0c\u89e3\u51b3\u73b0\u6709RLVR\u65b9\u6cd5\u4e2d\u7684\u7c97\u7c92\u5ea6\u5956\u52b1\u3001\u5956\u52b1\u566a\u58f0\u548c\u4f4e\u6548\u63a2\u7d22\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5b58\u5728\u7c97\u7c92\u5ea6\u5956\u52b1\u3001\u5956\u52b1\u566a\u58f0\u548c\u4f4e\u6548\u63a2\u7d22\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u71b5\u5d29\u6e83\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u589e\u5f3aLLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "ICPO\u65b9\u6cd5\u8ba1\u7b97\u6bcf\u4e2a\u54cd\u5e94\u7684\u504f\u597d\u4f18\u52bf\u5206\u6570\uff0c\u901a\u8fc7\u6bd4\u8f83\u540c\u4e00\u8f93\u5165\u63d0\u793a\u4e0b\u591a\u4e2a\u54cd\u5e94\u7684\u76f8\u5bf9\u751f\u6210\u6982\u7387\uff0c\u5e76\u5c06\u8be5\u5206\u6570\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7ed3\u5408\u6765\u6307\u5bfc\u63a2\u7d22\u8fc7\u7a0b\u3002", "result": "\u5728\u56db\u4e2a\u901a\u7528\u9886\u57df\u57fa\u51c6\u548c\u4e09\u4e2a\u6570\u5b66\u57fa\u51c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cICPO\u76f8\u6bd4GRPO\u80fd\u7a33\u5b9a\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u504f\u597d\u4f18\u52bf\u5206\u6570\u4e0d\u4ec5\u80fd\u7f13\u89e3\u7c97\u7c92\u5ea6\u5956\u52b1\u548c\u5956\u52b1\u566a\u58f0\u95ee\u9898\uff0c\u8fd8\u80fd\u6709\u6548\u6291\u5236\u8fc7\u5ea6\u81ea\u4fe1\u9519\u8bef\uff0c\u589e\u5f3a\u88ab\u4f4e\u4f30\u9ad8\u8d28\u91cf\u54cd\u5e94\u7684\u76f8\u5bf9\u4f18\u52bf\uff0c\u9632\u6b62\u6a21\u578b\u8fc7\u5ea6\u62df\u5408\u7279\u5b9a\u7b56\u7565\uff0c\u4fc3\u8fdb\u66f4\u5f7b\u5e95\u7684\u63a2\u7d22\u3002"}}
{"id": "2511.20809", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.20809", "abs": "https://arxiv.org/abs/2511.20809", "authors": ["Ozgur Kara", "Yujia Chen", "Ming-Hsuan Yang", "James M. Rehg", "Wen-Sheng Chu", "Du Tran"], "title": "Layer-Aware Video Composition via Split-then-Merge", "comment": "Project Webpage: https://split-then-merge.github.io", "summary": "We present Split-then-Merge (StM), a novel framework designed to enhance control in generative video composition and address its data scarcity problem. Unlike conventional methods relying on annotated datasets or handcrafted rules, StM splits a large corpus of unlabeled videos into dynamic foreground and background layers, then self-composes them to learn how dynamic subjects interact with diverse scenes. This process enables the model to learn the complex compositional dynamics required for realistic video generation. StM introduces a novel transformation-aware training pipeline that utilizes a multi-layer fusion and augmentation to achieve affordance-aware composition, alongside an identity-preservation loss that maintains foreground fidelity during blending. Experiments show StM outperforms SoTA methods in both quantitative benchmarks and in humans/VLLM-based qualitative evaluations. More details are available at our project page: https://split-then-merge.github.io", "AI": {"tldr": "\u63d0\u51faSplit-then-Merge (StM)\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u672a\u6807\u6ce8\u89c6\u9891\u5206\u5272\u4e3a\u524d\u666f\u548c\u80cc\u666f\u5c42\u5e76\u8fdb\u884c\u81ea\u7ec4\u5408\u5b66\u4e60\uff0c\u89e3\u51b3\u751f\u6210\u89c6\u9891\u5408\u6210\u7684\u63a7\u5236\u95ee\u9898\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u96c6\u6216\u624b\u5de5\u89c4\u5219\uff0c\u96be\u4ee5\u5b66\u4e60\u52a8\u6001\u4e3b\u4f53\u4e0e\u591a\u6837\u5316\u573a\u666f\u7684\u590d\u6742\u4ea4\u4e92\u5173\u7cfb\uff0c\u4e14\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "method": "\u5c06\u672a\u6807\u6ce8\u89c6\u9891\u5206\u5272\u4e3a\u52a8\u6001\u524d\u666f\u548c\u80cc\u666f\u5c42\uff0c\u901a\u8fc7\u81ea\u7ec4\u5408\u5b66\u4e60\u4e3b\u4f53\u4e0e\u573a\u666f\u7684\u4ea4\u4e92\uff1b\u91c7\u7528\u53d8\u6362\u611f\u77e5\u8bad\u7ec3\u7ba1\u9053\uff0c\u7ed3\u5408\u591a\u5c42\u878d\u5408\u589e\u5f3a\u548c\u8eab\u4efd\u4fdd\u6301\u635f\u5931\uff0c\u5b9e\u73b0\u611f\u77e5\u80fd\u529b\u611f\u77e5\u7684\u5408\u6210\u3002", "result": "\u5728\u5b9a\u91cf\u57fa\u51c6\u6d4b\u8bd5\u548c\u4eba\u7c7b/VLLM\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "StM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u89c6\u9891\u5408\u6210\u7684\u63a7\u5236\u6311\u6218\uff0c\u901a\u8fc7\u5b66\u4e60\u524d\u666f\u80cc\u666f\u7684\u4ea4\u4e92\u5173\u7cfb\u5b9e\u73b0\u4e86\u66f4\u771f\u5b9e\u7684\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2511.21033", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21033", "abs": "https://arxiv.org/abs/2511.21033", "authors": ["Linze Chen", "Yufan Cai", "Zhe Hou", "Jinsong Dong"], "title": "Towards Trustworthy Legal AI through LLM Agents and Formal Reasoning", "comment": null, "summary": "The rationality of law manifests in two forms: substantive rationality, which concerns the fairness or moral desirability of outcomes, and formal rationality, which requires legal decisions to follow explicitly stated, general, and logically coherent rules. Existing LLM-based systems excel at surface-level text analysis but lack the guarantees required for principled jurisprudence. We introduce L4M, a novel framework that combines adversarial LLM agents with SMT-solver-backed proofs to unite the interpretive flexibility of natural language with the rigor of symbolic verification. The pipeline consists of three phases: (1) Statute Formalization, where domain-specific prompts convert legal provisions into logical formulae; (2) Dual Fact and Statute Extraction, in which prosecutor- and defense-aligned LLMs independently map case narratives to fact tuples and statutes, ensuring role isolation; and (3) Solver-Centric Adjudication, where an autoformalizer compiles both parties' arguments into logic constraints, and unsat cores trigger iterative self-critique until a satisfiable formula is achieved, which is then verbalized by a Judge-LLM into a transparent verdict and optimized sentence. Experimental results on public benchmarks show that our system surpasses advanced LLMs including GPT-o4-mini, DeepSeek-V3, and Claude 4 as well as state-of-the-art Legal AI baselines, while providing rigorous and explainable symbolic justifications.", "AI": {"tldr": "L4M\u662f\u4e00\u4e2a\u7ed3\u5408\u5bf9\u6297\u6027LLM\u4ee3\u7406\u548cSMT\u6c42\u89e3\u5668\u7684\u6cd5\u5f8b\u63a8\u7406\u6846\u67b6\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u7684\u7075\u6d3b\u6027\u4e0e\u7b26\u53f7\u9a8c\u8bc1\u7684\u4e25\u8c28\u6027\u76f8\u7ed3\u5408\uff0c\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u5148\u8fdbLLM\u548c\u6cd5\u5f8bAI\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u64c5\u957f\u8868\u9762\u6587\u672c\u5206\u6790\uff0c\u4f46\u7f3a\u4e4f\u539f\u5219\u6027\u6cd5\u7406\u5b66\u6240\u9700\u7684\u4fdd\u8bc1\u3002\u6cd5\u5f8b\u7406\u6027\u4f53\u73b0\u5728\u5b9e\u8d28\u7406\u6027\uff08\u7ed3\u679c\u516c\u5e73\u6027\uff09\u548c\u5f62\u5f0f\u7406\u6027\uff08\u9075\u5faa\u660e\u786e\u89c4\u5219\uff09\u4e24\u65b9\u9762\uff0c\u9700\u8981\u5c06\u81ea\u7136\u8bed\u8a00\u7684\u89e3\u91ca\u7075\u6d3b\u6027\u4e0e\u7b26\u53f7\u9a8c\u8bc1\u7684\u4e25\u8c28\u6027\u7edf\u4e00\u8d77\u6765\u3002", "method": "\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u6cd5\u89c4\u5f62\u5f0f\u5316\uff1a\u5c06\u6cd5\u5f8b\u6761\u6b3e\u8f6c\u6362\u4e3a\u903b\u8f91\u516c\u5f0f\uff1b2) \u53cc\u91cd\u4e8b\u5b9e\u548c\u6cd5\u89c4\u63d0\u53d6\uff1a\u68c0\u5bdf\u5b98\u548c\u8fa9\u62a4\u65b9\u5bf9\u9f50\u7684LLM\u72ec\u7acb\u4ece\u6848\u4f8b\u53d9\u8ff0\u4e2d\u63d0\u53d6\u4e8b\u5b9e\u5143\u7ec4\u548c\u6cd5\u89c4\uff1b3) \u6c42\u89e3\u5668\u4e2d\u5fc3\u88c1\u51b3\uff1a\u5c06\u53cc\u65b9\u8bba\u8bc1\u7f16\u8bd1\u4e3a\u903b\u8f91\u7ea6\u675f\uff0c\u901a\u8fc7\u4e0d\u6ee1\u8db3\u6838\u5fc3\u89e6\u53d1\u8fed\u4ee3\u81ea\u6211\u6279\u8bc4\uff0c\u6700\u7ec8\u7531\u6cd5\u5b98LLM\u5c06\u53ef\u6ee1\u8db3\u516c\u5f0f\u8f6c\u5316\u4e3a\u900f\u660e\u5224\u51b3\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u7cfb\u7edf\u8d85\u8d8a\u4e86GPT-4-mini\u3001DeepSeek-V3\u3001Claude 4\u7b49\u5148\u8fdbLLM\u4ee5\u53ca\u6700\u5148\u8fdb\u7684\u6cd5\u5f8bAI\u57fa\u7ebf\uff0c\u540c\u65f6\u63d0\u4f9b\u4e25\u8c28\u4e14\u53ef\u89e3\u91ca\u7684\u7b26\u53f7\u5316\u7406\u7531\u3002", "conclusion": "L4M\u6846\u67b6\u6210\u529f\u5730\u5c06\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u7075\u6d3b\u6027\u4e0e\u7b26\u53f7\u9a8c\u8bc1\u7684\u4e25\u8c28\u6027\u76f8\u7ed3\u5408\uff0c\u4e3a\u6cd5\u5f8b\u63a8\u7406\u63d0\u4f9b\u4e86\u65e2\u5177\u6709\u89e3\u91ca\u7075\u6d3b\u6027\u53c8\u5177\u5907\u5f62\u5f0f\u5316\u4fdd\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cd5\u5f8bAI\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2511.20814", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20814", "abs": "https://arxiv.org/abs/2511.20814", "authors": ["Md Tanvirul Alam", "Saksham Aggarwal", "Justin Yang Chae", "Nidhi Rastogi"], "title": "SPHINX: A Synthetic Environment for Visual Perception and Reasoning", "comment": null, "summary": "We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.", "AI": {"tldr": "Sphinx\u662f\u4e00\u4e2a\u9488\u5bf9\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u6838\u5fc3\u8ba4\u77e5\u539f\u8bed\u7684\u5408\u6210\u73af\u5883\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5316\u751f\u6210\u5305\u542b\u56fe\u6848\u3001\u56fe\u5757\u3001\u56fe\u8868\u3001\u56fe\u6807\u548c\u51e0\u4f55\u539f\u8bed\u7684\u8c1c\u9898\uff0c\u5e76\u914d\u6709\u53ef\u9a8c\u8bc1\u7684\u771f\u5b9e\u89e3\uff0c\u652f\u6301\u7cbe\u786e\u8bc4\u4f30\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6784\u5efa\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u7f3a\u4e4f\u80fd\u591f\u7cbe\u786e\u8bc4\u4f30\u6838\u5fc3\u8ba4\u77e5\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u7cfb\u7edf\u8bc4\u4f30\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u7684\u5408\u6210\u73af\u5883\u3002", "method": "\u4f7f\u7528\u7a0b\u5e8f\u5316\u751f\u6210\u65b9\u6cd5\u521b\u5efa\u5305\u542b25\u79cd\u4efb\u52a1\u7c7b\u578b\u7684\u8c1c\u9898\uff0c\u6db5\u76d6\u5bf9\u79f0\u68c0\u6d4b\u3001\u51e0\u4f55\u53d8\u6362\u3001\u7a7a\u95f4\u63a8\u7406\u3001\u56fe\u8868\u89e3\u91ca\u548c\u5e8f\u5217\u9884\u6d4b\u7b49\u8ba4\u77e5\u539f\u8bed\uff0c\u6bcf\u4e2a\u8c1c\u9898\u90fd\u914d\u6709\u53ef\u9a8c\u8bc1\u7684\u771f\u5b9e\u89e3\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5373\u4f7f\u662f\u5f53\u524d\u6700\u5148\u8fdb\u7684GPT-5\u6a21\u578b\u5728Sphinx\u57fa\u51c6\u4e0a\u4ec5\u8fbe\u523051.1%\u51c6\u786e\u7387\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u8868\u73b0\u3002\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u6027\u3002", "conclusion": "Sphinx\u57fa\u51c6\u6709\u6548\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6838\u5fc3\u8ba4\u77e5\u80fd\u529b\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u800cRLVR\u65b9\u6cd5\u5c55\u793a\u4e86\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.21064", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21064", "abs": "https://arxiv.org/abs/2511.21064", "authors": ["Chujie Wang", "Jianyu Lu", "Zhiyuan Luo", "Xi Chen", "Chu He"], "title": "OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection", "comment": null, "summary": "Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD's lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent's state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.", "AI": {"tldr": "OVOD-Agent\u5c06\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u4ece\u88ab\u52a8\u7684\u7c7b\u522b\u5339\u914d\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u7684\u89c6\u89c9\u63a8\u7406\u548c\u81ea\u6211\u8fdb\u5316\u68c0\u6d4b\uff0c\u901a\u8fc7\u89c6\u89c9\u601d\u7ef4\u94fe\u548c\u5f31\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709OVOD\u65b9\u6cd5\u867d\u7136\u5728\u591a\u6a21\u6001\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u4f46\u63a8\u7406\u4ecd\u5c40\u9650\u4e8e\u56fa\u5b9a\u7c7b\u522b\u540d\u79f0\uff0c\u5bfc\u81f4\u591a\u6a21\u6001\u8bad\u7ec3\u4e0e\u5355\u6a21\u6001\u63a8\u7406\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002\u6587\u672c\u7a7a\u95f4\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002", "method": "\u63d0\u51faOVOD-Agent\u6846\u67b6\uff0c\u91c7\u7528\u89c6\u89c9\u601d\u7ef4\u94fe\u8fdb\u884c\u53ef\u89e3\u91ca\u63a8\u7406\uff0c\u5c06\u89c6\u89c9\u4e0a\u4e0b\u6587\u5efa\u6a21\u4e3a\u5f31\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5305\u542bBandit\u6a21\u5757\u751f\u6210\u63a2\u7d22\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u5956\u52b1\u6a21\u578b\u4f18\u5316\u5f62\u6210\u95ed\u73af\u3002", "result": "\u5728COCO\u548cLVIS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOVOD-Agent\u5728\u5404\u79cdOVOD\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5747\u80fd\u63d0\u4f9b\u4e00\u81f4\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u7a00\u6709\u7c7b\u522b\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "OVOD-Agent\u6846\u67b6\u901a\u8fc7\u4e3b\u52a8\u89c6\u89c9\u63a8\u7406\u548c\u81ea\u6211\u8fdb\u5316\u68c0\u6d4b\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.20821", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20821", "abs": "https://arxiv.org/abs/2511.20821", "authors": ["Samuele Dell'Erba", "Andrew D. Bagdanov"], "title": "Training-Free Diffusion Priors for Text-to-Image Generation via Optimization-based Visual Inversion", "comment": "11 pages, 7 figures, technical report (preprint)", "summary": "Diffusion models have established the state-of-the-art in text-to-image generation, but their performance often relies on a diffusion prior network to translate text embeddings into the visual manifold for easier decoding. These priors are computationally expensive and require extensive training on massive datasets. In this work, we challenge the necessity of a trained prior at all by employing Optimization-based Visual Inversion (OVI), a training-free and data-free alternative, to replace the need for a prior. OVI initializes a latent visual representation from random pseudo-tokens and iteratively optimizes it to maximize the cosine similarity with input textual prompt embedding. We further propose two novel constraints, a Mahalanobis-based and a Nearest-Neighbor loss, to regularize the OVI optimization process toward the distribution of realistic images. Our experiments, conducted on Kandinsky 2.2, show that OVI can serve as an alternative to traditional priors. More importantly, our analysis reveals a critical flaw in current evaluation benchmarks like T2I-CompBench++, where simply using the text embedding as a prior achieves surprisingly high scores, despite lower perceptual quality. Our constrained OVI methods improve visual fidelity over this baseline, with the Nearest-Neighbor approach proving particularly effective, achieving quantitative scores comparable to or higher than the state-of-the-art data-efficient prior, indicating that the idea merits further investigation. The code will be publicly available upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u89c6\u89c9\u53cd\u8f6c(OVI)\u65b9\u6cd5\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u66ff\u4ee3\u4f20\u7edf\u6269\u6563\u5148\u9a8c\u7f51\u7edc\uff0c\u901a\u8fc7\u4f18\u5316\u6f5c\u5728\u89c6\u89c9\u8868\u793a\u6765\u5339\u914d\u6587\u672c\u5d4c\u5165\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u7ea6\u675f\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u4f9d\u8d56\u8ba1\u7b97\u6602\u8d35\u7684\u5148\u9a8c\u7f51\u7edc\uff0c\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u3002\u672c\u7814\u7a76\u6311\u6218\u8fd9\u79cd\u5fc5\u8981\u6027\uff0c\u5bfb\u6c42\u65e0\u9700\u8bad\u7ec3\u548c\u6570\u636e\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4f18\u5316\u89c6\u89c9\u53cd\u8f6c(OVI)\u65b9\u6cd5\uff0c\u4ece\u968f\u673a\u4f2a\u6807\u8bb0\u521d\u59cb\u5316\u6f5c\u5728\u8868\u793a\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u6700\u5927\u5316\u4e0e\u6587\u672c\u5d4c\u5165\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3002\u63d0\u51fa\u9a6c\u6c0f\u8ddd\u79bb\u548c\u6700\u8fd1\u90bb\u635f\u5931\u4e24\u79cd\u7ea6\u675f\u6765\u6b63\u5219\u5316\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u5728Kandinsky 2.2\u4e0a\u7684\u5b9e\u9a8c\u8868\u660eOVI\u53ef\u66ff\u4ee3\u4f20\u7edf\u5148\u9a8c\u3002\u5206\u6790\u53d1\u73b0\u5f53\u524d\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u7f3a\u9677\uff0c\u6587\u672c\u5d4c\u5165\u76f4\u63a5\u4f5c\u4e3a\u5148\u9a8c\u4e5f\u80fd\u83b7\u5f97\u9ad8\u5206\u4f46\u611f\u77e5\u8d28\u91cf\u8f83\u4f4e\u3002\u7ea6\u675fOVI\u65b9\u6cd5\u63d0\u5347\u4e86\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u6700\u8fd1\u90bb\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "OVI\u4f5c\u4e3a\u4f20\u7edf\u5148\u9a8c\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u662f\u6700\u8fd1\u90bb\u7ea6\u675f\u65b9\u6cd5\u6548\u679c\u663e\u8457\uff0c\u8868\u660e\u8fd9\u4e00\u601d\u8def\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2511.21260", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21260", "abs": "https://arxiv.org/abs/2511.21260", "authors": ["Joseph Y. Halpern", "Rafael Pass"], "title": "Causality Without Causal Models", "comment": "In Proceedings TARK 2025, arXiv:2511.20540", "summary": "Perhaps the most prominent current definition of (actual) causality is due to Halpern and Pearl.  It is defined using causal models (also known as structural equations models).  We abstract the definition, extracting its key features, so that it can be applied to any other model where counterfactuals are defined. By abstracting the definition, we gain a number of benefits. Not only can we apply the definition in a wider range of models, including ones that allow, for example, backtracking, but we can apply the definition to determine if A is a cause of B  even if A and B are formulas involving disjunctions, negations, beliefs, and nested counterfactuals (none of which can be handled by the Halpern-Pearl definition). Moreover, we can extend the ideas to getting an abstract definition of explanation that can be applied beyond causal models. Finally, we gain a deeper understanding of features of the definition  even in causal models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Halpern-Pearl\u56e0\u679c\u5b9a\u4e49\u7684\u62bd\u8c61\u5316\u7248\u672c\uff0c\u4f7f\u5176\u80fd\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u6a21\u578b\uff0c\u5305\u62ec\u5904\u7406\u6790\u53d6\u3001\u5426\u5b9a\u3001\u4fe1\u5ff5\u548c\u5d4c\u5957\u53cd\u4e8b\u5b9e\u7b49\u590d\u6742\u60c5\u51b5\u3002", "motivation": "Halpern-Pearl\u7684\u56e0\u679c\u5b9a\u4e49\u5c40\u9650\u4e8e\u56e0\u679c\u6a21\u578b\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u903b\u8f91\u516c\u5f0f\u548c\u66f4\u5e7f\u6cdb\u7684\u6a21\u578b\u7c7b\u578b\uff0c\u9700\u8981\u62bd\u8c61\u5316\u4ee5\u6269\u5c55\u5e94\u7528\u8303\u56f4\u3002", "method": "\u901a\u8fc7\u63d0\u53d6Halpern-Pearl\u56e0\u679c\u5b9a\u4e49\u7684\u5173\u952e\u7279\u5f81\uff0c\u6784\u5efa\u62bd\u8c61\u5316\u5b9a\u4e49\u6846\u67b6\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u4efb\u4f55\u5b9a\u4e49\u4e86\u53cd\u4e8b\u5b9e\u7684\u6a21\u578b\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u80fd\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u6a21\u578b\u7684\u56e0\u679c\u5b9a\u4e49\uff0c\u5305\u62ec\u652f\u6301\u56de\u6eaf\u7684\u6a21\u578b\uff0c\u5e76\u80fd\u5904\u7406\u6790\u53d6\u3001\u5426\u5b9a\u3001\u4fe1\u5ff5\u548c\u5d4c\u5957\u53cd\u4e8b\u5b9e\u7b49\u590d\u6742\u903b\u8f91\u516c\u5f0f\u3002", "conclusion": "\u62bd\u8c61\u5316\u65b9\u6cd5\u4e0d\u4ec5\u6269\u5c55\u4e86\u56e0\u679c\u5b9a\u4e49\u7684\u5e94\u7528\u8303\u56f4\uff0c\u8fd8\u6df1\u5316\u4e86\u5bf9\u56e0\u679c\u5b9a\u4e49\u7279\u5f81\u7684\u7406\u89e3\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u89e3\u91ca\u6982\u5ff5\u7684\u5b9a\u4e49\u3002"}}
