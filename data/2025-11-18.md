<div id=toc></div>

# Table of Contents

- [cs.NE](#cs.NE) [Total: 6]
- [cs.CV](#cs.CV) [Total: 296]
- [cs.AI](#cs.AI) [Total: 82]


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [1] [Phase-Coded Memory and Morphological Resonance: A Next-Generation Retrieval-Augmented Generator Architecture](https://arxiv.org/abs/2511.11848)
*Denis V. Saklakov*

Main category: cs.NE

TL;DR: 提出了一种基于相位编码记忆和形态语义共振的认知RAG架构，通过将意义编码为具有幅度-相位结构的复杂波模式来超越transformer的上下文长度限制。


<details>
  <summary>Details</summary>
Motivation: 解决transformer模型在上下文长度方面的限制，消除顺序token依赖性，大幅减少内存和计算开销，通过基于频率的语义访问实现无限有效上下文。

Method: 采用三层设计：形态映射器将输入转换为语义波形；场记忆层以分布式全息痕迹存储知识并通过相位干涉检索；非上下文生成器通过共振而非固定上下文产生连贯输出。

Result: 该方法消除了顺序token依赖，大大减少了内存和计算开销，通过基于频率的语义访问实现了无限有效上下文。

Conclusion: 该认知RAG架构在能量、存储和时间方面实现了显著节省，为超越传统transformer限制提供了新的理论框架和实现方法。

Abstract: This paper introduces a cognitive Retrieval-Augmented Generator (RAG) architecture that transcends transformer context-length limitations through phase-coded memory and morphological-semantic resonance. Instead of token embeddings, the system encodes meaning as complex wave patterns with amplitude-phase structure. A three-tier design is presented: a Morphological Mapper that transforms inputs into semantic waveforms, a Field Memory Layer that stores knowledge as distributed holographic traces and retrieves it via phase interference, and a Non-Contextual Generator that produces coherent output guided by resonance rather than fixed context. This approach eliminates sequential token dependence, greatly reduces memory and computational overhead, and enables unlimited effective context through frequency-based semantic access. The paper outlines theoretical foundations, pseudocode implementation, and experimental evidence from related complex-valued neural models, emphasizing substantial energy, storage, and time savings.

</details>


### [2] [Benchmarking that Matters: Rethinking Benchmarking for Practical Impact](https://arxiv.org/abs/2511.12264)
*Anna V. Kononova,Niki van Stein,Olaf Mersmann,Thomas Bäck,Thomas Bartz-Beielstein,Tobias Glasmachers,Michael Hellwig,Sebastian Krey,Jakub Kůdela,Boris Naujoks,Leonard Papenmeier,Elena Raponi,Quentin Renau,Jeroen Rook,Lennart Schäpermeier,Diederick Vermetten,Daniela Zaharie*

Main category: cs.NE

TL;DR: 当前进化计算基准测试与现实需求存在脱节，需要建立以真实世界问题为核心的动态基准测试生态系统


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试套件（如BBOB和CEC）虽然能隔离算法现象，但无法反映实际连续和混合整数优化问题的结构、约束和信息限制，导致基准测试被误用于竞赛、自动算法选择和工业决策

Method: 提出以精心策划的真实世界启发基准为核心，配备从业者可访问的特征空间和社区维护的性能数据库的愿景

Result: 识别出现有基准测试实践和工具的关键差距，包括真实世界启发问题有限、高级特征缺失、多目标和噪声设置挑战等

Conclusion: 真正的进展需要协调努力：建立一个随着真实世界洞察而演化的动态基准测试生态系统，同时支持科学理解和工业应用

Abstract: Benchmarking has driven scientific progress in Evolutionary Computation, yet current practices fall short of real-world needs. Widely used synthetic suites such as BBOB and CEC isolate algorithmic phenomena but poorly reflect the structure, constraints, and information limitations of continuous and mixed-integer optimization problems in practice. This disconnect leads to the misuse of benchmarking suites for competitions, automated algorithm selection, and industrial decision-making, despite these suites being designed for different purposes.
  We identify key gaps in current benchmarking practices and tooling, including limited availability of real-world-inspired problems, missing high-level features, and challenges in multi-objective and noisy settings. We propose a vision centered on curated real-world-inspired benchmarks, practitioner-accessible feature spaces and community-maintained performance databases. Real progress requires coordinated effort: A living benchmarking ecosystem that evolves with real-world insights and supports both scientific understanding and industrial use.

</details>


### [3] [Random-Key Metaheuristic and Linearization for the Quadratic Multiple Constraints Variable-Sized Bin Packing Problem](https://arxiv.org/abs/2511.12367)
*Natalia A. Santos,Marlon Jeske,Antonio A. Chaves*

Main category: cs.NE

TL;DR: 提出了两种解决QMC-VSBPP问题的方法：线性化数学公式和RKO-ACO算法，显著提升了求解质量和效率。


<details>
  <summary>Details</summary>
Motivation: 解决具有多维度容量、异构容器类型和项目间二次交互成本的复杂组合优化问题，推进该领域的研究水平。

Method: 1) 引入线性化数学公式消除二次项，使用Gurobi等精确求解器计算强下界；2) 在随机密钥优化框架内开发连续域蚁群优化算法RKO-ACO，结合自适应Q学习参数控制和高效局部搜索。

Result: 线性化模型产生比原始二次公式更紧的下界，RKO-ACO算法在所有基准实例上匹配或改进了文献中的最佳解，为大规模实例建立了新的上界。

Conclusion: 为未来研究提供了新的参考值，证明了进化和随机密钥元启发式方法在解决复杂二次包装问题中的有效性。

Abstract: This paper addresses the Quadratic Multiple Constraints Variable-Sized Bin Packing Problem (QMC-VSBPP), a challenging combinatorial optimization problem that generalizes the classical bin packing by incorporating multiple capacity dimensions, heterogeneous bin types, and quadratic interaction costs between items. We propose two complementary methods that advance the current state-of-the-art. First, a linearized mathematical formulation is introduced to eliminate quadratic terms, enabling the use of exact solvers such as Gurobi to compute strong lower bounds - reported here for the first time for this problem. Second, we develop RKO-ACO, a continuous-domain Ant Colony Optimization algorithm within the Random-Key Optimization framework, enhanced with adaptive Q-learning parameter control and efficient local search. Extensive computational experiments on benchmark instances show that the proposed linearized model produces significantly tighter lower bounds than the original quadratic formulation, while RKO-ACO consistently matches or improves upon all best-known solutions in the literature, establishing new upper bounds for large-scale instances. These results provide new reference values for future studies and demonstrate the effectiveness of evolutionary and random-key metaheuristic approaches for solving complex quadratic packing problems. Source code and data available at https://github.com/nataliaalves03/RKO-ACO

</details>


### [4] [Evolving Prompts for Toxicity Search in Large Language Models](https://arxiv.org/abs/2511.12487)
*Onkar Shelar,Travis Desell*

Main category: cs.NE

TL;DR: ToxSearch是一个黑盒进化框架，通过进化提示词来测试大语言模型的安全性，发现即使经过安全对齐的模型仍容易受到对抗性提示的攻击。


<details>
  <summary>Details</summary>
Motivation: 大语言模型即使经过安全对齐后，仍然容易受到对抗性提示的攻击而生成有害内容，需要系统性的红队测试方法来评估模型安全性。

Method: 采用黑盒进化框架，在同步稳态循环中进化提示词，使用多种操作符（词汇替换、否定、回译、改写和两种语义交叉操作符），并通过审核预言机提供适应性指导。

Result: 词汇替换操作符在产量-方差权衡方面表现最佳，语义相似性交叉作为精确的低吞吐量插入器，全局改写表现出高方差但拒绝成本较高。在LLaMA 3.1 8B上进化的精英提示词在跨模型迁移中观察到毒性大约减半，较小的LLaMA 3.2变体表现出最强的抵抗性。

Conclusion: 小的可控扰动是系统性红队测试的有效载体，防御措施应预期对抗性提示的跨模型重用，而不仅仅是单一模型的硬化。

Abstract: Large Language Models remain vulnerable to adversarial prompts that elicit toxic content even after safety alignment. We present ToxSearch, a black-box evolutionary framework that tests model safety by evolving prompts in a synchronous steady-state loop. The system employs a diverse set of operators, including lexical substitutions, negation, back-translation, paraphrasing, and two semantic crossover operators, while a moderation oracle provides fitness guidance. Operator-level analysis shows heterogeneous behavior: lexical substitutions offer the best yield-variance trade-off, semantic-similarity crossover acts as a precise low-throughput inserter, and global rewrites exhibit high variance with elevated refusal costs. Using elite prompts evolved on LLaMA 3.1 8B, we observe practically meaningful but attenuated cross-model transfer, with toxicity roughly halving on most targets, smaller LLaMA 3.2 variants showing the strongest resistance, and some cross-architecture models retaining higher toxicity. These results suggest that small, controllable perturbations are effective vehicles for systematic red-teaming and that defenses should anticipate cross-model reuse of adversarial prompts rather than focusing only on single-model hardening.

</details>


### [5] [On Counts and Densities of Homogeneous Bent Functions: An Evolutionary Approach](https://arxiv.org/abs/2511.12652)
*Claude Carlet,Marko Ðurasevic,Domagoj Jakobovic,Luca Mariot,Stjepan Picek,Alexandr Polujan*

Main category: cs.NE

TL;DR: 使用进化算法演化齐次bent布尔函数，引入齐次bent函数密度概念，成功找到不同变量数的二次和三次bent函数


<details>
  <summary>Details</summary>
Motivation: 具有强密码学性质（如高非线性和代数次数）的布尔函数对密码系统安全至关重要，需要探索代数构造和元启发式方法的设计

Method: 采用进化算法演化齐次bent布尔函数，引入齐次bent函数密度概念来辅助算法设计

Result: 成功找到不同变量数的二次和三次bent函数

Conclusion: 进化算法可以有效用于演化齐次bent布尔函数，密度概念有助于算法设计

Abstract: Boolean functions with strong cryptographic properties, such as high nonlinearity and algebraic degree, are important for the security of stream and block ciphers. These functions can be designed using algebraic constructions or metaheuristics. This paper examines the use of Evolutionary Algorithms (EAs) to evolve homogeneous bent Boolean functions, that is, functions whose algebraic normal form contains only monomials of the same degree and that are maximally nonlinear. We introduce the notion of density of homogeneous bent functions, facilitating the algorithmic design that results in finding quadratic and cubic bent functions in different numbers of variables.

</details>


### [6] [DS-ATGO: Dual-Stage Synergistic Learning via Forward Adaptive Threshold and Backward Gradient Optimization for Spiking Neural Networks](https://arxiv.org/abs/2511.13050)
*Jiaqiang Jiang,Wenfeng Xu,Jing Fan,Rui Yan*

Main category: cs.NE

TL;DR: 提出一种双阶段协同学习算法，通过前向自适应阈值和后向动态替代梯度，解决SNN训练中膜电位分布变化导致的梯度信号减弱问题。


<details>
  <summary>Details</summary>
Motivation: SNN直接训练中，神经元膜电位分布随时间变化并偏离发放阈值，导致发放不平衡和梯度信号减弱，影响网络性能。

Method: 前向传播中基于膜电位动态分布自适应调整阈值，后向传播中动态优化替代梯度以增强梯度估计，实现时空对齐。

Result: 实验结果显示性能显著提升，神经元在每个时间步发放稳定比例的脉冲，深层神经元获得梯度的比例增加。

Conclusion: 该方法通过自适应阈值和动态替代梯度有效平衡了SNN的脉冲发放，缓解了梯度信息损失问题。

Abstract: Brain-inspired spiking neural networks (SNNs) are recognized as a promising avenue for achieving efficient, low-energy neuromorphic computing. Direct training of SNNs typically relies on surrogate gradient (SG) learning to estimate derivatives of non-differentiable spiking activity. However, during training, the distribution of neuronal membrane potentials varies across timesteps and progressively deviates toward both sides of the firing threshold. When the firing threshold and SG remain fixed, this may lead to imbalanced spike firing and diminished gradient signals, preventing SNNs from performing well. To address these issues, we propose a novel dual-stage synergistic learning algorithm that achieves forward adaptive thresholding and backward dynamic SG. In forward propagation, we adaptively adjust thresholds based on the distribution of membrane potential dynamics (MPD) at each timestep, which enriches neuronal diversity and effectively balances firing rates across timesteps and layers. In backward propagation, drawing from the underlying association between MPD, threshold, and SG, we dynamically optimize SG to enhance gradient estimation through spatio-temporal alignment, effectively mitigating gradient information loss. Experimental results demonstrate that our method achieves significant performance improvements. Moreover, it allows neurons to fire stable proportions of spikes at each timestep and increases the proportion of neurons that obtain gradients in deeper layers.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [7] [Psychological stress during Examination and its estimation by handwriting in answer script](https://arxiv.org/abs/2511.11633)
*Abhijeet Kumar,Chetan Agarwal,Pronoy B. Neogi,Mayank Goswami*

Main category: cs.CV

TL;DR: 该研究结合笔迹学和人工智能，通过分析学生手写考试试卷来量化心理压力水平，使用OCR和基于Transformer的情感分析模型，提供超越传统评分系统的认知和情感状态洞察。


<details>
  <summary>Details</summary>
Motivation: 传统评分系统无法深入了解学生在考试期间的认知和情感状态，本研究旨在通过量化分析手写笔迹来评估学生的心理压力水平，为学术取证提供创新方法。

Method: 集成高分辨率图像处理、TrOCR和基于RoBERTa模型的情感熵融合，生成数值化压力指数；采用五模型投票机制和无监督异常检测确保鲁棒性。

Result: 开发了一个能够通过分析手写考试试卷来量化学生心理压力水平的创新框架，在学术取证领域具有应用价值。

Conclusion: 该方法为评估学生在考试期间的心理状态提供了数据驱动的创新途径，将笔迹学与人工智能技术相结合，在学术评估和心理监测方面具有重要潜力。

Abstract: This research explores the fusion of graphology and artificial intelligence to quantify psychological stress levels in students by analyzing their handwritten examination scripts. By leveraging Optical Character Recognition and transformer based sentiment analysis models, we present a data driven approach that transcends traditional grading systems, offering deeper insights into cognitive and emotional states during examinations. The system integrates high resolution image processing, TrOCR, and sentiment entropy fusion using RoBERTa based models to generate a numerical Stress Index. Our method achieves robustness through a five model voting mechanism and unsupervised anomaly detection, making it an innovative framework in academic forensics.

</details>


### [8] [Real-time pothole detection with onboard sensors and camera on vehicles](https://arxiv.org/abs/2511.11643)
*Aswath Muthuselvam,Jeevak Raj S,Mohanaprasad K*

Main category: cs.CV

TL;DR: 使用车辆传感器和SVM分类器实时检测道路坑洞，在2公里路段上达到98.1%的准确率


<details>
  <summary>Details</summary>
Motivation: 道路状况对日常通勤至关重要，车辆数量增加需要频繁评估路况以确保交通顺畅，小裂缝可能发展成大坑洞

Method: 利用车辆车载传感器收集数据，采用SVM分类器进行坑洞检测

Result: 在2公里路段（包含26个坑洞）上实现了98.1%的检测准确率

Conclusion: 该方法能够有效实时检测道路坑洞，为大规模坑洞管理提供有用数据

Abstract: Road conditions play an important role in our everyday commute. With the proliferating number of vehicles on the road each year, it has become necessary to access the road conditions very frequently, this would ensure that the traffic also flows smoothly. Even the smallest crack in the road could be easily be chipped into a large pothole due to changing surface temperatures of the road and from the force of vehicles riding over it. In this paper, we have addressed how we could better identify these potholes in realtime with the help of onboard sensors in vehicles so that the data could be useful for analysis and better management of potholes on a large scale. For the implementation, we used an SVM classifier to detect potholes, we achieved 98.1% accuracy based on data collected from a local road for about 2 km which had 26 potholes distributed along the road. Code is available at: https://github.com/aswathselvam/Potholes

</details>


### [9] [A Method for Identifying Farmland System Habitat Types Based on the Dynamic-Weighted Feature Fusion Network Model](https://arxiv.org/abs/2511.11659)
*Kesong Zheng,Zhi Song,Peizhou Li,Shuyi Yao,Zhenxing Bian*

Main category: cs.CV

TL;DR: 本研究针对耕地生态系统缺乏标准化栖息地分类系统、栖息地类型覆盖不完整以及现有模型无法有效整合语义和纹理特征的问题，开发了包含15类耕地系统栖息地的超高分辨率遥感图像数据集，并提出了动态加权特征融合网络（DWFF-Net），在构建的数据集上取得了优于基线网络的性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏耕地生态系统的标准化栖息地分类系统，栖息地类型覆盖不完整，现有模型无法有效整合语义和纹理特征，导致多尺度栖息地（如大尺度田块和微栖息地）分割精度不足、边界模糊。

Method: 开发了包含15类耕地系统栖息地的超高分辨率遥感图像数据集；提出了动态加权特征融合网络（DWFF-Net），编码器使用冻结参数的DINOv3提取基础特征，引入数据级自适应动态加权策略进行特征融合，解码器包含动态权重计算网络实现多层特征充分融合，采用混合损失函数优化模型训练。

Result: 在构建的数据集上，所提模型实现了0.6979的平均交并比（mIoU）和0.8049的F1分数，分别比基线网络提高了0.021和0.0161。消融研究进一步证实了多层特征融合的互补性，有效提高了田埂等微栖息地类别的IoU。

Conclusion: 本研究建立了基于自适应多层特征融合的耕地系统栖息地识别框架，能够以低成本实现亚米级精度的栖息地制图，为耕地景观的细粒度栖息地监测提供了强有力的技术支持。

Abstract: Addressing the current lack of a standardized habitat classification system for cultivated land ecosystems, incomplete coverage of habitat types, and the inability of existing models to effectively integrate semantic and texture features-resulting in insufficient segmentation accuracy and blurred boundaries for multi-scale habitats (e.g., large-scale field plots and micro-habitats)-this study developed a comprehensively annotated ultra-high-resolution remote sensing image dataset encompassing 15 categories of cultivated land system habitats. Furthermore, we propose a Dynamic-Weighted Feature Fusion Network (DWFF-Net). The encoder of this model utilizes a frozen-parameter DINOv3 to extract foundational features. By analyzing the relationships between different category images and feature maps, we introduce a data-level adaptive dynamic weighting strategy for feature fusion. The decoder incorporates a dynamic weight computation network to achieve thorough integration of multi-layer features, and a hybrid loss function is adopted to optimize model training. Experimental results on the constructed dataset demonstrate that the proposed model achieves a mean Intersection over Union (mIoU) of 0.6979 and an F1-score of 0.8049, outperforming the baseline network by 0.021 and 0.0161, respectively. Ablation studies further confirm the complementary nature of multi-layer feature fusion, which effectively improves the IoU for micro-habitat categories such as field ridges. This study establishes a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at a low cost and providing robust technical support for fine-grained habitat monitoring in cultivated landscapes.

</details>


### [10] [AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2511.11662)
*Ziyuan Gao*

Main category: cs.CV

TL;DR: AGENet是一个用于医学图像少样本分割的新框架，通过边缘感知的测地距离学习来改进边界分割精度，特别适用于标注数据有限但需要精确边界分割的临床应用。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割需要大量标注数据，这在临床应用中形成了瓶颈。现有的少样本分割方法在医学图像边界分割上表现不佳，特别是当解剖结构相似且缺乏足够空间上下文时。

Method: AGENet结合三个主要组件：(1)边缘感知测地距离学习模块，通过迭代快速行进优化尊重解剖边界；(2)自适应原型提取，通过空间加权聚合捕获全局结构和局部边界细节；(3)自适应参数学习，自动适应不同器官特征。

Result: 在多个医学影像数据集上的广泛实验表明，该方法优于现有最先进方法，显著减少了边界误差，同时保持了计算效率。

Conclusion: AGENet为需要精确分割但标注数据有限的临床应用提供了一种高效解决方案，通过轻量级几何建模改进了医学图像少样本分割的边界精度。

Abstract: Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.

</details>


### [11] [EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance](https://arxiv.org/abs/2511.11700)
*Jiahui Wang,Haiyue Zhu,Haoren Guo,Abdullah Al Mamun,Cheng Xiang,Tong Heng Lee*

Main category: cs.CV

TL;DR: 提出EPSegFZ网络，无需预训练即可实现少样本和零样本3D点云语义分割，通过原型增强注意力、双相对位置编码和语言引导原型嵌入模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练阶段，限制了模型灵活性和适应性；同时未能充分利用支持集中的文本注释等信息，影响了少样本性能和零样本能力。

Method: 使用原型增强寄存器注意力模块和双相对位置编码的交叉注意力机制进行特征提取；引入语言引导原型嵌入模块利用文本信息；无需预训练。

Result: 在S3DIS和ScanNet基准测试中分别比最先进方法提升5.68%和3.82%。

Conclusion: EPSegFZ方法有效解决了预训练依赖问题，充分利用多模态信息，在少样本和零样本场景下均表现出色。

Abstract: Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.

</details>


### [12] [Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement](https://arxiv.org/abs/2511.11702)
*Lian He,Meng Liu,Qilang Ye,Yu Zhou,Xiang Deng,Gangyi Ding*

Main category: cs.CV

TL;DR: TASA是一个新颖的几何优化框架，通过联合利用2D语义线索和3D几何推理，以粗到细的方式实现3D场景级可供性分割，显著提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注对象级可供性或仅将2D预测提升到3D，忽略了点云中的丰富几何结构信息且计算成本高。需要解决语义推理和空间基础化的挑战。

Method: TASA框架包含任务感知的2D可供性检测模块和3D可供性细化模块。前者从语言和视觉输入识别可操作点，指导任务相关视图选择；后者整合2D语义先验与局部3D几何，生成准确且空间一致的3D可供性掩码。

Result: 在SceneFun3D数据集上的实验表明，TASA在场景级可供性分割的准确性和效率方面显著优于基线方法。

Conclusion: TASA通过几何优化的方法有效解决了3D场景级可供性分割问题，为具身智能体在复杂环境中的交互提供了有力支持。

Abstract: Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.

</details>


### [13] [LE-CapsNet: A Light and Enhanced Capsule Network](https://arxiv.org/abs/2511.11708)
*Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: LE-CapsNet是一个轻量级、增强版的胶囊网络，在CIFAR-10数据集上达到76.73%准确率，推理速度比CapsNet快4倍，在AffNIST数据集上达到94.3%准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管胶囊网络在检测重叠类别图像和变换图像方面优于CNN，但其速度慢、参数多、资源消耗大且准确率不如CNN。

Method: 提出LE-CapsNet作为CapsNet的轻量级增强变体，使用380万权重参数。

Result: 在CIFAR-10数据集上达到76.73%准确率，推理速度比CapsNet快4倍；在AffNIST数据集上达到94.3%准确率，优于CapsNet的90.52%。

Conclusion: LE-CapsNet是一个更轻量、更快、更准确的胶囊网络变体，对仿射变换图像具有更强的鲁棒性。

Abstract: Capsule Network (CapsNet) classifier has several advantages over CNNs, including better detection of images containing overlapping categories and higher accuracy on transformed images. Despite the advantages, CapsNet is slow due to its different structure. In addition, CapsNet is resource-hungry, includes many parameters and lags in accuracy compared to CNNs. In this work, we propose LE-CapsNet as a light, enhanced and more accurate variant of CapsNet. Using 3.8M weights, LECapsNet obtains 76.73% accuracy on the CIFAR-10 dataset while performing inference 4x faster than CapsNet. In addition, our proposed network is more robust at detecting images with affine transformations compared to CapsNet. We achieve 94.3% accuracy on the AffNIST dataset (compared to CapsNet 90.52%).

</details>


### [14] [Target-Balanced Score Distillation](https://arxiv.org/abs/2511.11710)
*Zhou Xu,Qi Wang,Yuxiao Yang,Luyuan Zhang,Zhang Liang,Yang Li*

Main category: cs.CV

TL;DR: 本文提出了Target-Balanced Score Distillation (TBSD)方法，通过多目标优化策略解决了Score Distillation Sampling (SDS)中纹理优化与形状保真度的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统SDS方法存在过饱和和过平滑问题，而引入负提示的变体方法面临纹理优化有限或纹理增益但形状失真的关键权衡。研究发现这种权衡根本上由负提示的使用方式决定。

Method: 提出TBSD方法，将生成建模为多目标优化问题，采用自适应策略来平衡纹理真实性和几何形状准确性。

Result: 大量实验表明，TBSD显著优于现有最先进方法，能够生成具有高保真纹理和几何精确形状的3D资产。

Conclusion: TBSD通过目标平衡策略有效解决了SDS方法中的纹理-形状权衡问题，实现了高质量的3D资产生成。

Abstract: Score Distillation Sampling (SDS) enables 3D asset generation by distilling priors from pretrained 2D text-to-image diffusion models, but vanilla SDS suffers from over-saturation and over-smoothing. To mitigate this issue, recent variants have incorporated negative prompts. However, these methods face a critical trade-off: limited texture optimization, or significant texture gains with shape distortion. In this work, we first conduct a systematic analysis and reveal that this trade-off is fundamentally governed by the utilization of the negative prompts, where Target Negative Prompts (TNP) that embed target information in the negative prompts dramatically enhancing texture realism and fidelity but inducing shape distortions. Informed by this key insight, we introduce the Target-Balanced Score Distillation (TBSD). It formulates generation as a multi-objective optimization problem and introduces an adaptive strategy that effectively resolves the aforementioned trade-off. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.

</details>


### [15] [CompressNAS : A Fast and Efficient Technique for Model Compression using Decomposition](https://arxiv.org/abs/2511.11716)
*Sudhakar Sah,Nikhil Chabbra,Matthieu Durnerin*

Main category: cs.CV

TL;DR: CompressNAS是一个基于神经架构搜索的框架，用于全局优化CNN模型的低秩张量分解，在保持精度的同时实现显著压缩。


<details>
  <summary>Details</summary>
Motivation: 深度CNN模型在微控制器和轻量级NPU上部署困难，现有低秩分解方法仅局部选择秩而忽略全局压缩与精度的权衡。

Method: 采用MicroNAS启发的方法，将秩选择视为全局搜索问题，使用快速精度估计器评估候选分解方案。

Result: 在ImageNet上压缩ResNet-18达8倍，精度损失小于4%；在COCO上压缩YOLOv5s达2倍无精度损失，压缩YOLOv5n达2倍仅损失2.5%精度。

Conclusion: 提出了新的压缩模型家族STResNet，与其他高效模型相比具有竞争力。

Abstract: Deep Convolutional Neural Networks (CNNs) are increasingly difficult to deploy on microcontrollers (MCUs) and lightweight NPUs (Neural Processing Units) due to their growing size and compute demands. Low-rank tensor decomposition, such as Tucker factorization, is a promising way to reduce parameters and operations with reasonable accuracy loss. However, existing approaches select ranks locally and often ignore global trade-offs between compression and accuracy. We introduce CompressNAS, a MicroNAS-inspired framework that treats rank selection as a global search problem. CompressNAS employs a fast accuracy estimator to evaluate candidate decompositions, enabling efficient yet exhaustive rank exploration under memory and accuracy constraints. In ImageNet, CompressNAS compresses ResNet-18 by 8x with less than 4% accuracy drop; on COCO, we achieve 2x compression of YOLOv5s without any accuracy drop and 2x compression of YOLOv5n with a 2.5% drop. Finally, we present a new family of compressed models, STResNet, with competitive performance compared to other efficient models.

</details>


### [16] [AdaptFly: Prompt-Guided Adaptation of Foundation Models for Low-Altitude UAV Networks](https://arxiv.org/abs/2511.11720)
*Jiao Chen,Haoyi Wang,Jianhua Tang,Junyi Wang*

Main category: cs.CV

TL;DR: AdaptFly是一个无需权重更新的提示引导测试时适应框架，通过轻量级token提示检索和梯度稀疏视觉提示优化，解决无人机在恶劣环境下的语义分割性能退化问题，实现通信高效的分布式感知。


<details>
  <summary>Details</summary>
Motivation: 低空无人机网络的语义分割模型在天气、光照和视角变化下性能快速退化，资源受限的无人机无法运行基于梯度的测试时适应，而资源充足的无人机独立适应会浪费共享经验。

Method: 提出两种互补适应模式：资源受限无人机使用轻量级token提示检索共享全局记忆；资源充足无人机使用协方差矩阵自适应进化策略进行梯度稀疏视觉提示优化。通过激活统计检测器触发适应，跨无人机知识池整合提示知识。

Result: 在UAVid和VDD基准测试及真实无人机部署中，AdaptFly显著提高了分割精度和鲁棒性，优于静态模型和最先进的TTA基线方法。

Conclusion: AdaptFly为新兴低空经济中的弹性、通信高效感知提供了一条实用路径，通过分布式协作适应机制解决了无人机网络中的语义分割挑战。

Abstract: Low-altitude Unmanned Aerial Vehicle (UAV) networks rely on robust semantic segmentation as a foundational enabler for distributed sensing-communication-control co-design across heterogeneous agents within the network. However, segmentation foundation models deteriorate quickly under weather, lighting, and viewpoint drift. Resource-limited UAVs cannot run gradient-based test-time adaptation, while resource-massive UAVs adapt independently, wasting shared experience. To address these challenges, we propose AdaptFly, a prompt-guided test-time adaptation framework that adjusts segmentation models without weight updates. AdaptFly features two complementary adaptation modes. For resource-limited UAVs, it employs lightweight token-prompt retrieval from a shared global memory. For resource-massive UAVs, it uses gradient-free sparse visual prompt optimization via Covariance Matrix Adaptation Evolution Strategy. An activation-statistic detector triggers adaptation, while cross-UAV knowledge pool consolidates prompt knowledge and enables fleet-wide collaboration with negligible bandwidth overhead. Extensive experiments on UAVid and VDD benchmarks, along with real-world UAV deployments under diverse weather conditions, demonstrate that AdaptFly significantly improves segmentation accuracy and robustness over static models and state-of-the-art TTA baselines. The results highlight a practical path to resilient, communication-efficient perception in the emerging low-altitude economy.

</details>


### [17] [Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video](https://arxiv.org/abs/2511.11725)
*Zekai Shi,Zhixi Cai,Kalin Stefanov*

Main category: cs.CV

TL;DR: 提出了一种基于人类视觉盲点的生物合理性掩码策略，用于从儿童视角数据中学习视觉表示和词-指称映射。


<details>
  <summary>Details</summary>
Motivation: 儿童学习第一词汇时面临词汇指称歧义问题，需要从生态有效的数据中学习视觉表示和词-指称映射。

Method: 使用基于掩码自编码器的视觉骨干网络，结合人类视觉盲点知识设计新型掩码策略，采用对比学习视频-文本模型学习词-指称映射。

Result: 提出的生物合理性掩码策略在从跨情境和时间扩展片段中学习词-指称映射方面至少与随机掩码同样有效。

Conclusion: 生物启发的掩码策略为从生态有效数据中学习视觉表示和语言获取提供了有前景的方向。

Abstract: Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes' field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes.

</details>


### [18] [GROVER: Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion](https://arxiv.org/abs/2511.11730)
*Yongjun Xiao,Dian Meng,Xinlei Huang,Yanran Liu,Shiwei Ruan,Ziyue Qiao,Xubin Zheng*

Main category: cs.CV

TL;DR: GROVER是一个用于空间多组学数据自适应融合的新框架，通过图卷积网络和对比学习解决多模态异质性问题。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学、蛋白质组学和表观基因组学缺乏病理形态学背景，需要与组织病理学图像整合以全面分析疾病组织。但多模态数据的异质性、分辨率不匹配和生物扰动给整合带来挑战。

Method: 使用基于Kolmogorov-Arnold网络的图卷积网络编码器捕获模态间非线性依赖关系，采用点-特征对对比学习策略优化跨模态对应关系，并设计动态专家路由机制自适应选择信息丰富的模态。

Result: 在真实世界空间组学数据集上的实验表明，GROVER优于现有最先进基线方法。

Conclusion: GROVER为多模态整合提供了一个稳健可靠的解决方案，能够有效处理空间多组学数据的异质性和整合挑战。

Abstract: Effectively modeling multimodal spatial omics data is critical for understanding tissue complexity and underlying biological mechanisms. While spatial transcriptomics, proteomics, and epigenomics capture molecular features, they lack pathological morphological context. Integrating these omics with histopathological images is therefore essential for comprehensive disease tissue analysis. However, substantial heterogeneity across omics, imaging, and spatial modalities poses significant challenges. Naive fusion of semantically distinct sources often leads to ambiguous representations. Additionally, the resolution mismatch between high-resolution histology images and lower-resolution sequencing spots complicates spatial alignment. Biological perturbations during sample preparation further distort modality-specific signals, hindering accurate integration. To address these challenges, we propose Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion (GROVER), a novel framework for adaptive integration of spatial multi-omics data. GROVER leverages a Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to capture the nonlinear dependencies between each modality and its associated spatial structure, thereby producing expressive, modality-specific embeddings. To align these representations, we introduce a spot-feature-pair contrastive learning strategy that explicitly optimizes the correspondence across modalities at each spot. Furthermore, we design a dynamic expert routing mechanism that adaptively selects informative modalities for each spot while suppressing noisy or low-quality inputs. Experiments on real-world spatial omics datasets demonstrate that GROVER outperforms state-of-the-art baselines, providing a robust and reliable solution for multimodal integration.

</details>


### [19] [Exposing DeepFakes via Hyperspectral Domain Mapping](https://arxiv.org/abs/2511.11732)
*Aditya Mehta,Swarnim Chaudhary,Pratik Narang,Jagat Sesh Challa*

Main category: cs.CV

TL;DR: HSI-Detect使用两阶段流程将RGB图像重建为31通道高光谱图像，在高光谱域进行深度伪造检测，相比RGB基线有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现代生成和扩散模型产生的图像高度逼真，可能误导人类感知和自动化检测系统。大多数检测方法只在RGB空间分析三个光谱通道，限制了检测能力。

Method: 提出HSI-Detect两阶段流程：1) 从标准RGB输入重建31通道高光谱图像；2) 在高光谱域进行检测。通过扩展输入表示到更密集的光谱带，放大RGB域中微弱或不可见的操作伪影。

Result: 在FaceForensics++数据集上的评估显示，相比仅使用RGB的基线方法，HSI-Detect实现了持续改进。

Conclusion: 光谱域映射在深度伪造检测方面具有前景，高光谱分析能够增强检测能力。

Abstract: Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.

</details>


### [20] [Toward bilipshiz geometric models](https://arxiv.org/abs/2511.11735)
*Yonatan Sverdlov,Eitan Rosen,Nadav Dym*

Main category: cs.CV

TL;DR: 本文研究了点云神经网络是否保持对称感知距离的双Lipschitz等价性，发现现有网络不满足此性质，提出了改进方法并在3D点云对应任务中验证了优势。


<details>
  <summary>Details</summary>
Motivation: 受等变学习文献启发，研究点云网络是否保持自然对称感知距离的双Lipschitz等价性，以提升模型性能。

Method: 分析Procrustes匹配度量和Hard Gromov Wasserstein距离的双Lipschitz等价性，修改现有不变网络以获得双Lipschitz保证。

Result: 发现两种对称感知距离本身不等价，现有网络不满足双Lipschitz性质，改进后的模型在3D点云对应任务中表现更优。

Conclusion: 通过确保点云网络的双Lipschitz性质，可以提升其在对称感知任务中的性能，为等变学习提供了新视角。

Abstract: Many neural networks for point clouds are, by design, invariant to the symmetries of this datatype: permutations and rigid motions. The purpose of this paper is to examine whether such networks preserve natural symmetry aware distances on the point cloud spaces, through the notion of bi-Lipschitz equivalence. This inquiry is motivated by recent work in the Equivariant learning literature which highlights the advantages of bi-Lipschitz models in other scenarios.
  We consider two symmetry aware metrics on point clouds: (a) The Procrustes Matching (PM) metric and (b) Hard Gromov Wasserstien distances. We show that these two distances themselves are not bi-Lipschitz equivalent, and as a corollary deduce that popular invariant networks for point clouds are not bi-Lipschitz with respect to the PM metric. We then show how these networks can be modified so that they do obtain bi-Lipschitz guarantees. Finally, we provide initial experiments showing the advantage of the proposed bi-Lipschitz model over standard invariant models, for the tasks of finding correspondences between 3D point clouds.

</details>


### [21] [Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models](https://arxiv.org/abs/2511.11751)
*Sanchit Sinha,Guangzhi Xiong,Zhenghao He,Aidong Zhang*

Main category: cs.CV

TL;DR: Concept-RuleNet是一个多代理神经符号系统，通过挖掘视觉概念来增强视觉基础，使用LLM生成可解释的一阶规则，在推理时结合黑盒模型输出，提高预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型虽然预测准确，但缺乏决策解释，容易产生幻觉事实，特别是在分布外数据上。神经符号框架虽然结合了黑盒感知和符号推理，但现有方法仅从任务标签中提取符号，缺乏视觉基础。

Method: 1) 多模态概念生成器从训练图像子集中挖掘判别性视觉概念；2) 使用视觉概念条件化符号发现；3) LLM推理代理将符号组合成可执行的一阶规则；4) 推理时视觉验证代理量化符号存在程度，结合黑盒模型输出执行规则。

Result: 在五个基准测试（包括两个医学影像任务和三个代表性不足的自然图像数据集）上，系统平均提升了最先进神经符号基线5%的性能，同时将规则中幻觉符号的出现减少了50%。

Conclusion: Concept-RuleNet通过重新建立视觉基础同时保持透明推理，有效提升了神经符号系统的性能和可解释性，减少了幻觉问题。

Abstract: Modern vision-language models (VLMs) deliver impressive predictive accuracy yet offer little insight into 'why' a decision is reached, frequently hallucinating facts, particularly when encountering out-of-distribution data. Neurosymbolic frameworks address this by pairing black-box perception with interpretable symbolic reasoning, but current methods extract their symbols solely from task labels, leaving them weakly grounded in the underlying visual data. In this paper, we introduce a multi-agent system - Concept-RuleNet that reinstates visual grounding while retaining transparent reasoning. Specifically, a multimodal concept generator first mines discriminative visual concepts directly from a representative subset of training images. Next, these visual concepts are utilized to condition symbol discovery, anchoring the generations in real image statistics and mitigating label bias. Subsequently, symbols are composed into executable first-order rules by a large language model reasoner agent - yielding interpretable neurosymbolic rules. Finally, during inference, a vision verifier agent quantifies the degree of presence of each symbol and triggers rule execution in tandem with outputs of black-box neural models, predictions with explicit reasoning pathways. Experiments on five benchmarks, including two challenging medical-imaging tasks and three underrepresented natural-image datasets, show that our system augments state-of-the-art neurosymbolic baselines by an average of 5% while also reducing the occurrence of hallucinated symbols in rules by up to 50%.

</details>


### [22] [Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition](https://arxiv.org/abs/2511.11754)
*Stanislav Selitskiy*

Main category: cs.CV

TL;DR: 提出了一种新型的隐式稀疏Transformer架构——Batch Transformers，通过对重要维度（主成分）进行注意力机制，显著减少了编码器-解码器ANN架构中的瓶颈大小。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer对序列或批处理的全部维度进行注意力计算，而Batch Transformers旨在通过关注重要维度来减少模型复杂度，提高效率。

Method: 采用隐式稀疏风格的Transformer变体，对重要维度（主成分）实施注意力机制，而非整个维度，从而减少编码器-解码器架构的瓶颈大小。

Result: 在面部识别任务的合成图像生成中，针对化妆和遮挡数据集进行了测试，结果表明该方法能够增加有限原始数据集的变异性。

Conclusion: Batch Transformers通过关注重要维度有效减少了模型复杂度，在数据增强和图像生成任务中表现出良好的性能。

Abstract: A novel Transformer variation architecture is proposed in the implicit sparse style. Unlike "traditional" Transformers, instead of attention to sequential or batch entities in their entirety of whole dimensionality, in the proposed Batch Transformers, attention to the "important" dimensions (primary components) is implemented. In such a way, the "important" dimensions or feature selection allows for a significant reduction of the bottleneck size in the encoder-decoder ANN architectures. The proposed architecture is tested on the synthetic image generation for the face recognition task in the case of the makeup and occlusion data set, allowing for increased variability of the limited original data set.

</details>


### [23] [Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing](https://arxiv.org/abs/2511.11780)
*Hossein Mohebbi,Mohammed Abdulrahman,Yanting Miao,Pascal Poupart,Suraj Kothawade*

Main category: cs.CV

TL;DR: Image-POSER是一个基于强化学习的框架，通过协调多个预训练文本-图像和图像-图像专家模型，动态分解长提示任务，并使用视觉语言模型进行结构化反馈监督，实现更好的图像生成效果。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型在处理长组合提示时表现不佳，缺乏可靠执行创意工作流程中典型复杂提示的能力。

Method: 采用反射强化学习框架，将图像合成和编辑建模为马尔可夫决策过程，动态协调预训练专家模型，通过视觉语言模型批评器提供结构化反馈监督每个步骤的对齐。

Result: 在行业标准和自定义基准测试中，Image-POSER在一致性、保真度和美学方面均优于基线模型（包括前沿模型），并在人类评估中持续获得偏好。

Conclusion: 强化学习可以赋予AI系统自主分解、重新排序和组合视觉模型的能力，朝着通用视觉助手的方向发展。

Abstract: Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.

</details>


### [24] [SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction](https://arxiv.org/abs/2511.11824)
*Zhongping Dong,Pengyang Yu,Shuangjian Li,Liming Chen,Mohand Tahar Kechadi*

Main category: cs.CV

TL;DR: SOTFormer是一个统一目标检测、跟踪和短期轨迹预测的端到端框架，使用恒定内存的时间transformer，在遮挡、尺度变化和时间漂移下保持稳定的身份传播。


<details>
  <summary>Details</summary>
Motivation: 解决在遮挡、尺度变化和时间漂移下，单目标跟踪和短期运动预测的时序一致性挑战，这些因素会破坏实时感知所需的时序连贯性。

Method: 使用基于真实标签初始化的记忆和烧入锚点损失来稳定初始化，通过轻量级时间注意力层跨帧优化嵌入表示，实现固定GPU内存的实时推理。

Result: 在Mini-LaSOT基准测试中达到76.3 AUC和53.7 FPS，在快速运动、尺度变化和遮挡情况下优于TrackFormer和MOTRv2等transformer基线模型。

Conclusion: SOTFormer通过统一的端到端框架，在保持恒定内存的同时，显著提升了单目标跟踪和短期预测的性能和效率。

Abstract: Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.

</details>


### [25] [MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning](https://arxiv.org/abs/2511.11837)
*Fatemeh Elhambakhsh,Gaurav Ameta,Aditi Roy,Hyunwoong Ko*

Main category: cs.CV

TL;DR: 提出了MP-GFormer，一种3D几何感知的动态图变换器，通过注意力机制将演化的3D几何表示集成到动态图学习中，用于预测机加工操作序列。


<details>
  <summary>Details</summary>
Motivation: 现有的动态图学习方法在机加工工艺规划中虽然能捕捉时空依赖关系，但无法融入零件的三维几何信息，缺乏领域感知能力。

Method: 使用StereoLithography表面网格表示每个机加工操作后的零件3D几何，边界表示方法用于初始3D设计，通过注意力机制集成几何信息到动态图变换器中。

Result: 在合成数据集上评估，相比最先进方法，在主操作和子操作预测准确率上分别提升了24%和36%。

Conclusion: MP-GFormer通过整合3D几何信息显著提升了机加工操作序列预测的性能，证明了3D几何感知在工艺规划中的重要性。

Abstract: Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\% and 36\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.

</details>


### [26] [Defending Unauthorized Model Merging via Dual-Stage Weight Protection](https://arxiv.org/abs/2511.11851)
*Wei-Jia Chen,Min-Yen Tsai,Cheng-Yi Lee,Chia-Mu Yu*

Main category: cs.CV

TL;DR: MergeGuard是一个主动的双阶段权重保护框架，通过破坏模型合并兼容性来防止未经授权的模型合并，同时保持受保护模型的性能。


<details>
  <summary>Details</summary>
Motivation: 预训练模型和开放存储库的快速扩散使得模型合并成为一种便捷但有风险的做法，未经授权的模型合并不仅侵犯知识产权，还破坏模型所有权和问责制。

Method: 采用双阶段方法：第一阶段通过L2正则化优化重新分配任务相关信息；第二阶段注入结构化扰动以错配任务子空间，破坏损失景观中的曲率兼容性。

Result: 在视觉（ViT-L-14）和语言（Llama2、Gemma2、Mistral）模型上的广泛实验表明，MergeGuard可将合并模型准确率降低高达90%，而受保护模型的性能损失不到1.5%。

Conclusion: MergeGuard通过重塑模型参数几何形状，使合并模型崩溃为破坏性干扰，同时保持受保护模型完全功能，有效解决了未经授权模型合并的问题。

Abstract: The rapid proliferation of pretrained models and open repositories has made model merging a convenient yet risky practice, allowing free-riders to combine fine-tuned models into a new multi-capability model without authorization. Such unauthorized model merging not only violates intellectual property rights but also undermines model ownership and accountability. To address this issue, we present MergeGuard, a proactive dual-stage weight protection framework that disrupts merging compatibility while maintaining task fidelity. In the first stage, we redistribute task-relevant information across layers via L2-regularized optimization, ensuring that important gradients are evenly dispersed. In the second stage, we inject structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, these stages reshape the model's parameter geometry such that merged models collapse into destructive interference while the protected model remains fully functional. Extensive experiments on both vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models demonstrate that MergeGuard reduces merged model accuracy by up to 90% with less than 1.5% performance loss on the protected model.

</details>


### [27] [FocusSDF: Boundary-Aware Learning for Medical Image Segmentation via Signed Distance Supervision](https://arxiv.org/abs/2511.11864)
*Muzammal Shafique,Nasir Rahim,Jamil Ahmad,Mohammad Siadat,Khalid Malik,Ghaus Malik*

Main category: cs.CV

TL;DR: 提出FocusSDF损失函数，基于符号距离函数自适应地为边界区域分配更高权重，解决医学图像分割中边界保持的挑战。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在临床实践中至关重要，但现有模型大多未显式编码边界信息，导致边界保持成为持续挑战。

Method: 引入基于符号距离函数的FocusSDF损失函数，通过自适应为靠近病灶或器官边界的像素分配更高权重，使网络专注于边界区域。

Result: 在涵盖脑动脉瘤、中风、肝脏和乳腺肿瘤分割任务的多个数据集上，FocusSDF在五个最先进模型上的实验结果显示其性能优于现有基于距离变换的损失函数。

Conclusion: FocusSDF损失函数在医学图像分割中表现出优越性能，能有效提升边界区域的关注度。

Abstract: Segmentation of medical images constitutes an essential component of medical image analysis, providing the foundation for precise diagnosis and efficient therapeutic interventions in clinical practices. Despite substantial progress, most segmentation models do not explicitly encode boundary information; as a result, making boundary preservation a persistent challenge in medical image segmentation. To address this challenge, we introduce FocusSDF, a novel loss function based on the signed distance functions (SDFs), which redirects the network to concentrate on boundary regions by adaptively assigning higher weights to pixels closer to the lesion or organ boundary, effectively making it boundary aware. To rigorously validate FocusSDF, we perform extensive evaluations against five state-of-the-art medical image segmentation models, including the foundation model MedSAM, using four distance-based loss functions across diverse datasets covering cerebral aneurysm, stroke, liver, and breast tumor segmentation tasks spanning multiple imaging modalities. The experimental results consistently demonstrate the superior performance of FocusSDF over existing distance transform based loss functions.

</details>


### [28] [Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)](https://arxiv.org/abs/2511.11882)
*Simon Durand,Samuel Foucher,Alexandre Delplanque,Joëlle Taillon,Jérôme Théau*

Main category: cs.CV

TL;DR: 本研究探讨了使用合成图像补充有限训练数据来改善麝牛检测的效果，在零样本和少样本设置下验证了合成图像对提高深度学习目标检测模型性能的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统野生动物调查方法资源密集且受限于后勤挑战，而深度学习目标检测模型在稀疏分布物种（如麝牛）上的应用受到小数据集的限制。

Method: 比较了仅使用真实图像的基线模型与5个零样本和5个少样本模型，这些模型在训练集中逐步加入更多合成图像。

Result: 在零样本模型中，添加合成图像提高了检测性能；在少样本模型中，结合真实和合成图像能获得更好的召回率和略高的整体准确率。

Conclusion: 合成图像在数据稀缺时能够训练准确的目标检测模型，为监测稀有或难以接近的物种提供了重要视角，并可在没有真实数据时启动模型，随着时间推移获取真实图像后逐步优化。

Abstract: Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.

</details>


### [29] [Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation](https://arxiv.org/abs/2511.11890)
*Camila Machado de Araujo,Egon P. B. S. Borges,Ricardo Marcelo Canteiro Grangeiro,Allan Pinto*

Main category: cs.CV

TL;DR: 开发了Harpia CUDA处理库，为Annotat3D提供可扩展的交互式3D分割功能，支持超出单GPU内存容量的大型数据集处理


<details>
  <summary>Details</summary>
Motivation: 高分辨率体成像技术生成的大型数据集对现有处理工具的效率、分割和交互探索能力提出了挑战

Method: 基于CUDA开发Harpia处理库，具有严格内存控制、原生分块执行和GPU加速的过滤、标注和量化工具套件

Result: 相比NVIDIA cuCIM和scikit-image等框架，在处理速度、内存效率和可扩展性方面有显著提升

Conclusion: 该系统结合交互式人机界面和高效GPU资源管理，特别适合共享HPC基础设施中的协作科学成像工作流

Abstract: High-resolution volumetric imaging techniques, such as X-ray tomography and advanced microscopy, generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration. This work introduces new capabilities to Annotat3D through Harpia, a new CUDA-based processing library designed to support scalable, interactive segmentation workflows for large 3D datasets in high-performance computing (HPC) and remote-access environments. Harpia features strict memory control, native chunked execution, and a suite of GPU-accelerated filtering, annotation, and quantification tools, enabling reliable operation on datasets exceeding single-GPU memory capacity. Experimental results demonstrate significant improvements in processing speed, memory efficiency, and scalability compared to widely used frameworks such as NVIDIA cuCIM and scikit-image. The system's interactive, human-in-the-loop interface, combined with efficient GPU resource management, makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.

</details>


### [30] [Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks](https://arxiv.org/abs/2511.11898)
*Arnav Singhvi,Vasiliki Bikia,Asad Aali,Akshay Chaudhari,Roxana Daneshjou*

Main category: cs.CV

TL;DR: 本文提出使用DSPy框架对医疗视觉语言模型进行自动化提示优化，在5个医学影像任务中实现了53%的中位数相对性能提升，最高可达3400%的改进。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言基础模型在医疗任务上表现不佳，而微调需要大量数据和计算资源，手动提示工程难以泛化。需要一种能够利用模型嵌入知识且不依赖人工提示设计的可扩展方法。

Method: 采用DSPy框架进行结构化自动提示优化，在放射学、胃肠病学和皮肤病学的5个医学影像任务上评估了10个开源VLM和4种提示优化技术。

Result: 优化后的管道在零样本基线基础上实现了53%的中位数相对改进，在零样本性能较低的任务上最大提升达300%至3400%。

Conclusion: 自动化提示优化在医疗AI系统中具有巨大潜力，能够显著提升临床图像解释的准确性，减少对提示设计的依赖，让临床医生专注于患者护理和临床决策。

Abstract: Vision-language foundation models (VLMs) show promise for diverse imaging tasks but often underperform on medical benchmarks. Prior efforts to improve performance include model finetuning, which requires large domain-specific datasets and significant compute, or manual prompt engineering, which is hard to generalize and often inaccessible to medical institutions seeking to deploy these tools. These challenges motivate interest in approaches that draw on a model's embedded knowledge while abstracting away dependence on human-designed prompts to enable scalable, weight-agnostic performance improvements. To explore this, we adapt the Declarative Self-improving Python (DSPy) framework for structured automated prompt optimization in medical vision-language systems through a comprehensive, formal evaluation. We implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance is low. These results highlight the substantial potential of applying automated prompt optimization to medical AI systems, demonstrating significant gains for vision-based applications requiring accurate clinical image interpretation. By reducing dependence on prompt design to elicit intended outputs, these techniques allow clinicians to focus on patient care and clinical decision-making. Furthermore, our experiments offer scalability and preserve data privacy, demonstrating performance improvement on open-source VLMs. We publicly release our evaluation pipelines to support reproducible research on specialized medical tasks, available at https://github.com/DaneshjouLab/prompt-triage-lab.

</details>


### [31] [PI-NAIM: Path-Integrated Neural Adaptive Imputation Model](https://arxiv.org/abs/2511.11908)
*Afifa Khaled,Ebrahim Hamid Sumiea*

Main category: cs.CV

TL;DR: PI-NAIM是一个双路径架构，根据缺失复杂性动态路由样本到优化的插补方法，结合统计插补和神经网络，在医学影像和多模态临床数据中实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像和多模态临床设置经常面临诊断流程中模态缺失的挑战，现有插补方法要么缺乏表示能力，要么计算成本高昂。

Method: 提出双路径架构：智能路径路由将低缺失样本导向高效统计插补（MICE），复杂模式导向强大神经网络（GAIN）；跨路径注意力融合利用缺失感知嵌入智能结合两个分支；端到端联合优化插补准确性和下游任务性能。

Result: 在MIMIC-III和多模态基准测试中实现最先进性能：RMSE为0.108（基准方法为0.119-0.152），死亡率预测AUROC达到0.812。

Conclusion: PI-NAIM的模块化设计能够无缝集成到处理不完整传感器测量、缺失模态或损坏输入的视觉流程中，为现实场景提供统一解决方案。

Abstract: Medical imaging and multi-modal clinical settings often face the challange of missing modality in their diagnostic pipelines. Existing imputation methods either lack representational capacity or are computationally expensive. We propose PI-NAIM, a novel dual-path architecture that dynamically routes samples to optimized imputation approaches based on missingness complexity. Our framework integrates: (1) intelligent path routing that directs low missingness samples to efficient statistical imputation (MICE) and complex patterns to powerful neural networks (GAIN with temporal analysis); (2) cross-path attention fusion that leverages missingness-aware embeddings to intelligently combine both branches; and (3) end-to-end joint optimization of imputation accuracy and downstream task performance. Extensive experiments on MIMIC-III and multimodal benchmarks demonstrate state-of-the-art performance, achieving RMSE of 0.108 (vs. baselines' 0.119-0.152) and substantial gains in downstream tasks with an AUROC of 0.812 for mortality prediction. PI-NAIM's modular design enables seamless integration into vision pipelines handling incomplete sensor measurements, missing modalities, or corrupted inputs, providing a unified solution for real-world scenario. The code is publicly available at https://github.com/AfifaKhaled/PI-NAIM-Path-Integrated-Neural-Adaptive-Imputation-Model

</details>


### [32] [Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models](https://arxiv.org/abs/2511.11910)
*Siyou Li,Huanan Wu,Juexi Shao,Yinghao Ma,Yujian Gan,Yihao Luo,Yuwei Wang,Dong Nie,Lu Wang,Wengqing Wu,Le Zhang,Massimo Poesio,Juntao Yu*

Main category: cs.CV

TL;DR: QTSplus是一个轻量级视觉token选择模块，通过动态选择与文本查询最相关的视觉证据来解决长视频理解中的计算复杂度问题，显著减少视觉token数量并提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在长视频理解中面临的挑战，即视觉token数量随视频长度线性增长导致的注意力成本、内存和延迟爆炸问题。

Method: QTSplus通过交叉注意力对视觉token评分，基于查询复杂度预测保留预算，使用可微分直通估计器选择Top-n token，并通过小型重编码器保持时间顺序。

Result: 在Qwen2.5-VL中集成QTSplus，视觉流压缩达89%，端到端延迟减少28%，在八个长视频理解基准测试中保持接近原始模型的准确度，在TempCompass方向准确度上提升20.5分。

Conclusion: QTSplus是一种有效且通用的机制，可在保持任务相关证据的同时，将MLLM扩展到真实世界长视频场景。

Abstract: Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.
  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \textbf{89\%} and reduces end-to-end latency by \textbf{28\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \textbf{+20.5} and \textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.
  We will make all code, data, and trained models' weights publicly available.

</details>


### [33] [From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing](https://arxiv.org/abs/2511.11944)
*Ling Wang,Yunfan Lu,Wenzong Ma,Huizai Yao,Pengteng Li,Hui Xiong*

Main category: cs.CV

TL;DR: 首次使用事件相机进行图像去雾，提出事件引导的扩散模型，通过将事件的高动态范围信息转移到RGB图像中来提升去雾效果。


<details>
  <summary>Details</summary>
Motivation: 传统基于RGB的去雾方法受限于有限的动态范围，容易丢失结构和光照细节。事件相机具有更高的动态范围和微秒级延迟，更适合处理雾霾场景。

Method: 设计事件引导模块，将稀疏的高动态范围事件特征（如边缘、角点）映射到扩散模型的潜在空间，在生成过程中提供精确的结构指导。

Result: 在两个基准测试集和自建的重度雾霾无人机数据集上取得了最先进的去雾效果。

Conclusion: 事件引导的扩散模型能够有效利用事件相机的高动态范围信息，显著提升去雾图像的质量和真实感。

Abstract: Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.

</details>


### [34] [Evaluation of Attention Mechanisms in U-Net Architectures for Semantic Segmentation of Brazilian Rock Art Petroglyphs](https://arxiv.org/abs/2511.11959)
*Leonardi Melo,Luís Gustavo,Dimmy Magalhães,Lucciani Vieira,Mauro Araújo*

Main category: cs.CV

TL;DR: 比较三种基于U-Net的架构在巴西岩画岩刻语义分割中的性能，其中Attention-Residual BEGL-UNet表现最佳，Dice得分为0.710。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估不同U-Net架构在考古遗产数字保护中的有效性，特别是针对巴西岩画岩刻的语义分割任务。

Method: 比较了三种U-Net架构：(1) BEGL-UNet；(2) Attention-Residual BEGL-UNet（结合残差块和门控注意力机制）；(3) Spatial Channel Attention BEGL-UNet（使用空间通道注意力模块）。所有方法都采用BEGL损失函数，并在巴西Poço da Bebidinha考古遗址图像上使用5折交叉验证进行实验。

Result: Attention-Residual BEGL-UNet表现最佳，Dice得分为0.710，验证损失为0.067，召回率为0.854。Spatial Channel Attention BEGL-UNet的Dice得分为0.707，召回率为0.857。基线BEGL-UNet的Dice得分为0.690。注意力机制相比基线提升了2.5-2.9%的Dice得分。

Conclusion: 注意力机制在考古遗产数字保护中具有显著效果，能够有效提升岩画岩刻语义分割的性能。

Abstract: This study presents a comparative analysis of three U-Net-based architectures for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites. The investigated architectures were: (1) BEGL-UNet with Border-Enhanced Gaussian Loss function; (2) Attention-Residual BEGL-UNet, incorporating residual blocks and gated attention mechanisms; and (3) Spatial Channel Attention BEGL-UNet, which employs spatial-channel attention modules based on Convolutional Block Attention Module. All implementations employed the BEGL loss function combining binary cross-entropy with Gaussian edge enhancement. Experiments were conducted on images from the Poço da Bebidinha Archaeological Complex, Piauí, Brazil, using 5-fold cross-validation. Among the architectures, Attention-Residual BEGL-UNet achieved the best overall performance with Dice Score of 0.710, validation loss of 0.067, and highest recall of 0.854. Spatial Channel Attention BEGL-UNet obtained comparable performance with DSC of 0.707 and recall of 0.857. The baseline BEGL-UNet registered DSC of 0.690. These results demonstrate the effectiveness of attention mechanisms for archaeological heritage digital preservation, with Dice Score improvements of 2.5-2.9% over the baseline.

</details>


### [35] [From Classification to Cross-Modal Understanding: Leveraging Vision-Language Models for Fine-Grained Renal Pathology](https://arxiv.org/abs/2511.11984)
*Zhenhao Guo,Rachit Saluja,Tianyuan Yao,Quan Liu,Junchao Zhu,Haibo Wang,Daniel Reisenbüchler,Yuankai Huo,Benjamin Liechty,David J. Pisapia,Kenji Ikemura,Steven Salvatoree,Surya Seshane,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: 该研究评估了视觉语言模型在细粒度肾小球亚型分类中的表现，发现在数据稀缺的临床场景下，病理学专业化的视觉语言骨干网络结合标准微调是最有效的起点。


<details>
  <summary>Details</summary>
Motivation: 细粒度肾小球亚型分类对肾脏活检解读至关重要，但临床上有价值的标签稀缺且难以获取。现有计算方法主要在完全监督下进行粗粒度疾病分类，不清楚视觉语言模型如何在数据约束下适应临床有意义的亚型分类。

Method: 将细粒度肾小球亚型分类建模为临床现实的少样本问题，系统评估病理学专业化和通用视觉语言模型，分析分类性能（准确率、AUC、F1）和学习表示的几何结构，检查图像与文本嵌入的特征对齐以及肾小球亚型的可分离性。

Result: 病理学专业化的视觉语言骨干网络结合标准微调是最有效的起点。即使每个肾小球亚型只有4-8个标注样本，这些模型也能开始捕捉区别，在区分度和校准方面显示出显著提升，尽管额外监督仍能带来渐进改进。正负样本之间的区分与图像-文本对齐同等重要。

Conclusion: 监督水平和适应策略共同塑造诊断性能和多模态结构，为模型选择、适应策略和标注投资提供指导。

Abstract: Fine-grained glomerular subtyping is central to kidney biopsy interpretation, but clinically valuable labels are scarce and difficult to obtain. Existing computational pathology approaches instead tend to evaluate coarse diseased classification under full supervision with image-only models, so it remains unclear how vision-language models (VLMs) should be adapted for clinically meaningful subtyping under data constraints. In this work, we model fine-grained glomerular subtyping as a clinically realistic few-shot problem and systematically evaluate both pathology-specialized and general-purpose vision-language models under this setting. We assess not only classification performance (accuracy, AUC, F1) but also the geometry of the learned representations, examining feature alignment between image and text embeddings and the separability of glomerular subtypes. By jointly analyzing shot count, model architecture and domain knowledge, and adaptation strategy, this study provides guidance for future model selection and training under real clinical data constraints. Our results indicate that pathology-specialized vision-language backbones, when paired with the vanilla fine-tuning, are the most effective starting point. Even with only 4-8 labeled examples per glomeruli subtype, these models begin to capture distinctions and show substantial gains in discrimination and calibration, though additional supervision continues to yield incremental improvements. We also find that the discrimination between positive and negative examples is as important as image-text alignment. Overall, our results show that supervision level and adaptation strategy jointly shape both diagnostic performance and multimodal structure, providing guidance for model selection, adaptation strategies, and annotation investment.

</details>


### [36] [BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups](https://arxiv.org/abs/2511.11989)
*Songsong Zhang,Chuanqi Tang,Hongguang Zhang,Guijian Tang,Minglong Li,Xueqiong Li,Shaowu Yang,Yuanxi Peng,Wenjing Yang,Jing Zhao*

Main category: cs.CV

TL;DR: 提出了一种突破面部特写限制的IPPG方法，通过身份-语义分离的双线推理管道、身份自适应融合策略和身份聚合前置模块，实现身份保真度和场景语义创作的协同优化。


<details>
  <summary>Details</summary>
Motivation: 现有IPPG方法过度强调面部区域，导致输出被面部特写主导，存在视觉叙事性弱、复杂文本提示下语义一致性差的问题，核心限制在于身份特征嵌入削弱了生成模型的语义表达能力。

Method: 设计了身份-语义分离的双线推理管道，提出身份自适应融合策略将身份-语义融合延迟到噪声预测阶段，引入身份聚合前置模块来聚合身份信息并替换随机初始化。

Result: 实验验证该方法在超越面部特写的IPPG任务中实现稳定有效的性能，无需手动掩码或微调即可高效生成，可作为即插即用组件快速部署到现有IPPG框架中。

Conclusion: 该方法解决了对面部特写的过度依赖，促进了电影级角色-场景创作，为相关领域提供了更丰富的个性化生成能力。

Abstract: Identity-Preserving Personalized Generation (IPPG) has advanced film production and artistic creation, yet existing approaches overemphasize facial regions, resulting in outputs dominated by facial close-ups.These methods suffer from weak visual narrativity and poor semantic consistency under complex text prompts, with the core limitation rooted in identity (ID) feature embeddings undermining the semantic expressiveness of generative models. To address these issues, this paper presents an IPPG method that breaks the constraint of facial close-ups, achieving synergistic optimization of identity fidelity and scene semantic creation. Specifically, we design a Dual-Line Inference (DLI) pipeline with identity-semantic separation, resolving the representation conflict between ID and semantics inherent in traditional single-path architectures. Further, we propose an Identity Adaptive Fusion (IdAF) strategy that defers ID-semantic fusion to the noise prediction stage, integrating adaptive attention fusion and noise decision masking to avoid ID embedding interference on semantics without manual masking. Finally, an Identity Aggregation Prepending (IdAP) module is introduced to aggregate ID information and replace random initializations, further enhancing identity preservation. Experimental results validate that our method achieves stable and effective performance in IPPG tasks beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. As a plug-and-play component, it can be rapidly deployed in existing IPPG frameworks, addressing the over-reliance on facial close-ups, facilitating film-level character-scene creation, and providing richer personalized generation capabilities for related domains.

</details>


### [37] [Dynamic Parameter Optimization for Highly Transferable Transformation-Based Attacks](https://arxiv.org/abs/2511.11993)
*Jiaming Liang,Chi-Man Pun*

Main category: cs.CV

TL;DR: 本文提出了一种动态参数优化(DPO)方法，通过分析迁移性随参数强度变化的三种动态模式，显著提升了基于变换攻击的迁移性，同时将参数优化复杂度从O(mn)降低到O(nlogm)。


<details>
  <summary>Details</summary>
Motivation: 现有基于变换的攻击方法存在三个主要问题：1)仅考虑低迭代设置，无法反映高迭代下的真实性能；2)对不同代理模型、迭代次数和任务使用统一参数，限制了迁移性；3)传统网格搜索参数优化计算复杂度高，限制了进一步优化。

Method: 首先通过实证研究揭示迁移性随参数强度变化的三种动态模式，提出同心衰减模型(CDM)解释这些模式。然后基于上升-下降模式设计高效的动态参数优化(DPO)方法。

Result: 在不同代理模型、迭代次数和任务上的综合实验表明，DPO能显著提升基于变换攻击的迁移性。

Conclusion: 提出的动态参数优化方法有效解决了现有变换攻击的参数优化盲点，显著提升了攻击迁移性，同时大幅降低了计算复杂度。

Abstract: Despite their wide application, the vulnerabilities of deep neural networks raise societal concerns. Among them, transformation-based attacks have demonstrated notable success in transfer attacks. However, existing attacks suffer from blind spots in parameter optimization, limiting their full potential. Specifically, (1) prior work generally considers low-iteration settings, yet attacks perform quite differently at higher iterations, so characterizing overall performance based only on low-iteration results is misleading. (2) Existing attacks use uniform parameters for different surrogate models, iterations, and tasks, which greatly impairs transferability. (3) Traditional transformation parameter optimization relies on grid search. For n parameters with m steps each, the complexity is O(mn). Large computational overhead limits further optimization of parameters. To address these limitations, we conduct an empirical study with various transformations as baselines, revealing three dynamic patterns of transferability with respect to parameter strength. We further propose a novel Concentric Decay Model (CDM) to effectively explain these patterns. Building on these insights, we propose an efficient Dynamic Parameter Optimization (DPO) based on the rise-then-fall pattern, reducing the complexity to O(nlogm). Comprehensive experiments on existing transformation-based attacks across different surrogate models, iterations, and tasks demonstrate that our DPO can significantly improve transferability.

</details>


### [38] [LithoSeg: A Coarse-to-Fine Framework for High-Precision Lithography Segmentation](https://arxiv.org/abs/2511.12005)
*Xinyu He,Botong Zhao,Bingbing Li,Shujing Lyu,Jiwei Shen,Yue Lu*

Main category: cs.CV

TL;DR: 提出LithoSeg，一种用于光刻图像分割的粗到细网络，通过SAM模型和轻量级MLP实现高精度分割和测量


<details>
  <summary>Details</summary>
Motivation: 现有光刻SEM图像分割方法缺乏必要的精度和鲁棒性，限制了实际应用，需要更精确的沟槽轮廓分割和跨不同图案几何形状的一致性能

Method: 两阶段方法：粗阶段使用带有人工干预引导的SAM模型获得鲁棒性；细阶段将2D分割重构为1D回归问题，使用粗掩模采样沟槽法向轮廓，用轻量级MLP进行逐点细化

Result: LithoSeg在分割精度和计量精度方面均优于先前方法，同时需要更少的监督

Conclusion: 该方法为实际应用提供了有前景的解决方案，能够提高半导体制造良率

Abstract: Accurate segmentation and measurement of lithography scanning electron microscope (SEM) images are crucial for ensuring precise process control, optimizing device performance, and advancing semiconductor manufacturing yield. Lithography segmentation requires pixel-level delineation of groove contours and consistent performance across diverse pattern geometries and process window. However, existing methods often lack the necessary precision and robustness, limiting their practical applicability. To overcome this challenge, we propose LithoSeg, a coarse-to-fine network tailored for lithography segmentation. In the coarse stage, we introduce a Human-in-the-Loop Bootstrapping scheme for the Segment Anything Model (SAM) to attain robustness with minimal supervision. In the subsequent fine stage, we recast 2D segmentation as 1D regression problem by sampling groove-normal profiles using the coarse mask and performing point-wise refinement with a lightweight MLP. LithoSeg outperforms previous approaches in both segmentation accuracy and metrology precision while requiring less supervision, offering promising prospects for real-world applications.

</details>


### [39] [Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy](https://arxiv.org/abs/2511.12006)
*Kai-Wen K. Yang,Andrew Bai,Alexandra Bermudez,Yunqi Hong,Zoe Latham,Iris Sloan,Michael Liu,Vishrut Goyal,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.CV

TL;DR: 提出SIT-ADDA-Auto框架，仅调整早期卷积层而非整个网络，实现显微镜图像的无标签域适应，提升跨仪器和采集设置的性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在应用于新仪器或采集设置的显微镜图像时经常失败，传统对抗域适应方法会破坏已学习的语义表示。

Method: 仅适应最早期的卷积层并冻结更深层，结合浅层对抗对齐和预测不确定性来自动选择适应深度，无需目标标签。

Result: 在曝光、光照变化、跨仪器转移和多种染色条件下，SIT-ADDA在重建和下游分割方面优于完整编码器适应和非对抗基线方法，减少语义特征漂移。

Conclusion: 为显微镜中的无标签适应提供了设计规则，并为现场应用提供了解决方案。

Abstract: Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.

</details>


### [40] [Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis](https://arxiv.org/abs/2511.12018)
*Shounak Ray Chaudhuri,Arash Jahangiri,Christopher Paolini*

Main category: cs.CV

TL;DR: 提出了一种基于多摄像头计算机视觉的实时交通安全评估框架，通过计算后侵入时间(PET)来识别信号交叉口的高风险区域，在边缘设备上实现亚秒级精度的实时分析。


<details>
  <summary>Details</summary>
Motivation: 传统基于事故的交通安全分析受限于数据稀疏性和延迟问题，需要开发能够实时评估交通安全的创新方法。

Method: 使用四个同步摄像头提供连续视觉覆盖，在NVIDIA Jetson AGX Xavier设备上使用YOLOv11分割进行车辆检测，通过单应矩阵将检测到的车辆多边形转换为统一的鸟瞰图，并开发了像素级PET算法进行车辆位置测量。

Result: 框架能够在边缘设备上以平均2.68 FPS的速度识别高风险区域，生成800×800像素的对数热图，精度达到3.3平方厘米，实现亚秒级精度的实时吞吐量。

Conclusion: 验证了基于分散式视觉的PET分析在智能交通系统中的可行性，为高分辨率、实时和可扩展的交叉口安全评估提供了可复制的方法论。

Abstract: Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.

</details>


### [41] [LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension](https://arxiv.org/abs/2511.12020)
*Xianglong Shi,Silin Cheng,Sirui Zhao,Yunhan Jiang,Enhong Chen,Yang Liu,Sebastien Ourselin*

Main category: cs.CV

TL;DR: 提出了LIHE框架解决弱监督广义指代表达理解任务，通过两阶段方法处理表达式中零个或多个目标的情况，结合双曲几何和欧几里得几何的混合相似度模块防止语义崩溃。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督指代表达理解方法受限于一对一映射假设，无法处理现实场景中对应零个或多个目标的表达式，需要更实用的广义范式。

Method: LIHE框架包含两个阶段：指称解耦阶段预测目标数量并分解复杂表达式；指称定位阶段使用HEMix混合相似度模块结合欧几里得精确对齐和双曲几何层次建模能力。

Result: 在gRefCOCO和Ref-ZOM上建立了首个有效的弱监督WGREC基线，HEMix在标准REC基准上IoU@0.5提升达2.5%。

Conclusion: LIHE框架成功解决了弱监督广义指代表达理解任务，混合几何方法有效防止语义崩溃并保持细粒度概念区分。

Abstract: Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\%. The code is available at https://anonymous.4open.science/r/LIHE.

</details>


### [42] [Null-Space Diffusion Distillation for Efficient Photorealistic Lensless Imaging](https://arxiv.org/abs/2511.12024)
*Jose Reinaldo Cunha Santos A V Silva Neto,Hodaka Kawachi,Yasushi Yagi,Tomoya Nakamura*

Main category: cs.CV

TL;DR: 提出了Null-Space Diffusion Distillation (NSDD)方法，通过蒸馏迭代DDNM+求解器的零空间组件，在无需配对监督的情况下实现快速、逼真的无透镜图像重建。


<details>
  <summary>Details</summary>
Motivation: 现有无透镜相机重建方法依赖配对监督会导致域不匹配偏差，而通用扩散先验在噪声、高度复用和病态的无透镜反卷积设置中容易失效。

Method: 分离范围空间强制和零空间扩散先验更新，引入NSDD方法：单次学生模型蒸馏迭代DDNM+求解器的零空间组件，以无透镜测量和范围空间锚点为条件。

Result: 在Lensless-FFHQ和PhlatCam数据集上，NSDD是第二快的方法（仅次于Wiener），达到接近教师模型的感知质量（第二好LPIPS，低于DDNM+），优于DPS和经典凸基线方法。

Conclusion: NSDD在保持测量一致性的同时实现了逼真结果，无需配对监督，且运行时间和内存消耗大幅减少，为快速、无需真实值、逼真的无透镜成像提供了一条实用路径。

Abstract: State-of-the-art photorealistic reconstructions for lensless cameras often rely on paired lensless-lensed supervision, which can bias models due to lens-lensless domain mismatch. To avoid this, ground-truth-free diffusion priors are attractive; however, generic formulations tuned for conventional inverse problems often break under the noisy, highly multiplexed, and ill-posed lensless deconvolution setting. We observe that methods which separate range-space enforcement from null-space diffusion-prior updates yield stable, realistic reconstructions. Building on this, we introduce Null-Space Diffusion Distillation (NSDD): a single-pass student that distills the null-space component of an iterative DDNM+ solver, conditioned on the lensless measurement and on a range-space anchor. NSDD preserves measurement consistency and achieves photorealistic results without paired supervision at a fraction of the runtime and memory. On Lensless-FFHQ and PhlatCam, NSDD is the second fastest, behind Wiener, and achieves near-teacher perceptual quality (second-best LPIPS, below DDNM+), outperforming DPS and classical convex baselines. These results suggest a practical path toward fast, ground-truth-free, photorealistic lensless imaging.

</details>


### [43] [Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark](https://arxiv.org/abs/2511.12026)
*Rulin Zhou,Wenlong He,An Wang,Jianhang Zhang,Xuanhui Zeng,Xi Zhang,Chaowei Zhu,Haijun Hu,Hongliang Ren*

Main category: cs.CV

TL;DR: VL-SurgPT是首个大规模多模态手术点追踪数据集，结合视觉追踪与文本描述，在复杂手术场景中显著提升追踪准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有手术追踪数据集缺乏语义上下文，难以理解追踪失败机制，无法应对烟雾遮挡、镜面反射和组织变形等复杂视觉条件。

Method: 构建包含908个体内视频片段的多模态数据集，包括组织追踪和器械追踪；提出TG-SurgPT文本引导追踪方法，利用语义描述提升鲁棒性。

Result: 实验表明，整合点状态信息显著提高了追踪准确性和可靠性，特别是在视觉条件恶劣的场景中，传统纯视觉方法表现不佳。

Conclusion: 通过桥接视觉和语言模态，VL-SurgPT能够开发上下文感知的追踪系统，在挑战性术中条件下保持性能，推动计算机辅助手术应用发展。

Abstract: Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.

</details>


### [44] [GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory](https://arxiv.org/abs/2511.12027)
*Jeong Hun Yeo,Sangyun Chung,Sungjune Park,Dae Hoe Kim,Jinyoung Moon,Yong Man Ro*

Main category: cs.CV

TL;DR: 提出了GCAgent框架，通过结构化的事件记忆和感知-行动-反思循环来解决长视频理解中的长期依赖问题，在Video-MME基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在长视频理解方面存在token限制和难以捕捉长期时间依赖性的问题，无法有效捕获全局上下文和复杂事件关系。

Method: 引入GCAgent框架，核心创新是示意图和叙事情景记忆，将事件及其因果时间关系结构化建模；采用多阶段的感知-行动-反思循环，通过记忆管理器检索相关情景上下文。

Result: 在Video-MME长视频分割上比强基线模型提升23.5%准确率，在7B规模MLLM中达到最先进性能，长分割准确率73.4%，总体平均71.9%。

Conclusion: 基于智能体的推理范式和结构化记忆为认知启发的长视频理解提供了有效解决方案，显著提升了长视频理解能力。

Abstract: Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\% accuracy on the Long split and the highest overall average (71.9\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.

</details>


### [45] [VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation](https://arxiv.org/abs/2511.12030)
*Jun Zhou,Chi Xu,Kaifeng Tang,Yuting Ge,Tingrui Guo,Li Cheng*

Main category: cs.CV

TL;DR: 提出了一种联合视觉和物理线索的手-物体3D姿态估计框架，通过视觉-物理线索联合学习和候选姿态聚合，实现视觉一致且物理合理的姿态估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖视觉线索，常产生违反物理约束的结果；而引入物理推理的方法通常依赖后优化或不可微物理引擎，影响视觉一致性和端到端训练能力。

Method: 1) 视觉-物理线索联合学习：训练模型提取2D视觉线索和3D物理线索；2) 候选姿态聚合：通过扩散生成多个候选姿态，利用视觉和物理预测进行聚合，获得最终估计。

Result: 在姿态准确性和物理合理性方面显著优于现有最先进方法。

Conclusion: 该框架成功整合了视觉和物理线索，实现了视觉一致且物理合理的手-物体姿态估计。

Abstract: Estimating the 3D poses of hands and objects from a single RGB image is a fundamental yet challenging problem, with broad applications in augmented reality and human-computer interaction. Existing methods largely rely on visual cues alone, often producing results that violate physical constraints such as interpenetration or non-contact. Recent efforts to incorporate physics reasoning typically depend on post-optimization or non-differentiable physics engines, which compromise visual consistency and end-to-end trainability. To overcome these limitations, we propose a novel framework that jointly integrates visual and physical cues for hand-object pose estimation. This integration is achieved through two key ideas: 1) joint visual-physical cue learning: The model is trained to extract 2D visual cues and 3D physical cues, thereby enabling more comprehensive representation learning for hand-object interactions; 2) candidate pose aggregation: A novel refinement process that aggregates multiple diffusion-generated candidate poses by leveraging both visual and physical predictions, yielding a final estimate that is visually consistent and physically plausible. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art approaches in both pose accuracy and physical plausibility.

</details>


### [46] [Improved Masked Image Generation with Knowledge-Augmented Token Representations](https://arxiv.org/abs/2511.12032)
*Guotao Liang,Baoquan Zhang,Zhiyuan Wen,Zihao Han,Yunming Ye*

Main category: cs.CV

TL;DR: KA-MIG框架通过引入token级语义依赖的显式知识作为先验，提升掩码图像生成的性能，包括三种知识图谱和轻量级融合机制。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖模型自身学习视觉token序列的语义依赖，但直接学习这种依赖很困难，因为单个token缺乏明确语义且序列通常很长。

Method: 提出KA-MIG框架，引入三种token知识图谱（共现图、语义相似图、位置token不兼容图），设计图感知编码器学习token和位置感知表示，并通过轻量级融合机制集成到现有MIG方法中。

Result: 实验结果表明，该方法在ImageNet上的类条件图像生成任务中优于现有MIG方法。

Conclusion: 通过引入先验知识，KA-MIG有效增强了模型捕捉语义依赖的能力，提高了生成质量。

Abstract: Masked image generation (MIG) has demonstrated remarkable efficiency and high-fidelity images by enabling parallel token prediction. Existing methods typically rely solely on the model itself to learn semantic dependencies among visual token sequences. However, directly learning such semantic dependencies from data is challenging because the individual tokens lack clear semantic meanings, and these sequences are usually long. To address this limitation, we propose a novel Knowledge-Augmented Masked Image Generation framework, named KA-MIG, which introduces explicit knowledge of token-level semantic dependencies (\emph{i.e.}, extracted from the training data) as priors to learn richer representations for improving performance. In particular, we explore and identify three types of advantageous token knowledge graphs, including two positive and one negative graphs (\emph{i.e.}, the co-occurrence graph, the semantic similarity graph, and the position-token incompatibility graph). Based on three prior knowledge graphs, we design a graph-aware encoder to learn token and position-aware representations. After that, a lightweight fusion mechanism is introduced to integrate these enriched representations into the existing MIG methods. Resorting to such prior knowledge, our method effectively enhances the model's ability to capture semantic dependencies, leading to improved generation quality. Experimental results demonstrate that our method improves upon existing MIG for class-conditional image generation on ImageNet.

</details>


### [47] [Calibrated Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2511.12034)
*Xiaohao Liu,Xiaobo Xia,Jiaheng Wei,Shuo Yang,Xiu Su,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出CalMRL方法解决多模态表示学习中因模态缺失导致的锚点偏移问题，通过表示级插补和双步学习来校准不完整对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要所有模态都存在才能实现跨模态对齐，无法有效利用普遍存在的缺失模态数据集。从锚点偏移的理论视角分析了模态缺失导致的对齐偏差问题。

Method: 提出CalMRL方法，利用模态先验和内在联系在表示层面对缺失模态进行插补，采用双步学习策略和共享潜变量的闭式后验分布解来优化。

Result: 理论验证了锚点偏移的缓解和收敛性，实验表明CalMRL优于现有方法，为吸收缺失模态数据提供了新的灵活性。

Conclusion: CalMRL有效解决了多模态表示学习中模态缺失带来的对齐问题，通过校准对齐机制实现了更好的多模态协同效果。

Abstract: Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.

</details>


### [48] [SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images](https://arxiv.org/abs/2511.12040)
*Xinyuan Hu,Changyue Shi,Chuxiao Yang,Minghao Chen,Jiajun Ding,Tao Wei,Chen Wei,Zhou Yu,Min Tan*

Main category: cs.CV

TL;DR: SRSplat是一个前馈式3D重建框架，能从少量低分辨率图像重建高分辨率3D场景，通过结合外部高质量参考图像和内部纹理线索来补偿纹理信息不足。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在稀疏低分辨率图像3D重建中无法恢复精细纹理细节的问题，因为低分辨率输入缺乏高频信息。

Method: 1) 使用MLLMs和扩散模型为每个场景构建特定参考图库；2) 引入参考引导特征增强模块对齐融合LR输入和参考图像特征；3) 训练解码器预测高斯基元；4) 引入纹理感知密度控制自适应调整高斯密度。

Result: 在RealEstate10K、ACID和DTU等多个数据集上优于现有方法，并展现出强大的跨数据集和跨分辨率泛化能力。

Conclusion: SRSplat通过有效结合外部参考信息和内部纹理线索，成功解决了稀疏低分辨率图像3D重建中的纹理细节恢复问题。

Abstract: Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.

</details>


### [49] [FedSDA: Federated Stain Distribution Alignment for Non-IID Histopathological Image Classification](https://arxiv.org/abs/2511.12044)
*Cheng-Chang Tsai,Kai-Wen Cheng,Chun-Shien Lu*

Main category: cs.CV

TL;DR: 提出FedSDA方法，通过扩散模型和染色分离技术对齐联邦学习中非IID组织病理图像的染色分布，解决数据分布偏移问题，同时避免隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中非IID组织病理图像的数据分布偏移是一个重要挑战，现有方法关注有限，需要从数据分布角度解决这一问题。

Method: 基于扩散模型拟合数据分布，利用染色分离提取关键特征，在联邦学习框架中对齐各客户端的染色分布到目标分布。

Result: 实验表明FedSDA能有效提升基线方法性能，优于其他从数据分布角度解决非IID问题的方法。

Conclusion: FedSDA为计算病理学社区提供了有价值的实用见解，能有效缓解联邦学习中的非IID数据问题。

Abstract: Federated learning (FL) has shown success in collaboratively training a model among decentralized data resources without directly sharing privacy-sensitive training data. Despite recent advances, non-IID (non-independent and identically distributed) data poses an inevitable challenge that hinders the use of FL. In this work, we address the issue of non-IID histopathological images with feature distribution shifts from an intuitive perspective that has only received limited attention. Specifically, we address this issue from the perspective of data distribution by solely adjusting the data distributions of all clients. Building on the success of diffusion models in fitting data distributions and leveraging stain separation to extract the pivotal features that are closely related to the non-IID properties of histopathological images, we propose a Federated Stain Distribution Alignment (FedSDA) method. FedSDA aligns the stain distribution of each client with a target distribution in an FL framework to mitigate distribution shifts among clients. Furthermore, considering that training diffusion models on raw data in FL has been shown to be susceptible to privacy leakage risks, we circumvent this problem while still effectively achieving alignment. Extensive experimental results show that FedSDA is not only effective in improving baselines that focus on mitigating disparities across clients' model updates but also outperforms baselines that address the non-IID data issues from the perspective of data distribution. We show that FedSDA provides valuable and practical insights for the computational pathology community.

</details>


### [50] [DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging](https://arxiv.org/abs/2511.12047)
*Huimin Cheng,Xiaowei Yu,Shushan Wu,Luyang Fang,Chao Cao,Jing Zhang,Tianming Liu,Dajiang Zhu,Wenxuan Zhong,Ping Ma*

Main category: cs.CV

TL;DR: DCMM-Transformer是一种新颖的视觉Transformer架构，通过将度校正混合成员模型作为自注意力中的加性偏置，在医学图像分析中有效利用解剖结构信息。


<details>
  <summary>Details</summary>
Motivation: 医学图像存在潜在的解剖分组（如器官、组织和病理区域），但标准ViT无法有效利用这些结构信息。现有方法如SBM-Transformer存在不可微分、训练不稳定和无法建模复杂社区结构的问题。

Method: 提出DCMM-Transformer，将度校正混合成员模型作为自注意力中的加性偏置引入，以完全可微分和可解释的方式建模社区结构和度异质性，避免了乘性掩码和二元采样。

Result: 在包括脑部、胸部、乳腺和眼部等多种医学成像数据集上的综合实验表明，该方法具有优越的性能和泛化能力。

Conclusion: 所学习的组结构和结构化注意力调制显著增强了可解释性，产生了具有解剖意义和语义一致性的注意力图。

Abstract: Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.

</details>


### [51] [DeiTFake: Deepfake Detection Model using DeiT Multi-Stage Training](https://arxiv.org/abs/2511.12048)
*Saksham Kumar,Ashish Singh,Srinivasarao Thota,Sunil Kumar Singh,Chandan Kumar*

Main category: cs.CV

TL;DR: DeiTFake是一种基于DeiT变换器的深度伪造检测方法，采用两阶段渐进式训练策略，在OpenForensics数据集上达到99.22%的准确率和0.9997的AUROC，优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 深度伪造对数字媒体完整性构成重大威胁，需要开发更鲁棒的检测方法来识别操纵痕迹。

Method: 提出DeiT-based变换器架构和两阶段渐进训练策略：第一阶段使用标准增强进行迁移学习，第二阶段使用高级仿射和深度伪造特定增强进行微调。利用DeiT的知识蒸馏模型捕捉细微的操纵伪影。

Result: 在OpenForensics数据集(190,335张图像)上，第一阶段达到98.71%准确率，第二阶段达到99.22%准确率和0.9997 AUROC，优于最新的OpenForensics基准。

Conclusion: 该方法通过分析增强影响和训练计划，为面部深度伪造检测提供了实用的基准，证明了DeiTFake在检测深度伪造方面的有效性。

Abstract: Deepfakes are major threats to the integrity of digital media. We propose DeiTFake, a DeiT-based transformer and a novel two-stage progressive training strategy with increasing augmentation complexity. The approach applies an initial transfer-learning phase with standard augmentations followed by a fine-tuning phase using advanced affine and deepfake-specific augmentations. DeiT's knowledge distillation model captures subtle manipulation artifacts, increasing robustness of the detection model. Trained on the OpenForensics dataset (190,335 images), DeiTFake achieves 98.71\% accuracy after stage one and 99.22\% accuracy with an AUROC of 0.9997, after stage two, outperforming the latest OpenForensics baselines. We analyze augmentation impact and training schedules, and provide practical benchmarks for facial deepfake detection.

</details>


### [52] [UniABG: Unified Adversarial View Bridging and Graph Correspondence for Unsupervised Cross-View Geo-Localization](https://arxiv.org/abs/2511.12054)
*Cuiqun Chen,Qi Chen,Bin Yang,Xingyi Zhang*

Main category: cs.CV

TL;DR: UniABG是一个新颖的双阶段无监督跨视角地理定位框架，集成了对抗性视角桥接和图基对应校准，无需成对标注即可实现跨视角图像匹配。


<details>
  <summary>Details</summary>
Motivation: 解决跨视角地理定位中监督方法依赖大量成对标注的问题，以及无监督方法因跨视角域差异导致伪标签噪声的问题。

Method: 采用双阶段框架：1) 视角感知对抗桥接(VAAB)建模视角不变特征增强伪标签鲁棒性；2) 异构图滤波校准(HGFC)构建双跨视角结构图精化视角对应关系。

Result: 在University-1652数据集上卫星→无人机AP提升+10.63%，在SUES-200数据集上提升+16.73%，甚至超越监督基线方法。

Conclusion: UniABG在无监督跨视角地理定位中实现了最先进的性能，证明了对抗性视角桥接和图基对应校准的有效性。

Abstract: Cross-view geo-localization (CVGL) matches query images ($\textit{e.g.}$, drone) to geographically corresponding opposite-view imagery ($\textit{e.g.}$, satellite). While supervised methods achieve strong performance, their reliance on extensive pairwise annotations limits scalability. Unsupervised alternatives avoid annotation costs but suffer from noisy pseudo-labels due to intrinsic cross-view domain gaps. To address these limitations, we propose $\textit{UniABG}$, a novel dual-stage unsupervised cross-view geo-localization framework integrating adversarial view bridging with graph-based correspondence calibration. Our approach first employs View-Aware Adversarial Bridging (VAAB) to model view-invariant features and enhance pseudo-label robustness. Subsequently, Heterogeneous Graph Filtering Calibration (HGFC) refines cross-view associations by constructing dual inter-view structure graphs, achieving reliable view correspondence. Extensive experiments demonstrate state-of-the-art unsupervised performance, showing that UniABG improves Satellite $\rightarrow$ Drone AP by +10.63\% on University-1652 and +16.73\% on SUES-200, even surpassing supervised baselines. The source code is available at https://github.com/chenqi142/UniABG

</details>


### [53] [PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling](https://arxiv.org/abs/2511.12056)
*Sijie Wang,Qiang Wang,Shaohuai Shi*

Main category: cs.CV

TL;DR: PipeDiT是一个用于加速视频生成的流水线框架，通过序列并行、模块解耦和注意力协同处理等技术，显著降低了推理延迟和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 基于扩散变换器的视频生成模型虽然性能出色，但实际部署时面临推理速度慢和内存消耗高的问题，阻碍了其广泛应用。

Method: 提出了三种创新技术：1) PipeSP流水线算法实现序列并行；2) DeDiVAE将扩散模块和VAE模块解耦到不同GPU组；3) Aco注意力协同处理方法优化GPU资源利用。

Result: 在OpenSoraPlan和HunyuanVideo两个开源框架上的实验表明，PipeDiT在多种分辨率和时间步配置下实现了1.06x到4.02x的加速比。

Conclusion: PipeDiT框架有效解决了视频生成模型的推理延迟和内存瓶颈问题，为实际部署提供了可行的解决方案。

Abstract: Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.

</details>


### [54] [MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity](https://arxiv.org/abs/2511.12061)
*Zhichen Lai,Hua Lu,Huan Li,Jialiang Li,Christian S. Jensen*

Main category: cs.CV

TL;DR: MovSemCL是一个基于运动语义对比学习的轨迹相似度计算框架，通过运动语义特征提取、层次化表示和物理合理的增强策略，解决了现有方法在语义建模、计算效率和增强合理性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的轨迹相似度计算方法存在三个关键问题：轨迹语义和层次结构建模不足、点级编码计算成本高、以及使用物理上不合理的增强方法扭曲轨迹语义。

Method: 将原始GPS轨迹转换为运动语义特征并分割成片段，使用片段内和片段间注意力机制编码局部和全局轨迹模式，采用曲率引导的增强策略保留信息丰富的片段并屏蔽冗余片段。

Result: 在真实数据集上的实验表明，MovSemCL在相似度搜索任务中平均排名接近理想值1，在启发式近似任务中提升高达20.3%，同时推理延迟降低高达43.4%。

Conclusion: MovSemCL通过有效的运动语义建模、层次化表示和物理合理的增强策略，显著提升了轨迹相似度计算的性能和效率。

Abstract: Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.

</details>


### [55] [DCA-LUT: Deep Chromatic Alignment with 5D LUT for Purple Fringing Removal](https://arxiv.org/abs/2511.12066)
*Jialang Lu,Shuning Sun,Pu Wang,Chen Wu,Feng Gao,Lina Gong,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: DCA-LUT是首个基于深度学习的紫色边缘去除框架，通过色度感知坐标变换模块分离紫色边缘到专用维度，并使用5D查找表进行色彩校正，在合成和真实数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 紫色边缘是由相机镜头纵向色差引起的持久伪影，传统解决方案依赖复杂昂贵的复消色差镜头硬件和手工特征提取，忽略了数据驱动方法。

Method: 提出色度感知坐标变换模块学习图像自适应色彩空间，将紫色边缘解耦分离到专用维度，然后通过学习的紫色边缘通道指导亮度通道恢复，最后使用5D查找表进行非线性色彩映射。

Result: 在合成和真实数据集上的广泛实验表明，该方法在紫色边缘去除方面达到了最先进的性能。

Conclusion: DCA-LUT框架通过数据驱动方法有效解决了紫色边缘问题，无需昂贵硬件，实现了高质量的图像恢复。

Abstract: Purple fringing, a persistent artifact caused by Longitudinal Chromatic Aberration (LCA) in camera lenses, has long degraded the clarity and realism of digital imaging. Traditional solutions rely on complex and expensive apochromatic (APO) lens hardware and the extraction of handcrafted features, ignoring the data-driven approach. To fill this gap, we introduce DCA-LUT, the first deep learning framework for purple fringing removal. Inspired by the physical root of the problem, the spatial misalignment of RGB color channels due to lens dispersion, we introduce a novel Chromatic-Aware Coordinate Transformation (CA-CT) module, learning an image-adaptive color space to decouple and isolate fringing into a dedicated dimension. This targeted separation allows the network to learn a precise ``purple fringe channel", which then guides the accurate restoration of the luminance channel. The final color correction is performed by a learned 5D Look-Up Table (5D LUT), enabling efficient and powerful% non-linear color mapping. To enable robust training and fair evaluation, we constructed a large-scale synthetic purple fringing dataset (PF-Synth). Extensive experiments in synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance in purple fringing removal.

</details>


### [56] [Learning to Hear by Seeing: It's Time for Vision Language Models to Understand Artistic Emotion from Sight and Sound](https://arxiv.org/abs/2511.12077)
*Dengming Zhang,Weitao You,Jingxiong Li,Weishen Lin,Wenda Shi,Xue Zhao,Heda Zuo,Junxian Wu,Lingyun Sun*

Main category: cs.CV

TL;DR: VAEmotionLLM是一个两阶段框架，通过有限音频预训练教导视觉语言模型具备听觉能力，并增强跨模态情感理解。第一阶段通过视觉引导音频对齐实现听觉，第二阶段通过轻量级跨模态情感适配器增强情感理解。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉语言模型通常需要大规模音频预训练，限制了可扩展性；同时大多数工作忽视了艺术作品有意表达的情感，需要更好的跨模态情感理解能力。

Method: 两阶段框架：1) VG-Align通过将冻结的视觉通路蒸馏到新的音频通路，在同步音频-视频片段上对齐共享LLM的下一个token分布；2) EmoAdapter通过情感增强器和情感监督器注入情感敏感残差并应用情感监督。

Result: 在构建的ArtEmoBenchmark上实现了最先进的结果，超越了音频、视觉和音频-视觉基线模型。消融实验表明各组件具有互补性。

Conclusion: VAEmotionLLM成功实现了在有限音频预训练下赋予视觉模型听觉能力，并显著提升了跨模态情感理解性能，为艺术情感分析提供了有效解决方案。

Abstract: Emotion understanding is critical for making Large Language Models (LLMs) more general, reliable, and aligned with humans. Art conveys emotion through the joint design of visual and auditory elements, yet most prior work is human-centered or single-modality, overlooking the emotion intentionally expressed by the artwork. Meanwhile, current Audio-Visual Language Models (AVLMs) typically require large-scale audio pretraining to endow Visual Language Models (VLMs) with hearing, which limits scalability. We present Vision Anchored Audio-Visual Emotion LLM (VAEmotionLLM), a two-stage framework that teaches a VLM to hear by seeing with limited audio pretraining and to understand emotion across modalities. In Stage 1, Vision-Guided Audio Alignment (VG-Align) distills the frozen visual pathway into a new audio pathway by aligning next-token distributions of the shared LLM on synchronized audio-video clips, enabling hearing without a large audio dataset. In Stage 2, a lightweight Cross-Modal Emotion Adapter (EmoAdapter), composed of the Emotion Enhancer and the Emotion Supervisor, injects emotion-sensitive residuals and applies emotion supervision to enhance cross-modal emotion understanding. We also construct ArtEmoBenchmark, an art-centric emotion benchmark that evaluates content and emotion understanding under audio-only, visual-only, and audio-visual inputs. VAEmotionLLM achieves state-of-the-art results on ArtEmoBenchmark, outperforming audio-only, visual-only, and audio-visual baselines. Ablations show that the proposed components are complementary.

</details>


### [57] [Point Cloud Quantization through Multimodal Prompting for 3D Understanding](https://arxiv.org/abs/2511.12079)
*Hongxuan Li,Wencheng Zhu,Huiying Xu,Xinzhong Zhu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 提出了一种基于多模态提示的向量量化框架，通过文本嵌入作为原型先验，结合紧凑性和分离性约束，在点云分析中实现几何和语义信息的联合编码。


<details>
  <summary>Details</summary>
Motivation: 现有基于可训练向量或聚类质心的原型方法在代表性和可解释性方面存在不足，而多模态对齐在视觉语言模型中显示出潜力。

Method: 使用预训练模型的文本嵌入作为原型先验，通过多模态提示进行自适应优化，引入双约束量化空间和Gumbel-Softmax松弛实现可微离散化。

Result: 在ModelNet40和ScanObjectNN数据集上的广泛实验证明了该方法的优越有效性。

Conclusion: 该方法通过多模态提示驱动的量化框架，有效解决了原型代表性和可解释性问题，在点云分析中取得了优异性能。

Abstract: Vector quantization has emerged as a powerful tool in large-scale multimodal models, unifying heterogeneous representations through discrete token encoding. However, its effectiveness hinges on robust codebook design. Current prototype-based approaches relying on trainable vectors or clustered centroids fall short in representativeness and interpretability, even as multimodal alignment demonstrates its promise in vision-language models. To address these limitations, we propose a simple multimodal prompting-driven quantization framework for point cloud analysis. Our methodology is built upon two core insights: 1) Text embeddings from pre-trained models inherently encode visual semantics through many-to-one contrastive alignment, naturally serving as robust prototype priors; and 2) Multimodal prompts enable adaptive refinement of these prototypes, effectively mitigating vision-language semantic gaps. The framework introduces a dual-constrained quantization space, enforced by compactness and separation regularization, which seamlessly integrates visual and prototype features, resulting in hybrid representations that jointly encode geometric and semantic information. Furthermore, we employ Gumbel-Softmax relaxation to achieve differentiable discretization while maintaining quantization sparsity. Extensive experiments on the ModelNet40 and ScanObjectNN datasets clearly demonstrate the superior effectiveness of the proposed method.

</details>


### [58] [Supervised Multilabel Image Classification Using Residual Networks with Probabilistic Reasoning](https://arxiv.org/abs/2511.12082)
*Lokender Singh,Saksham Kumar,Chandan Kumar*

Main category: cs.CV

TL;DR: 提出了一种基于改进ResNet-101架构和概率推理的多标签图像分类方法，在COCO-2014数据集上实现了0.794 mAP的优异性能，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 多标签图像分类在计算机视觉中有广泛应用，但现有方法在处理标签依赖性和不确定性方面存在挑战，需要更有效的方法来提升分类准确性。

Method: 使用改进的ResNet-101架构，通过概率推理模拟标签依赖性和不确定性，将概率推理集成到深度学习模型中。

Result: 在COCO-2014数据集上达到0.794 mAP，优于ResNet-SRN（0.771）和Vision Transformer基线（0.785），在多标签分类任务中表现出色。

Conclusion: 将概率推理集成到深度学习模型中能有效解决多标签场景的挑战，该方法在多标签图像分类中取得了先进水平的成果。

Abstract: Multilabel image categorization has drawn interest recently because of its numerous computer vision applications. The proposed work introduces a novel method for classifying multilabel images using the COCO-2014 dataset and a modified ResNet-101 architecture. By simulating label dependencies and uncertainties, the approach uses probabilistic reasoning to improve prediction accuracy. Extensive tests show that the model outperforms earlier techniques and approaches to state-of-the-art outcomes in multilabel categorization. The work also thoroughly assesses the model's performance using metrics like precision-recall score and achieves 0.794 mAP on COCO-2014, outperforming ResNet-SRN (0.771) and Vision Transformer baselines (0.785). The novelty of the work lies in integrating probabilistic reasoning into deep learning models to effectively address the challenges presented by multilabel scenarios.

</details>


### [59] [SemanticStitch: Enhancing Image Coherence through Foreground-Aware Seam Carving](https://arxiv.org/abs/2511.12084)
*Ji-Ping Jin,Chen-Bin Feng,Rui Fan,Chi-Man Vong*

Main category: cs.CV

TL;DR: 提出SemanticStitch框架，通过融入语义先验来保护前景对象完整性，改善图像拼接质量


<details>
  <summary>Details</summary>
Motivation: 传统图像拼接方法因视角、位置差异和物体移动导致错位和视觉不一致，且忽略语义信息破坏前景连续性

Method: 基于深度学习的框架，引入新颖的损失函数强调显著对象的语义完整性，并创建两个专用真实世界数据集

Result: 实验结果显示相比传统技术有显著改进，为实际应用提供有力支持

Conclusion: SemanticStitch能够有效保护前景对象完整性并增强视觉一致性，在图像拼接任务中表现优异

Abstract: Image stitching often faces challenges due to varying capture angles, positional differences, and object movements, leading to misalignments and visual discrepancies. Traditional seam carving methods neglect semantic information, causing disruptions in foreground continuity. We introduce SemanticStitch, a deep learning-based framework that incorporates semantic priors of foreground objects to preserve their integrity and enhance visual coherence. Our approach includes a novel loss function that emphasizes the semantic integrity of salient objects, significantly improving stitching quality. We also present two specialized real-world datasets to evaluate our method's effectiveness. Experimental results demonstrate substantial improvements over traditional techniques, providing robust support for practical applications.

</details>


### [60] [Teaching Prompts to Coordinate: Hierarchical Layer-Grouped Prompt Tuning for Continual Learning](https://arxiv.org/abs/2511.12090)
*Shengqin Jiang,Tianqi Kong,Yuankai Qi,Haokui Zhang,Lina Yao,Quan Z. Sheng,Qingshan Liu,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出了一种分层分组提示调优方法用于持续学习，通过共享提示和根提示生成机制来减少层间独立性，提高模型稳定性并缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 传统基于提示的持续学习方法在每个层独立添加任务特定提示，虽然灵活但可能导致某些层不必要更新，容易覆盖先前任务的关键特征表示，增加灾难性遗忘风险。

Method: 分层分组提示调优：1) 同组层共享相似提示并通过位置编码调整；2) 使用单一任务特定根提示生成各层组的子提示，增强子提示间的协同性。

Result: 在四个基准测试上的广泛实验表明，该方法相比多个最先进方法取得了优越性能。

Conclusion: 所提出的分层分组提示调优方法通过减少层间独立性和增强提示协同性，有效提高了持续学习中的模型稳定性，缓解了灾难性遗忘问题。

Abstract: Prompt-based continual learning methods fine-tune only a small set of additional learnable parameters while keeping the pre-trained model's parameters frozen. It enables efficient adaptation to new tasks while mitigating the risk of catastrophic forgetting. These methods typically attach one independent task-specific prompt to each layer of pre-trained models to locally modulate its features, ensuring that the layer's representation aligns with the requirements of the new task. However, although introducing learnable prompts independently at each layer provides high flexibility for adapting to new tasks, this overly flexible tuning could make certain layers susceptible to unnecessary updates. As all prompts till the current task are added together as a final prompt for all seen tasks, the model may easily overwrite feature representations essential to previous tasks, which increases the risk of catastrophic forgetting. To address this issue, we propose a novel hierarchical layer-grouped prompt tuning method for continual learning. It improves model stability in two ways: (i) Layers in the same group share roughly the same prompts, which are adjusted by position encoding. This helps preserve the intrinsic feature relationships and propagation pathways of the pre-trained model within each group. (ii) It utilizes a single task-specific root prompt to learn to generate sub-prompts for each layer group. In this way, all sub-prompts are conditioned on the same root prompt, enhancing their synergy and reducing independence. Extensive experiments across four benchmarks demonstrate that our method achieves favorable performance compared with several state-of-the-art methods.

</details>


### [61] [Learning from Dense Events: Towards Fast Spiking Neural Networks Training via Event Dataset Distillatio](https://arxiv.org/abs/2511.12095)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Kun Wang,Xudong Jiang*

Main category: cs.CV

TL;DR: PACE是首个针对SNN和事件视觉的数据集蒸馏框架，通过压缩大型训练数据集为紧凑的合成数据集，显著降低SNN训练成本。


<details>
  <summary>Details</summary>
Motivation: SNN由于时间编码导致训练成本高昂，限制了实际部署。需要一种方法来降低训练成本，同时保持性能。

Method: PACE包含两个核心模块：ST-DSM使用残差膜电位来密集化基于脉冲的特征，并进行细粒度的时空匹配；PEQ-N提供即插即用的概率整数量化器。

Result: 在多个数据集上，PACE优于现有的核心集选择和数据集蒸馏基准方法，特别是在动态事件流和低/中等IPC情况下表现突出。在N-MNIST上达到84.4%准确率，约为完整训练集性能的85%，同时减少训练时间50倍以上，存储成本6000倍。

Conclusion: PACE能够生成紧凑的替代数据集，实现分钟级的SNN训练和高效的边缘部署，为SNN的实际应用提供了可行的解决方案。

Abstract: Event cameras sense brightness changes and output binary asynchronous event streams, attracting increasing attention. Their bio-inspired dynamics align well with spiking neural networks (SNNs), offering a promising energy-efficient alternative to conventional vision systems. However, SNNs remain costly to train due to temporal coding, which limits their practical deployment. To alleviate the high training cost of SNNs, we introduce \textbf{PACE} (Phase-Aligned Condensation for Events), the first dataset distillation framework to SNNs and event-based vision. PACE distills a large training dataset into a compact synthetic one that enables fast SNN training, which is achieved by two core modules: \textbf{ST-DSM} and \textbf{PEQ-N}. ST-DSM uses residual membrane potentials to densify spike-based features (SDR) and to perform fine-grained spatiotemporal matching of amplitude and phase (ST-SM), while PEQ-N provides a plug-and-play straight through probabilistic integer quantizer compatible with standard event-frame pipelines. Across DVS-Gesture, CIFAR10-DVS, and N-MNIST datasets, PACE outperforms existing coreset selection and dataset distillation baselines, with particularly strong gains on dynamic event streams and at low or moderate IPC. Specifically, on N-MNIST, it achieves \(84.4\%\) accuracy, about \(85\%\) of the full training set performance, while reducing training time by more than \(50\times\) and storage cost by \(6000\times\), yielding compact surrogates that enable minute-scale SNN training and efficient edge deployment.

</details>


### [62] [Sparse by Rule: Probability-Based N:M Pruning for Spiking Neural Networks](https://arxiv.org/abs/2511.12097)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Xudong Jiang,Dacheng Tao*

Main category: cs.CV

TL;DR: SpikeNM是首个面向SNN的半结构化N:M剪枝框架，通过M路基对数参数化和可微分top-k采样器，线性化计算复杂度，结合神经科学启发的资格蒸馏方法，在保持精度的同时实现硬件友好的稀疏模式。


<details>
  <summary>Details</summary>
Motivation: 解决SNN深度架构参数膨胀和计算成本高的问题，现有剪枝方法要么难以硬件加速（非结构化），要么缺乏灵活性且精度下降（结构化），需要一种平衡硬件友好性和精度的半结构化剪枝方案。

Method: 采用N:M半结构化剪枝框架，使用M路基对数参数化和可微分top-k采样器线性化计算复杂度；提出资格启发蒸馏（EID）方法，将时间累积信用转换为块级软目标，对齐掩码概率与脉冲动态。

Result: 在2:4稀疏度下，SpikeNM在主流数据集上保持甚至提升精度，同时产生硬件友好的稀疏模式，与固有脉冲稀疏性互补。

Conclusion: SpikeNM成功实现了SNN的高效半结构化剪枝，平衡了硬件加速需求和模型精度，为边缘部署提供了可行的解决方案。

Abstract: Brain-inspired Spiking neural networks (SNNs) promise energy-efficient intelligence via event-driven, sparse computation, but deeper architectures inflate parameters and computational cost, hindering their edge deployment. Recent progress in SNN pruning helps alleviate this burden, yet existing efforts fall into only two families: \emph{unstructured} pruning, which attains high sparsity but is difficult to accelerate on general hardware, and \emph{structured} pruning, which eases deployment but lack flexibility and often degrades accuracy at matched sparsity. In this work, we introduce \textbf{SpikeNM}, the first SNN-oriented \emph{semi-structured} \(N{:}M\) pruning framework that learns sparse SNNs \emph{from scratch}, enforcing \emph{at most \(N\)} non-zeros per \(M\)-weight block. To avoid the combinatorial space complexity \(\sum_{k=1}^{N}\binom{M}{k}\) growing exponentially with \(M\), SpikeNM adopts an \(M\)-way basis-logit parameterization with a differentiable top-\(k\) sampler, \emph{linearizing} per-block complexity to \(\mathcal O(M)\) and enabling more aggressive sparsification. Further inspired by neuroscience, we propose \emph{eligibility-inspired distillation} (EID), which converts temporally accumulated credits into block-wise soft targets to align mask probabilities with spiking dynamics, reducing sampling variance and stabilizing search under high sparsity. Experiments show that at \(2{:}4\) sparsity, SpikeNM maintains and even with gains across main-stream datasets, while yielding hardware-amenable patterns that complement intrinsic spike sparsity.

</details>


### [63] [DINOv3-Guided Cross Fusion Framework for Semantic-aware CT generation from MRI and CBCT](https://arxiv.org/abs/2511.12098)
*Xianhao Zhou,Jianghao Wu,Ku Zhao,Jinlong He,Huangxuan Zhao,Lei Chen,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

TL;DR: 提出DGCF框架，结合冻结的DINOv3 Transformer和可训练的CNN编码器-解码器，通过交叉融合模块平衡局部外观和上下文表示，在SynthRAD2023盆腔数据集上实现最先进的MRI→CT和CBCT→CT转换性能。


<details>
  <summary>Details</summary>
Motivation: 现有CNN模型缺乏全局语义理解，而Transformer在小规模医学数据集上容易过拟合，需要平衡局部特征和全局语义表示。

Method: 使用冻结的自监督DINOv3 Transformer与可训练CNN编码器-解码器分层融合，通过交叉融合模块整合全局表示和局部特征，并引入多级DINOv3感知损失。

Result: 在SynthRAD2023盆腔数据集上，DGCF在MS-SSIM、PSNR和基于分割的指标上均达到最先进性能，适用于MRI→CT和CBCT→CT转换任务。

Conclusion: 这是首个利用DINOv3表示进行医学图像转换的工作，展示了自监督Transformer指导在语义感知CT合成中的潜力。

Abstract: Generating synthetic CT images from CBCT or MRI has a potential for efficient radiation dose planning and adaptive radiotherapy. However, existing CNN-based models lack global semantic understanding, while Transformers often overfit small medical datasets due to high model capacity and weak inductive bias. To address these limitations, we propose a DINOv3-Guided Cross Fusion (DGCF) framework that integrates a frozen self-supervised DINOv3 Transformer with a trainable CNN encoder-decoder. It hierarchically fuses global representation of Transformer and local features of CNN via a learnable cross fusion module, achieving balanced local appearance and contextual representation. Furthermore, we introduce a Multi-Level DINOv3 Perceptual (MLDP) loss that encourages semantic similarity between synthetic CT and the ground truth in DINOv3's feature space. Experiments on the SynthRAD2023 pelvic dataset demonstrate that DGCF achieved state-of-the-art performance in terms of MS-SSIM, PSNR and segmentation-based metrics on both MRI$\rightarrow$CT and CBCT$\rightarrow$CT translation tasks. To the best of our knowledge, this is the first work to employ DINOv3 representations for medical image translation, highlighting the potential of self-supervised Transformer guidance for semantic-aware CT synthesis. The code is available at https://github.com/HiLab-git/DGCF.

</details>


### [64] [Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models](https://arxiv.org/abs/2511.12099)
*Tianle Cheng,Zeyan Zhang,Kaifeng Gao,Jun Xiao*

Main category: cs.CV

TL;DR: 提出Adaptive Begin-of-Video Tokens (ada-BOV)方法，用于自回归视频扩散模型，通过自适应层归一化调制吸收去噪的前帧，保持全局一致性并改善局部动态质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在生成长视频时存在两种主要范式：基于块的扩展方法存在去噪延迟和误差累积问题；流去噪方法虽然支持实时采样，但一致性脆弱且运动动态差。

Method: 1. 提出自适应视频起始标记(ada-BOV)，通过自适应层归一化调制吸收前帧信息；2. 流去噪细化策略，解耦采样轨迹长度与注意力窗口约束；3. 扰动增强训练噪声调度，平衡收敛速度与模型鲁棒性。

Result: 在多个指标上取得了令人信服的定性和定量结果，证明了方法的有效性。

Conclusion: 提出的ada-BOV方法能够有效解决自回归视频扩散模型中的一致性和动态质量问题，为长视频生成提供了可行的解决方案。

Abstract: Recent advancements in diffusion-based video generation have produced impressive and high-fidelity short videos. To extend these successes to generate coherent long videos, most video diffusion models (VDMs) generate videos in an autoregressive manner, i.e., generating subsequent frames conditioned on previous ones. There are generally two primary paradigms: chunk-based extension and stream denoising. The former directly concatenates previous clean frames as conditioning, suffering from denoising latency and error accumulation. The latter maintains the denoising sequence with monotonically increasing noise levels. In each denoising iteration, one clean frame is produced while a new pure noise is simultaneously appended, enabling live-stream sampling. However, it struggles with fragile consistency and poor motion dynamics. In this paper, we propose Adaptive Begin-of-Video Tokens (ada-BOV) for autoregressive VDMs. The BOV tokens are special learnable embeddings on VDMs. They adaptively absorb denoised preceding frames via an adaptive-layer-norm-like modulation. This design preserves the global consistency while allowing for flexible conditioning in dynamic scenarios. To ensure the quality of local dynamics essential in modulating BOV tokens, we further propose a refinement strategy for stream denoising. It decouples the sampling trajectory length from the attention window size constraint, leading to improved local guidance and overall imaging quality. We also propose a disturbance-augmented training noise schedule, which balances the convergence speed with model robustness for the stream denoising. Extensive experiments demonstrate that our method achieves compelling qualitative and quantitative results across multiple metrics.

</details>


### [65] [Did Models Sufficient Learn? Attribution-Guided Training via Subset-Selected Counterfactual Augmentation](https://arxiv.org/abs/2511.12100)
*Yannan Chen,Ruoyu Chen,Bin Zeng,Wei Wang,Shiming Liu,Qunli Zhang,Zheng Hu,Laiyuan Wang,Yaowei Wang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出SS-CA方法，通过反事实解释增强训练，解决视觉模型对关键特征过度依赖的问题，提升泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型仅依赖有限的关键特征进行预测，导致对分布变化或关键特征缺失敏感。模型与人类在识别反事实样本时存在差异，表明模型学习到的依赖关系不够因果。

Method: 基于LIMA归因方法开发Counterfactual LIMA，识别最小空间区域集，其移除可选择性改变模型预测。利用这些归因，提出数据增强策略，将识别区域替换为自然背景，并在增强和原始样本上联合训练模型。

Result: 在多个ImageNet变体上的实验表明，SS-CA提高了ID测试数据的泛化能力，在OOD基准（如ImageNet-R和ImageNet-S）上表现更优。在噪声等扰动下，SS-CA训练的模型也展现出增强的泛化能力。

Conclusion: SS-CA方法有效利用可解释性洞察纠正模型缺陷，提高性能和鲁棒性，证明将反事实解释整合到训练过程中的有效性。

Abstract: In current visual model training, models often rely on only limited sufficient causes for their predictions, which makes them sensitive to distribution shifts or the absence of key features. Attribution methods can accurately identify a model's critical regions. However, masking these areas to create counterfactuals often causes the model to misclassify the target, while humans can still easily recognize it. This divergence highlights that the model's learned dependencies may not be sufficiently causal. To address this issue, we propose Subset-Selected Counterfactual Augmentation (SS-CA), which integrates counterfactual explanations directly into the training process for targeted intervention. Building on the subset-selection-based LIMA attribution method, we develop Counterfactual LIMA to identify minimal spatial region sets whose removal can selectively alter model predictions. Leveraging these attributions, we introduce a data augmentation strategy that replaces the identified regions with natural background, and we train the model jointly on both augmented and original samples to mitigate incomplete causal learning. Extensive experiments across multiple ImageNet variants show that SS-CA improves generalization on in-distribution (ID) test data and achieves superior performance on out-of-distribution (OOD) benchmarks such as ImageNet-R and ImageNet-S. Under perturbations including noise, models trained with SS-CA also exhibit enhanced generalization, demonstrating that our approach effectively uses interpretability insights to correct model deficiencies and improve both performance and robustness.

</details>


### [66] [BdSL-SPOTER: A Transformer-Based Framework for Bengali Sign Language Recognition with Cultural Adaptation](https://arxiv.org/abs/2511.12103)
*Sayad Ibna Azad,Md. Atiqur Rahman*

Main category: cs.CV

TL;DR: BdSL-SPOTER是一个基于姿态的transformer框架，用于准确高效地识别孟加拉手语，在BdSLW60基准测试中达到97.92%的Top-1验证准确率，比基线模型提升22.82%，同时保持低计算成本。


<details>
  <summary>Details</summary>
Motivation: 为孟加拉手语开发一个准确且高效的手语识别系统，解决低资源区域手语识别问题，并为其他低资源区域手语提供可扩展的模型。

Method: 扩展SPOTER范式，采用文化特定的预处理，使用紧凑的四层transformer编码器，优化可学习位置编码，并应用课程学习来增强泛化能力和加速收敛。

Result: 在BdSLW60基准测试中达到97.92%的Top-1验证准确率，比Bi-LSTM基线提升22.82%，同时参数数量更少、FLOPs更低、FPS更高。

Conclusion: BdSL-SPOTER为现实世界的无障碍应用提供了一个实用框架，并为其他低资源区域手语提供了一个可扩展的模型。

Abstract: We introduce BdSL-SPOTER, a pose-based transformer framework for accurate and efficient recognition of Bengali Sign Language (BdSL). BdSL-SPOTER extends the SPOTER paradigm with cultural specific preprocessing and a compact four-layer transformer encoder featuring optimized learnable positional encodings, while employing curriculum learning to enhance generalization on limited data and accelerate convergence. On the BdSLW60 benchmark, it achieves 97.92% Top-1 validation accuracy, representing a 22.82% improvement over the Bi-LSTM baseline, all while keeping computational costs low. With its reduced number of parameters, lower FLOPs, and higher FPS, BdSL-SPOTER provides a practical framework for real-world accessibility applications and serves as a scalable model for other low-resource regional sign languages.

</details>


### [67] [TEMPO: Global Temporal Building Density and Height Estimation from Satellite Imagery](https://arxiv.org/abs/2511.12104)
*Tammy Glazer,Gilles Q. Hacheme,Akram Zaytar,Luana Marotti,Amy Michaels,Girmaw Abebe Tadesse,Kevin White,Rahul Dodhia,Andrew Zolli,Inbal Becker-Reshef,Juan M. Lavista Ferres,Caleb Robinson*

Main category: cs.CV

TL;DR: TEMPO是一个基于深度学习从高分辨率卫星图像生成的全球建筑密度和高度数据集，具有时间分辨率，覆盖2018年第一季度至2025年第二季度，空间分辨率为37.6米/像素。


<details>
  <summary>Details</summary>
Motivation: 需要大规模监测全球发展模式和气候影响，为全球韧性和适应工作提供支持，但现有方法计算成本高昂。

Method: 将现有建筑足迹和高度数据与季度PlanetScope卫星图像配对，训练多任务深度学习模型预测建筑密度和高度。

Result: 验证结果显示F1分数在85%-88%之间，时间稳定性高（五年趋势一致性得分0.96），计算成本远低于可比方法。

Conclusion: TEMPO能以较低计算成本捕获建成区的季度变化，为大规模监测发展模式和气候影响提供了可行方案。

Abstract: We present TEMPO, a global, temporally resolved dataset of building density and height derived from high-resolution satellite imagery using deep learning models. We pair building footprint and height data from existing datasets with quarterly PlanetScope basemap satellite images to train a multi-task deep learning model that predicts building density and building height at a 37.6-meter per pixel resolution. We apply this model to global PlanetScope basemaps from Q1 2018 through Q2 2025 to create global, temporal maps of building density and height. We validate these maps by comparing against existing building footprint datasets. Our estimates achieve an F1 score between 85% and 88% on different hand-labeled subsets, and are temporally stable, with a 0.96 five-year trend-consistency score. TEMPO captures quarterly changes in built settlements at a fraction of the computational cost of comparable approaches, unlocking large-scale monitoring of development patterns and climate impacts essential for global resilience and adaptation efforts.

</details>


### [68] [Fine-Grained DINO Tuning with Dual Supervision for Face Forgery Detection](https://arxiv.org/abs/2511.12107)
*Tianxiang Zhang,Peipeng Yu,Zhihua Xia,Longchen Dai,Xiaoyu Zhou,Hui Gao*

Main category: cs.CV

TL;DR: 提出DFF-Adapter方法，通过轻量级多头LoRA模块适配DINOv2，同时处理真实性检测和细粒度伪造方法分类，利用3.5M可训练参数达到或超越现有复杂方法的检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有DINOv2微调方法将其视为通用二元分类，忽略了不同深度伪造方法产生的独特伪影特征，需要更精细的方法来应对复杂伪造威胁。

Method: 在DINOv2的每个transformer块中集成轻量级多头LoRA模块，构建共享分支将细粒度伪造方法线索传播到真实性检测头，实现多任务协同优化。

Result: 仅使用3.5M可训练参数，该方法在检测准确率上达到或超越了当前复杂的state-of-the-art方法。

Conclusion: DFF-Adapter通过参数高效的方式成功提升了DINOv2在深度伪造检测中的性能，证明了细粒度伪造方法分类对增强真实性检测敏感性的重要性。

Abstract: The proliferation of sophisticated deepfakes poses significant threats to information integrity. While DINOv2 shows promise for detection, existing fine-tuning approaches treat it as generic binary classification, overlooking distinct artifacts inherent to different deepfake methods. To address this, we propose a DeepFake Fine-Grained Adapter (DFF-Adapter) for DINOv2. Our method incorporates lightweight multi-head LoRA modules into every transformer block, enabling efficient backbone adaptation. DFF-Adapter simultaneously addresses authenticity detection and fine-grained manipulation type classification, where classifying forgery methods enhances artifact sensitivity. We introduce a shared branch propagating fine-grained manipulation cues to the authenticity head. This enables multi-task cooperative optimization, explicitly enhancing authenticity discrimination with manipulation-specific knowledge. Utilizing only 3.5M trainable parameters, our parameter-efficient approach achieves detection accuracy comparable to or even surpassing that of current complex state-of-the-art methods.

</details>


### [69] [MediRound: Multi-Round Entity-Level Reasoning Segmentation in Medical Images](https://arxiv.org/abs/2511.12110)
*Qinyue Tong,Ziqian Lu,Jun Liu,Rui Zuo,Zheming Lu*

Main category: cs.CV

TL;DR: 提出MEMR-Seg任务，支持多轮实体级医学推理分割，构建MR-MedSeg数据集，开发MediRound基线模型，引入判断与校正机制解决错误传播问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法多为任务特定且缺乏交互性，文本提示方法局限于单轮对话，无法进行多轮推理。

Method: 构建MR-MedSeg数据集(17.7万多轮医学分割对话)，提出MediRound基线模型，引入轻量级判断与校正机制缓解多轮分割中的错误传播。

Result: 实验结果表明该方法有效解决MEMR-Seg任务，性能优于传统医学参考分割方法。

Conclusion: MEMR-Seg任务和MediRound模型为多轮医学推理分割提供了有效解决方案，显著提升了交互性和推理能力。

Abstract: Despite the progress in medical image segmentation, most existing methods remain task-specific and lack interactivity. Although recent text-prompt-based segmentation approaches enhance user-driven and reasoning-based segmentation, they remain confined to single-round dialogues and fail to perform multi-round reasoning. In this work, we introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg), a new task that requires generating segmentation masks through multi-round queries with entity-level reasoning. To support this task, we construct MR-MedSeg, a large-scale dataset of 177K multi-round medical segmentation dialogues, featuring entity-based reasoning across rounds. Furthermore, we propose MediRound, an effective baseline model designed for multi-round medical reasoning segmentation. To mitigate the inherent error propagation in the chain-like pipeline of multi-round segmentation, we introduce a lightweight yet effective Judgment & Correction Mechanism during model inference. Experimental results demonstrate that our method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.

</details>


### [70] [RadarMP: Motion Perception for 4D mmWave Radar in Autonomous Driving](https://arxiv.org/abs/2511.12117)
*Ruiqi Cheng,Huijun Di,Jian Li,Feng Liu,Wei Liang*

Main category: cs.CV

TL;DR: RadarMP是一种利用4D毫米波雷达原始信号进行精确3D场景运动感知的新方法，通过统一架构联合建模雷达目标检测和运动估计任务，在恶劣天气条件下实现可靠的运动感知。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要精确的3D场景运动感知来提升安全性和可靠性。4D毫米波雷达具有全天候工作能力，但其稀疏和噪声点云往往导致运动感知不精确，特别是在光学传感器性能下降的恶劣天气条件下。

Method: 提出RadarMP方法，使用两帧连续的低级雷达回波信号，在统一架构中联合建模雷达目标检测和运动估计任务，实现一致的雷达点云生成和逐点3D场景流预测。设计了基于多普勒频移和回波强度的自监督损失函数，无需显式标注即可有效监督空间和运动一致性。

Result: 在公共数据集上的广泛实验表明，RadarMP在不同天气和光照条件下实现了可靠的运动感知，优于基于雷达的解耦运动感知流程，提升了全场景自动驾驶系统的感知能力。

Conclusion: RadarMP通过联合建模雷达目标检测和运动估计，利用自监督学习有效解决了4D毫米波雷达稀疏和噪声问题，为自动驾驶系统在恶劣天气条件下提供了可靠的3D场景运动感知能力。

Abstract: Accurate 3D scene motion perception significantly enhances the safety and reliability of an autonomous driving system. Benefiting from its all-weather operational capability and unique perceptual properties, 4D mmWave radar has emerged as an essential component in advanced autonomous driving. However, sparse and noisy radar points often lead to imprecise motion perception, leaving autonomous vehicles with limited sensing capabilities when optical sensors degrade under adverse weather conditions. In this paper, we propose RadarMP, a novel method for precise 3D scene motion perception using low-level radar echo signals from two consecutive frames. Unlike existing methods that separate radar target detection and motion estimation, RadarMP jointly models both tasks in a unified architecture, enabling consistent radar point cloud generation and pointwise 3D scene flow prediction. Tailored to radar characteristics, we design specialized self-supervised loss functions guided by Doppler shifts and echo intensity, effectively supervising spatial and motion consistency without explicit annotations. Extensive experiments on the public dataset demonstrate that RadarMP achieves reliable motion perception across diverse weather and illumination conditions, outperforming radar-based decoupled motion perception pipelines and enhancing perception capabilities for full-scenario autonomous driving systems.

</details>


### [71] [OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description](https://arxiv.org/abs/2511.12131)
*Quanxing Xu,Ling Zhou,Feifei Zhang,Jinyu Tian,Rubing Huang*

Main category: cs.CV

TL;DR: 提出了OAD-Promoter方法，通过减轻语言偏见和增强域迁移鲁棒性来改进基于LLM的视觉问答系统。


<details>
  <summary>Details</summary>
Motivation: LLM在视觉问答中依赖大规模训练数据会继承语言偏见，导致预测不可靠且在分布外泛化方面表现不佳。

Method: 包含三个模块：对象集中示例生成模块生成全局描述和对象集中样本；记忆知识辅助模块检索相关知识处理分布外样本；OAD提示整合模块输出优化LLM推理。

Result: 实验表明OAD-Promoter显著提升了基于LLM的VQA方法在少样本和零样本设置下的性能，达到了新的最先进水平。

Conclusion: OAD-Promoter通过减轻语言偏见和增强域迁移鲁棒性，有效提升了LLM在视觉问答中的表现。

Abstract: Large Language Models (LLMs) have become a crucial tool in Visual Question Answering (VQA) for handling knowledge-intensive questions in few-shot or zero-shot scenarios. However, their reliance on massive training datasets often causes them to inherit language biases during the acquisition of knowledge. This limitation imposes two key constraints on existing methods: (1) LLM predictions become less reliable due to bias exploitation, and (2) despite strong knowledge reasoning capabilities, LLMs still struggle with out-of-distribution (OOD) generalization. To address these issues, we propose Object Attribute Description Promoter (OAD-Promoter), a novel approach for enhancing LLM-based VQA by mitigating language bias and improving domain-shift robustness. OAD-Promoter comprises three components: the Object-concentrated Example Generation (OEG) module, the Memory Knowledge Assistance (MKA) module, and the OAD Prompt. The OEG module generates global captions and object-concentrated samples, jointly enhancing visual information input to the LLM and mitigating bias through complementary global and regional visual cues. The MKA module assists the LLM in handling OOD samples by retrieving relevant knowledge from stored examples to support questions from unseen domains. Finally, the OAD Prompt integrates the outputs of the preceding modules to optimize LLM inference. Experiments demonstrate that OAD-Promoter significantly improves the performance of LLM-based VQA methods in few-shot or zero-shot settings, achieving new state-of-the-art results.

</details>


### [72] [Compression and Inference of Spiking Neural Networks on Resource-Constrained Hardware](https://arxiv.org/abs/2511.12136)
*Karol C. Jurzec,Tomasz Szydlo,Maciej Wielgosz*

Main category: cs.CV

TL;DR: 提出了一个轻量级的C语言运行时系统，用于在边缘设备上进行SNN推理，通过优化减少延迟和内存使用，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: SNN的事件驱动特性在时间处理和能效方面具有优势，但在资源受限的硬件上训练和部署仍然具有挑战性。

Method: 将SNNTorch训练的模型转换为紧凑的C表示；使用静态、缓存友好的数据布局和预分配避免解释器和分配开销；利用稀疏脉冲活动修剪不活跃的神经元和突触。

Result: 在N-MNIST和ST-MNIST上的实验显示与Python基线功能相当，在桌面CPU上实现约10倍加速，通过修剪获得额外增益，内存大幅减少，可在微控制器上部署。

Conclusion: 当与优化的运行时和脉冲驱动的模型压缩相结合时，SNN可以在传统嵌入式平台上高效执行。

Abstract: Spiking neural networks (SNNs) communicate via discrete spikes in time rather than continuous activations. Their event-driven nature offers advantages for temporal processing and energy efficiency on resource-constrained hardware, but training and deployment remain challenging. We present a lightweight C-based runtime for SNN inference on edge devices and optimizations that reduce latency and memory without sacrificing accuracy. Trained models exported from SNNTorch are translated to a compact C representation; static, cache-friendly data layouts and preallocation avoid interpreter and allocation overheads. We further exploit sparse spiking activity to prune inactive neurons and synapses, shrinking computation in upstream convolutional layers. Experiments on N-MNIST and ST-MNIST show functional parity with the Python baseline while achieving ~10 speedups on desktop CPU and additional gains with pruning, together with large memory reductions that enable microcontroller deployment (Arduino Portenta H7). Results indicate that SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression. Code: https://github.com/karol-jurzec/snn-generator/

</details>


### [73] [MAVIS: A Benchmark for Multimodal Source Attribution in Long-form Visual Question Answering](https://arxiv.org/abs/2511.12142)
*Seokwon Song,Minsu Park,Gunhee Kim*

Main category: cs.CV

TL;DR: MAVIS是首个评估多模态源归属系统的基准，包含157K视觉问答实例，每个答案都有事实级引用。研究发现多模态RAG比单模态RAG生成更丰富流畅的答案，但在图像文档的根基性方面较弱，存在信息丰富性与根基性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注纯文本场景，忽视了多模态在源归属中的作用。源归属通过为每个陈述提供引用来源来增强AI生成答案的可信度。

Method: 开发MAVIS基准数据集，包含157K视觉问答实例，每个答案都有事实级引用标注。提出三个维度的自动评估指标：信息丰富性、根基性和流畅性。

Result: 多模态RAG比单模态RAG生成更丰富流畅的答案；但在图像文档的根基性方面较弱；不同提示方法在信息丰富性和根基性之间存在权衡。

Conclusion: 缓解图像文档解释中的上下文偏见是未来研究的关键方向。数据集和实验代码已开源。

Abstract: Source attribution aims to enhance the reliability of AI-generated answers by including references for each statement, helping users validate the provided answers. However, existing work has primarily focused on text-only scenario and largely overlooked the role of multimodality. We introduce MAVIS, the first benchmark designed to evaluate multimodal source attribution systems that understand user intent behind visual questions, retrieve multimodal evidence, and generate long-form answers with citations. Our dataset comprises 157K visual QA instances, where each answer is annotated with fact-level citations referring to multimodal documents. We develop fine-grained automatic metrics along three dimensions of informativeness, groundedness, and fluency, and demonstrate their strong correlation with human judgments. Our key findings are threefold: (1) LVLMs with multimodal RAG generate more informative and fluent answers than unimodal RAG, but they exhibit weaker groundedness for image documents than for text documents, a gap amplified in multimodal settings. (2) Given the same multimodal documents, there is a trade-off between informativeness and groundedness across different prompting methods. (3) Our proposed method highlights mitigating contextual bias in interpreting image documents as a crucial direction for future research. The dataset and experimental code are available at https://github.com/seokwon99/MAVIS

</details>


### [74] [Breaking the Modality Wall: Time-step Mixup for Efficient Spiking Knowledge Transfer from Static to Event Domain](https://arxiv.org/abs/2511.12150)
*Yuqi Xie,Shuhan Ye,Yi Yu,Chong Wang,Qixin Zhang,Jiazhen Xu,Le Shen,Yuanbin Qian,Jiangbo Qian,Guoqi Li*

Main category: cs.CV

TL;DR: TMKT是一个跨模态训练框架，通过时间步混合策略在RGB和DVS输入之间进行插值，结合轻量级模态感知目标，实现更平滑的知识迁移和更好的脉冲图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机和脉冲神经网络的结合有望实现能效视觉智能，但稀缺的事件数据和DVS输出的稀疏性阻碍了有效训练。RGB到DVS的先验知识迁移由于模态间分布差异大而表现不佳。

Method: 提出时间步混合知识迁移框架，包含概率时间步混合策略，在RGB和DVS输入的不同时间步进行插值，并引入模态感知引导和混合比感知两个轻量级目标来对齐时序特征。

Result: 在多个基准测试和不同SNN骨干网络上进行的广泛实验表明，该方法能够实现更平滑的知识迁移，缓解训练中的模态不匹配问题，并在脉冲图像分类任务中取得优越性能。

Conclusion: TMKT通过时间步混合策略和模态感知目标，有效解决了跨模态知识迁移中的分布差异问题，为事件相机和脉冲神经网络的结合提供了有效的训练框架。

Abstract: The integration of event cameras and spiking neural networks (SNNs) promises energy-efficient visual intelligence, yet scarce event data and the sparsity of DVS outputs hinder effective training. Prior knowledge transfers from RGB to DVS often underperform because the distribution gap between modalities is substantial. In this work, we present Time-step Mixup Knowledge Transfer (TMKT), a cross-modal training framework with a probabilistic Time-step Mixup (TSM) strategy. TSM exploits the asynchronous nature of SNNs by interpolating RGB and DVS inputs at various time steps to produce a smooth curriculum within each sequence, which reduces gradient variance and stabilizes optimization with theoretical analysis. To employ auxiliary supervision from TSM, TMKT introduces two lightweight modality-aware objectives, Modality Aware Guidance (MAG) for per-frame source supervision and Mixup Ratio Perception (MRP) for sequence-level mix ratio estimation, which explicitly align temporal features with the mixing schedule. TMKT enables smoother knowledge transfer, helps mitigate modality mismatch during training, and achieves superior performance in spiking image classification tasks. Extensive experiments across diverse benchmarks and multiple SNN backbones, together with ablations, demonstrate the effectiveness of our method.

</details>


### [75] [FIA-Edit: Frequency-Interactive Attention for Efficient and High-Fidelity Inversion-Free Text-Guided Image Editing](https://arxiv.org/abs/2511.12151)
*Kaixiang Yang,Boyang Shen,Xin Li,Yuchen Dai,Yuxuan Luo,Yueran Ma,Wei Fang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: FIA-Edit是一个基于频率交互注意力的免反演图像编辑框架，通过频率表示交互和特征注入模块实现高保真编辑，并在医疗图像增强中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流的免反演方法虽然效率高，但缺乏对源图像信息的有效整合，导致背景保留差、空间不一致和过度编辑等问题。

Method: 提出频率交互注意力框架，包含频率表示交互模块（在自注意力中交换源和目标特征的频率分量）和特征注入模块（在交叉注意力中显式引入源侧查询、键、值和文本嵌入）。

Result: 在RTX 4090上每张512*512图像编辑仅需约6秒，在视觉质量、背景保真度和可控性方面优于现有方法，并在医疗出血分类任务中取得显著提升。

Conclusion: FIA-Edit实现了高效高保真的文本引导图像编辑，并首次将该技术扩展到临床应用中，为医疗数据增强开辟了新途径。

Abstract: Text-guided image editing has advanced rapidly with the rise of diffusion models. While flow-based inversion-free methods offer high efficiency by avoiding latent inversion, they often fail to effectively integrate source information, leading to poor background preservation, spatial inconsistencies, and over-editing due to the lack of effective integration of source information. In this paper, we present FIA-Edit, a novel inversion-free framework that achieves high-fidelity and semantically precise edits through a Frequency-Interactive Attention. Specifically, we design two key components: (1) a Frequency Representation Interaction (FRI) module that enhances cross-domain alignment by exchanging frequency components between source and target features within self-attention, and (2) a Feature Injection (FIJ) module that explicitly incorporates source-side queries, keys, values, and text embeddings into the target branch's cross-attention to preserve structure and semantics. Comprehensive and extensive experiments demonstrate that FIA-Edit supports high-fidelity editing at low computational cost (~6s per 512 * 512 image on an RTX 4090) and consistently outperforms existing methods across diverse tasks in visual quality, background fidelity, and controllability. Furthermore, we are the first to extend text-guided image editing to clinical applications. By synthesizing anatomically coherent hemorrhage variations in surgical images, FIA-Edit opens new opportunities for medical data augmentation and delivers significant gains in downstream bleeding classification. Our project is available at: https://github.com/kk42yy/FIA-Edit.

</details>


### [76] [Codebook-Centric Deep Hashing: End-to-End Joint Learning of Semantic Hash Centers and Neural Hash Function](https://arxiv.org/abs/2511.12162)
*Shuo Yin,Zhiyuan Yin,Yuqing Hou,Rui Liu,Yong Chen,Dell Zhang*

Main category: cs.CV

TL;DR: 提出了Center-Reassigned Hashing (CRH)端到端框架，通过动态重分配预设码本中的哈希中心来避免随机初始化问题，同时联合优化哈希函数，无需显式的中心优化阶段。


<details>
  <summary>Details</summary>
Motivation: 现有的基于哈希中心的方法存在随机初始化忽略类间语义关系的问题，而两阶段方法又带来额外复杂性和性能下降。

Method: CRH框架从预设码本中动态重分配哈希中心，使用多头机制增强哈希中心的表示能力，端到端联合优化哈希函数。

Result: 在三个基准数据集上的实验表明，CRH能够学习到具有语义意义的哈希中心，并在检索任务中优于最先进的深度哈希方法。

Conclusion: CRH通过动态重分配哈希中心和端到端优化，有效解决了现有方法的局限性，实现了更好的语义保持和检索性能。

Abstract: Hash center-based deep hashing methods improve upon pairwise or triplet-based approaches by assigning fixed hash centers to each class as learning targets, thereby avoiding the inefficiency of local similarity optimization. However, random center initialization often disregards inter-class semantic relationships. While existing two-stage methods mitigate this by first refining hash centers with semantics and then training the hash function, they introduce additional complexity, computational overhead, and suboptimal performance due to stage-wise discrepancies. To address these limitations, we propose $\textbf{Center-Reassigned Hashing (CRH)}$, an end-to-end framework that $\textbf{dynamically reassigns hash centers}$ from a preset codebook while jointly optimizing the hash function. Unlike previous methods, CRH adapts hash centers to the data distribution $\textbf{without explicit center optimization phases}$, enabling seamless integration of semantic relationships into the learning process. Furthermore, $\textbf{a multi-head mechanism}$ enhances the representational capacity of hash centers, capturing richer semantic structures. Extensive experiments on three benchmarks demonstrate that CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.

</details>


### [77] [Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective](https://arxiv.org/abs/2511.12170)
*Wang Luo,Di Wu,Hengyuan Na,Yinlin Zhu,Miao Hu,Guocong Quan*

Main category: cs.CV

TL;DR: 提出了一种新的点云补全范式Completion-by-Correction，通过预训练图像到3D模型生成拓扑完整的形状先验，然后在特征空间进行校正，实现结构一致且与观测对齐的重建。


<details>
  <summary>Details</summary>
Motivation: 传统基于修复的补全方法由于几何和语义约束有限，容易产生结构不一致和拓扑伪影。需要重新思考点云补全任务，从无约束合成转向引导式精炼。

Method: 提出PGNet多阶段框架：双特征编码以支撑生成先验，合成结构对齐的粗粒度支架，通过分层校正逐步精炼几何细节。

Result: 在ShapeNetViPC数据集上，PGNet在平均Chamfer距离上比现有最佳方法降低23.5%，F-score提高7.1%。

Conclusion: Completion-by-Correction范式通过将补全任务重新定义为引导式精炼而非无约束合成，能够实现更结构一致和观测对齐的重建效果。

Abstract: Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).

</details>


### [78] [MixAR: Mixture Autoregressive Image Generation](https://arxiv.org/abs/2511.12181)
*Jinyuan Hu,Jiayou Zhang,Shaobo Cui,Kun Zhang,Guangyi Chen*

Main category: cs.CV

TL;DR: MixAR是一个新颖的自回归图像生成框架，通过混合离散和连续标记来提升生成质量，解决了纯连续自回归建模的挑战。


<details>
  <summary>Details</summary>
Motivation: 自回归方法在图像生成中取得了显著成功，但离散标记的量化过程和有限码本大小会丢失细粒度信息，限制了生成保真度。虽然连续潜在空间建模能提供更高质量，但连续表示存在于广阔无结构空间中，给高效自回归建模带来挑战。

Method: MixAR框架利用混合训练范式，将离散标记作为先验指导注入连续自回归建模。研究了多种离散-连续混合策略：自注意力(DC-SA)、交叉注意力(DC-CA)以及简单的DC-Mix方法（用信息丰富的离散对应物替换同质掩码标记）。还提出了训练-推理混合(TI-Mix)来弥合训练和生成分布之间的差距。

Result: 实验表明DC-Mix策略在计算效率和生成保真度之间取得了良好平衡，TI-Mix带来了持续改进。

Conclusion: MixAR通过混合离散和连续表示，有效解决了连续自回归建模的挑战，在保持计算效率的同时提升了生成质量。

Abstract: Autoregressive (AR) approaches, which represent images as sequences of discrete tokens from a finite codebook, have achieved remarkable success in image generation. However, the quantization process and the limited codebook size inevitably discard fine-grained information, placing bottlenecks on fidelity. Motivated by this limitation, recent studies have explored autoregressive modeling in continuous latent spaces, which offers higher generation quality. Yet, unlike discrete tokens constrained by a fixed codebook, continuous representations lie in a vast and unstructured space, posing significant challenges for efficient autoregressive modeling. To address these challenges, we introduce MixAR, a novel framework that leverages mixture training paradigms to inject discrete tokens as prior guidance for continuous AR modeling. MixAR is a factorized formulation that leverages discrete tokens as prior guidance for continuous autoregressive prediction. We investigate several discrete-continuous mixture strategies, including self-attention (DC-SA), cross-attention (DC-CA), and a simple approach (DC-Mix) that replaces homogeneous mask tokens with informative discrete counterparts. Moreover, to bridge the gap between ground-truth training tokens and inference tokens produced by the pre-trained AR model, we propose Training-Inference Mixture (TI-Mix) to achieve consistent training and generation distributions. In our experiments, we demonstrate a favorable balance of the DC-Mix strategy between computational efficiency and generation fidelity, and consistent improvement of TI-Mix.

</details>


### [79] [MMRINet: Efficient Mamba-Based Segmentation with Dual-Path Refinement for Low-Resource MRI Analysis](https://arxiv.org/abs/2511.12193)
*Abdelrahman Elsayed,Ahmed Jaheen,Mohammad Yaqub*

Main category: cs.CV

TL;DR: MMRINet是一个轻量级脑肿瘤分割架构，使用线性复杂度的Mamba状态空间模型替代二次复杂度的注意力机制，在BraTS-Lighthouse SSA 2025中仅用约250万参数就实现了0.752的平均Dice分数和12.23的平均HD95。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的环境中，深度3D网络的计算成本过高，需要开发轻量级但高效的脑肿瘤分割方法。

Method: 使用线性复杂度的Mamba状态空间模型替代注意力机制，提出双路径特征精炼模块（DPFR）和渐进式特征聚合（PFA）进行多尺度融合。

Result: 在BraTS-Lighthouse SSA 2025中达到平均Dice分数0.752和平均HD95 12.23，仅需约250万参数。

Conclusion: MMRINet展示了在低资源临床环境中实现高效准确分割的潜力。

Abstract: Automated brain tumor segmentation in multi-parametric MRI remains challenging in resource-constrained settings where deep 3D networks are computationally prohibitive. We propose MMRINet, a lightweight architecture that replaces quadratic-complexity attention with linear-complexity Mamba state-space models for efficient volumetric context modeling. Novel Dual-Path Feature Refinement (DPFR) modules maximize feature diversity without additional data requirements, while Progressive Feature Aggregation (PFA) enables effective multi-scale fusion. In the BraTS-Lighthouse SSA 2025, our model achieves strong performance with an average Dice score of (0.752) and an average HD95 of (12.23) with only ~2.5M parameters, demonstrating efficient and accurate segmentation suitable for low-resource clinical environments. Our GitHub repository can be accessed here: github.com/BioMedIA-MBZUAI/MMRINet.

</details>


### [80] [Cross-View Cross-Modal Unsupervised Domain Adaptation for Driver Monitoring System](https://arxiv.org/abs/2511.12196)
*Aditi Bhalla,Christian Hellert,Enkelejda Kasneci*

Main category: cs.CV

TL;DR: 提出了一种两阶段跨视角、跨模态的无监督域自适应框架，用于解决驾驶员活动识别中的视角变化和域偏移问题，显著提升了模型在真实部署中的性能。


<details>
  <summary>Details</summary>
Motivation: 驾驶员分心是交通事故的主要原因，现有深度学习方法在真实部署中面临视角变化和域偏移的挑战，需要同时解决跨视角泛化和无监督域自适应问题。

Method: 两阶段框架：第一阶段使用对比学习在多视角数据中学习视角不变和动作区分特征；第二阶段使用信息瓶颈损失进行跨模态域自适应，无需新域的标注数据。

Result: 使用Video Swin和MViT视频变换器在Drive&Act数据集上评估，相比监督对比学习方法在RGB视频数据上的top-1准确率提升近50%，比仅使用无监督域自适应方法提升5%。

Conclusion: 该联合框架能够有效解决驾驶员活动识别中的跨视角和跨模态挑战，为模型在多样化车辆配置中的稳健部署提供了可行方案。

Abstract: Driver distraction remains a leading cause of road traffic accidents, contributing to thousands of fatalities annually across the globe. While deep learning-based driver activity recognition methods have shown promise in detecting such distractions, their effectiveness in real-world deployments is hindered by two critical challenges: variations in camera viewpoints (cross-view) and domain shifts such as change in sensor modality or environment. Existing methods typically address either cross-view generalization or unsupervised domain adaptation in isolation, leaving a gap in the robust and scalable deployment of models across diverse vehicle configurations. In this work, we propose a novel two-phase cross-view, cross-modal unsupervised domain adaptation framework that addresses these challenges jointly on real-time driver monitoring data. In the first phase, we learn view-invariant and action-discriminative features within a single modality using contrastive learning on multi-view data. In the second phase, we perform domain adaptation to a new modality using information bottleneck loss without requiring any labeled data from the new domain. We evaluate our approach using state-of-the art video transformers (Video Swin, MViT) and multi modal driver activity dataset called Drive&Act, demonstrating that our joint framework improves top-1 accuracy on RGB video data by almost 50% compared to a supervised contrastive learning-based cross-view method, and outperforms unsupervised domain adaptation-only methods by up to 5%, using the same video transformer backbone.

</details>


### [81] [Bridging Granularity Gaps: Hierarchical Semantic Learning for Cross-domain Few-shot Segmentation](https://arxiv.org/abs/2511.12200)
*Sujun Sun,Haowen Gu,Cheng Xie,Yanxu Ren,Mingwu Ren,Haofeng Zhang*

Main category: cs.CV

TL;DR: 提出了一种层次化语义学习框架来解决跨域少样本分割中的语义粒度差异问题，通过双重风格随机化和层次化语义挖掘模块提升模型对不同粒度语义的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有的跨域少样本分割方法主要关注源域和目标域之间的风格差异，但忽略了分割粒度差异，导致对目标域中新类别的语义区分能力不足。

Method: 使用双重风格随机化模块模拟目标域数据的风格差异，层次化语义挖掘模块利用多尺度超像素引导模型学习不同粒度的类内一致性和类间区分性，以及原型置信度调制阈值模块缓解分割模糊问题。

Result: 在四个流行的目标域数据集上的实验表明，该方法达到了最先进的性能。

Conclusion: 提出的层次化语义学习框架有效解决了跨域少样本分割中的语义粒度差异问题，显著提升了模型性能。

Abstract: Cross-domain Few-shot Segmentation (CD-FSS) aims to segment novel classes from target domains that are not involved in training and have significantly different data distributions from the source domain, using only a few annotated samples, and recent years have witnessed significant progress on this task. However, existing CD-FSS methods primarily focus on style gaps between source and target domains while ignoring segmentation granularity gaps, resulting in insufficient semantic discriminability for novel classes in target domains. Therefore, we propose a Hierarchical Semantic Learning (HSL) framework to tackle this problem. Specifically, we introduce a Dual Style Randomization (DSR) module and a Hierarchical Semantic Mining (HSM) module to learn hierarchical semantic features, thereby enhancing the model's ability to recognize semantics at varying granularities. DSR simulates target domain data with diverse foreground-background style differences and overall style variations through foreground and global style randomization respectively, while HSM leverages multi-scale superpixels to guide the model to mine intra-class consistency and inter-class distinction at different granularities. Additionally, we also propose a Prototype Confidence-modulated Thresholding (PCMT) module to mitigate segmentation ambiguity when foreground and background are excessively similar. Extensive experiments are conducted on four popular target domain datasets, and the results demonstrate that our method achieves state-of-the-art performance.

</details>


### [82] [OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs](https://arxiv.org/abs/2511.12201)
*Feng Chen,Yefei He,Shaoxuan He,Yuanyu He,Jing Liu,Lequan Lin,Akide Liu,Zhaoyang Li,Jiyuan Zhang,Zhenbang Sun,Bohan Zhuang,Qi Wu*

Main category: cs.CV

TL;DR: OmniSparse是一个训练感知的细粒度稀疏注意力框架，用于长视频MLLMs，通过动态令牌预算分配在训练和推理中实现高效处理。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏注意力方法主要针对推理时加速，但无法弥合训练-推理差距，且缺乏跨多个维度（查询、键值、头）的细粒度令牌选择能力，导致性能次优和加速收益有限。

Method: OmniSparse包含三个自适应互补机制：(1)通过惰性-主动分类进行查询选择；(2)基于头级动态预算分配的KV选择；(3)KV缓存精简以减少头级冗余。

Result: 实验结果显示，OmniSparse在保持全注意力性能的同时，实现了预填充阶段2.7倍加速和解码阶段2.4倍内存减少。

Conclusion: OmniSparse框架有效解决了训练-推理差距问题，通过细粒度稀疏注意力机制在长视频MLLMs中实现了显著的加速和内存优化。

Abstract: Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding.

</details>


### [83] [LSS3D: Learnable Spatial Shifting for Consistent and High-Quality 3D Generation from Single-Image](https://arxiv.org/abs/2511.12202)
*Zhuojiang Cai,Yiheng Zhang,Meitong Guo,Mingdao Wang,Yuwang Wang*

Main category: cs.CV

TL;DR: 提出LSS3D方法，通过可学习空间偏移解决多视图扩散3D生成中的形状纹理不对齐问题，提升非正面视角输入的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有多视图扩散3D生成方法存在形状纹理不对齐问题，导致几何细节不完整和纹理重影，且对倾斜视角输入鲁棒性差

Method: 为每个视图分配可学习空间偏移参数，通过重建网格引导调整视图实现空间一致性，并加入输入视图作为额外约束增强鲁棒性

Result: 在几何和纹理评估指标上取得领先结果，对更灵活的输入视角具有良好表现

Conclusion: LSS3D方法能有效处理多视图不一致性和非正面输入视角，生成更完整几何细节和清晰纹理的高质量3D内容

Abstract: Recently, multi-view diffusion-based 3D generation methods have gained significant attention. However, these methods often suffer from shape and texture misalignment across generated multi-view images, leading to low-quality 3D generation results, such as incomplete geometric details and textural ghosting. Some methods are mainly optimized for the frontal perspective and exhibit poor robustness to oblique perspective inputs. In this paper, to tackle the above challenges, we propose a high-quality image-to-3D approach, named LSS3D, with learnable spatial shifting to explicitly and effectively handle the multiview inconsistencies and non-frontal input view. Specifically, we assign learnable spatial shifting parameters to each view, and adjust each view towards a spatially consistent target, guided by the reconstructed mesh, resulting in high-quality 3D generation with more complete geometric details and clean textures. Besides, we include the input view as an extra constraint for the optimization, further enhancing robustness to non-frontal input angles, especially for elevated viewpoint inputs. We also provide a comprehensive quantitative evaluation pipeline that can contribute to the community in performance comparisons. Extensive experiments demonstrate that our method consistently achieves leading results in both geometric and texture evaluation metrics across more flexible input viewpoints.

</details>


### [84] [GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction](https://arxiv.org/abs/2511.12204)
*Jiaqi Wu,Yaosen Chen,Shuyuan Zhu*

Main category: cs.CV

TL;DR: 提出了Geometry-guided Multi-View Diffusion Model，通过提取多视角几何信息并调整几何特征强度，生成跨视角一致且细节丰富的图像。


<details>
  <summary>Details</summary>
Motivation: 现有基于单图像扩展的多视角图像生成方法在保持跨视角一致性和生成高分辨率输出方面面临计算挑战。

Method: 设计了多视角几何信息提取模块、解耦几何增强注意力机制、自适应学习策略、迭代精炼过程和动态几何信息强度调整机制。

Result: 模型能够生成跨视角一致且细节丰富的图像，提高了整体图像质量和细节保持能力。

Conclusion: 该方法有效解决了多视角图像生成中的一致性和细节问题，在3D重建、虚拟现实等领域具有重要应用价值。

Abstract: Multi-view image generation holds significant application value in computer vision, particularly in domains like 3D reconstruction, virtual reality, and augmented reality. Most existing methods, which rely on extending single images, face notable computational challenges in maintaining cross-view consistency and generating high-resolution outputs. To address these issues, we propose the Geometry-guided Multi-View Diffusion Model, which incorporates mechanisms for extracting multi-view geometric information and adjusting the intensity of geometric features to generate images that are both consistent across views and rich in detail. Specifically, we design a multi-view geometry information extraction module that leverages depth maps, normal maps, and foreground segmentation masks to construct a shared geometric structure, ensuring shape and structural consistency across different views. To enhance consistency and detail restoration during generation, we develop a decoupled geometry-enhanced attention mechanism that strengthens feature focus on key geometric details, thereby improving overall image quality and detail preservation. Furthermore, we apply an adaptive learning strategy that fine-tunes the model to better capture spatial relationships and visual coherence between the generated views, ensuring realistic results. Our model also incorporates an iterative refinement process that progressively improves the output quality through multiple stages of image generation. Finally, a dynamic geometry information intensity adjustment mechanism is proposed to adaptively regulate the influence of geometric data, optimizing overall quality while ensuring the naturalness of generated images. More details can be found on the project page: https://github.com/SobeyMIL/GeoMVD.com.

</details>


### [85] [A Novel AI-Driven System for Real-Time Detection of Mirror Absence, Helmet Non-Compliance, and License Plates Using YOLOv8 and OCR](https://arxiv.org/abs/2511.12206)
*Nishant Vasantkumar Hegde,Aditi Agarwal,Minal Moharir*

Main category: cs.CV

TL;DR: 基于YOLOv8和EasyOCR的AI系统，用于自动检测交通违规行为，包括头盔佩戴情况和摩托车后视镜存在性，并通过Streamlit界面实现实时监控。


<details>
  <summary>Details</summary>
Motivation: 手动执行头盔法律和车辆安全标准检查资源密集且不一致，需要自动化解决方案来提高执法效率和道路安全。

Method: 使用YOLOv8进行目标检测，EasyOCR进行车牌识别，在自定义标注数据集上训练，并通过图像预处理增强车牌识别能力。

Result: 模型整体精度0.9147，召回率0.886，mAP@50为0.843，mAP@50-95为0.503，表明在严格IoU阈值下仍具有强检测能力。

Conclusion: 该工作展示了实用有效的自动交通规则执法解决方案，并讨论了实际部署的考虑因素。

Abstract: Road safety is a critical global concern, with manual enforcement of helmet laws and vehicle safety standards (e.g., rear-view mirror presence) being resource-intensive and inconsistent. This paper presents an AI-powered system to automate traffic violation detection, significantly enhancing enforcement efficiency and road safety. The system leverages YOLOv8 for robust object detection and EasyOCR for license plate recognition. Trained on a custom dataset of annotated images (augmented for diversity), it identifies helmet non-compliance, the absence of rear-view mirrors on motorcycles, an innovative contribution to automated checks, and extracts vehicle registration numbers. A Streamlit-based interface facilitates real-time monitoring and violation logging. Advanced image preprocessing enhances license plate recognition, particularly under challenging conditions. Based on evaluation results, the model achieves an overall precision of 0.9147, a recall of 0.886, and a mean Average Precision (mAP@50) of 0.843. The mAP@50 95 of 0.503 further indicates strong detection capability under stricter IoU thresholds. This work demonstrates a practical and effective solution for automated traffic rule enforcement, with considerations for real-world deployment discussed.

</details>


### [86] [Mixture of States: Routing Token-Level Dynamics for Multimodal Generation](https://arxiv.org/abs/2511.12207)
*Haozhe Liu,Ding Liu,Mingchen Zhuge,Zijian Zhou,Tian Xie,Sen He,Yukang Yang,Shuming Liu,Yuren Cong,Jiadong Guo,Hongyu Xu,Ke Xu,Kam-Woh Ng,Juan C. Pérez,Juan-Manuel~Pérez-Rúa,Tao Xiang,Wei Liu,Shikun Liu,Jürgen Schmidhuber*

Main category: cs.CV

TL;DR: MoS是一种新颖的多模态扩散模型融合范式，通过可学习的token-wise路由器实现模态间状态交互，在文本到图像生成和编辑任务中达到SOTA效果，仅需3B-5B参数即可匹配或超越4倍大的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态扩散模型在模态融合方面存在效率低下的问题，需要开发一种更灵活、计算高效的融合范式来提升模型性能同时减少参数规模。

Method: 使用可学习的token-wise路由器创建模态间隐藏状态的交互，该路由器稀疏选择top-k隐藏状态，采用ε-greedy训练策略，实现token级特征与扩散轨迹的精确对齐。

Result: 在文本到图像生成和编辑任务中取得SOTA结果，仅用3B-5B参数即可匹配或超越参数规模大4倍的对比模型，计算开销可忽略不计。

Conclusion: MoS为多模态扩散模型提供了一种灵活且计算高效的扩展范式，在保持高性能的同时显著减少了模型参数规模。

Abstract: We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-$k$ hidden states and is trained with an $ε$-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to $4\times$ larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.

</details>


### [87] [FaNe: Towards Fine-Grained Cross-Modal Contrast with False-Negative Reduction and Text-Conditioned Sparse Attention](https://arxiv.org/abs/2511.12215)
*Peng Zhang,Zhihui Lai,Wenting Chen,Xu Wu,Heng Kong*

Main category: cs.CV

TL;DR: FaNe是一个语义增强的医学视觉语言预训练框架，通过语义感知的正样本挖掘、文本条件稀疏注意力池化和硬负样本感知对比损失，解决了假阴性和细粒度跨模态对齐不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言预训练方法存在由语义相似文本引起的假阴性问题，以及细粒度跨模态对齐不足的局限性。

Method: 1. 基于文本-文本相似度的语义感知正样本挖掘策略；2. 文本条件稀疏注意力池化模块实现细粒度图像-文本对齐；3. 硬负样本感知对比损失增强模态内区分度。

Result: 在五个下游医学影像基准测试中，FaNe在图像分类、目标检测和语义分割任务上均达到最先进性能。

Conclusion: FaNe框架通过解决假阴性和细粒度对齐问题，有效提升了医学视觉语言预训练的性能。

Abstract: Medical vision-language pre-training (VLP) offers significant potential for advancing medical image understanding by leveraging paired image-report data. However, existing methods are limited by Fa}lse Negatives (FaNe) induced by semantically similar texts and insufficient fine-grained cross-modal alignment. To address these limitations, we propose FaNe, a semantic-enhanced VLP framework. To mitigate false negatives, we introduce a semantic-aware positive pair mining strategy based on text-text similarity with adaptive normalization. Furthermore, we design a text-conditioned sparse attention pooling module to enable fine-grained image-text alignment through localized visual representations guided by textual cues. To strengthen intra-modal discrimination, we develop a hard-negative aware contrastive loss that adaptively reweights semantically similar negatives. Extensive experiments on five downstream medical imaging benchmarks demonstrate that FaNe achieves state-of-the-art performance across image classification, object detection, and semantic segmentation, validating the effectiveness of our framework.

</details>


### [88] [Suppressing VLM Hallucinations with Spectral Representation Filtering](https://arxiv.org/abs/2511.12220)
*Ameen Ali,Tamim Zoabi,Lior Wolf*

Main category: cs.CV

TL;DR: SRF是一种无需训练的后处理方法，通过分析特征协方差结构来抑制视觉语言模型中的幻觉现象，无需修改架构或增加推理开销。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型经常由于过度依赖语言先验和跨模态对齐不精确而产生幻觉，描述图像中不存在的对象、属性或关系。

Method: 通过特征协方差的特征分解识别低秩幻觉模式，然后使用软谱滤波器在深层vLLM层的前馈投影权重中衰减这些模式，均衡特征方差同时保持语义保真度。

Result: 在LLaVA-1.5、MiniGPT-4和mPLUG-Owl2等VLM模型上，SRF在MSCOCO、POPE-VQA等基准测试中持续降低幻觉率，在不降低描述质量的情况下达到最先进的忠实度。

Conclusion: SRF提供了一种轻量级、无需训练的方法来有效抑制VLM中的幻觉，具有零推理开销且无需架构修改的优势。

Abstract: Vision-language models (VLMs) frequently produce hallucinations in the form of descriptions of objects, attributes, or relations that do not exist in the image due to over-reliance on language priors and imprecise cross-modal grounding. We introduce Spectral Representation Filtering (SRF), a lightweight, training-free method to suppress such hallucinations by analyzing and correcting the covariance structure of the model's representations. SRF identifies low-rank hallucination modes through eigendecomposition of the covariance of the differences between features collected for truthful and hallucinatory captions, revealing structured biases in the feature space. A soft spectral filter then attenuates these modes in the feed-forward projection weights of deeper vLLM layers, equalizing feature variance while preserving semantic fidelity. Unlike decoding or retraining-based approaches, SRF operates entirely post-hoc, incurs zero inference overhead, and requires no architectural modifications. Across three families of VLMs (LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2), SRF consistently reduces hallucination rates on MSCOCO, POPE-VQA, and other visual tasks benchmarks, achieving state-of-the-art faithfulness without degrading caption quality.

</details>


### [89] [MiniGPT-Pancreas: Multimodal Large Language Model for Pancreas Cancer Classification and Detection](https://arxiv.org/abs/2412.15925)
*Andrea Moglia,Elia Clement Nastasio,Luca Mainardi,Pietro Cerveri*

Main category: cs.CV

TL;DR: MiniGPT-Pancreas是一个多模态大语言模型，通过级联微调在胰腺检测、肿瘤分类和肿瘤检测任务上表现良好，为临床医生提供胰腺癌诊断支持。


<details>
  <summary>Details</summary>
Motivation: 胰腺放射影像学面临器官小、边界模糊、形状和位置变异等挑战，需要开发交互式工具来支持临床诊断。

Method: 基于MiniGPT-v2多模态大语言模型，使用NIH、MSD和AbdomenCT-1k数据集的CT扫描和问题提示进行级联微调，实现胰腺检测、肿瘤分类和肿瘤检测。

Result: 在NIH和MSD数据集上胰腺检测IoU分别为0.595和0.550；胰腺癌分类准确率0.876；多器官检测中肝脏IoU最高为0.8399；胰腺肿瘤检测IoU为0.168。

Conclusion: MiniGPT-Pancreas是支持临床医生进行胰腺肿瘤图像分类的有前景解决方案，未来需要改进胰腺肿瘤检测性能。

Abstract: Problem: Pancreas radiological imaging is challenging due to the small size, blurred boundaries, and variability of shape and position of the organ among patients. Goal: In this work we present MiniGPT-Pancreas, a Multimodal Large Language Model (MLLM), as an interactive chatbot to support clinicians in pancreas cancer diagnosis by integrating visual and textual information. Methods: MiniGPT-v2, a general-purpose MLLM, was fine-tuned in a cascaded way for pancreas detection, tumor classification, and tumor detection with multimodal prompts combining questions and computed tomography scans from the National Institute of Health (NIH), and Medical Segmentation Decathlon (MSD) datasets. The AbdomenCT-1k dataset was used to detect the liver, spleen, kidney, and pancreas. Results: MiniGPT-Pancreas achieved an Intersection over Union (IoU) of 0.595 and 0.550 for the detection of pancreas on NIH and MSD datasets, respectively. For the pancreas cancer classification task on the MSD dataset, accuracy, precision, and recall were 0.876, 0.874, and 0.878, respectively. When evaluating MiniGPT-Pancreas on the AbdomenCT-1k dataset for multi-organ detection, the IoU was 0.8399 for the liver, 0.722 for the kidney, 0.705 for the spleen, and 0.497 for the pancreas. For the pancreas tumor detection task, the IoU score was 0.168 on the MSD dataset. Conclusions: MiniGPT-Pancreas represents a promising solution to support clinicians in the classification of pancreas images with pancreas tumors. Future research is needed to improve the score on the detection task, especially for pancreas tumors.

</details>


### [90] [Model Inversion Attack Against Deep Hashing](https://arxiv.org/abs/2511.12233)
*Dongdong Zhao,Qiben Xu,Ranxin Fang,Baogang Song*

Main category: cs.CV

TL;DR: 提出了DHMI，首个针对深度哈希的扩散模型反演框架，能在黑盒设置下成功重构高分辨率、高质量图像，揭示了深度哈希系统的严重隐私风险。


<details>
  <summary>Details</summary>
Motivation: 深度哈希虽然提高了检索效率，但带来了严重的隐私风险，特别是从哈希码重构原始训练数据的能力可能导致生物特征伪造和隐私泄露。然而，针对深度哈希模型的模型反演攻击尚未被探索。

Method: DHMI首先对辅助数据集进行聚类得到语义哈希中心作为代理锚点，然后引入代理引导的去噪优化方法，利用融合分类一致性和哈希邻近度的新攻击指标动态选择候选样本，通过代理模型集群指导候选样本的细化。

Result: 在多个数据集上的实验表明，DHMI即使在最具挑战性的黑盒设置下也能成功重构高分辨率、高质量图像，在黑盒场景下优于现有最先进的模型反演攻击方法。

Conclusion: DHMI证实了深度哈希系统存在严重的隐私风险，其方法在实际效果和隐私风险揭示方面都具有重要意义。

Abstract: Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversion attacks specifically targeting deep hashing models remain unexplored, leaving their security implications unexamined. This research gap stems from the inaccessibility of genuine training hash codes and the highly discrete Hamming space, which prevents existing methods from adapting to deep hashing. To address these challenges, we propose DHMI, the first diffusion-based model inversion framework designed for deep hashing. DHMI first clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors. It then introduces a surrogate-guided denoising optimization method that leverages a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples. A cluster of surrogate models guides the refinement of these candidates, ensuring the generation of high-fidelity and semantically consistent images. Experiments on multiple datasets demonstrate that DHMI successfully reconstructs high-resolution, high-quality images even under the most challenging black-box setting, where no training hash codes are available. Our method outperforms the existing state-of-the-art model inversion attacks in black-box scenarios, confirming both its practical efficacy and the critical privacy risks inherent in deep hashing systems.

</details>


### [91] [Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets](https://arxiv.org/abs/2511.12255)
*Huy M. Le,Dat Tien Nguyen,Phuc Binh Nguyen,Gia-Bao Le-Tran,Phu Truong Thien,Cuong Dinh,Minh Nguyen,Nga Nguyen,Thuy T. N. Nguyen,Huy Gia Ngo,Tan Nhat Nguyen,Binh T. Nguyen,Monojit Choudhury*

Main category: cs.CV

TL;DR: Fusionista2.0是一个优化的视频检索系统，通过重新设计核心模块和用户界面，在VBS挑战中实现了75%的检索时间减少，同时提高了准确性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: Video Browser Showdown (VBS)挑战要求系统在严格时间限制下提供准确结果，需要开发高效快速的视频检索系统。

Method: 使用ffmpeg进行快速关键帧提取，Vintern-1B-v3.5进行多语言OCR，faster-whisper进行实时语音识别，轻量级视觉语言模型进行问答，并重新设计用户界面提升响应性和易用性。

Result: 检索时间减少高达75%，准确性和用户满意度均有提升，证明了系统在大规模视频搜索中的竞争力。

Conclusion: Fusionista2.0是一个具有竞争力且用户友好的大规模视频搜索系统，在速度和准确性方面都有显著改进。

Abstract: The Video Browser Showdown (VBS) challenges systems to deliver accurate results under strict time constraints. To meet this demand, we present Fusionista2.0, a streamlined video retrieval system optimized for speed and usability. All core modules were re-engineered for efficiency: preprocessing now relies on ffmpeg for fast keyframe extraction, optical character recognition uses Vintern-1B-v3.5 for robust multilingual text recognition, and automatic speech recognition employs faster-whisper for real-time transcription. For question answering, lightweight vision-language models provide quick responses without the heavy cost of large models. Beyond these technical upgrades, Fusionista2.0 introduces a redesigned user interface with improved responsiveness, accessibility, and workflow efficiency, enabling even non-expert users to retrieve relevant content rapidly. Evaluations demonstrate that retrieval time was reduced by up to 75% while accuracy and user satisfaction both increased, confirming Fusionista2.0 as a competitive and user-friendly system for large-scale video search.

</details>


### [92] [Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment](https://arxiv.org/abs/2511.12256)
*Tolga Demiroglu,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: cs.CV

TL;DR: 提出基于MedSigLIP的提示条件框架，通过FiLM和多尺度池化注入文本先验，在LDCTIQA2023数据集上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 开发能够根据临床意图进行条件化学习的数据高效方法，实现快速适应医学图像质量评估任务

Method: 使用提示条件框架，通过FiLM注入文本先验，结合全局、局部和纹理感知池化的多尺度回归头，采用轻量级MLP融合和成对排序损失训练

Result: 在LDCTIQA2023数据集（1000张训练图像）上获得PLCC=0.9575、SROCC=0.9561、KROCC=0.8301，超越已发表的最佳挑战提交结果

Conclusion: 提示引导方法在医学图像质量评估中表现出色，证明了文本条件化学习的有效性

Abstract: We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.

</details>


### [93] [A Disease-Aware Dual-Stage Framework for Chest X-ray Report Generation](https://arxiv.org/abs/2511.12259)
*Puzhen Wu,Hexin Dong,Yi Lin,Yihao Ding,Yifan Peng*

Main category: cs.CV

TL;DR: 提出了一种新颖的双阶段疾病感知框架用于胸部X光报告生成，通过疾病感知语义标记和视觉-语言对齐来提升临床准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏足够的疾病感知能力和视觉-语言对齐，导致忽略关键病理特征且难以生成临床准确的报告。

Method: 双阶段框架：第一阶段学习疾病感知语义标记并通过对比学习对齐视觉和语言表示；第二阶段引入疾病-视觉注意力融合模块和双模态相似性检索机制。

Result: 在多个基准数据集上实现了最先进的性能，在临床准确性和语言质量方面均有显著提升。

Conclusion: 该疾病感知框架能够有效解决胸部X光报告生成中的关键挑战，为医学图像分析提供了更专业的解决方案。

Abstract: Radiology report generation from chest X-rays is an important task in artificial intelligence with the potential to greatly reduce radiologists' workload and shorten patient wait times. Despite recent advances, existing approaches often lack sufficient disease-awareness in visual representations and adequate vision-language alignment to meet the specialized requirements of medical image analysis. As a result, these models usually overlook critical pathological features on chest X-rays and struggle to generate clinically accurate reports. To address these limitations, we propose a novel dual-stage disease-aware framework for chest X-ray report generation. In Stage~1, our model learns Disease-Aware Semantic Tokens (DASTs) corresponding to specific pathology categories through cross-attention mechanisms and multi-label classification, while simultaneously aligning vision and language representations via contrastive learning. In Stage~2, we introduce a Disease-Visual Attention Fusion (DVAF) module to integrate disease-aware representations with visual features, along with a Dual-Modal Similarity Retrieval (DMSR) mechanism that combines visual and disease-specific similarities to retrieve relevant exemplars, providing contextual guidance during report generation. Extensive experiments on benchmark datasets (i.e., CheXpert Plus, IU X-ray, and MIMIC-CXR) demonstrate that our disease-aware framework achieves state-of-the-art performance in chest X-ray report generation, with significant improvements in clinical accuracy and linguistic quality.

</details>


### [94] [CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2511.12263)
*Jingyao Li,Jingyun Wang,Molin Tan,Haochen Wang,Cilin Yan,Likun Shi,Jiayin Cai,Xiaolong Jiang,Yao Hu*

Main category: cs.CV

TL;DR: CrossVid是首个专门评估多模态大语言模型在跨视频推理中时空推理能力的基准测试，包含4个高级维度和10个具体任务，提供5,331个视频和9,015个问答对。实验显示当前MLLMs在跨视频推理方面表现不佳，主要原因是无法整合和比较分布在多个视频中的证据。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准主要关注单视频分析，无法评估MLLMs同时推理多个视频的能力。虽然近期有评估多视角视频的基准，但其有限任务无法全面评估MLLMs在多样化真实世界跨视频推理场景中的表现。

Method: 构建CrossVid基准测试，包含广泛的层次化任务（4个高级维度、10个具体任务），提供5,331个视频和9,015个具有挑战性的问答对，涵盖单选、多选和开放式问题格式。

Result: 在各类开源和闭源MLLMs上的广泛实验表明，Gemini-2.5-Pro在CrossVid上表现最佳，平均准确率为50.4%。深入案例研究显示大多数当前MLLMs在跨视频推理任务上表现挣扎。

Conclusion: CrossVid有潜力指导未来在增强MLLMs跨视频推理能力方面的进展，当前MLLMs的主要挑战在于无法整合或比较分布在多个视频中的证据进行推理。

Abstract: Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.

</details>


### [95] [ZoomEarth: Active Perception for Ultra-High-Resolution Geospatial Vision-Language Tasks](https://arxiv.org/abs/2511.12267)
*Ruixun Liu,Bowen Fu,Jiayi Song,Kaiyu Li,Wanchen Li,Lanxuan Xue,Hui Qiao,Weizhan Zhang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: 提出了ZoomEarth框架，这是一个用于超高分辨率遥感图像处理的主动感知范式，通过自适应裁剪缩放和区域引导奖励机制，在LRS-GRO基准数据集上实现了最先进性能，并能与下游任务无缝集成。


<details>
  <summary>Details</summary>
Motivation: 现有动态分辨率和令牌剪枝方法受限于被动感知范式，在获取更精细视觉输入时会产生冗余。本文探索新的主动感知范式，使模型能够重新访问信息丰富的区域。

Method: 提出了ZoomEarth自适应裁剪缩放框架，采用区域引导奖励机制，通过监督微调和组相对策略优化进行训练。

Result: 在LRS-GRO基准上实现最先进性能，在零样本设置下在三个公共UHR遥感基准上表现优异，并能与云去除、去噪、分割和图像编辑等下游任务集成。

Conclusion: ZoomEarth框架展示了主动感知范式在超高分辨率遥感图像处理中的有效性，具有强大的多功能性和可扩展性。

Abstract: Ultra-high-resolution (UHR) remote sensing (RS) images offer rich fine-grained information but also present challenges in effective processing. Existing dynamic resolution and token pruning methods are constrained by a passive perception paradigm, suffering from increased redundancy when obtaining finer visual inputs. In this work, we explore a new active perception paradigm that enables models to revisit information-rich regions. First, we present LRS-GRO, a large-scale benchmark dataset tailored for active perception in UHR RS processing, encompassing 17 question types across global, region, and object levels, annotated via a semi-automatic pipeline. Building on LRS-GRO, we propose ZoomEarth, an adaptive cropping-zooming framework with a novel Region-Guided reward that provides fine-grained guidance. Trained via supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), ZoomEarth achieves state-of-the-art performance on LRS-GRO and, in the zero-shot setting, on three public UHR remote sensing benchmarks. Furthermore, ZoomEarth can be seamlessly integrated with downstream models for tasks such as cloud removal, denoising, segmentation, and image editing through simple tool interfaces, demonstrating strong versatility and extensibility.

</details>


### [96] [TM-UNet: Token-Memory Enhanced Sequential Modeling for Efficient Medical Image Segmentation](https://arxiv.org/abs/2511.12270)
*Yaxuan Jiao,Qing Xu,Yuxiang Luo,Xiangjian He,Zhen Chen,Wenting Duan*

Main category: cs.CV

TL;DR: TM-UNet是一种轻量级医学图像分割框架，通过多尺度token-memory块将2D空间特征转换为token序列，利用矩阵记忆单元选择性保留和传播判别性上下文信息，以线性复杂度实现高效全局推理。


<details>
  <summary>Details</summary>
Motivation: 尽管基于transformer的方法在医学图像分割中取得了显著成果，但其高计算成本阻碍了临床部署。为了解决这个问题，需要开发计算效率更高的分割方法。

Method: 提出多尺度token-memory块，通过空间扫描将2D特征转换为token序列，利用矩阵记忆单元选择性保留上下文信息，结合指数门控识别token有效性，并通过并行池化操作实现多尺度上下文提取。

Result: 在多种医学分割任务中，TM-UNet超越了最先进的方法，同时显著降低了计算成本。

Conclusion: TM-UNet通过创新的token-memory机制，在保持高性能的同时实现了高效的医学图像分割，为临床部署提供了可行的解决方案。

Abstract: Medical image segmentation is essential for clinical diagnosis and treatment planning. Although transformer-based methods have achieved remarkable results, their high computational cost hinders clinical deployment. To address this issue, we propose TM-UNet, a novel lightweight framework that integrates token sequence modeling with an efficient memory mechanism for efficient medical segmentation. Specifically, we introduce a multi-scale token-memory (MSTM) block that transforms 2D spatial features into token sequences through strategic spatial scanning, leveraging matrix memory cells to selectively retain and propagate discriminative contextual information across tokens. This novel token-memory mechanism acts as a dynamic knowledge store that captures long-range dependencies with linear complexity, enabling efficient global reasoning without redundant computation. Our MSTM block further incorporates exponential gating to identify token effectiveness and multi-scale contextual extraction via parallel pooling operations, enabling hierarchical representation learning without computational overhead. Extensive experiments demonstrate that TM-UNet outperforms state-of-the-art methods across diverse medical segmentation tasks with substantially reduced computation cost. The code is available at https://github.com/xq141839/TM-UNet.

</details>


### [97] [D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs](https://arxiv.org/abs/2511.12280)
*Shuochen Chang,Xiaofeng Zhang,Qingyang Liu,Li Niu*

Main category: cs.CV

TL;DR: 提出D$^{3}$ToM方法，通过动态合并冗余视觉token来加速扩散多模态大语言模型的推理速度，同时保持性能竞争力


<details>
  <summary>Details</summary>
Motivation: 扩散多模态大语言模型虽然具有强大的非自回归生成能力，但推理速度显著慢于自回归模型，因为每个去噪步骤都需要对整个序列进行双向自注意力计算，导致立方级解码复杂度

Method: 使用决策token构建重要性图，保留最显著token并合并其余token，通过相似性聚合动态缩短视觉token序列，且合并比例随去噪步骤动态变化

Result: 实验表明D$^{3}$ToM在加速推理的同时保持了竞争性性能

Conclusion: D$^{3}$ToM是一种有效的即插即用模块，能够显著提升扩散多模态大语言模型的推理效率

Abstract: Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.

</details>


### [98] [One target to align them all: LiDAR, RGB and event cameras extrinsic calibration for Autonomous Driving](https://arxiv.org/abs/2511.12291)
*Andrea Bertogalli,Giacomo Boracchi,Luca Magri*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态外参标定框架，能够同时估计事件相机、LiDAR和RGB相机之间的相对位姿，特别关注具有挑战性的事件相机标定问题。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶等应用中，精确的多传感器对齐至关重要。现有方法通常依赖分离的成对标定，难以实现高效准确的多模态传感器标定，特别是对于事件相机这种新型传感器。

Method: 设计并构建了新型3D标定靶标，包含平面特征、ChArUco图案和主动LED模式，分别针对LiDAR、RGB相机和事件相机的特性进行优化。实现了一次性联合外参标定流程。

Result: 在定制数据集上进行了广泛实验评估，使用先进的自动驾驶传感器设置进行记录，验证了方法的准确性和鲁棒性。

Conclusion: 该方法能够准确标定复杂的视觉系统，为自动驾驶等需要精确多传感器对齐的应用提供了有效的解决方案。

Abstract: We present a novel multi-modal extrinsic calibration framework designed to simultaneously estimate the relative poses between event cameras, LiDARs, and RGB cameras, with particular focus on the challenging event camera calibration. Core of our approach is a novel 3D calibration target, specifically designed and constructed to be concurrently perceived by all three sensing modalities. The target encodes features in planes, ChArUco, and active LED patterns, each tailored to the unique characteristics of LiDARs, RGB cameras, and event cameras respectively. This unique design enables a one-shot, joint extrinsic calibration process, in contrast to existing approaches that typically rely on separate, pairwise calibrations. Our calibration pipeline is designed to accurately calibrate complex vision systems in the context of autonomous driving, where precise multi-sensor alignment is critical. We validate our approach through an extensive experimental evaluation on a custom built dataset, recorded with an advanced autonomous driving sensor setup, confirming the accuracy and robustness of our method.

</details>


### [99] [Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method](https://arxiv.org/abs/2511.12301)
*Chi Liu,Jincheng Liu,Congcong Zhu,Minghao Wang,Sheng Shen,Jia Gu,Tianqing Zhu,Wanlei Zhou*

Main category: cs.CV

TL;DR: 本文提出了频率重校准(FreRec)方法来解决医学图像生成数据增强中的频率失配问题，通过统计高频替换和重构高频映射来改善生成图像质量，提升下游分类任务性能。


<details>
  <summary>Details</summary>
Motivation: 医学AI开发面临数据稀缺问题，生成数据增强(GDA)可合成医学图像但存在偏差风险，特别是频率失配问题会影响下游任务的可靠性。

Method: 提出FreRec方法，包含两个步骤：(1)统计高频替换(SHR)粗略对齐高频分量；(2)重构高频映射(RHM)提升图像质量并重建高频细节。

Result: 在脑部MRI、胸部X光、眼底图像等多个医学数据集上的实验表明，FreRec显著提升了下游医学图像分类性能。

Conclusion: FreRec是一个独立的后处理步骤，兼容任何生成模型，可无缝集成到常见的医学GDA流程中，有效解决频率失配问题。

Abstract: Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.

</details>


### [100] [LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors](https://arxiv.org/abs/2511.12304)
*Qifeng Chen,Jiarun Liu,Rengan Xie,Tao Tang,Sicong Du,Yiru Zhao,Yuchi Huo,Sheng Yang*

Main category: cs.CV

TL;DR: LiDAR-GS++是一种基于高斯泼溅的LiDAR重建方法，通过扩散先验增强，实现实时高保真重模拟，解决单次扫描重建不完整导致的伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于高斯泼溅的LiDAR渲染方法在推断新视角合成时会出现伪影，主要原因是单次扫描的重建不完整。

Method: 提出可控的LiDAR生成模型，基于粗略推断渲染生成额外的几何一致扫描，并采用有效的蒸馏机制进行扩展重建。

Result: 在多个公共数据集上的实验表明，LiDAR-GS++在插值和推断视角下均达到最先进性能，超越现有的GS和NeRF方法。

Conclusion: 通过将重建扩展到未充分拟合区域，该方法确保推断新视角的全局几何一致性，同时保留传感器捕获的详细场景表面。

Abstract: Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.

</details>


### [101] [Learning Time in Static Classifiers](https://arxiv.org/abs/2511.12321)
*Xi Ding,Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: 提出了一种简单有效的框架，为前馈分类器添加时间推理能力，无需修改模型架构或引入循环模块。通过支持-示例-查询学习范式，利用时间连贯轨迹学习类别特定时间原型，并通过可微分软DTW损失对齐预测序列。


<details>
  <summary>Details</summary>
Motivation: 现实世界视觉数据通常随时间逐渐演变，但传统分类器基于时间独立性假设训练，无法捕捉这种动态变化。

Method: 采用支持-示例-查询学习范式，将训练数据组织成时间连贯轨迹，学习类别特定时间原型，使用可微分软DTW损失对齐预测序列，并通过多目标函数促进语义一致性和时间平滑性。

Result: 在细粒度和超细粒度图像分类中提升性能，在视频异常检测中提供精确且时间一致的预测。

Conclusion: 该方法通过损失设计引入强时间归纳偏置，以模块化和数据高效的方式桥接静态和时间学习，仅需在预提取特征上使用简单分类器。

Abstract: Real-world visual data rarely presents as isolated, static instances. Instead, it often evolves gradually over time through variations in pose, lighting, object state, or scene context. However, conventional classifiers are typically trained under the assumption of temporal independence, limiting their ability to capture such dynamics. We propose a simple yet effective framework that equips standard feedforward classifiers with temporal reasoning, all without modifying model architectures or introducing recurrent modules. At the heart of our approach is a novel Support-Exemplar-Query (SEQ) learning paradigm, which structures training data into temporally coherent trajectories. These trajectories enable the model to learn class-specific temporal prototypes and align prediction sequences via a differentiable soft-DTW loss. A multi-term objective further promotes semantic consistency and temporal smoothness. By interpreting input sequences as evolving feature trajectories, our method introduces a strong temporal inductive bias through loss design alone. This proves highly effective in both static and temporal tasks: it enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection. Despite its simplicity, our approach bridges static and temporal learning in a modular and data-efficient manner, requiring only a simple classifier on top of pre-extracted features.

</details>


### [102] [SpaceVLM: Sub-Space Modeling of Negation in Vision-Language Models](https://arxiv.org/abs/2511.12331)
*Sepehr Kazemi Ranjbar,Kumail Alhamoud,Marzyeh Ghassemi*

Main category: cs.CV

TL;DR: 提出了一种无需训练的框架，将否定建模为联合嵌入空间中的子空间而非单点，显著提升了视觉语言模型对否定的理解能力，同时保持零样本性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在处理否定提示时表现不佳，现有方法通过微调大型否定数据集来解决，但这会损害模型在肯定提示上的零样本性能。

Method: 将否定建模为嵌入空间中的子空间，围绕A和N的嵌入构建两个球形帽区域，通过靠近A且远离N的区域中心方向来评分图像。

Result: 在检索、多项选择和文本到图像任务中，该方法将否定理解能力平均提升了约30%，缩小了肯定和否定提示之间的差距。

Conclusion: 该方法有效解决了VLMs的否定理解问题，同时保持了零样本性能，无需重新训练模型。

Abstract: Vision-Language Models (VLMs) struggle with negation. Given a prompt like "retrieve (or generate) a street scene without pedestrians," they often fail to respect the "not." Existing methods address this limitation by fine-tuning on large negation datasets, but such retraining often compromises the model's zero-shot performance on affirmative prompts. We show that the embedding space of VLMs, such as CLIP, can be divided into semantically consistent subspaces. Based on this property, we propose a training-free framework that models negation as a subspace in the joint embedding space rather than a single point (Figure 1). To find the matching image for a caption such as "A but not N," we construct two spherical caps around the embeddings of A and N, and we score images by the central direction of the region that is close to A and far from N. Across retrieval, MCQ, and text-to-image tasks, our method improves negation understanding by about 30% on average over prior methods. It closes the gap between affirmative and negated prompts while preserving the zero-shot performance that fine-tuned models fail to maintain. Code will be released upon publication.

</details>


### [103] [Ground Plane Projection for Improved Traffic Analytics at Intersections](https://arxiv.org/abs/2511.12342)
*Sajjad Pakdamansavoji,Kumar Vaibhav Jha,Baher Abdulhai,James H Elder*

Main category: cs.CV

TL;DR: 通过将基础设施摄像头检测到的车辆反投影到地面平面进行3D坐标分析，可以提高转弯运动计数的准确性。单摄像头系统反投影能获得更准确的轨迹分类和计数，多摄像头弱融合能进一步提升精度。


<details>
  <summary>Details</summary>
Motivation: 交叉口的准确转弯运动计数对于信号控制、交通管理和城市规划至关重要。传统基于图像平面的计算机视觉系统存在局限性，需要探索在地面平面进行3D坐标分析的潜在优势。

Method: 将基础设施摄像头检测到的车辆反投影到地面平面进行3D坐标分析，比较单摄像头和多摄像头弱融合两种方法。

Result: 单摄像头系统反投影能获得更准确的轨迹分类和转弯运动计数，多摄像头弱融合能进一步提升精度。

Conclusion: 交通分析应该在地面平面进行，而不是在图像平面。

Abstract: Accurate turning movement counts at intersections are important for signal control, traffic management and urban planning. Computer vision systems for automatic turning movement counts typically rely on visual analysis in the image plane of an infrastructure camera. Here we explore potential advantages of back-projecting vehicles detected in one or more infrastructure cameras to the ground plane for analysis in real-world 3D coordinates. For single-camera systems we find that back-projection yields more accurate trajectory classification and turning movement counts. We further show that even higher accuracy can be achieved through weak fusion of back-projected detections from multiple cameras. These results suggeest that traffic should be analyzed on the ground plane, not the image plane

</details>


### [104] [CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification](https://arxiv.org/abs/2511.12346)
*Asmit Bandyopadhyay,Anindita Das Bhattacharjee,Rakesh Das*

Main category: cs.CV

TL;DR: 提出了CLAReSNet混合架构，结合多尺度卷积提取和Transformer注意力，通过自适应潜在瓶颈降低计算复杂度，在高光谱图像分类中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决高光谱图像分类面临的高光谱维度、复杂光谱-空间相关性、训练样本有限且类别不平衡等关键挑战，克服CNN和Transformer单独应用的局限性。

Method: 使用多尺度卷积主干和增强的卷积注意力模块提取层次空间特征，结合双向RNN和多尺度光谱潜在注意力(MSLA)，通过自适应潜在令牌分配将复杂度从O(T²D)降至O(Tlog(T)D)。

Result: 在Indian Pines和Salinas数据集上分别达到99.71%和99.96%的总体准确率，显著超越HybridSN、SSRN和SpectralFormer等现有方法。

Conclusion: CLAReSNet在有限样本和严重类别不平衡条件下表现出色，学习到的嵌入具有优越的类间可分性和紧凑的类内聚类。

Abstract: Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\mathcal{O}(T^2D)$ to $\mathcal{O}(T\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under limited samples and severe class imbalance.

</details>


### [105] [Explainable AI-Generated Image Detection RewardBench](https://arxiv.org/abs/2511.12363)
*Michael Yang,Shijian Deng,William T. Doan,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CV

TL;DR: 提出了XAIGID-RewardBench基准，用于评估多模态大语言模型在判断AI生成图像检测解释质量方面的能力，发现当前最佳模型与人类水平仍有明显差距。


<details>
  <summary>Details</summary>
Motivation: 传统基于分类的AI生成图像检测方法无法提供人类专家可理解的解释，降低了检测工具的可信度和说服力。虽然MLLMs被用于解决此问题，但它们在判断自身或其他MLLMs生成的解释质量方面的能力尚未得到充分研究。

Method: 构建包含约3000个标注三元组的基准数据集，这些数据来自各种图像生成模型和作为检测器的MLLMs，用于评估MLLMs作为奖励模型（评判者）的能力。

Result: 当前最佳奖励模型在该基准上得分88.76%，而人类标注者间一致性达到98.30%，表明当前MLLMs的推理能力与人类水平仍存在明显差距。

Conclusion: 需要进一步改进MLLMs的推理能力以缩小与人类水平的差距，并分析了这些模型常见的错误类型。

Abstract: Conventional, classification-based AI-generated image detection methods cannot explain why an image is considered real or AI-generated in a way a human expert would, which reduces the trustworthiness and persuasiveness of these detection tools for real-world applications. Leveraging Multimodal Large Language Models (MLLMs) has recently become a trending solution to this issue. Further, to evaluate the quality of generated explanations, a common approach is to adopt an "MLLM as a judge" methodology to evaluate explanations generated by other MLLMs. However, how well those MLLMs perform when judging explanations for AI-generated image detection generated by themselves or other MLLMs has not been well studied. We therefore propose \textbf{XAIGID-RewardBench}, the first benchmark designed to evaluate the ability of current MLLMs to judge the quality of explanations about whether an image is real or AI-generated. The benchmark consists of approximately 3,000 annotated triplets sourced from various image generation models and MLLMs as policy models (detectors) to assess the capabilities of current MLLMs as reward models (judges). Our results show that the current best reward model scored 88.76\% on this benchmark (while human inter-annotator agreement reaches 98.30\%), demonstrating that a visible gap remains between the reasoning abilities of today's MLLMs and human-level performance. In addition, we provide an analysis of common pitfalls that these models frequently encounter. Code and benchmark are available at https://github.com/RewardBench/XAIGID-RewardBench.

</details>


### [106] [Constructing and Interpreting Digital Twin Representations for Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2511.12365)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: DT-R1是一个基于强化学习的框架，通过训练大语言模型构建视觉输入的数字孪生表示，作为视觉推理的统一方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理方法依赖任务特定的监督微调和架构设计，缺乏统一解决方案，限制了跨任务和跨模态的泛化能力。

Method: 使用GRPO强化学习训练大语言模型构建数字孪生表示，并通过新颖的奖励函数验证结构完整性和输出准确性。

Result: 在六个视觉推理基准测试中，涵盖两种模态和四种任务类型，DT-R1始终优于最先进的任务特定模型。

Conclusion: DT-R1开辟了通过数字孪生表示的强化学习实现视觉推理的新方向。

Abstract: Visual reasoning may require models to interpret images and videos and respond to implicit text queries across diverse output formats, from pixel-level segmentation masks to natural language descriptions. Existing approaches rely on supervised fine-tuning with task-specific architectures. For example, reasoning segmentation, grounding, summarization, and visual question answering each demand distinct model designs and training, preventing unified solutions and limiting cross-task and cross-modality generalization. Hence, we propose DT-R1, a reinforcement learning framework that trains large language models to construct digital twin representations of complex multi-modal visual inputs and then reason over these high-level representations as a unified approach to visual reasoning. Specifically, we train DT-R1 using GRPO with a novel reward that validates both structural integrity and output accuracy. Evaluations in six visual reasoning benchmarks, covering two modalities and four task types, demonstrate that DT-R1 consistently achieves improvements over state-of-the-art task-specific models. DT-R1 opens a new direction where visual reasoning emerges from reinforcement learning with digital twin representations.

</details>


### [107] [Fast Reasoning Segmentation for Images and Videos](https://arxiv.org/abs/2511.12368)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: FastReasonSeg是一种通过数字孪生表示实现高效推理分割的方法，使用蒸馏技术将大型多模态语言模型压缩到可在边缘设备部署的小型模型，在保持推理能力的同时大幅提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有推理分割方法需要数十亿参数的大型多模态语言模型，超出了边缘设备的计算能力。传统蒸馏方法无法有效传递多步推理能力，需要新的蒸馏策略。

Method: 采用数字孪生表示将感知与推理解耦，首先在教师模型生成的推理链上进行监督微调，然后通过强化学习微调，联合评估分割准确性和推理质量对齐。

Result: 在四个基准测试中达到最先进的推理分割性能，0.6B参数的蒸馏变体优于参数多20倍的模型，实现7.79 FPS吞吐量和仅2.1GB内存消耗。

Conclusion: 该方法实现了在资源受限环境中的实时推理分割部署，为具身AI系统在真实世界环境中的自主操作提供了可行方案。

Abstract: Reasoning segmentation enables open-set object segmentation via implicit text queries, therefore serving as a foundation for embodied agents that should operate autonomously in real-world environments. However, existing methods for reasoning segmentation require multimodal large language models with billions of parameters that exceed the computational capabilities of edge devices that typically deploy the embodied AI systems. Distillation offers a pathway to compress these models while preserving their capabilities. Yet, existing distillation approaches fail to transfer the multi-step reasoning capabilities that reasoning segmentation demands, as they focus on matching output predictions and intermediate features rather than preserving reasoning chains. The emerging paradigm of reasoning over digital twin representations presents an opportunity for more effective distillation by re-framing the problem. Consequently, we propose FastReasonSeg, which employs digital twin representations that decouple perception from reasoning to enable more effective distillation. Our distillation scheme first relies on supervised fine-tuning on teacher-generated reasoning chains. Then it is followed by reinforcement fine-tuning with joint rewards evaluating both segmentation accuracy and reasoning quality alignment. Experiments on two video (JiTBench, RVTBench) and two image benchmarks (ReasonSeg, LLM-Seg40K) demonstrate that our FastReasonSeg achieves state-of-the-art reasoning segmentation performance. Moreover, the distilled 0.6B variant outperforms models with 20 times more parameters while achieving 7.79 FPS throughput with only 2.1GB memory consumption. This efficiency enables deployment in resource-constrained environments to enable real-time reasoning segmentation.

</details>


### [108] [Changes in Real Time: Online Scene Change Detection with Multi-View Fusion](https://arxiv.org/abs/2511.12370)
*Chamuditha Jayanga Galappaththige,Jason Lai,Lloyd Windrim,Donald Dansereau,Niko Sünderhauf,Dimity Miller*

Main category: cs.CV

TL;DR: 提出首个姿态无关、无标签、多视角一致的在线场景变化检测方法，在10+FPS下超越现有离线方法的性能


<details>
  <summary>Details</summary>
Motivation: 现有的在线场景变化检测方法精度远低于离线方法，需要开发能在无约束视角下实时检测相关变化的高精度在线方法

Method: 使用自监督融合损失从多线索和观测推断场景变化，基于PnP的快速姿态估计，以及针对3D高斯泼溅场景表示的快速变化引导更新策略

Result: 在复杂真实世界数据集上的广泛实验表明，该方法超越了在线和离线基线方法，实现了新的最先进性能

Conclusion: 该方法首次实现了姿态无关、无标签、多视角一致的在线场景变化检测，在保持实时性的同时超越了离线方法的性能

Abstract: Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.

</details>


### [109] [Reasoning Text-to-Video Retrieval via Digital Twin Video Representations and Large Language Models](https://arxiv.org/abs/2511.12371)
*Yiqing Shen,Chenxiao Fan,Chenjia Li,Mathias Unberath*

Main category: cs.CV

TL;DR: 提出推理文本到视频检索新范式，通过数字孪生表示和LLM推理处理隐式查询，在多个基准测试中取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有方法只能处理显式查询，无法应对需要推理的隐式查询，限制了文本到视频检索的实际应用范围

Method: 两阶段框架：1）组合对齐分解子查询与数字孪生表示；2）LLM推理与即时精炼，调用专家模型填补信息空白

Result: 在ReasonT2VBench-135上达到81.2% R@1，比最强基线提升50多个百分点；在扩展配置上保持81.7% R@1，并在三个传统基准测试中达到最先进水平

Conclusion: 数字孪生表示和LLM推理的结合能够有效处理需要复杂推理的隐式查询，显著提升了文本到视频检索的性能

Abstract: The goal of text-to-video retrieval is to search large databases for relevant videos based on text queries. Existing methods have progressed to handling explicit queries where the visual content of interest is described explicitly; however, they fail with implicit queries where identifying videos relevant to the query requires reasoning. We introduce reasoning text-to-video retrieval, a paradigm that extends traditional retrieval to process implicit queries through reasoning while providing object-level grounding masks that identify which entities satisfy the query conditions. Instead of relying on vision-language models directly, we propose representing video content as digital twins, i.e., structured scene representations that decompose salient objects through specialist vision models. This approach is beneficial because it enables large language models to reason directly over long-horizon video content without visual token compression. Specifically, our two-stage framework first performs compositional alignment between decomposed sub-queries and digital twin representations for candidate identification, then applies large language model-based reasoning with just-in-time refinement that invokes additional specialist models to address information gaps. We construct a benchmark of 447 manually created implicit queries with 135 videos (ReasonT2VBench-135) and another more challenging version of 1000 videos (ReasonT2VBench-1000). Our method achieves 81.2% R@1 on ReasonT2VBench-135, outperforming the strongest baseline by greater than 50 percentage points, and maintains 81.7% R@1 on the extended configuration while establishing state-of-the-art results in three conventional benchmarks (MSR-VTT, MSVD, and VATEX).

</details>


### [110] [AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification](https://arxiv.org/abs/2511.12382)
*Ansh Makwe,Akansh Agrawal,Prateek Jain,Akshan Agrawal,Priyanka Bagade*

Main category: cs.CV

TL;DR: 提出了AGGRNet框架，通过提取信息性和非信息性特征来理解细粒度视觉模式，改善复杂医学图像分析任务的分类性能


<details>
  <summary>Details</summary>
Motivation: 医学图像分析面临类间相似性高、类内变异性大、标注数据稀缺等挑战，现有注意力模型难以有效区分细微类别，导致误诊

Method: AGGRNet框架提取信息性和非信息性特征，以更好地理解细粒度视觉模式

Result: 在多个医学影像数据集上达到最先进性能，在Kvasir数据集上比SOTA模型提升高达5%

Conclusion: AGGRNet能有效解决医学图像分析中细微类别区分问题，提升分类准确性

Abstract: Medical image analysis for complex tasks such as severity grading and disease subtype classification poses significant challenges due to intricate and similar visual patterns among classes, scarcity of labeled data, and variability in expert interpretations. Despite the usefulness of existing attention-based models in capturing complex visual patterns for medical image classification, underlying architectures often face challenges in effectively distinguishing subtle classes since they struggle to capture inter-class similarity and intra-class variability, resulting in incorrect diagnosis. To address this, we propose AGGRNet framework to extract informative and non-informative features to effectively understand fine-grained visual patterns and improve classification for complex medical image analysis tasks. Experimental results show that our model achieves state-of-the-art performance on various medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.

</details>


### [111] [Leveraging Quantum-Based Architectures for Robust Diagnostics](https://arxiv.org/abs/2511.12386)
*Shabnam Sodagari,Tommy Long*

Main category: cs.CV

TL;DR: 本研究使用混合量子-经典框架，结合预训练的ResNet50编码器和量子卷积神经网络，通过CT图像诊断和区分肾结石、囊肿和肿瘤，在8量子比特和12量子比特配置下均达到0.99的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 利用量子计算增强医学图像诊断性能，解决肾结石、囊肿和肿瘤的自动识别问题。

Method: 采用混合量子-经典框架：预处理的CT图像通过ResNet50提取特征，特征通过角度编码转换为量子比特，再由量子卷积神经网络处理。使用去噪和对比度增强预处理，并通过数据增强解决类别不平衡问题。

Result: 8量子比特和12量子比特配置均快速收敛且表现稳定，测试准确率达0.99。12量子比特配置在囊肿检测中实现完美召回率，肿瘤F1分数达0.9956，混淆矩阵显示各类别分类可靠，误分类极少。

Conclusion: 将经典预处理和深度特征提取与量子电路结合能够显著提升医学诊断性能，量子辅助诊断在医学图像分析中具有应用潜力。

Abstract: The objective of this study is to diagnose and differentiate kidney stones, cysts, and tumors using Computed Tomography (CT) images of the kidney. This study leverages a hybrid quantum-classical framework in this regard. We combine a pretrained ResNet50 encoder, with a Quantum Convolutional Neural Network (QCNN) to explore quantum-assisted diagnosis. We pre-process the kidney images using denoising and contrast limited adaptive histogram equalization to enhance feature extraction. We address class imbalance through data augmentation and weighted sampling. Latent features extracted by the encoder are transformed into qubits via angle encoding and processed by a QCNN. The model is evaluated on both 8-qubit and 12-qubit configurations. Both architectures achieved rapid convergence with stable learning curves and high consistency between training and validation performance. The models reached a test accuracy of 0.99, with the 12-qubit configuration providing improvements in overall recall and precision, particularly for Cyst and Tumor detection, where it achieved perfect recall for Cysts and a tumor F1-score of 0.9956. Confusion matrix analysis further confirmed reliable classification behavior across all classes, with very few misclassifications. Results demonstrate that integrating classical pre-processing and deep feature extraction with quantum circuits enhances medical diagnostic performance.

</details>


### [112] [Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation](https://arxiv.org/abs/2511.12389)
*Divake Kumar,Patrick Poggi,Sina Tayebati,Devashri Naik,Nilesh Ahuja,Amit Ranjan Trivedi*

Main category: cs.CV

TL;DR: 提出了一种轻量级推理时间框架，在深度特征空间中解耦偶然不确定性和认知不确定性，无需采样、集成或额外前向传播，显著减少计算成本并提高预测区间精度。


<details>
  <summary>Details</summary>
Motivation: 大多数估计器将所有不确定性模式压缩为单一置信度分数，无法可靠判断何时需要分配更多计算资源或调整推理过程。

Method: 使用正则化全局密度模型估计偶然不确定性，认知不确定性由三个互补组件构成：局部支持不足、流形谱崩溃和跨层特征不一致性，这些组件经验正交且无需额外计算。

Result: 在MOT17数据集上减少约60%计算成本且精度损失可忽略，不确定性分解比总体不确定性基线提高13.6个百分点的计算节省。

Conclusion: 该方法实现了实用的自调节视觉推理，通过正交不确定性分解显著提升计算效率。

Abstract: Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.

</details>


### [113] [MSLoRA: Multi-Scale Low-Rank Adaptation via Attention Reweighting](https://arxiv.org/abs/2511.12400)
*Xu Yang,Gady Agam*

Main category: cs.CV

TL;DR: MSLoRA是一个与主干网络无关的参数高效适配器，通过重新加权特征响应而不是重新调整主干网络来实现迁移学习，适用于CNN和ViT架构，仅需不到5%的主干参数。


<details>
  <summary>Details</summary>
Motivation: 现有的低秩适应方法主要局限于视觉变换器（ViTs），难以跨架构泛化，需要一种统一的方法来适应CNN和ViT架构。

Method: 结合低秩线性投影和多尺度非线性变换，通过点乘和残差连接融合，联合调制空间和通道注意力，保持预训练权重冻结。

Result: 在分类、检测和分割任务上持续提升迁移性能，实现稳定优化、快速收敛和强大的跨架构泛化能力。

Conclusion: MSLoRA通过重新加权而非重新调整，为冻结视觉主干网络的高效适应提供了一个简单通用的方法。

Abstract: We introduce MSLoRA, a backbone-agnostic, parameter-efficient adapter that reweights feature responses rather
  than re-tuning the underlying backbone. Existing low-rank adaptation methods are mostly confined to vision
  transformers (ViTs) and struggle to generalize across architectures. MSLoRA unifies adaptation for both convolutional neural networks (CNNs) and
  ViTs by combining a low-rank linear projection with a multi-scale nonlinear transformation that jointly
  modulates spatial and channel attention. The two components are fused through pointwise multiplication and
  a residual connection, yielding a lightweight module that shifts feature attention while keeping pretrained
  weights frozen.
  Extensive experiments demonstrate that MSLoRA consistently improves transfer performance on classification,
  detection, and segmentation tasks with roughly less than 5\% of backbone parameters.
  The design further enables stable optimization, fast convergence, and strong cross-architecture
  generalization. By reweighting rather than re-tuning, MSLoRA provides a simple and universal approach
  for efficient adaptation of frozen vision backbones.

</details>


### [114] [VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving](https://arxiv.org/abs/2511.12405)
*Hyunki Seong,Seongwoo Moon,Hojin Ahn,Jehun Kang,David Hyunchul Shim*

Main category: cs.CV

TL;DR: VLA-R是一个开放世界端到端自动驾驶框架，通过视觉-语言-动作检索范式，在非结构化户外环境中实现强泛化能力的自主驾驶。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶在非结构化户外环境中经常遇到训练时未见过的条件，需要强大的泛化能力来处理开放世界情况。

Method: 使用冻结的视觉语言模型进行开放世界检测和分割，通过Q-Former瓶颈聚合细粒度视觉表示和语言对齐的视觉特征，并采用视觉-动作对比学习方案对齐视觉语言和动作嵌入。

Result: 在真实世界机器人平台上的实验表明，即使在数据有限的情况下，该方法在非结构化、未见环境中表现出强大的泛化和探索性能。

Conclusion: VLA-R框架成功地将开放世界感知与视觉-动作检索相结合，为端到端自动驾驶在开放世界环境中的泛化问题提供了有效解决方案。

Abstract: Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.

</details>


### [115] [MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation](https://arxiv.org/abs/2511.12422)
*Nuolin Sun,Linyuan Wang,Haonan Wei,Lei Li,Bin Yan*

Main category: cs.CV

TL;DR: MFI-ResNet通过将ResNet的残差块替换为MeanFlow模块，采用压缩-扩展策略，在显著减少参数的同时提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 受流匹配模型MeanFlow的启发，探索生成式流场如何有效表征ResNet中的特征变换过程，为理解生成建模与判别学习的关系提供新视角。

Method: 压缩阶段：将每个ResNet阶段的多层结构简化为1-2个MeanFlow模块构建轻量元模型；扩展阶段：对前三个阶段采用选择性孵化策略，扩展到基准ResNet的残差块配置，保持最后阶段为MeanFlow形式并进行微调。

Result: 在CIFAR-10和CIFAR-100数据集上，相比ResNet-50，MFI-ResNet分别减少参数46.28%和45.59%，同时准确率提升0.23%和0.17%。

Conclusion: 生成式流场可以有效表征ResNet中的特征变换过程，为理解生成建模与判别学习的关系提供了新视角，同时实现了参数效率和判别性能的联合提升。

Abstract: ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.

</details>


### [116] [Self-Supervised Visual Prompting for Cross-Domain Road Damage Detection](https://arxiv.org/abs/2511.12410)
*Xi Xiao,Zhuxuanzi Wang,Mingqiao Mo,Chen Liu,Chenrui Ma,Yanshu Li,Smita Krishnaswamy,Xiao Wang,Tianyang Wang*

Main category: cs.CV

TL;DR: PROBE是一个自监督框架，通过视觉探测目标域无需标签，使用自监督提示增强模块和域感知提示对齐目标，在四个基准测试中优于监督、自监督和适应基线方法。


<details>
  <summary>Details</summary>
Motivation: 自动化路面缺陷检测的部署常受限于跨域泛化能力差。监督检测器需要昂贵的数据重标注，而标准自监督方法对域偏移仍然脆弱。

Method: 提出PROBE框架，包含自监督提示增强模块(SPEM)从无标签目标数据生成缺陷感知提示来指导冻结的ViT骨干网络，以及域感知提示对齐(DAPA)目标来对齐源域和目标域的提示条件表示。

Result: 在四个挑战性基准测试中，PROBE始终优于强监督、自监督和适应基线方法，实现了鲁棒的零样本迁移、改进的域变化弹性，以及在少样本适应中的高数据效率。

Conclusion: 自监督提示是构建可扩展和自适应视觉检测系统的实用方向。

Abstract: The deployment of automated pavement defect detection is often hindered by poor cross-domain generalization. Supervised detectors achieve strong in-domain accuracy but require costly re-annotation for new environments, while standard self-supervised methods capture generic features and remain vulnerable to domain shift. We propose \ours, a self-supervised framework that \emph{visually probes} target domains without labels. \ours introduces a Self-supervised Prompt Enhancement Module (SPEM), which derives defect-aware prompts from unlabeled target data to guide a frozen ViT backbone, and a Domain-Aware Prompt Alignment (DAPA) objective, which aligns prompt-conditioned source and target representations. Experiments on four challenging benchmarks show that \ours consistently outperforms strong supervised, self-supervised, and adaptation baselines, achieving robust zero-shot transfer, improved resilience to domain variations, and high data efficiency in few-shot adaptation. These results highlight self-supervised prompting as a practical direction for building scalable and adaptive visual inspection systems. Source code is publicly available: https://github.com/xixiaouab/PROBE/tree/main

</details>


### [117] [Real-Time Drivers' Drowsiness Detection and Analysis through Deep Learning](https://arxiv.org/abs/2511.12438)
*ANK Zaman,Prosenjit Chatterjee,Rajat Sharma*

Main category: cs.CV

TL;DR: 开发了一个基于深度卷积神经网络和OpenCV的实时驾驶员疲劳检测系统，通过分析面部特征（眼睛开合和打哈欠动作）来检测疲劳状态，并在检测到疲劳时发出警报。


<details>
  <summary>Details</summary>
Motivation: 长途驾驶容易导致驾驶员疲劳，疲劳驾驶对驾驶员和其他道路使用者的安全构成严重威胁，因此需要实时检测系统来预防事故。

Method: 使用OpenCV实时捕获驾驶员面部图像，分析面部关键点（眼睛开合和嘴巴动作），然后通过预训练的深度卷积神经网络模型检测疲劳状态。

Result: 在NTHU-DDD数据集和Yawn-Eye-Dataset上的疲劳检测分类准确率分别达到99.6%和97%。

Conclusion: 该方法提供了一种非侵入式、低成本且有效的疲劳检测解决方案，有望在智能汽车技术中应用，保护道路安全。

Abstract: A long road trip is fun for drivers. However, a long drive for days can be tedious for a driver to accommodate stringent deadlines to reach distant destinations. Such a scenario forces drivers to drive extra miles, utilizing extra hours daily without sufficient rest and breaks. Once a driver undergoes such a scenario, it occasionally triggers drowsiness during driving. Drowsiness in driving can be life-threatening to any individual and can affect other drivers' safety; therefore, a real-time detection system is needed. To identify fatigued facial characteristics in drivers and trigger the alarm immediately, this research develops a real-time driver drowsiness detection system utilizing deep convolutional neural networks (DCNNs) and OpenCV.Our proposed and implemented model takes real- time facial images of a driver using a live camera and utilizes a Python-based library named OpenCV to examine the facial images for facial landmarks like sufficient eye openings and yawn-like mouth movements. The DCNNs framework then gathers the data and utilizes a per-trained model to detect the drowsiness of a driver using facial landmarks. If the driver is identified as drowsy, the system issues a continuous alert in real time, embedded in the Smart Car technology.By potentially saving innocent lives on the roadways, the proposed technique offers a non-invasive, inexpensive, and cost-effective way to identify drowsiness. Our proposed and implemented DCNNs embedded drowsiness detection model successfully react with NTHU-DDD dataset and Yawn-Eye-Dataset with drowsiness detection classification accuracy of 99.6% and 97% respectively.

</details>


### [118] [Towards Rotation-only Imaging Geometry: Rotation Estimation](https://arxiv.org/abs/2511.12415)
*Xinrui Li,Qi Cai,Yuanxin Wu*

Main category: cs.CV

TL;DR: 提出了一种基于旋转流形的旋转优化框架，将平移表示为旋转的函数，在重投影误差基础上实现两视图和多视图的旋转估计，显著提升了SfM的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 基于姿态成像几何的成功，探索场景结构、旋转和平移之间的关键关系，将成像几何表示压缩到旋转流形上，以实现更准确、高效和可靠的3D视觉计算。

Method: 提出旋转优化框架，将平移表示为旋转的函数，基于重投影误差在两视图和多视图场景中进行旋转估计。

Result: 实验结果表明，该方法在旋转估计精度和鲁棒性方面优于当前最先进方法，甚至可与多次光束法平差迭代结果相媲美。

Conclusion: 这项工作为更准确、高效和可靠的3D视觉计算做出了贡献，证明了旋转流形表示在SfM中的有效性。

Abstract: Structure from Motion (SfM) is a critical task in computer vision, aiming to recover the 3D scene structure and camera motion from a sequence of 2D images. The recent pose-only imaging geometry decouples 3D coordinates from camera poses and demonstrates significantly better SfM performance through pose adjustment. Continuing the pose-only perspective, this paper explores the critical relationship between the scene structures, rotation and translation. Notably, the translation can be expressed in terms of rotation, allowing us to condense the imaging geometry representation onto the rotation manifold. A rotation-only optimization framework based on reprojection error is proposed for both two-view and multi-view scenarios. The experiment results demonstrate superior accuracy and robustness performance over the current state-of-the-art rotation estimation methods, even comparable to multiple bundle adjustment iteration results. Hopefully, this work contributes to even more accurate, efficient and reliable 3D visual computing.

</details>


### [119] [MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2511.12449)
*Zhanheng Nie,Chenghan Fu,Daoze Zhang,Junxian Wu,Wanxian Guan,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: MOON2.0是一个动态模态平衡的多模态表示学习框架，用于解决电商产品理解中的模态不平衡、对齐关系利用不足和数据噪声问题。


<details>
  <summary>Details</summary>
Motivation: 当前电商多模态大语言模型面临三个挑战：(i)模态混合训练导致的模态不平衡；(ii)产品内部视觉和文本信息对齐关系利用不足；(iii)电商多模态数据噪声处理能力有限。

Method: 提出MOON2.0框架，包含：模态驱动的专家混合模块、双级对齐方法、基于MLLM的图像-文本协同增强策略和动态样本过滤。

Result: 实验表明MOON2.0在MBE2.0基准和多个公共数据集上实现了最先进的零样本性能，注意力热图可视化显示其多模态对齐能力得到改善。

Conclusion: MOON2.0通过动态模态平衡和协同增强策略，有效提升了电商产品多模态理解能力。

Abstract: The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.

</details>


### [120] [Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance](https://arxiv.org/abs/2511.12419)
*Wenjie Li,Jinglei Shi,Jin Han,Heng Guo,Zhanyu Ma*

Main category: cs.CV

TL;DR: DHGM模型通过整合预训练扩散先验和高通滤波器，同时去除雨纹伪影并增强结构细节，解决了天气恢复与超分辨率之间的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界图像常因恶劣天气而退化，现有天气恢复方法可能牺牲对分析小物体至关重要的高频细节。简单的恢复和超分辨率级联方法难以解决两者之间的内在冲突：恢复旨在去除高频天气噪声，而超分辨率旨在从现有细节中幻觉高频纹理。

Method: 提出DHGM（基于扩散的高频引导模型），整合预训练扩散先验与高通滤波器，同时进行去雨伪影和结构细节增强。

Result: 大量实验证明DHGM在性能上优于现有方法，且成本更低。

Conclusion: DHGM能够有效生成干净且高分辨率的图像，解决了天气恢复与超分辨率之间的冲突问题。

Abstract: Clean images are crucial for visual tasks such as small object detection, especially at high resolutions. However, real-world images are often degraded by adverse weather, and weather restoration methods may sacrifice high-frequency details critical for analyzing small objects. A natural solution is to apply super-resolution (SR) after weather removal to recover both clarity and fine structures. However, simply cascading restoration and SR struggle to bridge their inherent conflict: removal aims to remove high-frequency weather-induced noise, while SR aims to hallucinate high-frequency textures from existing details, leading to inconsistent restoration contents. In this paper, we take deraining as a case study and propose DHGM, a Diffusion-based High-frequency Guided Model for generating clean and high-resolution images. DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. Extensive experiments demonstrate that DHGM achieves superior performance over existing methods, with lower costs.

</details>


### [121] [MaskAnyNet: Rethinking Masked Image Regions as Valuable Information in Supervised Learning](https://arxiv.org/abs/2511.12480)
*Jingshan Hong,Haigen Hu,Huihuang Zhang,Qianwei Zhou,Zhao Li*

Main category: cs.CV

TL;DR: 提出MaskAnyNet方法，将图像掩码区域作为辅助知识而非丢弃信息，通过重新学习机制同时利用可见和掩码区域，提升语义多样性并保留细粒度细节。


<details>
  <summary>Details</summary>
Motivation: 传统图像掩码存在像素利用不足和关键特征丢失问题，而MIM表明掩码区域可重建，具有与原图的强上下文一致性，可作为语义多样性来源。

Method: 提出MaskAnyNet，结合掩码和重新学习机制，通过额外分支从重新组合的掩码区域联合学习，利用掩码区域的语义多样性。

Result: 在CNN和Transformer骨干网络上的实验显示，在多个基准测试中均获得一致提升。

Conclusion: 该方法通过重用掩码内容有效提升了语义多样性，验证了掩码区域作为辅助知识的价值。

Abstract: In supervised learning, traditional image masking faces two key issues: (i) discarded pixels are underutilized, leading to a loss of valuable contextual information; (ii) masking may remove small or critical features, especially in fine-grained tasks. In contrast, masked image modeling (MIM) has demonstrated that masked regions can be reconstructed from partial input, revealing that even incomplete data can exhibit strong contextual consistency with the original image. This highlights the potential of masked regions as sources of semantic diversity. Motivated by this, we revisit the image masking approach, proposing to treat masked content as auxiliary knowledge rather than ignored. Based on this, we propose MaskAnyNet, which combines masking with a relearning mechanism to exploit both visible and masked information. It can be easily extended to any model with an additional branch to jointly learn from the recomposed masked region. This approach leverages the semantic diversity of the masked regions to enrich features and preserve fine-grained details. Experiments on CNN and Transformer backbones show consistent gains across multiple benchmarks. Further analysis confirms that the proposed method improves semantic diversity through the reuse of masked content.

</details>


### [122] [RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning](https://arxiv.org/abs/2511.12428)
*Jingqi Xu,Jingxi Lu,Chenghao Li,Sreetama Sarkar,Souvik Kundu,Peter A. Beerel*

Main category: cs.CV

TL;DR: RedVTP是一种针对扩散视觉语言模型(DVLMs)的响应驱动视觉token剪枝策略，通过利用掩码响应token的注意力来估计视觉token重要性，在保持准确性的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 扩散视觉语言模型(DVLMs)虽然支持并行token解码，但大量视觉token仍然严重阻碍推理效率。虽然视觉token剪枝在自回归VLMs中已有研究，但在DVLMs中仍未被充分探索。

Method: 提出RedVTP策略，利用掩码响应token的注意力来估计视觉token重要性，基于重要性分数在推理步骤间保持一致的观察，在第一步推理后剪枝不重要的视觉token。

Result: 在LLaDA-V和LaViDa模型上，RedVTP将token生成吞吐量分别提升高达186%和28.05%，推理延迟分别降低64.97%和21.87%，且准确率未受影响甚至有所提升。

Conclusion: RedVTP通过响应驱动的视觉token剪枝策略，有效解决了DVLMs的推理效率问题，在保持模型性能的同时显著提升了生成速度和降低了延迟。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.

</details>


### [123] [Fine-Grained Representation for Lane Topology Reasoning](https://arxiv.org/abs/2511.12590)
*Guoqing Xu,Yiheng Li,Yang Yang*

Main category: cs.CV

TL;DR: TopoFG是一个细粒度的车道拓扑推理框架，通过分层先验提取、区域聚焦解码器和鲁棒边界点拓扑推理三个模块，精确建模复杂车道结构并实现可靠拓扑预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常用单个查询表示每个车道，基于车道查询之间的相似性推断拓扑连接性，但这种方法难以准确建模复杂车道结构，导致拓扑预测不可靠。

Method: TopoFG将BEV特征到拓扑预测的过程分为三个阶段：分层先验提取器从BEV掩码提取全局空间先验和车道内关键点序列提取局部序列先验；区域聚焦解码器构建细粒度查询并采样RoI区域参考点；鲁棒边界点拓扑推理基于边界点查询特征建模车道连接性并应用拓扑去噪策略。

Result: 在OpenLane-V2基准测试中，TopoFG实现了最先进的性能，在subsetA上OLS达到48.0%，在subsetB上达到45.4%。

Conclusion: 通过将空间和序列先验集成到细粒度查询中，并对边界点拓扑推理应用去噪策略，该方法能够精确建模复杂车道结构并提供可信的拓扑预测。

Abstract: Precise modeling of lane topology is essential for autonomous driving, as it directly impacts navigation and control decisions.Existing methods typically represent each lane with a single query and infer topological connectivity based on the similarity between lane queries.However, this kind of design struggles to accurately model complex lane structures, leading to unreliable topology prediction.In this view, we propose a Fine-Grained lane topology reasoning framework (TopoFG).It divides the procedure from bird's-eye-view (BEV) features to topology prediction via fine-grained queries into three phases, i.e., Hierarchical Prior Extractor (HPE), Region-Focused Decoder (RFD), and Robust Boundary-Point Topology Reasoning (RBTR).Specifically, HPE extracts global spatial priors from the BEV mask and local sequential priors from in-lane keypoint sequences to guide subsequent fine-grained query modeling.RFD constructs fine-grained queries by integrating the spatial and sequential priors. It then samples reference points in RoI regions of the mask and applies cross-attention with BEV features to refine the query representations of each lane.RBTR models lane connectivity based on boundary-point query features and further employs a topological denoising strategy to reduce matching ambiguity.By integrating spatial and sequential priors into fine-grained queries and applying a denoising strategy to boundary-point topology reasoning, our method precisely models complex lane structures and delivers trustworthy topology predictions.Extensive experiments on the OpenLane-V2 benchmark demonstrate that TopoFG achieves new state-of-the-art performance, with an OLS of 48.0% on subsetA and 45.4% on subsetB.

</details>


### [124] [Text-Guided Channel Perturbation and Pretrained Knowledge Integration for Unified Multi-Modality Image Fusion](https://arxiv.org/abs/2511.12432)
*Xilai Li,Xiaosong Li,Weijun Jiang*

Main category: cs.CV

TL;DR: UP-Fusion是一个统一的多模态图像融合框架，通过通道扰动和预训练知识集成来解决模态差异导致的梯度冲突问题，提升融合质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 统一模型在多模态图像融合中由于模态差异大导致梯度冲突，限制性能；而引入模态特定编码器的方法又降低了不同融合任务间的泛化能力。

Method: 提出语义感知通道剪枝模块(SCPM)过滤和增强多模态特征通道；几何仿射调制模块(GAM)保持特征编码器的模态区分能力；文本引导通道扰动模块(TCPM)在解码时重塑通道分布。

Result: 大量实验表明，该算法在多模态图像融合和下游任务上均优于现有方法。

Conclusion: UP-Fusion框架通过通道扰动和预训练知识集成，有效解决了多模态图像融合中的梯度冲突和泛化问题，取得了优越性能。

Abstract: Multi-modality image fusion enhances scene perception by combining complementary information. Unified models aim to share parameters across modalities for multi-modality image fusion, but large modality differences often cause gradient conflicts, limiting performance. Some methods introduce modality-specific encoders to enhance feature perception and improve fusion quality. However, this strategy reduces generalisation across different fusion tasks. To overcome this limitation, we propose a unified multi-modality image fusion framework based on channel perturbation and pre-trained knowledge integration (UP-Fusion). To suppress redundant modal information and emphasize key features, we propose the Semantic-Aware Channel Pruning Module (SCPM), which leverages the semantic perception capability of a pre-trained model to filter and enhance multi-modality feature channels. Furthermore, we proposed the Geometric Affine Modulation Module (GAM), which uses original modal features to apply affine transformations on initial fusion features to maintain the feature encoder modal discriminability. Finally, we apply a Text-Guided Channel Perturbation Module (TCPM) during decoding to reshape the channel distribution, reducing the dependence on modality-specific channels. Extensive experiments demonstrate that the proposed algorithm outperforms existing methods on both multi-modality image fusion and downstream tasks.

</details>


### [125] [OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding](https://arxiv.org/abs/2511.12614)
*Artem Moroz,Vít Zeman,Martin Mikšík,Elizaveta Isianova,Miroslav David,Pavel Burget,Varun Burde*

Main category: cs.CV

TL;DR: 提出了一个统一的端到端框架，将物体检测和姿态估计与灵活的上线流程相结合。系统支持从3D CAD模型或通过多视角图像快速重建神经辐射场来生成物体表示，使用CNOS检测器定位目标物体，并通过新型姿态估计模块OPFormer推断6D姿态。


<details>
  <summary>Details</summary>
Motivation: 为了解决物体检测和姿态估计在实际应用中的灵活性问题，特别是在缺乏传统3D模型的情况下，需要一种能够从多视角图像快速构建物体表示的统一框架。

Method: 采用端到端框架，包含上线阶段生成物体表示（3D CAD模型或NeRF重建），CNOS检测器进行物体定位，以及基于transformer的OPFormer模块进行姿态估计。OPFormer利用基础模型提取特征，结合多视角模板和NOCS几何先验，通过解码器建立2D-3D对应关系。

Result: 在BOP基准测试中表现出色，在准确性和效率之间取得了良好平衡，证明了在模型驱动和无模型场景中的实际适用性。

Conclusion: 该集成系统提供了一个实用且灵活的解决方案，能够有效处理不同来源的物体表示，在复杂场景下实现准确的6D姿态估计。

Abstract: We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process. Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images. Given a test image, our system first employs the CNOS detector to localize target objects. For each detection, our novel pose estimation module, OPFormer, infers the precise 6D pose. The core of OPFormer is a transformer-based architecture that leverages a foundation model for robust feature extraction. It uniquely learns a comprehensive object representation by jointly encoding multiple template views and enriches these features with explicit 3D geometric priors using Normalized Object Coordinate Space (NOCS). A decoder then establishes robust 2D-3D correspondences to determine the final pose. Evaluated on the challenging BOP benchmarks, our integrated system demonstrates a strong balance between accuracy and efficiency, showcasing its practical applicability in both model-based and model-free scenarios.

</details>


### [126] [C3Net: Context-Contrast Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12627)
*Baber Jan,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: C3Net是一个用于伪装目标检测的双路径解码器架构，通过边缘细化路径和上下文定位路径协同工作，解决了伪装目标检测中的六个核心挑战，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 伪装目标检测面临传统分割方法和现代基础模型都无法有效处理的挑战，主要由于伪装目标与背景在颜色、纹理和模式上的高度相似性。论文识别了六个根本性挑战：内在相似性、边缘破坏、极端尺度变化、环境复杂性、上下文依赖性和显著-伪装目标区分。

Method: 提出C3Net双路径解码器架构：1) 边缘细化路径使用梯度初始化边缘增强模块从早期特征恢复精确边界；2) 上下文定位路径采用图像上下文引导机制实现内在显著性抑制；3) 注意力融合模块通过空间门控协同结合两条路径。

Result: 在COD10K数据集上S-measure达到0.898，CAMO数据集0.904，NC4K数据集0.913，均达到最先进性能，同时保持高效处理。

Conclusion: C3Net表明复杂多方面的检测挑战需要架构创新，通过专门组件协同工作实现全面覆盖，超越孤立改进。

Abstract: Camouflaged object detection identifies objects that blend seamlessly with their surroundings through similar colors, textures, and patterns. This task challenges both traditional segmentation methods and modern foundation models, which fail dramatically on camouflaged objects. We identify six fundamental challenges in COD: Intrinsic Similarity, Edge Disruption, Extreme Scale Variation, Environmental Complexities, Contextual Dependencies, and Salient-Camouflaged Object Disambiguation. These challenges frequently co-occur and compound the difficulty of detection, requiring comprehensive architectural solutions. We propose C3Net, which addresses all challenges through a specialized dual-pathway decoder architecture. The Edge Refinement Pathway employs gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features. The Contextual Localization Pathway utilizes our novel Image-based Context Guidance mechanism to achieve intrinsic saliency suppression without external models. An Attentive Fusion Module synergistically combines the two pathways via spatial gating. C3Net achieves state-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K, while maintaining efficient processing. C3Net demonstrates that complex, multifaceted detection challenges require architectural innovation, with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. Code, model weights, and results are available at https://github.com/Baber-Jan/C3Net.

</details>


### [127] [CoTBox-TTT: Grounding Medical VQA with Visual Chain-of-Thought Boxes During Test-time Training](https://arxiv.org/abs/2511.12446)
*Jiahe Qian,Yuhao Shen,Zhangtianyi Chen,Juexiao Zhou,Peisong Wang*

Main category: cs.CV

TL;DR: CoTBox-TTT是一种在推理时自适应的测试时训练方法，通过视觉思维链信号识别问题相关区域，并保持答案在原始图像和局部裁剪区域的一致性，无需额外标签即可提升医学VQA性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉问答系统在领域偏移下可靠性不足，产生缺乏图像证据支撑的答案，且重新训练或添加标签在实际部署中不实用。

Method: 采用测试时训练方法，仅更新少量连续软提示，通过视觉思维链识别问题相关区域，并确保原始图像和局部裁剪区域答案的一致性，所有骨干网络保持冻结。

Result: 在医学VQA实验中，该方法显著提升性能，例如在LLaVA模型上添加CoTBox-TTT后，在pathVQA上的闭式答案准确率提高了12.3%。

Conclusion: CoTBox-TTT是一种实用且无需标签的即插即用方法，适用于真实临床部署，能有效提升医学视觉问答系统的可靠性和证据基础。

Abstract: Medical visual question answering could support clinical decision making, yet current systems often fail under domain shift and produce answers that are weakly grounded in image evidence. This reliability gap arises when models attend to spurious regions and when retraining or additional labels are impractical at deployment time. We address this setting with CoTBox-TTT, an evidence-first test-time training approach that adapts a vision-language model at inference while keeping all backbones frozen. The method updates only a small set of continuous soft prompts. It identifies question-relevant regions through a visual chain-of-thought signal and encourages answer consistency across the original image and a localized crop. The procedure is label free, and plug and play with diverse backbones. Experiments on medical VQA show that the approach is practical for real deployments. For instance, adding CoTBox-TTT to LLaVA increases closed-ended accuracy by 12.3% on pathVQA.

</details>


### [128] [Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation](https://arxiv.org/abs/2511.12631)
*Yushe Cao,Dianxi Shi,Xing Fu,Xuechao Zou,Haikuo Peng,Xueqi Li,Chun Yu,Junliang Xing*

Main category: cs.CV

TL;DR: MDiTFace是一个基于扩散变换器的多模态人脸生成框架，通过统一标记化策略处理语义掩码和文本输入，采用解耦注意力机制减少计算开销，在面部保真度和条件一致性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的特征融合方法在多模态人脸生成中难以实现有效的跨模态交互，导致生成结果不理想。需要解决异构模态表示之间的差异问题。

Method: 提出MDiTFace框架：1）统一标记化策略处理语义掩码和文本输入；2）堆叠多元变换器块实现同步多模态特征交互；3）设计解耦注意力机制，分离掩码标记和时间嵌入的隐式依赖，分为动态和静态路径，可缓存和重用静态路径特征。

Result: 实验表明MDiTFace在面部保真度和条件一致性方面显著优于其他竞争方法，同时将掩码条件引入的额外计算开销减少了94%以上。

Conclusion: MDiTFace通过统一的标记化策略和创新的解耦注意力机制，有效解决了多模态人脸生成中的跨模态交互问题，在保持性能的同时大幅降低了计算成本。

Abstract: While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.

</details>


### [129] [BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections](https://arxiv.org/abs/2511.12676)
*Subin Varghese,Joshua Gao,Asad Ur Rahman,Vedhus Hoskere*

Main category: cs.CV

TL;DR: 提出了BridgeEQA基准测试，用于评估在真实桥梁检查场景中的具身问答能力，包含2200个开放词汇问答对，并提出了新的评估指标和EMVR方法来解决现有模型的性能差距。


<details>
  <summary>Details</summary>
Motivation: 解决在真实世界环境中部署具身代理进行问答的困难，特别是缺乏能够忠实捕捉实际操作条件的基准测试。桥梁检查领域自然地需要多尺度推理、长距离空间理解和复杂语义关系。

Method: 提出了BridgeEQA基准测试，基于专业检查报告和NBI条件评级。同时提出了Embodied Memory Visual Reasoning (EMVR)方法，将检查建模为在基于图像的场景图上的顺序导航，代理通过马尔可夫决策过程遍历视图、比较证据和推理。

Result: 对最先进的视觉语言模型的评估揭示了在情景记忆EQA设置下的显著性能差距。EMVR方法在基准测试上表现出优于基线模型的性能。

Conclusion: 桥梁检查是开放词汇具身问答的一个有前景的领域，BridgeEQA基准测试为评估具身代理在真实世界环境中的能力提供了重要工具，EMVR方法为解决这一挑战提供了有效途径。

Abstract: Deploying embodied agents that can answer questions about their surroundings in realistic real-world settings remains difficult, partly due to the scarcity of benchmarks that faithfully capture practical operating conditions. We propose infrastructure inspection as a compelling domain for open-vocabulary Embodied Question Answering (EQA): it naturally demands multi-scale reasoning, long-range spatial understanding, and complex semantic relationships, while offering unique evaluation advantages via standardized National Bridge Inventory (NBI) condition ratings (0-9), professional inspection reports, and egocentric imagery.
  We introduce BridgeEQA, a benchmark of 2,200 open-vocabulary question-answer pairs (in the style of OpenEQA) grounded in professional inspection reports across 200 real-world bridge scenes with 47.93 images on average per scene. Questions require synthesizing visual evidence across multiple images and aligning responses with NBI condition ratings. We further propose a new EQA metric Image Citation Relevance to evaluate the ability of a model to cite relevant images.
  Evaluations of state-of-the-art vision-language models reveal substantial performance gaps under episodic memory EQA settings. To address this, we propose Embodied Memory Visual Reasoning (EMVR), which formulates inspection as sequential navigation over an image-based scene graph: images are nodes, and an agent takes actions to traverse views, compare evidence, and reason within a Markov decision process. EMVR shows strong performance over the baselines. We publicly release both the dataset and code.

</details>


### [130] [DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions](https://arxiv.org/abs/2511.12452)
*Xiaoyu Lin,Aniket Ghorpade,Hansheng Zhu,Justin Qiu,Dea Rrozhani,Monica Lama,Mick Yang,Zixuan Bian,Ruohan Ren,Alan B. Hong,Jiatao Gu,Chris Callison-Burch*

Main category: cs.CV

TL;DR: DenseAnnotate是一个音频驱动的在线标注平台，能够高效创建图像和3D资产的密集细粒度标注，解决了传统文本标注在表达性、速度和视觉特征捕捉方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型的训练数据主要依赖稀疏的互联网挖掘或手动输入标注，这些方法只能捕捉图像视觉内容的一小部分。密集标注更有价值但稀缺，传统文本标注管道在表达性、速度和专业领域（如多元文化图像和3D资产标注）方面存在不足。

Method: 开发了DenseAnnotate平台，采用音频驱动方法，标注者通过口述观察并同步将口语短语链接到图像区域或3D场景部分。平台整合了语音转文字转录和注意力区域标记功能。

Result: 通过超过1000名标注者的案例研究，构建了包含3,531张图像、898个3D场景和7,460个3D对象的多模态数据集，涵盖20种语言的音频对齐密集标注。基于该数据集训练的模型在多语言能力上提升5%，文化对齐能力提升47%，3D空间能力提升54%。

Conclusion: DenseAnnotate平台为未来视觉语言研究提供了可行方法，能够应用于各种任务和多样化数据类型，显著提升了标注质量和模型性能。

Abstract: With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.

</details>


### [131] [R$^{2}$Seg: Training-Free OOD Medical Tumor Segmentation via Anatomical Reasoning and Statistical Rejection](https://arxiv.org/abs/2511.12691)
*Shuaike Shen,Ke Liu,Jiaqing Xie,Shangde Gao,Chunhua Shen,Ge Liu,Mireia Crispin-Ortuzar,Shangqi Gao*

Main category: cs.CV

TL;DR: R²Seg是一个无需训练的两阶段框架，通过推理-拒绝机制提升医学图像分割模型在分布外肿瘤上的鲁棒性，有效抑制假阳性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割基础模型在分布外数据上表现不佳，容易产生碎片化假阳性结果，需要提升模型在OOD肿瘤上的鲁棒性。

Method: 采用两阶段方法：1）推理阶段使用LLM引导的解剖推理定位器官锚点并生成多尺度ROI；2）拒绝阶段对基础模型生成的候选区域进行双样本统计测试，仅保留与正常组织显著不同的候选区域。

Result: 在多中心多模态肿瘤分割基准测试中，R²Seg在Dice系数、特异性和灵敏度方面显著优于强基线模型和原始基础模型。

Conclusion: R²Seg框架无需参数更新，兼容零更新测试时增强，能有效提升基础模型在OOD肿瘤分割中的性能，避免灾难性遗忘。

Abstract: Foundation models for medical image segmentation struggle under out-of-distribution (OOD) shifts, often producing fragmented false positives on OOD tumors. We introduce R$^{2}$Seg, a training-free framework for robust OOD tumor segmentation that operates via a two-stage Reason-and-Reject process. First, the Reason step employs an LLM-guided anatomical reasoning planner to localize organ anchors and generate multi-scale ROIs. Second, the Reject step applies two-sample statistical testing to candidates generated by a frozen foundation model (BiomedParse) within these ROIs. This statistical rejection filter retains only candidates significantly different from normal tissue, effectively suppressing false positives. Our framework requires no parameter updates, making it compatible with zero-update test-time augmentation and avoiding catastrophic forgetting. On multi-center and multi-modal tumor segmentation benchmarks, R$^{2}$Seg substantially improves Dice, specificity, and sensitivity over strong baselines and the original foundation models. Code are available at https://github.com/Eurekashen/R2Seg.

</details>


### [132] [Co-Layout: LLM-driven Co-optimization for Interior Layout](https://arxiv.org/abs/2511.12474)
*Chucheng Xiang,Ruchao Bao,Biyin Feng,Wenzheng Wu,Zhongyuan Liu,Yirui Guan,Ligang Liu*

Main category: cs.CV

TL;DR: 提出结合大语言模型和网格整数规划的自动化室内设计框架，联合优化房间布局和家具摆放


<details>
  <summary>Details</summary>
Motivation: 解决现有两阶段设计流程在解决方案质量和计算效率方面的不足，实现更优的室内设计自动化

Method: 使用LLM提取设计约束并编码到网格表示中，采用粗到细优化策略，通过整数规划联合优化布局

Result: 在多样化场景中显著优于现有两阶段设计流程，通过粗到细策略实现计算效率提升

Conclusion: 该联合优化框架在室内设计自动化方面表现出优越性能，为智能设计系统提供了有效解决方案

Abstract: We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.

</details>


### [133] [HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language Models](https://arxiv.org/abs/2511.12693)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: HEDGE是一个统一的幻觉检测框架，通过视觉扰动、语义聚类和不确定性度量来检测视觉语言模型的幻觉问题，在多种模型架构上表现出稳定的检测性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在开放域视觉问答中容易产生幻觉，需要系统性的检测方法来评估多模态可靠性。

Method: 结合受控视觉扰动、语义聚类（蕴含和嵌入两种方法）和鲁棒不确定性度量，构建可复现的检测流水线。

Result: 在VQA-RAD和KvasirVQA-x1数据集上评估三种VLMs，发现统一融合模型（如Qwen2.5-VL）检测性能最佳，VASE度量结合嵌入聚类在适度采样预算下提供最鲁棒的幻觉信号。

Conclusion: HEDGE将幻觉检测构建为几何鲁棒性问题，为评估多模态可靠性提供了原则性、计算感知的基础框架。

Abstract: Vision-language models (VLMs) enable open-ended visual question answering but remain prone to hallucinations. We present HEDGE, a unified framework for hallucination detection that combines controlled visual perturbations, semantic clustering, and robust uncertainty metrics. HEDGE integrates sampling, distortion synthesis, clustering (entailment- and embedding-based), and metric computation into a reproducible pipeline applicable across multimodal architectures.
  Evaluations on VQA-RAD and KvasirVQA-x1 with three representative VLMs (LLaVA-Med, Med-Gemma, Qwen2.5-VL) reveal clear architecture- and prompt-dependent trends. Hallucination detectability is highest for unified-fusion models with dense visual tokenization (Qwen2.5-VL) and lowest for architectures with restricted tokenization (Med-Gemma). Embedding-based clustering often yields stronger separation when applied directly to the generated answers, whereas NLI-based clustering remains advantageous for LLaVA-Med and for longer, sentence-level responses. Across configurations, the VASE metric consistently provides the most robust hallucination signal, especially when paired with embedding clustering and a moderate sampling budget (n ~ 10-15). Prompt design also matters: concise, label-style outputs offer clearer semantic structure than syntactically constrained one-sentence responses.
  By framing hallucination detection as a geometric robustness problem shaped jointly by sampling scale, prompt structure, model architecture, and clustering strategy, HEDGE provides a principled, compute-aware foundation for evaluating multimodal reliability. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE .

</details>


### [134] [Which Way from B to A: The role of embedding geometry in image interpolation for Stable Diffusion](https://arxiv.org/abs/2511.12757)
*Nicholas Karris,Luke Durell,Javier Flores,Tegan Emerson*

Main category: cs.CV

TL;DR: 论文发现Stable Diffusion的CLIP嵌入具有置换不变性，可视为Wasserstein空间中的点云而非欧几里得空间中的矩阵。通过将嵌入插值问题重构为最优传输问题，能计算嵌入之间的最短路径，生成更平滑连贯的中间图像。


<details>
  <summary>Details</summary>
Motivation: Stable Diffusion对CLIP嵌入矩阵的行具有置换不变性，这启发了将嵌入解释为Wasserstein空间中的点云而非欧几里得空间中的矩阵的新视角，为理解嵌入空间的几何结构开辟了新可能性。

Method: 将嵌入插值问题重新定义为最优传输问题，通过求解该问题计算嵌入之间的最短路径（测地线），从而在嵌入空间中实现更自然和几何平滑的过渡。

Result: 实验表明，基于最优传输的插值方法相比其他标准插值方法能产生更平滑的图像插值，生成的中间图像更加连贯。

Conclusion: 将嵌入视为点云（而非矩阵）能更好地反映和利用嵌入空间的几何结构，最优传输方法确实能提供更平滑的图像插值效果。

Abstract: It can be shown that Stable Diffusion has a permutation-invariance property with respect to the rows of Contrastive Language-Image Pretraining (CLIP) embedding matrices. This inspired the novel observation that these embeddings can naturally be interpreted as point clouds in a Wasserstein space rather than as matrices in a Euclidean space. This perspective opens up new possibilities for understanding the geometry of embedding space. For example, when interpolating between embeddings of two distinct prompts, we propose reframing the interpolation problem as an optimal transport problem. By solving this optimal transport problem, we compute a shortest path (or geodesic) between embeddings that captures a more natural and geometrically smooth transition through the embedding space. This results in smoother and more coherent intermediate (interpolated) images when rendered by the Stable Diffusion generative model. We conduct experiments to investigate this effect, comparing the quality of interpolated images produced using optimal transport to those generated by other standard interpolation methods. The novel optimal transport--based approach presented indeed gives smoother image interpolations, suggesting that viewing the embeddings as point clouds (rather than as matrices) better reflects and leverages the geometry of the embedding space.

</details>


### [135] [Towards Temporal Fusion Beyond the Field of View for Camera-based Semantic Scene Completion](https://arxiv.org/abs/2511.12498)
*Jongseong Bae,Junwoo Ha,Jinnyeong Heo,Yeongin Lee,Ha Young Kim*

Main category: cs.CV

TL;DR: 提出了C3DFusion模块，通过显式对齐当前和历史帧的3D点特征来生成隐藏区域感知的3D特征几何，有效解决了相机基3D语义场景补全方法在重建车辆侧边关键区域方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于相机的3D语义场景补全方法主要关注增强帧内区域，但在重建车辆侧边关键区域方面存在困难，尽管历史帧通常包含这些不可见区域的有价值上下文信息。

Method: 提出C3DFusion模块，通过历史上下文模糊化和当前中心特征密集化两种互补技术进行增强的时间融合，抑制不准确历史点特征的噪声并增强当前点特征的体积贡献。

Result: 在SemanticKITTI和SSCBench-KITTI-360数据集上显著优于最先进方法，并在其他基线模型上表现出强大的泛化能力。

Conclusion: C3DFusion模块简单集成到标准SSC架构中即可展示强大效果，为3D语义场景补全提供了有效的隐藏区域重建解决方案。

Abstract: Recent camera-based 3D semantic scene completion (SSC) methods have increasingly explored leveraging temporal cues to enrich the features of the current frame. However, while these approaches primarily focus on enhancing in-frame regions, they often struggle to reconstruct critical out-of-frame areas near the sides of the ego-vehicle, although previous frames commonly contain valuable contextual information about these unseen regions. To address this limitation, we propose the Current-Centric Contextual 3D Fusion (C3DFusion) module, which generates hidden region-aware 3D feature geometry by explicitly aligning 3D-lifted point features from both current and historical frames. C3DFusion performs enhanced temporal fusion through two complementary techniques-historical context blurring and current-centric feature densification-which suppress noise from inaccurately warped historical point features by attenuating their scale, and enhance current point features by increasing their volumetric contribution. Simply integrated into standard SSC architectures, C3DFusion demonstrates strong effectiveness, significantly outperforming state-of-the-art methods on the SemanticKITTI and SSCBench-KITTI-360 datasets. Furthermore, it exhibits robust generalization, achieving notable performance gains when applied to other baseline models.

</details>


### [136] [Lightweight Optimal-Transport Harmonization on Edge Devices](https://arxiv.org/abs/2511.12785)
*Maria Larchenko,Dmitry Guskov,Alexander Lobashev,Georgy Derevyanko*

Main category: cs.CV

TL;DR: 提出一种轻量级颜色协调方法MKL-Harmonizer，支持设备端实时推理，用于增强现实中的无缝图像合成


<details>
  <summary>Details</summary>
Motivation: 增强现实中需要实时颜色协调算法，但现有方法难以集成到AR流水线中，因为缺乏实时解决方案

Method: 基于经典最优传输理论，训练紧凑编码器预测Monge-Kantorovich传输映射

Result: 在真实AR合成图像上，该方法获得最佳综合评分，并发布了专用AR数据集和采集工具包

Conclusion: 该方法成功解决了AR中的颜色协调问题，支持设备端实时处理

Abstract: Color harmonization adjusts the colors of an inserted object so that it perceptually matches the surrounding image, resulting in a seamless composite. The harmonization problem naturally arises in augmented reality (AR), yet harmonization algorithms are not currently integrated into AR pipelines because real-time solutions are scarce. In this work, we address color harmonization for AR by proposing a lightweight approach that supports on-device inference. For this, we leverage classical optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map. We benchmark our MKL-Harmonizer algorithm against state-of-the-art methods and demonstrate that for real composite AR images our method achieves the best aggregated score. We release our dedicated AR dataset of composite images with pixel-accurate masks and data-gathering toolkit to support further data acquisition by researchers.

</details>


### [137] [Visible Structure Retrieval for Lightweight Image-Based Relocalisation](https://arxiv.org/abs/2511.12503)
*Fereidoon Zangeneh,Leonard Bruns,Amit Dekel,Alessandro Pieropan,Patric Jensfelt*

Main category: cs.CV

TL;DR: 提出一种新的视觉定位范式，通过神经网络直接从图像观测中检索可见的3D结构点，避免依赖图像检索或搜索启发式方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于结构的定位方法在大场景中需要复杂的搜索启发式或图像检索，导致计算复杂或存储需求随观测数量增长。

Method: 设计可见结构检索网络，通过前向传播直接从查询图像中获取地图中可见的3D结构点子集，减少2D-3D对应搜索空间。

Result: 该方法在定位精度上达到state-of-the-art水平，同时计算和存储开销更低。

Conclusion: 提出的新范式使基于结构的重定位更加可行，提供了一种更紧凑高效的解决方案。

Abstract: Accurate camera pose estimation from an image observation in a previously mapped environment is commonly done through structure-based methods: by finding correspondences between 2D keypoints on the image and 3D structure points in the map. In order to make this correspondence search tractable in large scenes, existing pipelines either rely on search heuristics, or perform image retrieval to reduce the search space by comparing the current image to a database of past observations. However, these approaches result in elaborate pipelines or storage requirements that grow with the number of past observations. In this work, we propose a new paradigm for making structure-based relocalisation tractable. Instead of relying on image retrieval or search heuristics, we learn a direct mapping from image observations to the visible scene structure in a compact neural network. Given a query image, a forward pass through our novel visible structure retrieval network allows obtaining the subset of 3D structure points in the map that the image views, thus reducing the search space of 2D-3D correspondences. We show that our proposed method enables performing localisation with an accuracy comparable to the state of the art, while requiring lower computational and storage footprint.

</details>


### [138] [MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12810)
*Leena Alghamdi,Muhammad Usman,Hafeez Anwar,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: 提出了一种多尺度递归网络MSRNet，通过金字塔视觉变换器提取多尺度特征，结合注意力机制和递归解码策略，在伪装目标检测任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前伪装目标检测方法在复杂场景中，特别是对小目标和多目标检测仍存在困难，需要改进检测精度。

Method: 使用金字塔视觉变换器作为骨干网络提取多尺度特征，通过注意力机制融合多尺度信息，采用递归解码策略和细粒度融合单元优化特征表示。

Result: 在两个基准数据集上达到最先进性能，在另外两个数据集上排名第二，能够有效检测小目标和多目标。

Conclusion: 多尺度学习和递归特征优化的联合策略能够显著提升伪装目标检测性能，特别是在复杂场景下。

Abstract: Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at \href{https://github.com/linaagh98/MSRNet}{https://github.com/linaagh98/MSRNet}.

</details>


### [139] [DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection](https://arxiv.org/abs/2511.12511)
*Jialiang Shen,Jiyang Zheng,Yunqi Xue,Huajie Chen,Yu Yao,Hui Kang,Ruiqi Liu,Helin Gong,Yang Yang,Dadong Wang,Tongliang Liu*

Main category: cs.CV

TL;DR: 提出基于师生知识蒸馏的模糊鲁棒AI生成图像检测框架，通过冻结教师模型(DINOv3)来指导学生在模糊图像上学习一致表示，显著提升在运动模糊条件下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器在真实世界运动模糊条件下性能严重下降，因为模糊会扭曲精细纹理并抑制高频伪影，限制了实际应用。

Method: 使用在清晰图像上训练的DINOv3作为教师模型，冻结其权重以保持泛化能力，然后通过知识蒸馏将教师的特征和logit响应传递给在模糊图像上训练的学生模型。

Result: 在运动模糊和清晰条件下都达到了最先进的性能，表现出改进的泛化能力和真实世界适用性。

Conclusion: 该方法有效解决了AI生成图像检测在运动模糊条件下的性能下降问题，为真实世界应用提供了鲁棒的解决方案。

Abstract: With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in real-world settings. We address this limitation with a blur-robust AIGI detection framework based on teacher-student knowledge distillation. A high-capacity teacher (DINOv3), trained on clean (i.e., sharp) images, provides stable and semantically rich representations that serve as a reference for learning. By freezing the teacher to maintain its generalization ability, we distill its feature and logit responses from sharp images to a student trained on blurred counterparts, enabling the student to produce consistent representations under motion degradation. Extensive experiments benchmarks show that our method achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability. Source codes will be released at: https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection.

</details>


### [140] [SAGA: Source Attribution of Generative AI Videos](https://arxiv.org/abs/2511.12834)
*Rohit Kundu,Vishal Mohanty,Hao Xiong,Shan Jia,Athula Balachandran,Amit K. Roy-Chowdhury*

Main category: cs.CV

TL;DR: SAGA是首个用于AI生成视频来源归因的综合性框架，能识别具体的生成模型，提供五个层级的归因分析，并采用数据高效的预训练策略，仅需0.5%的标注数据即可达到全监督性能。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的普及导致超逼真合成视频泛滥，传统二元真伪检测器已无法应对，迫切需要能识别具体生成模型的来源归因方法。

Method: 提出新颖的视频transformer架构，利用鲁棒视觉基础模型特征捕捉时空伪影；引入数据高效的预训练-归因策略；提出时序注意力签名(T-Sigs)可解释性方法。

Result: 在公共数据集上的广泛实验表明，SAGA在合成视频溯源方面设立了新基准，在跨域场景下也表现优异，仅用0.5%的源标注数据就能达到最先进的归因性能。

Conclusion: SAGA为法证和监管应用提供了关键的可解释洞察，是首个能大规模进行AI生成视频来源归因的综合框架。

Abstract: The proliferation of generative AI has led to hyper-realistic synthetic videos, escalating misuse risks and outstripping binary real/fake detectors. We introduce SAGA (Source Attribution of Generative AI videos), the first comprehensive framework to address the urgent need for AI-generated video source attribution at a large scale. Unlike traditional detection, SAGA identifies the specific generative model used. It uniquely provides multi-granular attribution across five levels: authenticity, generation task (e.g., T2V/I2V), model version, development team, and the precise generator, offering far richer forensic insights. Our novel video transformer architecture, leveraging features from a robust vision foundation model, effectively captures spatio-temporal artifacts. Critically, we introduce a data-efficient pretrain-and-attribute strategy, enabling SAGA to achieve state-of-the-art attribution using only 0.5\% of source-labeled data per class, matching fully supervised performance. Furthermore, we propose Temporal Attention Signatures (T-Sigs), a novel interpretability method that visualizes learned temporal differences, offering the first explanation for why different video generators are distinguishable. Extensive experiments on public datasets, including cross-domain scenarios, demonstrate that SAGA sets a new benchmark for synthetic video provenance, providing crucial, interpretable insights for forensic and regulatory applications.

</details>


### [141] [MdaIF: Robust One-Stop Multi-Degradation-Aware Image Fusion with Language-Driven Semantics](https://arxiv.org/abs/2511.12525)
*Jing Li,Yifan Wang,Jiafeng Yan,Renlong Zhang,Bin Yang*

Main category: cs.CV

TL;DR: 提出了一个基于大语言模型的退化感知图像融合框架MdaIF，用于处理多退化场景下的红外与可见光图像融合问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个问题：1) 未考虑恶劣天气条件下可见光图像的退化，影响融合性能；2) 依赖固定网络架构，难以适应多样化退化场景。

Method: 采用混合专家系统处理多退化场景，利用预训练视觉语言模型提取语义先验，提出退化感知通道注意力模块，并通过语义先验和通道调制特征指导专家路由。

Result: 大量实验验证了MdaIF的有效性，在复杂退化场景下表现出优于现有最先进方法的性能。

Conclusion: MdaIF框架能够有效处理多退化场景下的图像融合问题，通过退化感知和自适应网络架构实现了鲁棒的融合性能。

Abstract: Infrared and visible image fusion aims to integrate complementary multi-modal information into a single fused result. However, existing methods 1) fail to account for the degradation visible images under adverse weather conditions, thereby compromising fusion performance; and 2) rely on fixed network architectures, limiting their adaptability to diverse degradation scenarios. To address these issues, we propose a one-stop degradation-aware image fusion framework for multi-degradation scenarios driven by a large language model (MdaIF). Given the distinct scattering characteristics of different degradation scenarios (e.g., haze, rain, and snow) in atmospheric transmission, a mixture-of-experts (MoE) system is introduced to tackle image fusion across multiple degradation scenarios. To adaptively extract diverse weather-aware degradation knowledge and scene feature representations, collectively referred to as the semantic prior, we employ a pre-trained vision-language model (VLM) in our framework. Guided by the semantic prior, we propose degradation-aware channel attention module (DCAM), which employ degradation prototype decomposition to facilitate multi-modal feature interaction in channel domain. In addition, to achieve effective expert routing, the semantic prior and channel-domain modulated features are utilized to guide the MoE, enabling robust image fusion in complex degradation scenarios. Extensive experiments validate the effectiveness of our MdaIF, demonstrating superior performance over SOTA methods.

</details>


### [142] [Video Finetuning Improves Reasoning Between Frames](https://arxiv.org/abs/2511.12868)
*Ruiqi Yang,Tian Yun,Zihan Wang,Ellie Pavlick*

Main category: cs.CV

TL;DR: 论文提出视觉思维链(vCoT)方法，通过生成连续帧间的事件描述来增强多模态大语言模型的视频理解能力，比较了仅图像模型与视频微调模型的性能差异。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视频理解方面往往只是简单地拼接帧标记，缺乏对帧间过渡的显式建模，需要研究视频微调对模型能力的实际提升。

Method: 提出视觉思维链(vCoT)方法，生成连续帧之间的过渡事件描述作为显式推理过程，系统比较图像模型与视频微调模型在有无过渡线索情况下的表现。

Result: vCoT显著提升了仅图像模型在长视频问答任务上的性能，但对视频微调模型增益有限，表明后者已隐式掌握了帧间过渡推理能力。视频模型还能将时序推理能力迁移到静态图像任务中。

Conclusion: 视频微调使模型能够隐式学习帧间过渡推理，这种时序推理能力可以迁移到静态视觉推理任务，为多模态模型的时间理解提供了新视角。

Abstract: Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.

</details>


### [143] [D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation](https://arxiv.org/abs/2511.12528)
*Zheyuan Zhang,Jiwei Zhang,Boyu Zhou,Linzhimeng Duan,Hong Chen*

Main category: cs.CV

TL;DR: 提出D²-VPR框架，通过知识蒸馏和可变形聚合器在保持视觉基础模型强大特征提取能力的同时，显著减少模型参数和计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决DINOv2等视觉基础模型在视觉位置识别中虽然性能优越但模型复杂度和计算开销大，难以在资源受限设备上部署的问题。

Method: 采用两阶段训练策略（知识蒸馏+微调），引入蒸馏恢复模块对齐师生模型特征空间，设计基于自上而下注意力的可变形聚合器动态调整感兴趣区域。

Result: 在保持竞争力的性能同时，参数减少约64.2%，FLOPs减少约62.6%（相比CricaVPR）。

Conclusion: D²-VPR框架在视觉位置识别任务中实现了更好的性能-效率权衡，为资源受限环境下的部署提供了可行方案。

Abstract: Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.

</details>


### [144] [DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2511.12908)
*Junbo Zou,Haotian Xia,Zhen Ye,Shengjie Zhang,Christopher Lai,Vicente Ordonez,Weining Shen,Hanjie Chen*

Main category: cs.CV

TL;DR: DeepSport是首个端到端训练的多模态大语言模型框架，专为多任务、多运动的视频理解设计，通过主动迭代推理和专门帧提取工具实现"视频思考"，在6.7k问题测试基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 体育视频理解面临独特挑战，需要模型感知高速动态、理解复杂规则和长时序上下文。现有方法要么单一运动中心化，要么局限于特定任务，或依赖无训练范式缺乏稳健学习推理过程。

Method: 提出数据蒸馏管道从10个数据源合成高质量思维链轨迹，创建78k训练数据；采用两阶段训练策略：监督微调后接强化学习，使用新颖的门控工具使用奖励优化推理过程；从被动帧处理转向主动迭代推理。

Result: 在6.7k问题测试基准上的广泛实验表明，DeepSport达到最先进性能，显著优于专有模型和开源模型的基线。

Conclusion: 这项工作为领域特定视频推理建立了新基础，以应对多样化体育的复杂性。

Abstract: Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.

</details>


### [145] [ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding](https://arxiv.org/abs/2511.12530)
*Yuan Zhou,Litao Hua,Shilong Jin,Wentao Huang,Haoran Duan*

Main category: cs.CV

TL;DR: 提出了ReaSon框架，通过因果信息瓶颈(CIB)将关键帧选择定义为优化问题，确保所选关键帧既具有预测充分性又满足因果必要性。


<details>
  <summary>Details</summary>
Motivation: 由于视觉语言模型的输入token限制和视频帧中相关信息的时间稀疏性，关键帧选择对于视频理解至关重要。需要选择既信息丰富又具有因果决定性的关键帧。

Method: 使用可学习的策略网络从视觉相关候选帧池中选择关键帧来捕捉预测充分性，然后通过反事实干预评估因果必要性。设计符合CIB原则的复合奖励，通过强化学习指导选择策略。

Result: 在NExT-QA、EgoSchema和Video-MME上的广泛实验表明，在有限帧设置下，ReaSon始终优于现有的最先进方法。

Conclusion: ReaSon框架通过因果信息瓶颈有效解决了关键帧选择问题，验证了其有效性和泛化能力。

Abstract: Keyframe selection has become essential for video understanding with vision-language models (VLMs) due to limited input tokens and the temporal sparsity of relevant information across video frames. Video understanding often relies on effective keyframes that are not only informative but also causally decisive. To this end, we propose Reinforced Causal Search with Information Bottleneck (ReaSon), a framework that formulates keyframe selection as an optimization problem with the help of a novel Causal Information Bottleneck (CIB), which explicitly defines keyframes as those satisfying both predictive sufficiency and causal necessity. Specifically, ReaSon employs a learnable policy network to select keyframes from a visually relevant pool of candidate frames to capture predictive sufficiency, and then assesses causal necessity via counterfactual interventions. Finally, a composite reward aligned with the CIB principle is designed to guide the selection policy through reinforcement learning. Extensive experiments on NExT-QA, EgoSchema, and Video-MME demonstrate that ReaSon consistently outperforms existing state-of-the-art methods under limited-frame settings, validating its effectiveness and generalization ability.

</details>


### [146] [PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos](https://arxiv.org/abs/2511.12935)
*Dianbing Xi,Guoyuan An,Jingsen Zhu,Zhijian Liu,Yuan Liu,Ruiyuan Zhang,Jiayuan Lu,Rui Wang,Yuchi Huo*

Main category: cs.CV

TL;DR: PFAvatar是一种从"每日穿搭"照片重建高质量3D虚拟形象的新方法，通过两阶段流程：基于姿势感知扩散模型的少样本微调和NeRF表示的3D虚拟形象蒸馏，实现快速个性化重建。


<details>
  <summary>Details</summary>
Motivation: 现有的3D虚拟形象重建方法通常将图像分割为服装、配饰等组件进行组装，容易导致不一致性。"每日穿搭"照片中多样的姿势、遮挡和复杂背景给重建带来挑战。

Method: 第一阶段：使用预训练的ControlNet进行姿势估计，结合新颖的条件先验保持损失(CPPL)，端到端学习完整外观细节。第二阶段：基于NeRF的虚拟形象表示，通过规范SMPL-X空间采样和多分辨率3D-SDS进行优化。

Result: 方法在5分钟内完成个性化，比先前方法快48倍。在重建保真度、细节保持和对遮挡/截断的鲁棒性方面优于最先进方法，支持虚拟试穿、动画和视频重演等下游应用。

Conclusion: PFAvatar通过避免分解直接建模全身外观，结合连续辐射场表示，在真实世界"每日穿搭"相册的3D虚拟形象生成方面取得了实际进展，展示了方法的通用性和实用价值。

Abstract: We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day'' (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.

</details>


### [147] [HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models](https://arxiv.org/abs/2511.12547)
*Zhiguang Lu,Qianqian Xu,Peisong Wen,Siran Da,Qingming Huang*

Main category: cs.CV

TL;DR: 提出HiGFA方法，通过分层引导的扩散模型进行细粒度数据增强，在采样过程的不同阶段使用不同强度的文本、轮廓和分类器引导，以生成既多样又忠实于类别特征的合成图像。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在细粒度任务中难以准确捕捉类别定义性细微特征的问题，避免标准文本引导方法生成误导性样本导致分类器性能下降。

Method: HiGFA方法：早期阶段使用强文本和轮廓引导建立整体场景和结构，后期阶段激活细粒度分类器引导并基于预测置信度动态调制所有引导信号强度。

Result: 在多个细粒度视觉分类数据集上的实验证明了HiGFA的有效性。

Conclusion: HiGFA通过分层、置信度驱动的引导协调，能够智能平衡全局结构形成与精确细节优化，生成多样且忠实的合成图像。

Abstract: Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.

</details>


### [148] [EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics](https://arxiv.org/abs/2511.12962)
*Daniel Cavadia*

Main category: cs.CV

TL;DR: EndoSight AI是一个深度学习架构，用于内窥镜手术中实时检测胃肠道息肉，在检测和分割任务上表现出色，并能在GPU上实现实时推理。


<details>
  <summary>Details</summary>
Motivation: 在内窥镜手术中精确实时检测胃肠道息肉对于结直肠癌的早期诊断和预防至关重要。

Method: 利用公开的Hyper-Kvasir数据集，采用深度学习架构进行息肉定位和边界分割，并引入热感知程序确保模型鲁棒性。

Result: 系统在息肉检测上达到88.3%的平均精度(mAP)，分割任务Dice系数达69%，GPU推理速度超过35帧/秒。

Conclusion: 该集成AI解决方案设计用于无缝部署到内窥镜工作流程中，有望提高胃肠道医疗的诊断准确性和临床决策能力。

Abstract: Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.

</details>


### [149] [EmoVerse: A MLLMs-Driven Emotion Representation Dataset for Interpretable Visual Emotion Analysis](https://arxiv.org/abs/2511.12554)
*Yijie Guo,Dexiang Hong,Weidong Chen,Zihan She,Cheng Ye,Xiaojun Chang,Zhendong Mao*

Main category: cs.CV

TL;DR: EmoVerse是一个大规模开源数据集，通过多层次的基于知识图谱的注释实现可解释的视觉情感分析，包含219k+图像，支持离散和连续情感表示，并提出了可解释的情感分析模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉情感分析研究缺乏开源和可解释的数据集，通常只给整张图像分配单一离散情感标签，无法揭示视觉元素如何影响情感。

Method: 提出EmoVerse数据集，将情感分解为背景-属性-主体三元组，并将每个元素定位到视觉区域；采用多阶段标注流程确保高质量标注；提出可解释模型将视觉线索映射到维度情感空间。

Result: 构建了包含219k+图像的大规模数据集，支持词级和主体级情感推理，提供分类情感状态和维度情感空间的双重标注。

Conclusion: EmoVerse数据集、标注流程和模型为推进可解释的高层情感理解提供了全面基础。

Abstract: Visual Emotion Analysis (VEA) aims to bridge the affective gap between visual content and human emotional responses. Despite its promise, progress in this field remains limited by the lack of open-source and interpretable datasets. Most existing studies assign a single discrete emotion label to an entire image, offering limited insight into how visual elements contribute to emotion. In this work, we introduce EmoVerse, a large-scale open-source dataset that enables interpretable visual emotion analysis through multi-layered, knowledge-graph-inspired annotations. By decomposing emotions into Background-Attribute-Subject (B-A-S) triplets and grounding each element to visual regions, EmoVerse provides word-level and subject-level emotional reasoning. With over 219k images, the dataset further includes dual annotations in Categorical Emotion States (CES) and Dimensional Emotion Space (DES), facilitating unified discrete and continuous emotion representation. A novel multi-stage pipeline ensures high annotation reliability with minimal human effort. Finally, we introduce an interpretable model that maps visual cues into DES representations and provides detailed attribution explanations. Together, the dataset, pipeline, and model form a comprehensive foundation for advancing explainable high-level emotion understanding.

</details>


### [150] [CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models](https://arxiv.org/abs/2511.12964)
*Mehrab Mustafy Rahman,Jayanth Mohan,Tiberiu Sosea,Cornelia Caragea*

Main category: cs.CV

TL;DR: 提出CalibrateMix方法，通过有针对性的mixup策略改善半监督学习模型的校准性能，在保持分类准确率的同时降低预期校准误差。


<details>
  <summary>Details</summary>
Motivation: 现有半监督学习方法存在校准不佳的问题，模型会产生过度自信的预测。虽然mixup在监督学习中能改善校准，但在半监督学习中由于伪标签的不可靠性，随机mixup面临挑战。

Method: 利用训练动态识别'易学'和'难学'样本，然后对易学和难学样本进行有针对性的mixup操作。

Result: 在多个基准图像数据集上的实验结果表明，该方法相比现有半监督学习方法实现了更低的预期校准误差和更高的准确率。

Conclusion: CalibrateMix方法有效解决了半监督学习中的校准问题，在保持或提升分类性能的同时显著改善了模型的校准质量。

Abstract: Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.

</details>


### [151] [SEMC: Structure-Enhanced Mixture-of-Experts Contrastive Learning for Ultrasound Standard Plane Recognition](https://arxiv.org/abs/2511.12559)
*Qing Cai,Guihao Yan,Fan Zhang,Cheng Zhang,Zhi Liu*

Main category: cs.CV

TL;DR: 提出了SEMC框架，结合结构感知特征融合和专家引导对比学习，用于超声标准平面识别，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效利用浅层结构信息，且难以通过图像增强生成的对比样本捕捉细粒度语义差异，导致超声标准平面识别效果不佳。

Method: 提出SEMC框架：1）语义-结构融合模块(SSFM)利用多尺度结构信息对齐浅层和深层特征；2）专家混合对比识别模块(MCRM)使用MoE机制进行分层对比学习和分类。

Result: 在内部数据集和两个公共数据集上的实验表明，SEMC在各种指标上均优于现有最先进方法。

Conclusion: SEMC框架通过结构增强和专家混合对比学习，有效提升了超声标准平面识别的性能，特别是在结构细节感知和类别区分能力方面。

Abstract: Ultrasound standard plane recognition is essential for clinical tasks such as disease screening, organ evaluation, and biometric measurement. However, existing methods fail to effectively exploit shallow structural information and struggle to capture fine-grained semantic differences through contrastive samples generated by image augmentations, ultimately resulting in suboptimal recognition of both structural and discriminative details in ultrasound standard planes. To address these issues, we propose SEMC, a novel Structure-Enhanced Mixture-of-Experts Contrastive learning framework that combines structure-aware feature fusion with expert-guided contrastive learning. Specifically, we first introduce a novel Semantic-Structure Fusion Module (SSFM) to exploit multi-scale structural information and enhance the model's ability to perceive fine-grained structural details by effectively aligning shallow and deep features. Then, a novel Mixture-of-Experts Contrastive Recognition Module (MCRM) is designed to perform hierarchical contrastive learning and classification across multi-level features using a mixture-of-experts (MoE) mechanism, further improving class separability and recognition performance. More importantly, we also curate a large-scale and meticulously annotated liver ultrasound dataset containing six standard planes. Extensive experimental results on our in-house dataset and two public datasets demonstrate that SEMC outperforms recent state-of-the-art methods across various metrics.

</details>


### [152] [UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective](https://arxiv.org/abs/2511.12988)
*Furui Xu,Shaobo Wang,Jiajun Zhang,Chenghao Sun,Haixiang Tang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出了UNSEEN框架，从泛化角度进行数据集剪枝，通过使用未在训练中见过的模型来评估样本重要性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集剪枝方法主要基于训练阶段的模型性能来评估样本重要性，导致样本分数分布密集、区分度低，难以有效选择代表性样本。

Method: UNSEEN框架：1）从泛化角度评估样本，使用未见过该样本的模型进行评分；2）扩展到多步场景，通过在不同coreset上训练的模型进行增量选择；3）动态优化coreset质量。

Result: 在CIFAR-10、CIFAR-100和ImageNet-1K上显著优于现有SOTA方法，在ImageNet-1K上减少30%训练数据仍能实现无损性能。

Conclusion: 从泛化角度进行数据集剪枝比传统拟合中心方法更有效，UNSEEN框架能显著提升剪枝效果并保持模型性能。

Abstract: The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model's performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\%.

</details>


### [153] [Through-Foliage Surface-Temperature Reconstruction for early Wildfire Detection](https://arxiv.org/abs/2511.12572)
*Mohamed Youssef,Lukas Brunner,Klaus Rundhammer,Gerald Czech,Oliver Bimber*

Main category: cs.CV

TL;DR: 提出了一种结合信号处理和机器学习的方法，通过遮挡的森林植被重建地表温度，用于无人机自主监测野火，实现烟雾或火焰可见前的地面火灾早期检测。


<details>
  <summary>Details</summary>
Motivation: 实现全自动空中野火监测，让自主无人机能够在烟雾或火焰可见之前早期检测地面火灾。合成孔径传感虽然减轻了树冠和阳光的遮挡，但引入了热模糊，掩盖了实际地表温度。

Method: 训练视觉状态空间模型从模糊数据中恢复部分遮挡土壤和火灾热点的细微热信号。通过将潜在扩散模型集成到向量量化中，从真实野火记录生成大量真实地表温度模拟数据，并通过温度增强和程序化热森林模拟进一步扩展。

Result: 在模拟数据上，相比传统热成像和未校正SA成像，RMSE降低了2到2.5倍。在野外实验中，对高温热点的改进更为显著，相比传统热成像RMSE增益达12.8倍，相比未校正SA图像增益达2.6倍。

Conclusion: 该方法能够重建火灾和人体特征的完整形态，而传统成像因部分遮挡而失效。模型还展示了在其他热信号（如搜救中的人体特征）上的泛化能力。

Abstract: We introduce a novel method for reconstructing surface temperatures through occluding forest vegetation by combining signal processing and machine learning. Our goal is to enable fully automated aerial wildfire monitoring using autonomous drones, allowing for the early detection of ground fires before smoke or flames are visible. While synthetic aperture (SA) sensing mitigates occlusion from the canopy and sunlight, it introduces thermal blur that obscures the actual surface temperatures. To address this, we train a visual state space model to recover the subtle thermal signals of partially occluded soil and fire hotspots from this blurred data. A key challenge was the scarcity of real-world training data. We overcome this by integrating a latent diffusion model into a vector quantized to generated a large volume of realistic surface temperature simulations from real wildfire recordings, which we further expanded through temperature augmentation and procedural thermal forest simulation. On simulated data across varied ambient and surface temperatures, forest densities, and sunlight conditions, our method reduced the RMSE by a factor of 2 to 2.5 compared to conventional thermal and uncorrected SA imaging. In field experiments focused on high-temperature hotspots, the improvement was even more significant, with a 12.8-fold RMSE gain over conventional thermal and a 2.6-fold gain over uncorrected SA images. We also demonstrate our model's generalization to other thermal signals, such as human signatures for search and rescue. Since simple thresholding is frequently inadequate for detecting subtle thermal signals, the morphological characteristics are equally essential for accurate classification. Our experiments demonstrated another clear advantage: we reconstructed the complete morphology of fire and human signatures, whereas conventional imaging is defeated by partial occlusion.

</details>


### [154] [SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias](https://arxiv.org/abs/2511.13005)
*Wenqian Ye,Di Wang,Guangtao Zheng,Bohan Liu,Aidong Zhang*

Main category: cs.CV

TL;DR: 提出SAGE方法，通过引导式提示选择缓解CLIP模型中的多模态虚假偏见，无需训练或微调即可提升零样本分类的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: CLIP等大视觉语言模型存在多模态虚假偏见，倾向于依赖虚假特征（如背景）而非核心特征进行推理，这严重影响了在分布外数据上的鲁棒性。现有方法需要下游数据微调或先验知识，限制了CLIP的开箱即用性。

Method: 提出SAGE方法：理论分析多模态虚假偏见的影响，通过探索提示模板空间，选择能诱导最大类别语义分离的提示，无需训练、微调或外部标注。

Result: 在4个真实世界基准数据集和5个主流骨干模型上的实验表明，SAGE一致提升了零样本性能和泛化能力，优于现有零样本方法。

Conclusion: SAGE是一种简单有效的零样本方法，能够缓解多模态虚假偏见，提高模型鲁棒性，且无需任何外部知识或模型更新。

Abstract: Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.

</details>


### [155] [Beyond Pixels: Semantic-aware Typographic Attack for Geo-Privacy Protection](https://arxiv.org/abs/2511.12575)
*Jiayi Zhu,Yihao Huang,Yue Cao,Xiaojun Jia,Qing Guo,Felix Juefei-Xu,Geguang Pu,Bin Wang*

Main category: cs.CV

TL;DR: 提出了一种基于语义感知的排版攻击方法，通过在图像外部添加欺骗性文本，有效保护用户的地理位置隐私，同时保持图像视觉质量。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型能够从社交媒体图像中推断用户地理位置，造成严重的隐私泄露威胁。现有的对抗性扰动方法需要强失真才能有效，但会显著降低图像视觉质量。

Method: 采用两阶段语义感知排版攻击，在图像视觉内容外部添加欺骗性文本，研究哪些文本语义能有效干扰地理位置推断。

Result: 在三个数据集上的广泛实验表明，该方法显著降低了五种最先进商业LVLMs的地理位置预测准确率。

Conclusion: 该方法建立了一种实用且视觉保持的保护策略，有效应对新兴的地理隐私威胁。

Abstract: Large Visual Language Models (LVLMs) now pose a serious yet overlooked privacy threat, as they can infer a social media user's geolocation directly from shared images, leading to unintended privacy leakage. While adversarial image perturbations provide a potential direction for geo-privacy protection, they require relatively strong distortions to be effective against LVLMs, which noticeably degrade visual quality and diminish an image's value for sharing. To overcome this limitation, we identify typographical attacks as a promising direction for protecting geo-privacy by adding text extension outside the visual content. We further investigate which textual semantics are effective in disrupting geolocation inference and design a two-stage, semantics-aware typographical attack that generates deceptive text to protect user privacy. Extensive experiments across three datasets demonstrate that our approach significantly reduces geolocation prediction accuracy of five state-of-the-art commercial LVLMs, establishing a practical and visually-preserving protection strategy against emerging geo-privacy threats.

</details>


### [156] [MeanFlow Transformers with Representation Autoencoders](https://arxiv.org/abs/2511.13019)
*Zheyuan Hu,Chieh-Hsin Lai,Ge Wu,Yuki Mitsufuji,Stefano Ermon*

Main category: cs.CV

TL;DR: 提出了一种在表示自编码器（RAE）潜在空间中训练MeanFlow的高效方法，通过一致性中期训练和两阶段方案解决了梯度爆炸问题，无需复杂指导即可实现高效少步生成。


<details>
  <summary>Details</summary>
Motivation: MeanFlow在潜在空间中训练计算量大且不稳定，SD-VAE解码器在推理时成本高，且需要复杂的指导超参数进行类别条件生成。

Method: 在RAE潜在空间中训练，采用一致性中期训练进行轨迹感知初始化，使用两阶段方案：从预训练流匹配教师蒸馏加速收敛，可选引导阶段使用单点速度估计器减少偏差。

Result: 在ImageNet 256上实现1步FID 2.03，优于vanilla MF的3.43，采样GFLOPS减少38%，总训练成本降低83%；在ImageNet 512上达到1步FID 3.23，在所有基线中GFLOPS最低。

Conclusion: 该方法简化了训练配置，移除了指导需求，在训练和采样中都减少了计算量，实现了高效稳定的少步生成。

Abstract: MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.

</details>


### [157] [TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction](https://arxiv.org/abs/2511.12578)
*Yukuo Ma,Cong Liu,Junke Wang,Junqi Liu,Haibin Huang,Zuxuan Wu,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TempoMaster是一个新颖的长视频生成框架，将长视频生成建模为下一帧率预测，通过从低帧率到高帧率的渐进式生成实现高效并行合成。


<details>
  <summary>Details</summary>
Motivation: 解决长视频生成中的长程时间一致性问题，同时保持高效的生成能力。

Method: 首先生成低帧率视频作为粗粒度蓝图，然后逐步提高帧率以细化视觉细节和运动连续性；在每帧率级别使用双向注意力，在帧率间进行自回归预测。

Result: 在长视频生成任务中达到了新的最先进水平，在视觉质量和时间质量方面表现出色。

Conclusion: TempoMaster通过帧率渐进式生成策略，成功实现了长视频的高质量高效生成，在长程时间一致性方面表现优异。

Abstract: We present TempoMaster, a novel framework that formulates long video generation as next-frame-rate prediction. Specifically, we first generate a low-frame-rate clip that serves as a coarse blueprint of the entire video sequence, and then progressively increase the frame rate to refine visual details and motion continuity. During generation, TempoMaster employs bidirectional attention within each frame-rate level while performing autoregression across frame rates, thus achieving long-range temporal coherence while enabling efficient and parallel synthesis. Extensive experiments demonstrate that TempoMaster establishes a new state-of-the-art in long video generation, excelling in both visual and temporal quality.

</details>


### [158] [SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction](https://arxiv.org/abs/2511.13020)
*Yufei Wen,Yuting Zhang,Jingdan Kang,Hao Ren,Weibin Cheng,Jintai Chen,Kaishun Wu*

Main category: cs.CV

TL;DR: 提出SpectralAdapt半监督域适应框架，通过光谱密度掩码和光谱端元表示对齐技术，解决医疗领域高光谱图像重建中的数据稀缺和域差异问题。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像在医疗领域潜力巨大，但获取成本高且技术复杂。现有通用领域数据集丰富，而人体高光谱数据稀缺，限制了医疗应用的发展。

Method: 提出SpectralAdapt框架，包含光谱密度掩码(SDM)自适应掩码RGB通道，以及光谱端元表示对齐(SERA)使用物理可解释的端元作为域不变锚点指导预测。

Result: 在基准数据集上实验表明，该方法在光谱保真度、跨域泛化能力和训练稳定性方面均有持续改进。

Conclusion: 半监督域适应是医疗高光谱成像的高效解决方案，SpectralAdapt框架有效缓解了域偏移、光谱退化和数据稀缺问题。

Abstract: Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.

</details>


### [159] [Rank-Aware Agglomeration of Foundation Models for Immunohistochemistry Image Cell Counting](https://arxiv.org/abs/2511.12588)
*Zuqi Huang,Mengxin Tian,Huan Liu,Wentao Li,Baobao Liang,Jie Wu,Fang Yan,Zhaoqing Tang,Zhongyu Li*

Main category: cs.CV

TL;DR: 提出CountIHC框架，通过排名感知的教师选择策略和多模态微调，实现免疫组化图像中多类细胞的高精度计数


<details>
  <summary>Details</summary>
Motivation: 免疫组化图像中细胞计数对癌症诊断至关重要，但现有方法难以处理染色重叠、形态多样性和多类计数问题，基础模型潜力未充分挖掘

Method: 使用排名感知教师选择策略评估基础模型的计数能力，通过多模态微调将任务转化为视觉语言对齐，利用语义锚点指导类别特异性密度图回归

Result: 在12种免疫组化生物标志物和5种组织类型上超越现有最优方法，与病理学家评估高度一致，在H&E染色数据上也表现良好

Conclusion: CountIHC框架有效解决了免疫组化图像中多类细胞计数挑战，具有良好的可扩展性和临床应用价值

Abstract: Accurate cell counting in immunohistochemistry (IHC) images is critical for quantifying protein expression and aiding cancer diagnosis. However, the task remains challenging due to the chromogen overlap, variable biomarker staining, and diverse cellular morphologies. Regression-based counting methods offer advantages over detection-based ones in handling overlapped cells, yet rarely support end-to-end multi-class counting. Moreover, the potential of foundation models remains largely underexplored in this paradigm. To address these limitations, we propose a rank-aware agglomeration framework that selectively distills knowledge from multiple strong foundation models, leveraging their complementary representations to handle IHC heterogeneity and obtain a compact yet effective student model, CountIHC. Unlike prior task-agnostic agglomeration strategies that either treat all teachers equally or rely on feature similarity, we design a Rank-Aware Teacher Selecting (RATS) strategy that models global-to-local patch rankings to assess each teacher's inherent counting capacity and enable sample-wise teacher selection. For multi-class cell counting, we introduce a fine-tuning stage that reformulates the task as vision-language alignment. Discrete semantic anchors derived from structured text prompts encode both category and quantity information, guiding the regression of class-specific density maps and improving counting for overlapping cells. Extensive experiments demonstrate that CountIHC surpasses state-of-the-art methods across 12 IHC biomarkers and 5 tissue types, while exhibiting high agreement with pathologists' assessments. Its effectiveness on H&E-stained data further confirms the scalability of the proposed method.

</details>


### [160] [Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations](https://arxiv.org/abs/2511.13081)
*Yehonatan Elisha,Seffi Cohen,Oren Barkan,Noam Koenigstein*

Main category: cs.CV

TL;DR: 提出了RFxG分类法来系统化显著性图解释，通过参考框架和粒度两个维度组织解释方法，并开发了新的评估指标来全面评估解释质量。


<details>
  <summary>Details</summary>
Motivation: 显著性图在深度学习中广泛用于视觉解释，但缺乏对其目的和用户查询一致性的共识，这阻碍了解释方法的有效评估和实际应用。

Method: 引入RFxG分类法框架，包含参考框架（点对点vs对比解释）和粒度（细粒度类级vs粗粒度组级）两个维度，并提出了四种新的忠实性评估指标。

Result: 评估了十种最先进的显著性方法、四种模型架构和三个数据集，揭示了现有评估指标过度关注点对点忠实性而忽视对比推理和语义粒度的问题。

Conclusion: 通过转向用户意图驱动的评估，为开发既忠实于模型行为又与人类理解复杂性相匹配的视觉解释提供了概念基础和实践工具。

Abstract: Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods.We address this gap by introducing the Reference-Frame $\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise ("Why this prediction?") and contrastive ("Why this and not an alternative?") explanations.Granularity: Ranging from fine-grained class-level (e.g., "Why Husky?") to coarse-grained group-level (e.g., "Why Dog?") interpretations.Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets.By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.

</details>


### [161] [Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack](https://arxiv.org/abs/2511.13132)
*Chenyang Li,Wenbing Tang,Yihao Huang,Sinong Simon Zhan,Ming Hu,Xiaojun Jia,Yang Liu*

Main category: cs.CV

TL;DR: 提出了基于室内光照的对抗攻击框架ILA，通过操纵全局光照来破坏视觉语言导航代理的性能，包括静态和动态两种攻击模式。


<details>
  <summary>Details</summary>
Motivation: 现有对抗评估依赖不现实的纹理扰动，而室内光照作为常见但被忽视的场景属性，对导航有重要影响，需要研究其对抗性影响。

Method: 设计了ILA黑盒攻击框架，包含静态光照攻击（SILA，恒定光照强度）和动态光照攻击（DILA，关键时刻开关灯），模拟真实家庭光照使用场景。

Result: 在两个最先进的VLN模型和三个导航任务上的评估显示，ILA显著提高了失败率并降低了轨迹效率，揭示了VLN代理对现实室内光照变化的脆弱性。

Conclusion: ILA框架揭示了VLN代理在现实室内光照变化下的潜在脆弱性，为提升导航系统的鲁棒性提供了重要见解。

Abstract: Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.

</details>


### [162] [Seg-VAR: Image Segmentation with Visual Autoregressive Modeling](https://arxiv.org/abs/2511.12594)
*Rongkun Zheng,Lu Qi,Xi Chen,Yi Wang,Kun Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Seg-VAR将分割任务重新定义为条件自回归掩码生成问题，通过多阶段训练策略在多个分割任务上超越之前的判别式和生成式方法


<details>
  <summary>Details</summary>
Motivation: 视觉自回归模型在图像生成方面已取得进展，但其在需要精确空间感知的分割任务中的潜力尚未被探索

Method: 提出Seg-VAR框架，包含图像编码器、空间感知的seglat编码器和解码器，采用多阶段训练策略：先学习seglat表示，然后优化潜在变换，最后对齐图像编码器与seglat分布

Result: 实验表明Seg-VAR在各种分割任务和验证基准上优于之前的判别式和生成式方法

Conclusion: 通过将分割构建为序列层次预测任务，Seg-VAR为将自回归推理集成到空间感知视觉系统中开辟了新途径

Abstract: While visual autoregressive modeling (VAR) strategies have shed light on image generation with the autoregressive models, their potential for segmentation, a task that requires precise low-level spatial perception, remains unexplored. Inspired by the multi-scale modeling of classic Mask2Former-based models, we propose Seg-VAR, a novel framework that rethinks segmentation as a conditional autoregressive mask generation problem. This is achieved by replacing the discriminative learning with the latent learning process. Specifically, our method incorporates three core components: (1) an image encoder generating latent priors from input images, (2) a spatial-aware seglat (a latent expression of segmentation mask) encoder that maps segmentation masks into discrete latent tokens using a location-sensitive color mapping to distinguish instances, and (3) a decoder reconstructing masks from these latents. A multi-stage training strategy is introduced: first learning seglat representations via image-seglat joint training, then refining latent transformations, and finally aligning image-encoder-derived latents with seglat distributions. Experiments show Seg-VAR outperforms previous discriminative and generative methods on various segmentation tasks and validation benchmarks. By framing segmentation as a sequential hierarchical prediction task, Seg-VAR opens new avenues for integrating autoregressive reasoning into spatial-aware vision systems. Code will be available at https://github.com/rkzheng99/Seg-VAR.

</details>


### [163] [Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks](https://arxiv.org/abs/2511.13145)
*Cesar Portocarrero Rodriguez,Laura Vandeweyen,Yosuke Yamamoto*

Main category: cs.CV

TL;DR: 本文探讨了使用计算机视觉技术进行道路损坏分割，评估了GAN生成合成数据的效果，并比较了CNN和MaskFormer模型的性能，发现MaskFormer在mAP50和IoU指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 美国道路基础设施状况不佳（评级D），传统的人工或激光检测方法效率低下且成本高昂。随着自动驾驶车辆实时视觉数据的可用性增加，有机会应用计算机视觉方法进行先进的道路监测。

Method: 1. 使用生成对抗网络（GANs）生成合成数据评估其对模型训练的有用性；2. 应用卷积神经网络（CNNs）进行道路损坏分割；3. 研究基于transformer的MaskFormer模型。

Result: GAN生成的数据能提高模型性能；MaskFormer在mAP50和IoU两个指标上优于CNN模型。

Conclusion: 计算机视觉方法特别是MaskFormer模型在道路损坏分割方面具有显著优势，为基础设施修复工作提供了有价值的指导。

Abstract: The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.

</details>


### [164] [LoRA-Enhanced Vision Transformer for Single Image based Morphing Attack Detection via Knowledge Distillation from EfficientNet](https://arxiv.org/abs/2511.12602)
*Ria Shekhawat,Sushrut Patwardhan,Raghavendra Ramachandra,Praveen Kumar Chandaliya,Kishor P. Upla*

Main category: cs.CV

TL;DR: 提出了一种基于师生框架的单图像形态攻击检测方法，使用CNN教师模型优化ViT学生模型，并集成LoRA进行微调以提高效率。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统对安全至关重要，但容易受到形态攻击的威胁，其中合成图像融合了多个个体的生物特征。

Method: 采用师生框架，CNN教师模型优化ViT学生模型，集成LoRA进行微调以降低计算成本。在三个公开人脸数据集上构建形态数据集，包含十种不同形态生成算法。

Result: 与六种最先进的S-MAD技术相比，该方法在检测性能和计算效率方面均表现出优越性。

Conclusion: 所提出的方法在形态攻击检测方面具有卓越性能和高计算效率，为人脸识别系统的安全性提供了有效保障。

Abstract: Face Recognition Systems (FRS) are critical for security but remain vulnerable to morphing attacks, where synthetic images blend biometric features from multiple individuals. We propose a novel Single-Image Morphing Attack Detection (S-MAD) approach using a teacher-student framework, where a CNN-based teacher model refines a ViT-based student model. To improve efficiency, we integrate Low-Rank Adaptation (LoRA) for fine-tuning, reducing computational costs while maintaining high detection accuracy. Extensive experiments are conducted on a morphing dataset built from three publicly available face datasets, incorporating ten different morphing generation algorithms to assess robustness. The proposed method is benchmarked against six state-of-the-art S-MAD techniques, demonstrating superior detection performance and computational efficiency.

</details>


### [165] [SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration](https://arxiv.org/abs/2511.13168)
*Haodong Wang,Tao Zhuo,Xiuwei Zhang,Hanlin Yin,Wencong Wu,Yanning Zhang*

Main category: cs.CV

TL;DR: SOMA是一个SAR-光学图像密集配准框架，通过集成结构梯度先验和混合匹配策略，显著提升了配准精度


<details>
  <summary>Details</summary>
Motivation: SAR和光学图像因成像机制和视觉特征差异导致像素级配准困难，现有深度学习方法未能有效利用梯度信息

Method: 提出特征梯度增强器(FGE)在特征空间嵌入多尺度多方向梯度滤波器，以及全局-局部仿射流匹配器(GLAM)结合仿射变换和流式精炼的粗到细架构

Result: 在SEN1-2数据集上CMR@1px提升12.29%，在GFGE_SO数据集上提升18.50%，表现出强鲁棒性和良好泛化能力

Conclusion: SOMA通过有效利用梯度信息和混合匹配策略，显著改善了SAR-光学图像的配准性能

Abstract: Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.

</details>


### [166] [Pixels or Positions? Benchmarking Modalities in Group Activity Recognition](https://arxiv.org/abs/2511.12606)
*Drishya Karki,Merey Ramazanova,Anthony Cioppa,Silvio Giancola,Bernard Ghanem*

Main category: cs.CV

TL;DR: 提出了SoccerNet-GAR多模态数据集，包含足球世界杯2022的94,285个同步标注的群体活动，比较了视频和追踪两种模态在群体活动识别中的表现，发现基于追踪的方法在准确率、速度和参数效率方面都优于视频方法。


<details>
  <summary>Details</summary>
Motivation: 群体活动识别主要研究视频模态，但追踪模态作为紧凑、以智能体为中心的信号能明确编码空间交互，目前缺乏标准化的多模态基准来公平比较这两种模态的性能。

Method: 构建了SoccerNet-GAR数据集，包含同步的视频和追踪数据；定义了统一的评估协议，比较了视频分类器和基于图神经网络的追踪分类器，其中追踪模型采用了角色感知图架构，通过位置边和时间注意力直接编码战术结构。

Result: 追踪模型达到67.2%的平衡准确率，而最佳视频基线为58.1%；追踪模型训练速度快4.25倍，参数少438倍（197K vs 86.3M）。

Conclusion: 追踪模态在群体活动识别中优于视频模态，强调了模态选择和角色感知建模的重要性，为未来研究提供了新的见解。

Abstract: Group Activity Recognition (GAR) is well studied on the video modality for surveillance and indoor team sports (e.g., volleyball, basketball). Yet, other modalities such as agent positions and trajectories over time, i.e. tracking, remain comparatively under-explored despite being compact, agent-centric signals that explicitly encode spatial interactions. Understanding whether pixel (video) or position (tracking) modalities leads to better group activity recognition is therefore important to drive further research on the topic. However, no standardized benchmark currently exists that aligns broadcast video and tracking data for the same group activities, leading to a lack of apples-to-apples comparison between these modalities for GAR. In this work, we introduce SoccerNet-GAR, a multimodal dataset built from the $64$ matches of the football World Cup 2022. Specifically, the broadcast videos and player tracking modalities for $94{,}285$ group activities are synchronized and annotated with $10$ categories. Furthermore, we define a unified evaluation protocol to benchmark two strong unimodal approaches: (i) a competitive video-based classifiers and (ii) a tracking-based classifiers leveraging graph neural networks. In particular, our novel role-aware graph architecture for tracking-based GAR directly encodes tactical structure through positional edges and temporal attention. Our tracking model achieves $67.2\%$ balanced accuracy compared to $58.1\%$ for the best video baseline, while training $4.25 \times$ faster with $438 \times$ fewer parameters ($197K$ \vs $86.3M$). This study provides new insights into the relative strengths of pixels and positions for group activity recognition. Overall, it highlights the importance of modality choice and role-aware modeling for GAR.

</details>


### [167] [GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models](https://arxiv.org/abs/2511.13259)
*Yushuo Zheng,Jiangyong Ying,Huiyu Duan,Chunyi Li,Zicheng Zhang,Jing Liu,Xiaohong Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: GeoX-Bench是一个用于评估大型多模态模型在跨视角地理定位和姿态估计任务中能力的基准测试，包含10,859个全景-卫星图像对和755,976个问答对，覆盖128个城市的49个国家。


<details>
  <summary>Details</summary>
Motivation: 尽管大型多模态模型在多种任务上表现出色，但其在跨视角地理定位和姿态估计领域的能力尚未被探索，而这些能力对导航、自动驾驶等应用具有重要价值。

Method: 构建包含全景-卫星图像对和问答对的综合基准测试GeoX-Bench，评估25个最先进的大型多模态模型，并探索指令调优对模型能力的提升效果。

Result: 当前大型多模态模型在地理定位任务上表现良好，但在更复杂的姿态估计任务上效果显著下降；通过在GeoX-Bench训练数据上进行指令调优可以显著提升模型的跨视角地理感知能力。

Conclusion: GeoX-Bench揭示了当前大型多模态模型在跨视角地理定位和姿态估计方面的局限性，特别是姿态估计任务需要进一步改进，指令调优是提升模型能力的有效方法。

Abstract: Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \textit{etc}. To bridge this gap, we introduce \textbf{GeoX-Bench}, a comprehensive \underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \underline{cross}-view \underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.

</details>


### [168] [Open-World Test-Time Adaptation with Hierarchical Feature Aggregation and Attention Affine](https://arxiv.org/abs/2511.12607)
*Ziqiong Liu,Yushun Tang,Junyang Ji,Zhihai He*

Main category: cs.CV

TL;DR: 提出分层阶梯网络和注意力仿射网络来提升测试时适应性，通过OOD检测和自适应注意力机制增强模型在分布偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决测试时适应方法在遇到未见类别样本时性能下降的问题，避免将OOD样本误分类为已知类别导致预测精度下降和适应过程受损。

Method: 使用分层阶梯网络从所有Transformer层提取OOD特征，结合加权概率融合增强OOD检测；引入注意力仿射网络自适应优化自注意力机制；采用加权熵机制动态抑制低置信度样本影响。

Result: 在基准数据集上的实验结果表明，该方法显著提升了在广泛使用的分类数据集上的性能。

Conclusion: 提出的方法有效提升了模型在测试时面对分布偏移和未见类别样本时的适应能力和分类性能。

Abstract: Test-time adaptation (TTA) refers to adjusting the model during the testing phase to cope with changes in sample distribution and enhance the model's adaptability to new environments. In real-world scenarios, models often encounter samples from unseen (out-of-distribution, OOD) categories. Misclassifying these as known (in-distribution, ID) classes not only degrades predictive accuracy but can also impair the adaptation process, leading to further errors on subsequent ID samples. Many existing TTA methods suffer substantial performance drops under such conditions. To address this challenge, we propose a Hierarchical Ladder Network that extracts OOD features from class tokens aggregated across all Transformer layers. OOD detection performance is enhanced by combining the original model prediction with the output of the Hierarchical Ladder Network (HLN) via weighted probability fusion. To improve robustness under domain shift, we further introduce an Attention Affine Network (AAN) that adaptively refines the self-attention mechanism conditioned on the token information to better adapt to domain drift, thereby improving the classification performance of the model on datasets with domain shift. Additionally, a weighted entropy mechanism is employed to dynamically suppress the influence of low-confidence samples during adaptation. Experimental results on benchmark datasets show that our method significantly improves the performance on the most widely used classification datasets.

</details>


### [169] [Computer Vision based group activity detection and action spotting](https://arxiv.org/abs/2511.13315)
*Narthana Sivalingam,Santhirarajah Sivasthigan,Thamayanthi Mahendranathan,G. M. R. I. Godaliyadda,M. P. B. Ekanayake,H. M. V. R. Herath*

Main category: cs.CV

TL;DR: 提出一个结合深度学习和图关系推理的群体活动检测框架，通过Mask R-CNN定位演员，融合掩码信息精炼特征，构建演员关系图编码外观和位置关系，使用图卷积网络预测个体动作和群体活动。


<details>
  <summary>Details</summary>
Motivation: 多人场景中的群体活动检测面临复杂的人际交互、遮挡和时间外观变化的挑战，需要有效建模个体间关系。

Method: 使用Mask R-CNN进行演员定位，多骨干网络提取特征，RoIAlign保持空间对齐，融合掩码信息精炼特征，构建演员关系图编码外观相似性和位置关系，图卷积网络进行关系推理。

Result: 在Collective Activity数据集上的实验表明，该方法在拥挤和非拥挤场景下都提高了识别性能。

Conclusion: 该方法展示了结合分割、特征提取和关系图推理在复杂视频理解任务中的潜力。

Abstract: Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.

</details>


### [170] [Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images](https://arxiv.org/abs/2511.13353)
*Lucas Gabriel Telesco,Danila Nejamkin,Estefanía Mata,Francisco Filizzola,Kevin Wignall,Lucía Franco Troilo,María de los Angeles Cenoz,Melissa Thompson,Mercedes Leguía,Ignacio Larrabide,José Ignacio Orlando*

Main category: cs.CV

TL;DR: 提出了一种混合半监督学习方法，通过结合整体质量的人工标签和质量细节的伪标签，在多任务框架下改进视网膜图像质量评估，无需大量手动标注即可获得更可解释的模型。


<details>
  <summary>Details</summary>
Motivation: 现有视网膜图像质量评估工具大多只分类整体图像质量，而不指示采集缺陷以指导重新采集，这主要是由于详细标注成本高昂。

Method: 使用在小型数据集上训练的教师模型生成伪标签，然后在多任务设置中使用这些伪标签微调预训练模型，采用ResNet-18骨干网络。

Result: 多任务模型在EyeQ数据集上F1得分为0.875（基线为0.863），在DeepDRiD数据集上为0.778（基线为0.763），匹配或超越现有方法。在大多数细节预测任务中与教师模型性能统计相当。

Conclusion: 所提出的半监督方法不仅改进了整体质量评估，还提供了关于采集条件（光照、清晰度、对比度）的可解释反馈，增强了可解释性且无需额外手动标注成本，提供了临床可操作的输出来指导图像重新采集。

Abstract: Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.

</details>


### [171] [Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model](https://arxiv.org/abs/2511.13387)
*Fei Kong*

Main category: cs.CV

TL;DR: 本文提出了广义去噪扩散压缩模型（gDDCM），将DDCM扩展到主流的扩散模型及其变体，包括DDPM、基于分数的模型、一致性模型和整流流，实现了更好的图像压缩性能。


<details>
  <summary>Details</summary>
Motivation: DDCM虽然利用DDPM和特定噪声集实现了图像压缩，但无法应用于DDPM以外的其他扩散模型方法，限制了其通用性。

Method: 提出gDDCM方法，通过将反向过程中的随机噪声替换为按预定规则从特定集合采样的噪声，将DDCM扩展到多种主流扩散模型。

Result: 在CIFAR-10和LSUN Bedroom数据集上的实验结果表明，该方法成功将DDCM推广到多种扩散模型，并实现了性能提升。

Conclusion: gDDCM有效扩展了DDCM的应用范围，使其能够适用于多种主流扩散模型，并在图像压缩任务中取得改进的性能。

Abstract: Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.

</details>


### [172] [Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)](https://arxiv.org/abs/2511.13397)
*Nikos Theodoridis,Tim Brophy,Reenu Mohandas,Ganesh Sistu,Fiachra Collins,Anthony Scanlan,Ciaran Eising*

Main category: cs.CV

TL;DR: DTPQA是一个专门用于评估视觉语言模型在交通场景中感知能力的视觉问答基准，包含合成和真实世界两部分，并带有距离标注来分析模型性能随距离增加而下降的情况。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶等安全关键领域应用视觉语言模型时，需要确保它们具备强大的感知能力，特别是在复杂交通场景和远距离物体识别方面。

Method: 创建了包含合成基准（使用模拟器生成）和真实世界基准（基于真实交通场景图像）的DTPQA数据集，每个样本包含图像、问题、真实答案和物体距离信息。

Result: 提供了完整的DTPQA数据集和用于生成同类数据的Python脚本，能够系统评估VLMs在交通感知任务中的表现。

Conclusion: DTPQA基准能够有效评估视觉语言模型在交通场景中的感知能力，特别是分析模型性能随物体距离增加而下降的趋势，为安全关键领域的应用提供重要评估工具。

Abstract: The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.

</details>


### [173] [Denoising Vision Transformer Autoencoder with Spectral Self-Regularization](https://arxiv.org/abs/2511.12633)
*Xunzhi Xiang,Xingye Tian,Guiyu Zhang,Yabo Chen,Shaofeng Zhang,Xuebo Wang,Xin Tao,Qi Fan*

Main category: cs.CV

TL;DR: 提出Denoising-VAE，通过频谱自正则化策略抑制高维潜在空间中的冗余高频噪声，提升扩散模型的生成质量和训练收敛速度。


<details>
  <summary>Details</summary>
Motivation: 解决高维VAE潜在空间在重建保真度和生成性能之间的优化困境，特别是高频冗余分量阻碍扩散模型训练收敛的问题。

Method: 使用频谱自正则化策略抑制冗余高频噪声，提出Denoising-VAE（基于ViT的自编码器），并引入频谱对齐策略优化生成模型训练。

Result: 在ImageNet 256×256基准上，扩散模型收敛速度提升约2倍，重建质量达到SOTA（rFID=0.28，PSNR=27.26），生成性能具有竞争力（gFID=1.82）。

Conclusion: Denoising-VAE通过抑制潜在空间中的高频噪声，有效解决了高维VAE的优化困境，实现了更好的重建和生成性能平衡。

Abstract: Variational autoencoders (VAEs) typically encode images into a compact latent space, reducing computational cost but introducing an optimization dilemma: a higher-dimensional latent space improves reconstruction fidelity but often hampers generative performance. Recent methods attempt to address this dilemma by regularizing high-dimensional latent spaces using external vision foundation models (VFMs). However, it remains unclear how high-dimensional VAE latents affect the optimization of generative models. To our knowledge, our analysis is the first to reveal that redundant high-frequency components in high-dimensional latent spaces hinder the training convergence of diffusion models and, consequently, degrade generation quality. To alleviate this problem, we propose a spectral self-regularization strategy to suppress redundant high-frequency noise while simultaneously preserving reconstruction quality. The resulting Denoising-VAE, a ViT-based autoencoder that does not rely on VFMs, produces cleaner, lower-noise latents, leading to improved generative quality and faster optimization convergence. We further introduce a spectral alignment strategy to facilitate the optimization of Denoising-VAE-based generative models. Our complete method enables diffusion models to converge approximately 2$\times$ faster than with SD-VAE, while achieving state-of-the-art reconstruction quality (rFID = 0.28, PSNR = 27.26) and competitive generation performance (gFID = 1.82) on the ImageNet 256$\times$256 benchmark.

</details>


### [174] [TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing](https://arxiv.org/abs/2511.13399)
*Yuchen Bao,Yiting Wang,Wenjian Huang,Haowei Wang,Shen Chen,Taiping Yao,Shouhong Ding,Jianguo Zhang*

Main category: cs.CV

TL;DR: 提出了TripleFDS框架和SCB Synthesis数据集，通过三重特征解耦实现场景文本编辑，在保持视觉一致性的同时支持文本内容、样式和背景的灵活编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可编辑属性解耦方面不完整，通常只能编辑文本内容，限制了可控性和视觉一致性。

Method: 使用SCB Group作为基本训练单元，通过组间对比正则化和组内多特征正交性实现三重特征解耦，在合成阶段进行特征重映射以防止重建中的'捷径'现象。

Result: 在主流STE基准测试中达到最先进的图像保真度（SSIM 44.54）和文本准确率（ACC 93.58%），并支持样式替换和背景转移等新操作。

Conclusion: TripleFDS通过完整的三重特征解耦实现了更灵活的场景文本编辑，在保持视觉一致性的同时提升了编辑能力。

Abstract: Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the "SCB Group", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent "shortcut" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS

</details>


### [175] [Medical Knowledge Intervention Prompt Tuning for Medical Image Classification](https://arxiv.org/abs/2511.12639)
*Ye Du,Nanxi Yu,Shujun Wang*

Main category: cs.CV

TL;DR: CILMP是一种将大型语言模型(LLM)集成到视觉语言模型(VLM)提示调优中的方法，通过提取疾病特定表征并创建实例自适应提示，在医学图像分类任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示调优方法无法精确区分不同类型的医学概念，在医学图像分类任务中缺乏特定疾病相关特征。LLM在提供专业医学知识方面表现出色，因此考虑将LLM集成到提示调优过程中。

Method: 提出CILMP方法：从LLM提取疾病特定表征，在低秩线性子空间进行干预，创建疾病特定提示；加入条件机制为每个医学图像生成实例自适应提示。

Result: 在多个医学图像数据集上的广泛实验表明，CILMP始终优于最先进的提示调优方法，证明了其有效性。

Conclusion: CILMP成功地将LLM与VLM连接起来，促进了医学知识向VLM提示的转移，为医学图像分类任务提供了有效的解决方案。

Abstract: Vision-language foundation models (VLMs) have shown great potential in feature transfer and generalization across a wide spectrum of medical-related downstream tasks. However, fine-tuning these models is resource-intensive due to their large number of parameters. Prompt tuning has emerged as a viable solution to mitigate memory usage and reduce training time while maintaining competitive performance. Nevertheless, the challenge is that existing prompt tuning methods cannot precisely distinguish different kinds of medical concepts, which miss essentially specific disease-related features across various medical imaging modalities in medical image classification tasks. We find that Large Language Models (LLMs), trained on extensive text corpora, are particularly adept at providing this specialized medical knowledge. Motivated by this, we propose incorporating LLMs into the prompt tuning process. Specifically, we introduce the CILMP, Conditional Intervention of Large Language Models for Prompt Tuning, a method that bridges LLMs and VLMs to facilitate the transfer of medical knowledge into VLM prompts. CILMP extracts disease-specific representations from LLMs, intervenes within a low-rank linear subspace, and utilizes them to create disease-specific prompts. Additionally, a conditional mechanism is incorporated to condition the intervention process on each individual medical image, generating instance-adaptive prompts and thus enhancing adaptability. Extensive experiments across diverse medical image datasets demonstrate that CILMP consistently outperforms state-of-the-art prompt tuning methods, demonstrating its effectiveness. Code is available at https://github.com/usr922/cilmp.

</details>


### [176] [Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline](https://arxiv.org/abs/2511.13442)
*Rui Zuo,Qinyue Tong,Zhe-Ming Lu,Ziqian Lu*

Main category: cs.CV

TL;DR: Foresee是一个无需训练的MLLM图像伪造分析框架，通过类型先验驱动策略和灵活特征检测器，在多种伪造类型上实现优越的定位精度和丰富的文本解释。


<details>
  <summary>Details</summary>
Motivation: 现有图像伪造检测方法泛化能力差且可解释性有限，而大规模训练的MLLM方法计算成本高，未能充分利用原始MLLM的内在泛化潜力。

Method: 提出训练免费的Foresee框架，采用类型先验驱动策略和灵活特征检测器(FFD)模块，专门处理复制-移动操作，无需额外训练即可实现轻量级推理。

Result: 在多种伪造类型(复制-移动、拼接、移除、局部增强、深度伪造和AIGC编辑)上超越现有方法，同时实现优越的定位精度和更全面的文本解释。

Conclusion: Foresee有效释放了原始MLLM在取证领域的潜力，具有更强的泛化能力，为图像伪造分析提供了高效且可解释的解决方案。

Abstract: With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.

</details>


### [177] [DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry](https://arxiv.org/abs/2511.12653)
*Cheng Liao*

Main category: cs.CV

TL;DR: DPVO-QAT++是一个层次化量化优化框架，通过可学习尺度参数化、前端FP16/FP32和后端全精度的异构精度设计，以及GPU原生核融合技术，显著降低了视觉SLAM系统的计算开销，同时保持轨迹精度。


<details>
  <summary>Details</summary>
Motivation: 深度学习视觉SLAM系统具有出色的几何推理能力，但计算开销过大限制了在资源受限自主平台上的部署。

Method: 采用可学习尺度参数化、异构精度设计（前端浮点伪量化，后端全精度）和GPU原生核融合技术。

Result: 在TartanAir数据集上平均FPS提升52.1%，延迟降低29.1%，GPU峰值内存减少64.9%；在EuRoC数据集上FPS提升30.1%，延迟降低23.1%，内存减少37.7%，同时保持与原始模型相当的轨迹精度。

Conclusion: 该框架有效弥合了高精度深度视觉里程计与实用部署效率要求之间的差距，为实际嵌入式平台应用提供了可行的工程范例。

Abstract: Deep learning-based Visual SLAM (vSLAM) systems exhibit exceptional geometric reasoning capabilities, yet their prohibitive computational overhead severely restricts deployment on resource-constrained autonomous platforms. This paper presents a hierarchical quantization optimization framework, DPVO-QAT++ (DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry). Through the synergistic integration of learnable scale parameterization, a heterogeneous precision design for the Visual Odometry (VO) front-end and back-end (front-end floating-point fake quantization with FP16/FP32; back-end full precision), and GPU-native kernel fusion for fake quantization (custom CUDA kernels), our framework significantly reduces memory footprint and increases processing speed while preserving the trajectory accuracy of the original model. On the TartanAir dataset, our framework achieves an average FPS increase of 52.1%, a 29.1% reduction in median latency, and a 64.9% reduction in peak GPU memory reservation, while maintaining trajectory accuracy (ATE) comparable to the original DPVO model across 32 validation sequences. On the EuRoC dataset, it realizes an average FPS increase of 30.1%, a 23.1% reduction in median latency, and a 37.7% reduction in peak GPU memory reservation, maintaining comparable trajectory accuracy (ATE) across 11 validation sequences. Experimental results demonstrate that DPVO-QAT++ effectively bridges the gap between high-precision deep VO and the efficiency requirements for practical deployment, offering a viable engineering paradigm for the application of this technology on real-world embedded platforms.
  Keywords: Visual Odometry, Heterogeneous Precision Architecture, Quantization-Aware Training, CUDA Kernel Fusion, Scale-Only Training, Deep Patch Visual Odometry, GPU-Native Kernel Fusion.

</details>


### [178] [Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling](https://arxiv.org/abs/2511.13478)
*Adam Hazimeh,Ke Wang,Mark Collier,Gilles Baechler,Efi Kokiopoulou,Pascal Frossard*

Main category: cs.CV

TL;DR: SliDer是一个使用视觉语言模型将幻灯片图像转换为可编辑SVG格式的框架，解决了现有几何光栅矢量化方法无法保持高层次语义结构的问题。


<details>
  <summary>Details</summary>
Motivation: 多媒体文档通常以静态光栅格式分发，限制了编辑和定制。现有方法无法保持图像和文本元素之间的语义区分，导致结构丢失。

Method: 使用视觉语言模型检测和提取光栅输入中的图像和文本元素属性，将其组织成连贯的SVG格式，并通过迭代推理过程优化预测。

Result: SliDer实现了0.069的重建LPIPS，在82.9%的情况下被人类评估者认为优于最强的零样本VLM基线。

Conclusion: SliDer能够有效地将幻灯片图像转换为紧凑且可编辑的SVG表示，为语义文档反渲染提供了可行的解决方案。

Abstract: Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.

</details>


### [179] [Toward Real-world Text Image Forgery Localization: Structured and Interpretable Data Synthesis](https://arxiv.org/abs/2511.12658)
*Zeqin Yu,Haotao Xie,Jian Zhang,Jiangqun Ni,Wenkan Su,Jiwu Huang*

Main category: cs.CV

TL;DR: 提出FSTS框架，通过傅里叶级数建模真实篡改行为，合成多样化的训练数据，显著提升文本图像伪造定位模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有T-IFL方法由于真实数据集规模有限，且合成数据与真实篡改存在分布差异，导致泛化能力差。

Method: 收集16,750个真实篡改实例，通过结构化流程记录编辑痕迹，建立分层建模框架，用傅里叶级数思想表示篡改参数和群体分布，合成训练数据。

Result: 在四个评估协议上的广泛实验表明，使用FSTS数据训练的模型在真实数据集上实现了显著改进的泛化性能。

Conclusion: FSTS框架能够合成反映真实篡改痕迹的多样化训练数据，有效解决文本图像伪造定位的泛化问题。

Abstract: Existing Text Image Forgery Localization (T-IFL) methods often suffer from poor generalization due to the limited scale of real-world datasets and the distribution gap caused by synthetic data that fails to capture the complexity of real-world tampering. To tackle this issue, we propose Fourier Series-based Tampering Synthesis (FSTS), a structured and interpretable framework for synthesizing tampered text images. FSTS first collects 16,750 real-world tampering instances from five representative tampering types, using a structured pipeline that records human-performed editing traces via multi-format logs (e.g., video, PSD, and editing logs). By analyzing these collected parameters and identifying recurring behavioral patterns at both individual and population levels, we formulate a hierarchical modeling framework. Specifically, each individual tampering parameter is represented as a compact combination of basis operation-parameter configurations, while the population-level distribution is constructed by aggregating these behaviors. Since this formulation draws inspiration from the Fourier series, it enables an interpretable approximation using basis functions and their learned weights. By sampling from this modeled distribution, FSTS synthesizes diverse and realistic training data that better reflect real-world forgery traces. Extensive experiments across four evaluation protocols demonstrate that models trained with FSTS data achieve significantly improved generalization on real-world datasets. Dataset is available at \href{https://github.com/ZeqinYu/FSTS}{Project Page}.

</details>


### [180] [Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks](https://arxiv.org/abs/2511.13545)
*Md. Iqbal Hossain,Afia Sajeeda,Neeresh Kumar Perla,Ming Shao*

Main category: cs.CV

TL;DR: 提出一种增强多模态对比学习模型对抗后门攻击鲁棒性的创新策略，能够识别后门触发器、定位受害样本和标签，并通过精心策划的微调数据集修复中毒模型


<details>
  <summary>Details</summary>
Motivation: 多模态深度学习模型（如CLIP）容易受到后门攻击，现有防御方法通常需要从头训练或使用大量数据集进行微调，且无法精确定位受影响的特定标签

Method: 引入图像分割"预言机"作为中毒CLIP输出的监督器，开发两种算法：1）区分CLIP和预言机知识以识别潜在触发器；2）定位受影响标签和受害样本，并策划紧凑的微调数据集

Result: 在视觉识别基准测试上的广泛实验表明，该策略在基于CLIP的后门防御中有效

Conclusion: 该方法能够有效识别后门触发器并修复中毒的CLIP模型，消除后门影响

Abstract: The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.

</details>


### [181] [Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans](https://arxiv.org/abs/2511.12662)
*Hongbin Huang,Junwei Li,Tianxin Xie,Zhuang Li,Cekai Weng,Yaodong Yang,Yue Luo,Li Liu,Jing Tang,Zhijing Shao,Zeyu Wang*

Main category: cs.CV

TL;DR: 提出一个高保真、实时的对话数字人系统，结合逼真的3D虚拟形象、人物驱动的语音合成和知识驱动的对话生成，通过异步执行管道实现低延迟多模态交互。


<details>
  <summary>Details</summary>
Motivation: 解决在交互应用中同时实现视觉真实感和实时响应性的挑战，为通信、教育和娱乐等沉浸式应用提供可信的数字人类。

Method: 采用异步执行管道协调多模态组件，结合检索增强方法（历史增强和意图路由）来维持对话流畅性和高效知识访问，支持唤醒词检测和情感表达韵律。

Result: 开发了一个集成系统，能够实现响应迅速且可信的数字人类交互，支持自然及时的对话体验。

Conclusion: 该系统通过整合视觉真实感、语音表达和知识对话，为沉浸式应用提供了可行的实时数字人解决方案。

Abstract: High-fidelity digital humans are increasingly used in interactive applications, yet achieving both visual realism and real-time responsiveness remains a major challenge. We present a high-fidelity, real-time conversational digital human system that seamlessly combines a visually realistic 3D avatar, persona-driven expressive speech synthesis, and knowledge-grounded dialogue generation. To support natural and timely interaction, we introduce an asynchronous execution pipeline that coordinates multi-modal components with minimal latency. The system supports advanced features such as wake word detection, emotionally expressive prosody, and highly accurate, context-aware response generation. It leverages novel retrieval-augmented methods, including history augmentation to maintain conversational flow and intent-based routing for efficient knowledge access. Together, these components form an integrated system that enables responsive and believable digital humans, suitable for immersive applications in communication, education, and entertainment.

</details>


### [182] [Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification](https://arxiv.org/abs/2511.13575)
*Linhan Zhou,Shuang Li,Neng Dong,Yonghang Tai,Yafei Zhang,Huafeng Li*

Main category: cs.CV

TL;DR: 提出了分层提示学习（HPL）框架，通过任务感知提示建模联合优化图像到图像（I2I）和文本到图像（T2I）的行人重识别任务，解决了现有方法分别处理这两个任务导致的表示纠缠和性能次优问题。


<details>
  <summary>Details</summary>
Motivation: 现有的行人重识别方法通常将I2I和T2I任务分开处理，这可能导致表示纠缠和性能不佳。I2I强调判别性身份学习，而T2I需要准确的跨模态语义对齐，两者共享检索目标但面临不同挑战。

Method: 1. 任务路由Transformer：在共享视觉编码器中引入双分类令牌，为I2I和T2I分支分别路由特征；2. 分层提示生成方案：整合身份级可学习令牌和实例级伪文本令牌；3. 跨模态提示正则化：在提示令牌空间强制语义对齐，确保伪提示保留源模态特征同时增强跨模态可转移性。

Result: 在多个ReID基准测试上的广泛实验验证了该方法的有效性，在I2I和T2I任务上都达到了最先进的性能。

Conclusion: HPL框架通过统一处理I2I和T2I任务，利用任务感知提示建模有效解决了表示纠缠问题，在两个任务上都取得了优异的性能。

Abstract: Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.

</details>


### [183] [DensePercept-NCSSD: Vision Mamba towards Real-time Dense Visual Perception with Non-Causal State Space Duality](https://arxiv.org/abs/2511.12671)
*Tushar Anand,Advik Sinha,Abhijit Das*

Main category: cs.CV

TL;DR: 提出了一种基于非因果选择性状态空间的实时光学流和视差估计模型，用于密集感知任务，在保持高精度的同时显著减少推理时间和GPU使用。


<details>
  <summary>Details</summary>
Motivation: 解决实时应用中光学流和视差估计的高计算成本和推理时间问题，提供统一的实时3D密集感知解决方案。

Method: 使用非因果Mamba块融合成对输入图像，构建快速高效的模型来管理实时应用中的约束条件。

Result: 模型在保持高精度的同时显著减少了推理时间和GPU使用，在真实场景中验证了其有效性。

Conclusion: 提出的模型适用于统一、实时且准确的3D密集感知估计任务，代码和模型已开源。

Abstract: In this work, we propose an accurate and real-time optical flow and disparity estimation model by fusing pairwise input images in the proposed non-causal selective state space for dense perception tasks. We propose a non-causal Mamba block-based model that is fast and efficient and aptly manages the constraints present in a real-time applications. Our proposed model reduces inference times while maintaining high accuracy and low GPU usage for optical flow and disparity map generation. The results and analysis, and validation in real-life scenario justify that our proposed model can be used for unified real-time and accurate 3D dense perception estimation tasks. The code, along with the models, can be found at https://github.com/vimstereo/DensePerceptNCSSD

</details>


### [184] [VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping](https://arxiv.org/abs/2511.13587)
*Haotian Dong,Ye Li,Rongwei Lu,Chen Tang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: 提出VVS框架，通过部分验证跳过来加速视觉自回归生成模型，减少目标模型前向传递次数2.8倍，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归生成模型虽然性能强大，但逐令牌预测范式导致推理延迟高。现有推测解码方法无法直接减少前向传递次数，限制了加速潜力。

Method: 基于视觉令牌可互换性，提出VVS框架，包含三个模块：动态截断的验证自由令牌选择器、令牌级特征缓存与重用、细粒度跳过步调度。

Result: 相比原始自回归解码，VVS将目标模型前向传递次数减少2.8倍，同时保持竞争力的生成质量。

Conclusion: VVS提供了优于传统推测解码框架的速度-质量权衡，具有重塑推测解码范式的强大潜力。

Abstract: Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its "draft one step, then verify one step" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.

</details>


### [185] [Appreciate the View: A Task-Aware Evaluation Framework for Novel View Synthesis](https://arxiv.org/abs/2511.12675)
*Saar Stern,Ido Sobol,Or Litany*

Main category: cs.CV

TL;DR: 提出了一个任务感知的新视角合成评估框架PRISM，包含基于参考的D_PRISM和无参考的MMD_PRISM两个指标，能可靠识别错误生成结果并与人类偏好一致。


<details>
  <summary>Details</summary>
Motivation: 现有新视角合成评估指标难以准确评估生成图像是否既真实又忠实于源视图和视角变换，标准指标经常错误地排名不正确的结果。

Method: 利用Zero123强基础模型的特征，结合轻量级调优步骤增强判别能力，开发了基于参考和无参考的两个互补评估指标。

Result: 在Toys4K、GSO和OmniObject3D三个基准测试中，MMD_PRISM产生了清晰稳定的模型排名，较低分数始终表示更强的模型。

Conclusion: 该框架为新视角合成评估提供了原则性和实用的方法，为更可靠的新视角合成进展铺平了道路。

Abstract: The goal of Novel View Synthesis (NVS) is to generate realistic images of a given content from unseen viewpoints. But how can we trust that a generated image truly reflects the intended transformation? Evaluating its reliability remains a major challenge. While recent generative models, particularly diffusion-based approaches, have significantly improved NVS quality, existing evaluation metrics struggle to assess whether a generated image is both realistic and faithful to the source view and intended viewpoint transformation. Standard metrics, such as pixel-wise similarity and distribution-based measures, often mis-rank incorrect results as they fail to capture the nuanced relationship between the source image, viewpoint change, and generated output. We propose a task-aware evaluation framework that leverages features from a strong NVS foundation model, Zero123, combined with a lightweight tuning step to enhance discrimination. Using these features, we introduce two complementary evaluation metrics: a reference-based score, $D_{\text{PRISM}}$, and a reference-free score, $\text{MMD}_{\text{PRISM}}$. Both reliably identify incorrect generations and rank models in agreement with human preference studies, addressing a fundamental gap in NVS evaluation. Our framework provides a principled and practical approach to assessing synthesis quality, paving the way for more reliable progress in novel view synthesis. To further support this goal, we apply our reference-free metric to six NVS methods across three benchmarks: Toys4K, Google Scanned Objects (GSO), and OmniObject3D, where $\text{MMD}_{\text{PRISM}}$ produces a clear and stable ranking, with lower scores consistently indicating stronger models.

</details>


### [186] [Alpha Divergence Losses for Biometric Verification](https://arxiv.org/abs/2511.13621)
*Dimitrios Koutsianos,Ladislav Mosner,Yannis Panagakis,Themos Stafylakis*

Main category: cs.CV

TL;DR: 本文提出了两种基于α-散度的边缘损失函数：Q-Margin和A3M，用于提升人脸和说话人验证性能，特别是在低误接受率下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的α-散度损失函数虽然能诱导稀疏解，但难以集成角度边缘，而角度边缘对于验证任务至关重要。

Method: 通过参考度量（先验概率）或对数（未归一化对数似然）两种方式集成角度边缘，提出了Q-Margin和A3M两种新损失函数，并使用原型重新初始化策略解决A3M的训练不稳定性。

Result: 在IJB-B、IJB-C人脸验证基准和VoxCeleb说话人验证上取得显著性能提升，特别是在低误接受率下表现优于强基线。

Conclusion: 提出的方法为高安全性应用（如银行认证）提供了有效的解决方案，能够显著减少误认证。

Abstract: Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $α$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $α>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $α$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.

</details>


### [187] [UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity](https://arxiv.org/abs/2511.13714)
*Junwei Yu,Trevor Darrell,XuDong Wang*

Main category: cs.CV

TL;DR: UnSAMv2通过无监督学习实现任意粒度分割，仅需6K未标注图像和0.02%额外参数，显著提升SAM-2在交互式、全图和视频分割任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决SAM模型在控制分割粒度方面的局限性，用户通常需要手动细化结果才能达到所需细节水平，而收集所有粒度的密集标注成本过高。

Method: 扩展UnSAM的分治策略，发现丰富的掩码-粒度对，引入新颖的粒度控制嵌入，实现精确连续的分割尺度控制。

Result: 在11个基准测试中显著改进：NoC90从5.69降至4.75，1-IoU从58.0提升至73.1，AR1000从49.6提升至68.3。

Conclusion: 少量未标注数据结合粒度感知的自监督学习方法可以释放视觉基础模型的潜力。

Abstract: The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\text{NoC}_{90}$ (5.69 $\rightarrow$ 4.75), 1-IoU (58.0 $\rightarrow$ 73.1), and $\text{AR}_{1000}$ (49.6 $\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.

</details>


### [188] [Scaling Spatial Intelligence with Multimodal Foundation Models](https://arxiv.org/abs/2511.13719)
*Zhongang Cai,Ruisi Wang,Chenyang Gu,Fanyi Pu,Junxiang Xu,Yubo Wang,Wanqi Yin,Zhitao Yang,Chen Wei,Qingping Sun,Tongxi Zhou,Jiaqi Li,Hui En Pang,Oscar Qian,Yukun Wei,Zhiqian Lin,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Xiangyu Fan,Hanming Deng,Lewei Lu,Liang Pan,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: SenseNova-SI系列多模态基础模型通过构建800万样本的数据集，在空间智能任务上取得突破性表现，同时保持强大的通用多模态理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态基础模型取得了显著进展，但在空间智能方面仍存在明显不足，需要专门提升这方面的能力。

Method: 基于现有多模态基础模型（Qwen3-VL、InternVL3、Bagel），系统构建包含800万多样化样本的SenseNova-SI-8M数据集，采用严格的空间能力分类法。

Result: 在多个空间智能基准测试中表现优异：VSI-Bench 68.7%、MMSI 43.3%、MindCube 85.6%、ViewSpatial 54.6%、SITE 50.1%，同时保持强大的通用多模态理解能力（MMBench-En 84.9%）。

Conclusion: SenseNova-SI项目展示了通过大规模多样化数据训练可以显著提升空间智能能力，并观察到早期涌现的泛化能力。该项目将持续更新，所有模型将公开发布以促进相关研究。

Abstract: Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.

</details>


### [189] [X-VMamba: Explainable Vision Mamba](https://arxiv.org/abs/2511.12694)
*Mohamed A. Mabrok,Yalda Zafari*

Main category: cs.CV

TL;DR: 提出了一个基于可控性的可解释性框架，用于理解视觉状态空间模型(SSMs)如何处理空间信息，通过两种方法量化输入序列对内部状态动态的影响。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏类似注意力机制的透明度，理解视觉SSMs如何处理空间信息具有挑战性，需要开发可解释性方法来分析这些模型的工作原理。

Method: 提出了两种互补方法：适用于任何SSM架构的基于雅可比矩阵的方法，以及针对对角SSMs的基于格拉姆矩阵的方法，两者都在单次前向传播中运行，具有线性复杂度。

Result: 在三种不同医学成像模态上的实验表明，SSMs自然实现了从早期层的扩散低层纹理到深层聚焦的临床有意义模式的分层特征细化。

Conclusion: 该框架将可控性分析确立为跨所有领域的SSMs的统一、基础性可解释性范式。

Abstract: State Space Models (SSMs), particularly the Mamba architecture, have recently emerged as powerful alternatives to Transformers for sequence modeling, offering linear computational complexity while achieving competitive performance. Yet, despite their effectiveness, understanding how these Vision SSMs process spatial information remains challenging due to the lack of transparent, attention-like mechanisms. To address this gap, we introduce a controllability-based interpretability framework that quantifies how different parts of the input sequence (tokens or patches) influence the internal state dynamics of SSMs. We propose two complementary formulations: a Jacobian-based method applicable to any SSM architecture that measures influence through the full chain of state propagation, and a Gramian-based approach for diagonal SSMs that achieves superior speed through closed-form analytical solutions. Both methods operate in a single forward pass with linear complexity, requiring no architectural modifications or hyperparameter tuning. We validate our framework through experiments on three diverse medical imaging modalities, demonstrating that SSMs naturally implement hierarchical feature refinement from diffuse low-level textures in early layers to focused, clinically meaningful patterns in deeper layers. Our analysis reveals domain-specific controllability signatures aligned with diagnostic criteria, progressive spatial selectivity across the network hierarchy, and the substantial influence of scanning strategies on attention patterns. Beyond medical imaging, we articulate applications spanning computer vision, natural language processing, and cross-domain tasks. Our framework establishes controllability analysis as a unified, foundational interpretability paradigm for SSMs across all domains. Code and analysis tools will be made available upon publication

</details>


### [190] [Counting Through Occlusion: Framework for Open World Amodal Counting](https://arxiv.org/abs/2511.12702)
*Safaeid Hossain Arib,Rabeya Akter,Abdul Monaf Chowdhury,Md Jubair Ahmed Sourov,Md Mehedi Hasan*

Main category: cs.CV

TL;DR: CountOCC是一个处理遮挡条件下物体计数的模态计数框架，通过多模态引导重建被遮挡物体特征，在遮挡场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有物体计数方法在遮挡场景下表现不佳，因为主干网络会编码遮挡表面而非目标物体，导致特征表示损坏。

Method: 使用分层多模态引导显式重建被遮挡物体特征，整合可见片段的空间上下文与文本和视觉嵌入的语义先验，在多个金字塔层级生成类别区分性特征，并引入视觉等价性目标确保遮挡和非遮挡视图产生空间对齐的注意力图。

Result: 在FSC 147数据集上，验证集和测试集的MAE分别降低26.72%和20.80%；在CARPK数据集上MAE降低49.89%；在CAPTUREReal数据集上MAE降低28.79%。

Conclusion: CountOCC通过互补机制在遮挡条件下保持区分性特征，实现了跨视觉领域的鲁棒模态计数。

Abstract: Object counting has achieved remarkable success on visible instances, yet state-of-the-art (SOTA) methods fail under occlusion, a pervasive challenge in real world deployment. This failure stems from a fundamental architectural limitation where backbone networks encode occluding surfaces rather than target objects, thereby corrupting the feature representations required for accurate enumeration. To address this, we present CountOCC, an amodal counting framework that explicitly reconstructs occluded object features through hierarchical multimodal guidance. Rather than accepting degraded encodings, we synthesize complete representations by integrating spatial context from visible fragments with semantic priors from text and visual embeddings, generating class-discriminative features at occluded locations across multiple pyramid levels. We further introduce a visual equivalence objective that enforces consistency in attention space, ensuring that both occluded and unoccluded views of the same scene produce spatially aligned gradient-based attention maps. Together, these complementary mechanisms preserve discriminative properties essential for accurate counting under occlusion. For rigorous evaluation, we establish occlusion-augmented versions of FSC 147 and CARPK spanning both structured and unstructured scenes. CountOCC achieves SOTA performance on FSC 147 with 26.72% and 20.80% MAE reduction over prior baselines under occlusion in validation and test, respectively. CountOCC also demonstrates exceptional generalization by setting new SOTA results on CARPK with 49.89% MAE reduction and on CAPTUREReal with 28.79% MAE reduction, validating robust amodal counting across diverse visual domains. Code will be released soon.

</details>


### [191] [FSDAM: Few-Shot Driving Attention Modeling via Vision-Language Coupling](https://arxiv.org/abs/2511.12708)
*Kaiser Hamid,Can Cui,Khandakar Ashrafi Akbar,Ziran Wang,Nade Liang*

Main category: cs.CV

TL;DR: FSDAM是一个少样本驾驶员注意力建模框架，仅需约100个标注样本即可实现联合注意力预测和描述生成，比现有方法少两个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖大规模注视数据集来学习驾驶员注意力模式，但这些数据集收集成本高、整理耗时。需要开发在数据受限场景下仍能有效工作的驾驶员注意力系统。

Method: 采用双路径架构，分别处理空间预测和描述生成，通过跨模态对齐保持语义一致性。

Result: FSDAM在注意力预测方面达到竞争性性能，生成连贯且上下文感知的解释，并在多个驾驶基准测试中展示出强大的零样本泛化能力。

Conclusion: 研究表明，在有限监督下可以实现有效的注意力条件生成，为在数据受限场景中实际部署可解释的驾驶员注意力系统开辟了新可能性。

Abstract: Understanding where drivers look and why they shift their attention is essential for autonomous systems that read human intent and justify their actions. Most existing models rely on large-scale gaze datasets to learn these patterns; however, such datasets are labor-intensive to collect and time-consuming to curate. We present FSDAM (Few-Shot Driver Attention Modeling), a framework that achieves joint attention prediction and caption generation with approximately 100 annotated examples, two orders of magnitude fewer than existing approaches. Our approach introduces a dual-pathway architecture where separate modules handle spatial prediction and caption generation while maintaining semantic consistency through cross-modal alignment. Despite minimal supervision, FSDAM achieves competitive performance on attention prediction, generates coherent, and context-aware explanations. The model demonstrates robust zero-shot generalization across multiple driving benchmarks. This work shows that effective attention-conditioned generation is achievable with limited supervision, opening new possibilities for practical deployment of explainable driver attention systems in data-constrained scenarios.

</details>


### [192] [Backdoor Attacks on Open Vocabulary Object Detectors via Multi-Modal Prompt Tuning](https://arxiv.org/abs/2511.12735)
*Ankita Raj,Chetan Arora*

Main category: cs.CV

TL;DR: TrAP是首个针对开放词汇目标检测器的后门攻击方法，通过联合优化图像和文本模态的提示参数与视觉触发器，实现轻量级后门注入。


<details>
  <summary>Details</summary>
Motivation: 随着开放词汇目标检测器在机器人、自动驾驶等高风险应用中的普及，理解其安全风险变得至关重要。本研究首次探讨了OVODs的后门攻击问题。

Method: 提出TrAP多模态后门注入策略，联合优化图像和文本模态的提示参数与视觉触发器，采用课程学习策略逐步缩小触发器尺寸。

Result: 在多个数据集上的实验表明，TrAP在对象误分类和对象消失攻击中均达到高攻击成功率，同时在下游数据集上相比零样本设置提高了干净图像性能。

Conclusion: TrAP揭示了提示调优引入的新攻击面，能够在不重新训练基础模型权重的情况下植入后门，同时保持模型的泛化能力。

Abstract: Open-vocabulary object detectors (OVODs) unify vision and language to detect arbitrary object categories based on text prompts, enabling strong zero-shot generalization to novel concepts. As these models gain traction in high-stakes applications such as robotics, autonomous driving, and surveillance, understanding their security risks becomes crucial. In this work, we conduct the first study of backdoor attacks on OVODs and reveal a new attack surface introduced by prompt tuning. We propose TrAP (Trigger-Aware Prompt tuning), a multi-modal backdoor injection strategy that jointly optimizes prompt parameters in both image and text modalities along with visual triggers. TrAP enables the attacker to implant malicious behavior using lightweight, learnable prompt tokens without retraining the base model weights, thus preserving generalization while embedding a hidden backdoor. We adopt a curriculum-based training strategy that progressively shrinks the trigger size, enabling effective backdoor activation using small trigger patches at inference. Experiments across multiple datasets show that TrAP achieves high attack success rates for both object misclassification and object disappearance attacks, while also improving clean image performance on downstream datasets compared to the zero-shot setting.

</details>


### [193] [Direct Visual Grounding by Directing Attention of Visual Tokens](https://arxiv.org/abs/2511.12738)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 本文发现视觉语言模型(VLMs)在最终层对视觉token关注不足，导致视觉问答错误。作者提出KL注意力损失(KLAL)来直接监督视觉token的注意力分布，通过KL散度对齐真实注意力图，显著提升了在几何任务、指向和指代表达理解等任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 标准的下一个token预测损失无法有效引导LLM模块关注相关的视觉token，导致视觉token在最终层获得很少注意力，这会造成视觉问答错误。需要更直接的监督来引导视觉token的注意力。

Method: 提出KL注意力损失(KLAL)，通过KL散度将视觉token的注意力分布与真实注意力图对齐。真实注意力图来自合成案例的任务几何或真实图像的标注(如边界框)，在LLM内部进行注意力监督而不需要新标签。KLAL与NTP损失结合使用。

Result: 在合成和真实世界数据上，该方法在几何任务、指向和指代表达理解方面取得了显著改进。还引入了一个新的数据集来评估VLMs的线条追踪能力，发现即使是商业VLMs在该任务上表现也不佳。

Conclusion: 直接监督视觉token的注意力分布可以有效提升VLMs在视觉任务上的性能，KL注意力损失为改善视觉-语言对齐提供了一种有效方法。

Abstract: Vision Language Models (VLMs) mix visual tokens and text tokens. A puzzling issue is the fact that visual tokens most related to the query receive little to no attention in the final layers of the LLM module of VLMs from the answer tokens, where all tokens are treated equally, in particular, visual and language tokens in the LLM attention layers. This fact may result in wrong answers to visual questions, as our experimental results confirm. It appears that the standard next-token prediction (NTP) loss provides an insufficient signal for directing attention to visual tokens. We hypothesize that a more direct supervision of the attention of visual tokens to corresponding language tokens in the LLM module of VLMs will lead to improved performance on visual tasks. To demonstrate that this is indeed the case, we propose a novel loss function that directly supervises the attention of visual tokens. It directly grounds the answer language tokens in images by directing their attention to the relevant visual tokens. This is achieved by aligning the attention distribution of visual tokens to ground truth attention maps with KL divergence. The ground truth attention maps are obtained from task geometry in synthetic cases or from standard grounding annotations (e.g., bounding boxes or point annotations) in real images, and are used inside the LLM for attention supervision without requiring new labels. The obtained KL attention loss (KLAL) when combined with NTP encourages VLMs to attend to relevant visual tokens while generating answer tokens. This results in notable improvements across geometric tasks, pointing, and referring expression comprehension on both synthetic and real-world data, as demonstrated by our experiments. We also introduce a new dataset to evaluate the line tracing abilities of VLMs. Surprisingly, even commercial VLMs do not perform well on this task.

</details>


### [194] [Deep Imbalanced Multi-Target Regression: 3D Point Cloud Voxel Content Estimation in Simulated Forests](https://arxiv.org/abs/2511.12740)
*Amirhossein Hassanzadeh,Bartosz Krawczyk,Michael Saunders,Rob Wible,Keith Krause,Dimah Dera,Jan van Aardt*

Main category: cs.CV

TL;DR: 该研究探索了从体素化LiDAR点云数据推断体素内目标占用百分比的可能性，提出了一种基于KPConv的多目标回归方法，并针对类别不平衡问题采用了密度相关性的成本敏感学习策略。


<details>
  <summary>Details</summary>
Motivation: 体素化虽然能降低LiDAR数据处理的计算成本，但会导致细尺度结构信息丢失。研究旨在探索能否从高级体素化数据中恢复低级的体素内容信息，特别是森林环境中不同目标（树皮、树叶、土壤等）的占用百分比。

Method: 采用基于核点卷积的多目标回归方法，结合成本敏感学习（密度相关性）处理类别不平衡问题，使用加权均方误差、焦点回归和正则化来优化KPConv模型。对体素尺寸（0.25-2米）进行敏感性分析。

Result: 敏感性分析显示，较大体素尺寸（如2米）由于变异性降低而误差较小，而较小体素尺寸（0.25或0.5米）误差较高，特别是在树冠层变异性最大的区域。对于树皮和树叶目标，小体素尺寸数据集的误差显著高于大体素尺寸数据集。

Conclusion: 体素尺寸的选择应基于具体应用需求。该研究填补了深度不平衡学习模型在多目标回归和森林3D LiDAR点云模拟数据集方面的空白。

Abstract: Voxelization is an effective approach to reduce the computational cost of processing Light Detection and Ranging (LiDAR) data, yet it results in a loss of fine-scale structural information. This study explores whether low-level voxel content information, specifically target occupancy percentage within a voxel, can be inferred from high-level voxelized LiDAR point cloud data collected from Digital Imaging and remote Sensing Image Generation (DIRSIG) software. In our study, the targets include bark, leaf, soil, and miscellaneous materials. We propose a multi-target regression approach in the context of imbalanced learning using Kernel Point Convolutions (KPConv). Our research leverages cost-sensitive learning to address class imbalance called density-based relevance (DBR). We employ weighted Mean Saquared Erorr (MSE), Focal Regression (FocalR), and regularization to improve the optimization of KPConv. This study performs a sensitivity analysis on the voxel size (0.25 - 2 meters) to evaluate the effect of various grid representations in capturing the nuances of the forest. This sensitivity analysis reveals that larger voxel sizes (e.g., 2 meters) result in lower errors due to reduced variability, while smaller voxel sizes (e.g., 0.25 or 0.5 meter) exhibit higher errors, particularly within the canopy, where variability is greatest. For bark and leaf targets, error values at smaller voxel size datasets (0.25 and 0.5 meter) were significantly higher than those in larger voxel size datasets (2 meters), highlighting the difficulty in accurately estimating within-canopy voxel content at fine resolutions. This suggests that the choice of voxel size is application-dependent. Our work fills the gap in deep imbalance learning models for multi-target regression and simulated datasets for 3D LiDAR point clouds of forests.

</details>


### [195] [SAGE: Saliency-Guided Contrastive Embeddings](https://arxiv.org/abs/2511.12744)
*Colton R. Crum,Adam Czajka*

Main category: cs.CV

TL;DR: SAGE是一种通过对比嵌入将人类显著性先验整合到神经网络训练中的方法，在图像空间之外使用潜在空间嵌入来引导训练，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有将显著性整合到模型训练的方法依赖内部模型机制，但研究表明这些机制可能不可靠。挑战主要源于仅将引导方法置于图像空间。

Method: 使用对比嵌入损失函数，对输入应用显著性保留和显著性退化信号增强，捕捉嵌入和模型logits的变化，通过对比三元组损失引导模型关注显著特征并忽略非显著特征。

Result: 在开放和封闭场景下，相比最先进的显著性方法，分类性能均有提升，SAGE在不同骨干网络上都有效，并显示出跨任务的广泛泛化能力。

Conclusion: 通过将人类显著性指导从图像空间转移到潜在空间，SAGE提供了一种更可靠的整合人类先验知识的方法，能有效提升模型性能。

Abstract: Integrating human perceptual priors into the training of neural networks has been shown to raise model generalization, serve as an effective regularizer, and align models with human expertise for applications in high-risk domains. Existing approaches to integrate saliency into model training often rely on internal model mechanisms, which recent research suggests may be unreliable. Our insight is that many challenges associated with saliency-guided training stem from the placement of the guidance approaches solely within the image space. Instead, we move away from the image space, use the model's latent space embeddings to steer human guidance during training, and we propose SAGE (Saliency-Guided Contrastive Embeddings): a loss function that integrates human saliency into network training using contrastive embeddings. We apply salient-preserving and saliency-degrading signal augmentations to the input and capture the changes in embeddings and model logits. We guide the model towards salient features and away from non-salient features using a contrastive triplet loss. Additionally, we perform a sanity check on the logit distributions to ensure that the model outputs match the saliency-based augmentations. We demonstrate a boost in classification performance across both open- and closed-set scenarios against SOTA saliency-based methods, showing SAGE's effectiveness across various backbones, and include experiments to suggest its wide generalization across tasks.

</details>


### [196] [RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.12767)
*Cătălin-Alexandru Rîpanu,Andrei-Theodor Hotnog,Giulia-Stefania Imbrea,Dumitru-Clementin Cercel*

Main category: cs.CV

TL;DR: 本文介绍了RoCoISLR数据集，这是首个用于罗马尼亚孤立手语识别的大规模标准化数据集，包含超过9,000个视频样本，涵盖近6,000个标准化词汇。通过评估7种最先进的视频识别模型，发现基于Transformer的架构优于卷积基线，Swin Transformer达到34.1%的Top-1准确率。


<details>
  <summary>Details</summary>
Motivation: 目前大多数手语识别数据集专注于美国手语，而罗马尼亚孤立手语识别缺乏大规模标准化数据集，这限制了该领域的研究进展。

Method: 创建RoCoISLR数据集，包含9,000多个视频样本和近6,000个标准化词汇。在一致的实验设置下评估7种视频识别模型：I3D、SlowFast、Swin Transformer、TimeSformer、Uniformer、VideoMAE和PoseConv3D。

Result: 基于Transformer的架构表现优于卷积基线，Swin Transformer获得最高Top-1准确率34.1%。研究还揭示了低资源手语中长尾类别分布的挑战。

Conclusion: RoCoISLR数据集为系统性的罗马尼亚孤立手语识别研究提供了初步基础，填补了该领域的空白。

Abstract: Automatic sign language recognition plays a crucial role in bridging the communication gap between deaf communities and hearing individuals; however, most available datasets focus on American Sign Language. For Romanian Isolated Sign Language Recognition (RoISLR), no large-scale, standardized dataset exists, which limits research progress. In this work, we introduce a new corpus for RoISLR, named RoCoISLR, comprising over 9,000 video samples that span nearly 6,000 standardized glosses from multiple sources. We establish benchmark results by evaluating seven state-of-the-art video recognition models-I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, and PoseConv3D-under consistent experimental setups, and compare their performance with that of the widely used WLASL2000 corpus. According to the results, transformer-based architectures outperform convolutional baselines; Swin Transformer achieved a Top-1 accuracy of 34.1%. Our benchmarks highlight the challenges associated with long-tail class distributions in low-resource sign languages, and RoCoISLR provides the initial foundation for systematic RoISLR research.

</details>


### [197] [Enhancing Neuro-Oncology Through Self-Assessing Deep Learning Models for Brain Tumor Unified Model for MRI Segmentation](https://arxiv.org/abs/2511.12801)
*Andrew Zhou*

Main category: cs.CV

TL;DR: 提出了一个不确定性感知的脑肿瘤分割框架，在nnUNet基础上增加体素级不确定性通道，同时结合正常脑结构和肿瘤分割，为临床手术决策提供更全面的信息。


<details>
  <summary>Details</summary>
Motivation: 解决当前深度学习方法在临床应用中缺乏不确定性估计和健康脑结构分割的问题，为手术规划提供更可靠的AI辅助。

Method: 在nnUNet中增加不确定性通道，使用BraTS2023数据集训练，结合正常和癌症数据集构建统一模型，实现肿瘤定位与解剖学背景的统一分割。

Result: 不确定性估计达到0.750的相关性和0.047的RMSD，肿瘤分割DSC为0.86，脑结构分割DSC为0.81，关键区域表现稳健。

Conclusion: 该框架首次输出肿瘤在自然环境中的分割结果叠加不确定性图，为临床手术决策提供关键洞察，帮助评估预测和修正错误。

Abstract: Accurate segmentation of brain tumors is vital for diagnosis, surgical planning, and treatment monitoring. Deep learning has advanced on benchmarks, but two issues limit clinical use: no uncertainty estimates for errors and no segmentation of healthy brain structures around tumors for surgery. Current methods fail to unify tumor localization with anatomical context and lack confidence scores. This study presents an uncertainty-aware framework augmenting nnUNet with a channel for voxel-wise uncertainty. Trained on BraTS2023, it yields a correlation of 0.750 and RMSD of 0.047 for uncertainty without hurting tumor accuracy. It predicts uncertainty in one pass, with no extra networks or inferences, aiding clinical decisions. For whole-brain context, a unified model combines normal and cancer datasets, achieving a DSC of 0.81 for brain structures and 0.86 for tumor, with robust key-region performance. Combining both innovations gives the first model outputting tumor in natural surroundings plus an overlaid uncertainty map. Visual checks of outputs show uncertainty offers key insights to evaluate predictions and fix errors, helping informed surgical decisions from AI.

</details>


### [198] [View-aware Cross-modal Distillation for Multi-view Action Recognition](https://arxiv.org/abs/2511.12870)
*Trung Thanh Nguyen,Yasutomo Kawanishi,Vijay John,Takahiro Komamizu,Ichiro Ide*

Main category: cs.CV

TL;DR: 提出了ViCoKD框架，通过知识蒸馏从多模态教师模型向模态和标注受限的学生模型传递知识，解决了部分重叠多视图动作识别中的视图不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多视图动作识别方法主要针对完全重叠的传感器设置，而部分重叠设置（动作仅在部分视图中可见）研究不足。现实场景中系统通常只有有限输入模态和序列级标注，而非密集帧级标注。

Method: 使用跨模态适配器和跨模态注意力，让学生模型能够利用多模态相关性；提出视图感知一致性模块，通过人类检测掩码和置信度加权的Jensen-Shannon散度来强制共视动作的预测对齐。

Result: 在真实世界MultiSensor-Home数据集上的实验表明，ViCoKD在多种骨干网络和环境设置下始终优于竞争性蒸馏方法，在受限条件下甚至超越了教师模型。

Conclusion: ViCoKD框架有效解决了部分重叠多视图动作识别中的模态限制和视图不对齐问题，在真实场景中表现出色。

Abstract: The widespread use of multi-sensor systems has increased research in multi-view action recognition. While existing approaches in multi-view setups with fully overlapping sensors benefit from consistent view coverage, partially overlapping settings where actions are visible in only a subset of views remain underexplored. This challenge becomes more severe in real-world scenarios, as many systems provide only limited input modalities and rely on sequence-level annotations instead of dense frame-level labels. In this study, we propose View-aware Cross-modal Knowledge Distillation (ViCoKD), a framework that distills knowledge from a fully supervised multi-modal teacher to a modality- and annotation-limited student. ViCoKD employs a cross-modal adapter with cross-modal attention, allowing the student to exploit multi-modal correlations while operating with incomplete modalities. Moreover, we propose a View-aware Consistency module to address view misalignment, where the same action may appear differently or only partially across viewpoints. It enforces prediction alignment when the action is co-visible across views, guided by human-detection masks and confidence-weighted Jensen-Shannon divergence between their predicted class distributions. Experiments on the real-world MultiSensor-Home dataset show that ViCoKD consistently outperforms competitive distillation methods across multiple backbones and environments, delivering significant gains and surpassing the teacher model under limited conditions.

</details>


### [199] [Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views](https://arxiv.org/abs/2511.12878)
*Junyi Ma,Wentao Bao,Jingyi Xu,Guanzhong Sun,Yu Zheng,Erhang Zhang,Xieyuanli Chen,Hesheng Wang*

Main category: cs.CV

TL;DR: 提出了EgoLoc方法，用于在自我中心视频中零样本定位手-物体接触和分离的时间戳，无需物体掩码和动词-名词分类法，在VR/AR和机器人操作任务中表现出良好性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注交互动作的行为范式（"如何交互"），但更挑战性的手与目标物体接触和分离关键时刻的捕捉（"何时交互"）仍未被充分探索，这对混合现实中的沉浸式交互体验和机器人运动规划至关重要。

Method: 提出了EgoLoc方法，引入手动力学引导采样生成高质量视觉提示，利用视觉语言模型识别接触/分离属性、定位特定时间戳，并提供闭环反馈进行进一步优化。

Result: 在公共数据集和新基准上的综合实验表明，EgoLoc在自我中心视频中实现了可信的时间交互定位，并有效促进了自我中心视觉和机器人操作任务中的多个下游应用。

Conclusion: EgoLoc消除了对物体掩码和动词-名词分类法的需求，实现了可泛化的零样本实现，为自我中心视频中的时间交互定位提供了有效解决方案。

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR applications and human-robot policy transfer. Existing research has mostly focused on modeling the behavior paradigm of interactive actions (i.e., "how to interact"). However, the more challenging and fine-grained problem of capturing the critical moments of contact and separation between the hand and the target object (i.e., "when to interact") is still underexplored, which is crucial for immersive interactive experiences in mixed reality and robotic motion planning. Therefore, we formulate this problem as temporal interaction localization (TIL). Some recent works extract semantic masks as TIL references, but suffer from inaccurate object grounding and cluttered scenarios. Although current temporal action localization (TAL) methods perform well in detecting verb-noun action segments, they rely on category annotations during training and exhibit limited precision in localizing hand-object contact/separation moments. To address these issues, we propose a novel zero-shot approach dubbed EgoLoc to localize hand-object contact and separation timestamps in egocentric videos. EgoLoc introduces hand-dynamics-guided sampling to generate high-quality visual prompts. It exploits the vision-language model to identify contact/separation attributes, localize specific timestamps, and provide closed-loop feedback for further refinement. EgoLoc eliminates the need for object masks and verb-noun taxonomies, leading to generalizable zero-shot implementation. Comprehensive experiments on the public dataset and our novel benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric videos. It is also validated to effectively facilitate multiple downstream applications in egocentric vision and robotic manipulation tasks. Code and relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [200] [Simple Lines, Big Ideas: Towards Interpretable Assessment of Human Creativity from Drawings](https://arxiv.org/abs/2511.12880)
*Zihao Lin,Zhenshan Shi,Sasa Zhao,Hanwei Zhu,Lingyu Zhu,Baoliang Chen,Lei Mo*

Main category: cs.CV

TL;DR: 提出一个基于数据驱动的自动可解释绘图创造力评估框架，通过多模态多任务学习同时预测创造力分数、分类内容类型和提取风格特征。


<details>
  <summary>Details</summary>
Motivation: 当前绘图创造力评估主要依赖专家主观评分，既费时又主观。基于认知科学理解，创造力可以从绘制内容（what）和绘制风格（how）两个维度体现。

Method: 首先扩充现有创造力标注数据集，增加内容类别标注；然后提出多模态多任务学习框架，引入条件学习机制，根据绘图的风格和语义线索动态调整视觉特征提取。

Result: 实验结果表明，该模型相比现有基于回归的方法达到了最先进的性能，并提供与人类判断一致的可解释可视化结果。

Conclusion: 该框架能够自动、可解释地评估绘图创造力，在心理学、教育和认知科学领域具有应用价值。

Abstract: Assessing human creativity through visual outputs, such as drawings, plays a critical role in fields including psychology, education, and cognitive science. However, current assessment practices still rely heavily on expert-based subjective scoring, which is both labor-intensive and inherently subjective. In this paper, we propose a data-driven framework for automatic and interpretable creativity assessment from drawings. Motivated by the cognitive understanding that creativity can emerge from both what is drawn (content) and how it is drawn (style), we reinterpret the creativity score as a function of these two complementary dimensions.Specifically, we first augment an existing creativity labeled dataset with additional annotations targeting content categories. Based on the enriched dataset, we further propose a multi-modal, multi-task learning framework that simultaneously predicts creativity scores, categorizes content types, and extracts stylistic features. In particular, we introduce a conditional learning mechanism that enables the model to adapt its visual feature extraction by dynamically tuning it to creativity-relevant signals conditioned on the drawing's stylistic and semantic cues.Experimental results demonstrate that our model achieves state-of-the-art performance compared to existing regression-based approaches and offers interpretable visualizations that align well with human judgments. The code and annotations will be made publicly available at https://github.com/WonderOfU9/CSCA_PRCV_2025

</details>


### [201] [ActVAR: Activating Mixtures of Weights and Tokens for Efficient Visual Autoregressive Generation](https://arxiv.org/abs/2511.12893)
*Kaixin Zhang,Ruiqing Yang,Yuan Zhang,Shan You,Tao Huang*

Main category: cs.CV

TL;DR: ActVAR是一个动态激活框架，通过双重稀疏化（权重和token序列）提升VAR模型效率，在ImageNet 256×256基准上实现21.2% FLOPs减少且性能损失最小。


<details>
  <summary>Details</summary>
Motivation: 解决VAR模型序列长度增长时计算成本急剧上升的问题，避免静态剪枝方法永久移除权重或token导致的性能下降和预训练依赖破坏。

Method: 将FFN分解为轻量级专家子网络，使用可学习路由器动态选择token特定专家子集；同时通过门控token选择器识别高更新潜力token进行计算，重构未选token以保持全局上下文和序列对齐；采用两阶段知识蒸馏策略训练。

Result: 在ImageNet 256×256基准测试中，ActVAR实现了高达21.2%的FLOPs减少，同时保持了最小的性能下降。

Conclusion: ActVAR通过动态激活机制有效平衡了VAR模型的效率与性能，为大规模视觉自回归模型提供了可行的加速方案。

Abstract: Visual Autoregressive (VAR) models enable efficient image generation via next-scale prediction but face escalating computational costs as sequence length grows. Existing static pruning methods degrade performance by permanently removing weights or tokens, disrupting pretrained dependencies. To address this, we propose ActVAR, a dynamic activation framework that introduces dual sparsity across model weights and token sequences to enhance efficiency without sacrificing capacity. ActVAR decomposes feedforward networks (FFNs) into lightweight expert sub-networks and employs a learnable router to dynamically select token-specific expert subsets based on content. Simultaneously, a gated token selector identifies high-update-potential tokens for computation while reconstructing unselected tokens to preserve global context and sequence alignment. Training employs a two-stage knowledge distillation strategy, where the original VAR model supervises the learning of routing and gating policies to align with pretrained knowledge. Experiments on the ImageNet $256\times 256$ benchmark demonstrate that ActVAR achieves up to $21.2\%$ FLOPs reduction with minimal performance degradation.

</details>


### [202] [Reconstructing 3D Scenes in Native High Dynamic Range](https://arxiv.org/abs/2511.12895)
*Kaixuan Zhang,Minxian Li,Mingwu Ren,Jiankang Deng,Xiatian Zhu*

Main category: cs.CV

TL;DR: 提出了首个直接从原生HDR观测数据重建3D场景的方法NH-3DGS，通过新颖的亮度-色度分解技术，在重建流程中完整保留高动态范围。


<details>
  <summary>Details</summary>
Motivation: 专业数字媒体制作需要HDR成像，但现有3D场景重建主要基于LDR数据，限制了在专业工作流程中的应用。现有方法依赖多曝光融合或逆色调映射，增加了捕获复杂性。

Method: 提出Native High dynamic range 3D Gaussian Splatting (NH-3DGS)，采用新颖的亮度-色度分解颜色表示，支持直接从原生HDR相机数据优化。

Result: 在合成和真实多视角HDR数据集上，NH-3DGS在重建质量和动态范围保持方面显著优于现有方法。

Conclusion: 该方法实现了直接从原生HDR捕获进行专业级3D重建，代码和数据集将公开。

Abstract: High Dynamic Range (HDR) imaging is essential for professional digital media creation, e.g., filmmaking, virtual production, and photorealistic rendering. However, 3D scene reconstruction has primarily focused on Low Dynamic Range (LDR) data, limiting its applicability to professional workflows. Existing approaches that reconstruct HDR scenes from LDR observations rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision. With the recent emergence of cameras that directly capture native HDR data in a single exposure, we present the first method for 3D scene reconstruction that directly models native HDR observations. We propose {\bf Native High dynamic range 3D Gaussian Splatting (NH-3DGS)}, which preserves the full dynamic range throughout the reconstruction pipeline. Our key technical contribution is a novel luminance-chromaticity decomposition of the color representation that enables direct optimization from native HDR camera data. We demonstrate on both synthetic and real multi-view HDR datasets that NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation, enabling professional-grade 3D reconstruction directly from native HDR captures. Code and datasets will be made available.

</details>


### [203] [FDP: A Frequency-Decomposition Preprocessing Pipeline for Unsupervised Anomaly Detection in Brain MRI](https://arxiv.org/abs/2511.12899)
*Hao Li,Zhenfeng Zhuang,Jingyu Lin,Yu Liu,Yifei Chen,Qiong Peng,Lequan Yu,Liansheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于频域分解预处理（FDP）的无监督脑MRI异常检测方法，通过分析病理特征的频域特性来提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于脑解剖结构的多样性和标注数据的稀缺性，脑MRI的无监督异常检测面临挑战。现有方法使用人工生成的噪声扰动来训练生成模型，但模拟的异常缺乏真实临床病变的生物物理保真度和形态复杂性。

Method: 通过系统性的频域分析发现异常具有独特的频域模式，提出频率分解预处理（FDP）框架，利用频域重建同时抑制病理特征并保持解剖结构完整性。FDP可与现有异常模拟技术无缝集成。

Result: 实验结果表明，FDP能一致提升异常检测性能，与LDM结合时DICE分数提高17.63%，并在多个基线方法上保持稳健改进。

Conclusion: FDP是首个利用频域重建进行无监督异常检测的方法，通过频域分析揭示了病理特征的独特性质，为脑MRI异常检测提供了新的有效解决方案。

Abstract: Due to the diversity of brain anatomy and the scarcity of annotated data, supervised anomaly detection for brain MRI remains challenging, driving the development of unsupervised anomaly detection (UAD) approaches. Current UAD methods typically utilize artificially generated noise perturbations on healthy MRIs to train generative models for normal anatomy reconstruction, enabling anomaly detection via residual mapping. However, such simulated anomalies lack the biophysical fidelity and morphological complexity characteristic of true clinical lesions. To advance UAD in brain MRI, we conduct the first systematic frequency-domain analysis of pathological signatures, revealing two key properties: (1) anomalies exhibit unique frequency patterns distinguishable from normal anatomy, and (2) low-frequency signals maintain consistent representations across healthy scans. These insights motivate our Frequency-Decomposition Preprocessing (FDP) framework, the first UAD method to leverage frequency-domain reconstruction for simultaneous pathology suppression and anatomical preservation. FDP can integrate seamlessly with existing anomaly simulation techniques, consistently enhancing detection performance across diverse architectures while maintaining diagnostic fidelity. Experimental results demonstrate that FDP consistently improves anomaly detection performance when integrated with existing methods. Notably, FDP achieves a 17.63% increase in DICE score with LDM while maintaining robust improvements across multiple baselines. The code is available at https://github.com/ls1rius/MRI_FDP.

</details>


### [204] [CASL: Curvature-Augmented Self-supervised Learning for 3D Anomaly Detection](https://arxiv.org/abs/2511.12909)
*Yaohua Zha,Xue Yuerong,Chunlin Fan,Yuansong Wang,Tao Dai,Ke Chen,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 提出了一种基于曲率增强自监督学习(CASL)的3D异常检测框架，该框架通过多尺度曲率提示引导解码器重建点云坐标，无需特定异常检测机制即可实现领先性能，并能泛化到其他3D理解任务。


<details>
  <summary>Details</summary>
Motivation: 现有3D异常检测方法缺乏泛化性，而通用自监督模型在异常检测任务上表现不佳，需要开发既能有效检测异常又具有良好泛化能力的3D模型。

Method: 基于U-Net架构，引入多尺度曲率提示来指导解码器预测点云空间坐标，通过重建范式进行自监督学习，然后通过简单的异常分类微调实现异常检测。

Result: 仅使用点曲率作为异常分数就超越了多个经典自监督和专用异常检测模型，CASL框架在异常检测任务上达到领先性能，同时学到的表示能很好地泛化到点云分类等标准3D理解任务。

Conclusion: 曲率在3D异常检测中起着关键作用，CASL框架证明了无需专用异常检测机制也能实现优异的检测性能，同时保持对通用3D任务的泛化能力。

Abstract: Deep learning-based 3D anomaly detection methods have demonstrated significant potential in industrial manufacturing. However, many approaches are specifically designed for anomaly detection tasks, which limits their generalizability to other 3D understanding tasks. In contrast, self-supervised point cloud models aim for general-purpose representation learning, yet our investigation reveals that these classical models are suboptimal at anomaly detection under the unified fine-tuning paradigm. This motivates us to develop a more generalizable 3D model that can effectively detect anomalies without relying on task-specific designs. Interestingly, we find that using only the curvature of each point as its anomaly score already outperforms several classical self-supervised and dedicated anomaly detection models, highlighting the critical role of curvature in 3D anomaly detection. In this paper, we propose a Curvature-Augmented Self-supervised Learning (CASL) framework based on a reconstruction paradigm. Built upon the classical U-Net architecture, our approach introduces multi-scale curvature prompts to guide the decoder in predicting the spatial coordinates of each point. Without relying on any dedicated anomaly detection mechanisms, it achieves leading detection performance through straightforward anomaly classification fine-tuning. Moreover, the learned representations generalize well to standard 3D understanding tasks such as point cloud classification. The code is available at https://github.com/zyh16143998882/CASL.

</details>


### [205] [Explore How to Inject Beneficial Noise in MLLMs](https://arxiv.org/abs/2511.12917)
*Ruishu Zhu,Sida Huang,Ziheng Jiao,Hongyuan Zhang*

Main category: cs.CV

TL;DR: 提出了一种通过注入有益随机噪声的多模态大语言模型微调方法MuNG，仅需调整1-2%的额外参数，性能超过全参数微调和其他现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法往往忽略跨模态异质性，限制了多模态大语言模型的潜力。

Method: 从变分推理角度重新构建MLLMs的推理过程，设计多模态噪声生成器动态分析图像-文本对的跨模态关系，生成任务自适应的有益噪声。

Result: 在QwenVL和LLaVA上的实验表明，该方法超越了全参数微调和其他现有微调方法，仅需调整约1-2%的额外参数。

Conclusion: 通过注入有益噪声可以有效抑制不相关语义成分，显著改善跨模态表示对齐，提升下游任务性能。

Abstract: Multimodal Large Language Models (MLLMs) have played an increasingly important role in multimodal intelligence. However, the existing fine-tuning methods often ignore cross-modal heterogeneity, limiting their full potential. In this work, we propose a novel fine-tuning strategy by injecting beneficial random noise, which outperforms previous methods and even surpasses full fine-tuning, with minimal additional parameters. The proposed Multimodal Noise Generator (MuNG) enables efficient modality fine-tuning by injecting customized noise into the frozen MLLMs. Specifically, we reformulate the reasoning process of MLLMs from a variational inference perspective, upon which we design a multimodal noise generator that dynamically analyzes cross-modal relationships in image-text pairs to generate task-adaptive beneficial noise. Injecting this type of noise into the MLLMs effectively suppresses irrelevant semantic components, leading to significantly improved cross-modal representation alignment and enhanced performance on downstream tasks. Experiments on two mainstream MLLMs, QwenVL and LLaVA, demonstrate that our method surpasses full-parameter fine-tuning and other existing fine-tuning approaches, while requiring adjustments to only about $1\sim2\%$ additional parameters. The relevant code is uploaded in the supplementary.

</details>


### [206] [CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation](https://arxiv.org/abs/2511.12919)
*Dexin Zuo,Ang Li,Wei Wang,Wenxian Yu,Danping Zou*

Main category: cs.CV

TL;DR: CoordAR是一个用于单参考视图6D姿态估计的自回归框架，通过将3D-3D对应关系建模为离散token序列，解决了现有方法在对称性和遮挡场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于单参考视图的6D姿态估计方法依赖实值坐标回归，存在全局一致性不足和对称/遮挡场景处理困难的问题，需要新的不确定性建模方法。

Method: 提出坐标图token化、模态解耦编码和自回归Transformer解码器，将3D-3D对应关系表示为离散token序列进行概率预测。

Result: 在多个基准测试中显著优于现有方法，在对称性、遮挡等现实挑战中表现出强鲁棒性。

Conclusion: CoordAR通过自回归概率建模有效解决了单参考6D姿态估计中的对称性和遮挡问题，为无3D模型的对象姿态估计提供了新思路。

Abstract: Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.

</details>


### [207] [Generative Photographic Control for Scene-Consistent Video Cinematic Editing](https://arxiv.org/abs/2511.12921)
*Huiqiang Sun,Liao Shen,Zhan Peng,Kun Wang,Size Wu,Yuhang Zang,Tianqi Liu,Zihao Huang,Xingyu Zeng,Zhiguo Cao,Wei Li,Chen Change Loy*

Main category: cs.CV

TL;DR: CineCtrl是首个提供专业相机参数精细控制的视频电影编辑框架，通过解耦交叉注意力机制分离相机运动和摄影输入，实现独立控制而不影响场景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有生成视频模型主要局限于相机运动控制，难以控制深度、曝光等摄影元素，而这些元素对电影叙事至关重要。

Method: 提出解耦交叉注意力机制分离相机运动和摄影输入；开发综合数据生成策略，结合模拟摄影效果和真实世界采集管道构建大规模数据集。

Result: 模型能够生成具有精确控制的用户指定摄影相机效果的高保真视频。

Conclusion: CineCtrl框架成功实现了对专业相机参数的精细控制，为生成视频模型提供了新的电影编辑能力。

Abstract: Cinematic storytelling is profoundly shaped by the artful manipulation of photographic elements such as depth of field and exposure. These effects are crucial in conveying mood and creating aesthetic appeal. However, controlling these effects in generative video models remains highly challenging, as most existing methods are restricted to camera motion control. In this paper, we propose CineCtrl, the first video cinematic editing framework that provides fine control over professional camera parameters (e.g., bokeh, shutter speed). We introduce a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, allowing fine-grained, independent control without compromising scene consistency. To overcome the shortage of training data, we develop a comprehensive data generation strategy that leverages simulated photographic effects with a dedicated real-world collection pipeline, enabling the construction of a large-scale dataset for robust model training. Extensive experiments demonstrate that our model generates high-fidelity videos with precisely controlled, user-specified photographic camera effects.

</details>


### [208] [Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes](https://arxiv.org/abs/2511.12932)
*Feng Lv,Haoxuan Feng,Zilu Zhang,Chunlong Xia,Yanfeng Li*

Main category: cs.CV

TL;DR: 提出了一个统一的文本驱动框架，通过可控掩码机制结合图像生成和编辑，利用车辆侧和路侧多视角数据增强交通场景几何多样性，采用两阶段训练策略和掩码区域加权损失来提升小尺度交通元素的生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决智能交通系统中文本驱动图像生成和编辑面临的挑战：交通元素语义丰富度不足、相机视角有限、合成图像视觉保真度低、文本描述与生成内容对齐差。

Method: 使用可控掩码机制统一图像生成和编辑任务；结合车辆侧和路侧多视角数据；采用两阶段训练策略（大规模粗粒度文本图像数据的概念学习 + 细粒度描述数据的微调）；引入掩码区域加权损失动态强调关键小区域。

Result: 在交通场景的文本驱动图像生成和编辑任务中取得了领先性能。

Conclusion: 该框架有效解决了交通场景图像生成中的多个关键问题，显著提升了生成质量和文本-图像对齐效果。

Abstract: With the rapid advancement of intelligent transportation systems, text-driven image generation and editing techniques have demonstrated significant potential in providing rich, controllable visual scene data for applications such as traffic monitoring and autonomous driving. However, several challenges remain, including insufficient semantic richness of generated traffic elements, limited camera viewpoints, low visual fidelity of synthesized images, and poor alignment between textual descriptions and generated content. To address these issues, we propose a unified text-driven framework for both image generation and editing, leveraging a controllable mask mechanism to seamlessly integrate the two tasks. Furthermore, we incorporate both vehicle-side and roadside multi-view data to enhance the geometric diversity of traffic scenes. Our training strategy follows a two-stage paradigm: first, we perform conceptual learning using large-scale coarse-grained text-image data; then, we fine-tune with fine-grained descriptive data to enhance text-image alignment and detail quality. Additionally, we introduce a mask-region-weighted loss that dynamically emphasizes small yet critical regions during training, thereby substantially enhancing the generation fidelity of small-scale traffic elements. Extensive experiments demonstrate that our method achieves leading performance in text-based image generation and editing within traffic scenes.

</details>


### [209] [ProtoAnomalyNCD: Prototype Learning for Multi-class Novel Anomaly Discovery in Industrial Scenarios](https://arxiv.org/abs/2511.12938)
*Botong Zhao,Qijun Shi,Shujing Lyu,Yue Lu*

Main category: cs.CV

TL;DR: ProtoAnomalyNCD是一个基于原型学习的框架，用于发现和分类多种类型的未见异常类别，可集成到各种异常检测方法中，通过目标区域定位和异常图引导注意力来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测方法主要判断是否存在异常，但实际应用需要发现和分类多种异常类型。由于工业异常语义细微且当前方法未能充分利用图像先验，直接聚类方法效果不佳。

Method: 1) 使用Grounded SAM和文本提示定位目标区域作为先验；2) 引入异常图引导注意力块，设计区域引导因子区分背景、目标区域和异常区域；3) 在统一原型学习框架下发现未见异常类别并实现多类型异常分类。

Result: 在MVTec AD、MTD和Real-IAD数据集上超越了最先进的方法。

Conclusion: 提出的方法能够有效发现未见异常类别，同时实现多类型异常分类，并扩展到检测未见异常值，实现了任务级统一。

Abstract: Existing industrial anomaly detection methods mainly determine whether an anomaly is present. However, real-world applications also require discovering and classifying multiple anomaly types. Since industrial anomalies are semantically subtle and current methods do not sufficiently exploit image priors, direct clustering approaches often perform poorly. To address these challenges, we propose ProtoAnomalyNCD, a prototype-learning-based framework for discovering unseen anomaly classes of multiple types that can be integrated with various anomaly detection methods. First, to suppress background clutter, we leverage Grounded SAM with text prompts to localize object regions as priors for the anomaly classification network. Next, because anomalies usually appear as subtle and fine-grained patterns on the product, we introduce an Anomaly-Map-Guided Attention block. Within this block, we design a Region Guidance Factor that helps the attention module distinguish among background, object regions, and anomalous regions. By using both localized product regions and anomaly maps as priors, the module enhances anomalous features while suppressing background noise and preserving normal features for contrastive learning. Finally, under a unified prototype-learning framework, ProtoAnomalyNCD discovers and clusters unseen anomaly classes while simultaneously enabling multi-type anomaly classification. We further extend our method to detect unseen outliers, achieving task-level unification. Our method outperforms state-of-the-art approaches on the MVTec AD, MTD, and Real-IAD datasets.

</details>


### [210] [Semi-Supervised High Dynamic Range Image Reconstructing via Bi-Level Uncertain Area Masking](https://arxiv.org/abs/2511.12939)
*Wei Jiang,Jiahao Cui,Yizheng Wu,Zhan Peng,Zhiyu Pan,Zhiguo Cao*

Main category: cs.CV

TL;DR: 提出了一种基于半监督学习的高动态范围图像重建方法，通过教师模型生成伪HDR标签，并使用不确定性掩码过滤不可靠区域，仅需6.7%的HDR真值即可达到全监督方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的HDR图像重建方法需要LDR-HDR图像对，但这些数据对难以获取，因此需要研究如何在有限HDR真值下实现可比性能的标注高效方法。

Method: 采用半监督学习框架，教师模型为无标签的LDR样本生成伪HDR标签，学生模型从伪标签学习。提出基于不确定性的像素级和块级掩码过程，过滤伪标签中的不可靠区域。

Result: 该方法不仅优于之前的标注高效算法，而且仅使用6.7%的HDR真值就能达到最新全监督方法的可比性能。

Conclusion: 提出的不确定性掩码半监督方法有效解决了伪标签中的确认偏差问题，显著减少了HDR重建对标注数据的依赖。

Abstract: Reconstructing high dynamic range (HDR) images from low dynamic range (LDR) bursts plays an essential role in the computational photography. Impressive progress has been achieved by learning-based algorithms which require LDR-HDR image pairs. However, these pairs are hard to obtain, which motivates researchers to delve into the problem of annotation-efficient HDR image reconstructing: how to achieve comparable performance with limited HDR ground truths (GTs). This work attempts to address this problem from the view of semi-supervised learning where a teacher model generates pseudo HDR GTs for the LDR samples without GTs and a student model learns from pseudo GTs. Nevertheless, the confirmation bias, i.e., the student may learn from the artifacts in pseudo HDR GTs, presents an impediment. To remove this impediment, an uncertainty-based masking process is proposed to discard unreliable parts of pseudo GTs at both pixel and patch levels, then the trusted areas can be learned from by the student. With this novel masking process, our semi-supervised HDR reconstructing method not only outperforms previous annotation-efficient algorithms, but also achieves comparable performance with up-to-date fully-supervised methods by using only 6.7% HDR GTs.

</details>


### [211] [Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention](https://arxiv.org/abs/2511.12940)
*Taiye Chen,Zihan Ding,Anjian Li,Christina Zhang,Zeqi Xiao,Yisen Wang,Chi Jin*

Main category: cs.CV

TL;DR: 提出RAD框架，通过将LSTM集成到扩散变换器中，解决长视频生成中的记忆遗忘和时空不一致问题，实现高效的长序列建模。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在生成长视频时，由于缺乏有效的记忆压缩和检索机制，在超出窗口大小后会出现遗忘和时空不一致的问题。

Method: 在扩散变换器框架中引入RNN（特别是LSTM），提出RAD框架，通过帧级自回归进行记忆更新和检索，确保训练和推理的一致性。

Result: 在Memory Maze和Minecraft数据集上的实验表明，RAD在长视频生成方面表现优异，LSTM在序列建模中展现出高效性。

Conclusion: RAD框架通过结合扩散模型和LSTM，有效解决了长视频生成中的记忆保持问题，为无限长视频生成提供了可行方案。

Abstract: Recent advancements in video generation have demonstrated the potential of using video diffusion models as world models, with autoregressive generation of infinitely long videos through masked conditioning. However, such models, usually with local full attention, lack effective memory compression and retrieval for long-term generation beyond the window size, leading to issues of forgetting and spatiotemporal inconsistencies. To enhance the retention of historical information within a fixed memory budget, we introduce a recurrent neural network (RNN) into the diffusion transformer framework. Specifically, a diffusion model incorporating LSTM with attention achieves comparable performance to state-of-the-art RNN blocks, such as TTT and Mamba2. Moreover, existing diffusion-RNN approaches often suffer from performance degradation due to training-inference gap or the lack of overlap across windows. To address these limitations, we propose a novel Recurrent Autoregressive Diffusion (RAD) framework, which executes frame-wise autoregression for memory update and retrieval, consistently across training and inference time. Experiments on Memory Maze and Minecraft datasets demonstrate the superiority of RAD for long video generation, highlighting the efficiency of LSTM in sequence modeling.

</details>


### [212] [T2I-Based Physical-World Appearance Attack against Traffic Sign Recognition Systems in Autonomous Driving](https://arxiv.org/abs/2511.12956)
*Chen Ma,Ningfei Wang,Junhao Zheng,Qing Guo,Qian Wang,Qi Alfred Chen,Chao Shen*

Main category: cs.CV

TL;DR: DiffSign是一个基于文本到图像(T2I)的交通标志识别(TSR)系统对抗攻击框架，通过集成CLIP损失和掩码提示来生成物理鲁棒、高效、可迁移、实用且隐蔽的外观攻击。


<details>
  <summary>Details</summary>
Motivation: 现有对抗外观攻击存在明显局限性：像素级扰动方法缺乏隐蔽性且过拟合特定模型，T2I扩散模型方法效果有限且泛化能力差。需要开发更有效的攻击方法来评估TSR系统的安全性。

Method: 提出精心设计的攻击流程，集成CLIP损失和掩码提示以提高攻击聚焦性和可控性；提出两种新颖的风格定制方法来引导视觉外观，提高跨域交通标志攻击泛化能力和攻击隐蔽性。

Result: 在不同真实世界条件下（距离、角度、光照条件和标志类别）进行广泛评估，平均物理世界攻击成功率达到83.3%，展现了DiffSign在攻击可迁移性方面的高效性。

Conclusion: DiffSign框架能够生成物理鲁棒、高效、可迁移、实用且隐蔽的外观攻击，为评估TSR系统安全性提供了有效工具。

Abstract: Traffic Sign Recognition (TSR) systems play a critical role in Autonomous Driving (AD) systems, enabling real-time detection of road signs, such as STOP and speed limit signs. While these systems are increasingly integrated into commercial vehicles, recent research has exposed their vulnerability to physical-world adversarial appearance attacks. In such attacks, carefully crafted visual patterns are misinterpreted by TSR models as legitimate traffic signs, while remaining inconspicuous or benign to human observers. However, existing adversarial appearance attacks suffer from notable limitations. Pixel-level perturbation-based methods often lack stealthiness and tend to overfit to specific surrogate models, resulting in poor transferability to real-world TSR systems. On the other hand, text-to-image (T2I) diffusion model-based approaches demonstrate limited effectiveness and poor generalization to out-of-distribution sign types.
  In this paper, we present DiffSign, a novel T2I-based appearance attack framework designed to generate physically robust, highly effective, transferable, practical, and stealthy appearance attacks against TSR systems. To overcome the limitations of prior approaches, we propose a carefully designed attack pipeline that integrates CLIP-based loss and masked prompts to improve attack focus and controllability. We also propose two novel style customization methods to guide visual appearance and improve out-of-domain traffic sign attack generalization and attack stealthiness. We conduct extensive evaluations of DiffSign under varied real-world conditions, including different distances, angles, light conditions, and sign categories. Our method achieves an average physical-world attack success rate of 83.3%, leveraging DiffSign's high effectiveness in attack transferability.

</details>


### [213] [GrOCE:Graph-Guided Online Concept Erasure for Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.12968)
*Ning Han,Zhenyu Ge,Feng Han,Yuhua Sun,Chengqing Li,Jingjing Chen*

Main category: cs.CV

TL;DR: GrOCE是一个无需训练的框架，通过基于图的语义推理实现精确自适应的概念擦除，在概念相似度和FID指标上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法要么依赖昂贵的微调，要么进行粗粒度的语义分离，往往会损害无关概念且难以适应不断发展的概念集合

Method: 将概念及其相互关系建模为动态语义图，包含动态拓扑图构建、自适应聚类识别和选择性边剪断三个组件

Result: 在概念相似度和Fréchet Inception距离指标上达到最先进性能，提供高效、准确且稳定的概念擦除

Conclusion: GrOCE无需重新训练即可实现精确、自适应的概念擦除，在保持全局语义的同时有效移除有害内容

Abstract: Concept erasure aims to remove harmful, inappropriate, or copyrighted content from text-to-image diffusion models while preserving non-target semantics. However, existing methods either rely on costly fine-tuning or apply coarse semantic separation, often degrading unrelated concepts and lacking adaptability to evolving concept sets. To alleviate this issue, we propose Graph-Guided Online Concept Erasure (GrOCE), a training-free framework that performs precise and adaptive concept removal through graph-based semantic reasoning. GrOCE models concepts and their interrelations as a dynamic semantic graph, enabling principled reasoning over dependencies and fine-grained isolation of undesired content. It comprises three components: (1) Dynamic Topological Graph Construction for incremental graph building, (2) Adaptive Cluster Identification for multi-hop traversal with similarity-decay scoring, and (3) Selective Edge Severing for targeted edge removal while preserving global semantics. Extensive experiments demonstrate that GrOCE achieves state-of-the-art performance on Concept Similarity (CS) and Fréchet Inception Distance (FID) metrics, offering efficient, accurate, and stable concept erasure without retraining.

</details>


### [214] [HiFusion: Hierarchical Intra-Spot Alignment and Regional Context Fusion for Spatial Gene Expression Prediction from Histopathology](https://arxiv.org/abs/2511.12969)
*Ziqiao Weng,Yaoyu Fang,Jiahe Qian,Xinkun Wang,Lee AD Cooper,Weidong Cai,Bo Zhou*

Main category: cs.CV

TL;DR: HiFusion是一个深度学习框架，通过分层建模和跨尺度融合从H&E染色图像预测空间转录组基因表达，在多个基准数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 空间转录组技术面临临床应用的障碍，包括技术复杂性和高昂成本。现有方法难以捕捉spots内的生物异质性，且在整合周围组织上下文信息时容易受到形态学噪声的影响。

Method: 提出HiFusion框架，包含两个互补组件：1) 分层spot内建模模块，通过多分辨率子块分解提取细粒度形态表示；2) 上下文感知跨尺度融合模块，使用交叉注意力选择性整合生物相关区域上下文。

Result: 在两个基准ST数据集上的广泛实验表明，HiFusion在2D切片交叉验证和更具挑战性的3D样本特定场景中均达到最先进性能。

Conclusion: HiFusion作为一个稳健、准确且可扩展的解决方案，具有从常规组织病理学进行ST推断的潜力。

Abstract: Spatial transcriptomics (ST) bridges gene expression and tissue morphology but faces clinical adoption barriers due to technical complexity and prohibitive costs. While computational methods predict gene expression from H&E-stained whole-slide images (WSIs), existing approaches often fail to capture the intricate biological heterogeneity within spots and are susceptible to morphological noise when integrating contextual information from surrounding tissue. To overcome these limitations, we propose HiFusion, a novel deep learning framework that integrates two complementary components. First, we introduce the Hierarchical Intra-Spot Modeling module that extracts fine-grained morphological representations through multi-resolution sub-patch decomposition, guided by a feature alignment loss to ensure semantic consistency across scales. Concurrently, we present the Context-aware Cross-scale Fusion module, which employs cross-attention to selectively incorporate biologically relevant regional context, thereby enhancing representational capacity. This architecture enables comprehensive modeling of both cellular-level features and tissue microenvironmental cues, which are essential for accurate gene expression prediction. Extensive experiments on two benchmark ST datasets demonstrate that HiFusion achieves state-of-the-art performance across both 2D slide-wise cross-validation and more challenging 3D sample-specific scenarios. These results underscore HiFusion's potential as a robust, accurate, and scalable solution for ST inference from routine histopathology.

</details>


### [215] [MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning](https://arxiv.org/abs/2511.12976)
*Yoonjae Seo,Ermal Elbasani,Jaehong Lee*

Main category: cs.CV

TL;DR: MCAQ-YOLO是一种基于形态复杂度的目标检测量化框架，通过五种形态学指标实现空间自适应比特分配，相比均匀量化在保持高效的同时显著提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络量化方法通常在整个空间区域使用统一的比特精度，忽略了视觉数据在结构和纹理复杂度上的异质性，导致在复杂区域量化损失较大。

Method: 使用五种形态学指标（分形维度、纹理熵、梯度方差、边缘密度和轮廓复杂度）来表征局部视觉形态，并基于这些指标与量化敏感性的相关性动态调整比特精度；同时采用基于课程学习的量化感知训练方案逐步增加量化难度。

Result: 在安全装备数据集上达到85.6% mAP@0.5，平均4.2比特，压缩比7.6倍，比均匀4比特量化mAP提高3.5个百分点，每张图像仅增加1.8毫秒运行时间；在COCO和Pascal VOC数据集上也获得一致性能提升。

Conclusion: 形态驱动的空间量化能够提升计算受限、安全关键视觉识别任务的效率和鲁棒性，形态复杂度与量化敏感性之间存在强相关性。

Abstract: Most neural network quantization methods apply uniform bit precision across spatial regions, ignoring the heterogeneous structural and textural complexity of visual data. This paper introduces MCAQ-YOLO, a morphological complexity-aware quantization framework for object detection. The framework employs five morphological metrics - fractal dimension, texture entropy, gradient variance, edge density, and contour complexity - to characterize local visual morphology and guide spatially adaptive bit allocation. By correlating these metrics with quantization sensitivity, MCAQ-YOLO dynamically adjusts bit precision according to spatial complexity. In addition, a curriculum-based quantization-aware training scheme progressively increases quantization difficulty to stabilize optimization and accelerate convergence. Experimental results demonstrate a strong correlation between morphological complexity and quantization sensitivity and show that MCAQ-YOLO achieves superior detection accuracy and convergence efficiency compared with uniform quantization. On a safety equipment dataset, MCAQ-YOLO attains 85.6 percent mAP@0.5 with an average of 4.2 bits and a 7.6x compression ratio, yielding 3.5 percentage points higher mAP than uniform 4-bit quantization while introducing only 1.8 ms of additional runtime overhead per image. Cross-dataset validation on COCO and Pascal VOC further confirms consistent performance gains, indicating that morphology-driven spatial quantization can enhance efficiency and robustness for computationally constrained, safety-critical visual recognition tasks.

</details>


### [216] [ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes](https://arxiv.org/abs/2511.12977)
*Yixuan Yang,Luyang Xie,Zhen Luo,Zixiang Zhao,Mingqi Gao,Feng Zheng*

Main category: cs.CV

TL;DR: ArtiWorld是一个从文本场景描述中自动识别可关节化物体并重建可执行URDF模型的流程，其核心Arti4URDF利用3D点云、大语言模型先验知识和URDF导向提示设计，将刚性物体转换为交互式关节化物体。


<details>
  <summary>Details</summary>
Motivation: 现有3D资产大多是刚性的，手动将其转换为关节化物体极其耗时耗力，需要自动化方法从场景中识别可关节化物体并转换为可执行URDF模型。

Method: 使用3D点云、大语言模型先验知识和URDF导向提示设计，通过Arti4URDF组件快速将刚性物体转换为交互式URDF关节化物体，同时保持原始3D形状。

Result: 在3D模拟物体、完整3D模拟场景和真实世界扫描场景三个层面评估，方法始终优于现有方法并达到最先进性能，保持物体几何形状并正确捕捉物体交互性。

Conclusion: 该方法为直接从现有3D资产构建交互式、机器人就绪的仿真环境提供了实用路径。

Abstract: Building interactive simulators and scalable robot-learning environments requires a large number of articulated assets. However, most existing 3D assets in simulation are rigid, and manually converting them into articulated objects is extremely labor- and cost-intensive. This raises a natural question: can we automatically identify articulable objects in a scene and convert them into articulated assets directly? In this paper, we present ArtiWorld, a scene-aware pipeline that localizes candidate articulable objects from textual scene descriptions and reconstructs executable URDF models that preserve the original geometry. At the core of this pipeline is Arti4URDF, which leverages 3D point cloud, prior knowledge of a large language model (LLM), and a URDF-oriented prompt design to rapidly convert rigid objects into interactive URDF-based articulated objects while maintaining their 3D shape. We evaluate ArtiWorld at three levels: 3D simulated objects, full 3D simulated scenes, and real-world scan scenes. Across all three settings, our method consistently outperforms existing approaches and achieves state-of-the-art performance, while preserving object geometry and correctly capturing object interactivity to produce usable URDF-based articulated models. This provides a practical path toward building interactive, robot-ready simulation environments directly from existing 3D assets. Code and data will be released.

</details>


### [217] [Concept Regions Matter: Benchmarking CLIP with a New Cluster-Importance Approach](https://arxiv.org/abs/2511.12978)
*Aishwarya Agarwal,Srikrishna Karanam,Vineet Gandhi*

Main category: cs.CV

TL;DR: 提出CCI方法，利用CLIP的patch嵌入将空间区域聚类成语义连贯的簇，通过掩蔽和评估预测变化来识别重要概念。该方法在忠实性基准测试中达到新SOTA，并引入COVAR基准来系统评估前景和背景变化的影响。


<details>
  <summary>Details</summary>
Motivation: 对比视觉语言模型如CLIP在零样本识别方面表现强劲，但容易受到虚假相关性的影响，特别是对背景的过度依赖。现有基准如CounterAnimals仅依赖准确度，隐含地将所有性能下降归因于背景相关性，这种假设是不完整的。

Method: 提出CCI方法：使用CLIP自身的patch嵌入将空间patch分组为语义连贯的簇，掩蔽这些簇，并评估模型预测的相对变化。结合GroundedSAM自动将预测分类为前景驱动或背景驱动。引入COVAR基准系统性地变化对象前景和背景。

Result: CCI在忠实性基准测试中创下新SOTA，在MS COCO检索的删除-AUC指标上带来超过两倍的改进。使用CCI和COVAR对18个CLIP变体进行了全面评估，揭示了视角变化、尺度偏移和细粒度对象混淆等误差来源。

Conclusion: CCI方法提供了关键诊断能力，结合COVAR基准为构建更稳健的视觉语言模型提供了方法论进展和实证证据，能够更全面地评估模型对前景和背景的依赖关系。

Abstract: Contrastive vision-language models (VLMs) such as CLIP achieve strong zero-shot recognition yet remain vulnerable to spurious correlations, particularly background over-reliance. We introduce Cluster-based Concept Importance (CCI), a novel interpretability method that uses CLIP's own patch embeddings to group spatial patches into semantically coherent clusters, mask them, and evaluate relative changes in model predictions. CCI sets a new state of the art on faithfulness benchmarks, surpassing prior methods by large margins; for example, it yields more than a twofold improvement on the deletion-AUC metric for MS COCO retrieval. We further propose that CCI, when combined with GroundedSAM, automatically categorizes predictions as foreground- or background-driven, providing a crucial diagnostic ability. Existing benchmarks such as CounterAnimals, however, rely solely on accuracy and implicitly attribute all performance degradation to background correlations. Our analysis shows this assumption to be incomplete, since many errors arise from viewpoint variation, scale shifts, and fine-grained object confusions. To disentangle these effects, we introduce COVAR, a benchmark that systematically varies object foregrounds and backgrounds. Leveraging CCI with COVAR, we present a comprehensive evaluation of eighteen CLIP variants, offering methodological advances and empirical evidence that chart a path toward more robust VLMs.

</details>


### [218] [Semantic Prioritization in Visual Counterfactual Explanations with Weighted Segmentation and Auto-Adaptive Region Selection](https://arxiv.org/abs/2511.12992)
*Lintong Zhang,Kang Yin,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 提出WSAE-Net方法，通过加权语义图和自适应候选编辑序列改进视觉反事实解释，提升语义相关性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统视觉反事实解释方法在替换图像区域时忽略语义相关性，损害模型可解释性并阻碍编辑流程。

Method: WSAE-Net包含两个关键创新：1）生成加权语义图以减少非语义特征单元计算；2）自适应候选编辑序列确定特征单元处理顺序。

Result: 通过全面实验验证，该方法表现出优越性能，有助于更清晰深入地理解视觉反事实解释。

Conclusion: WSAE-Net方法有效解决了传统方法中语义相关性不足的问题，为视觉反事实解释提供了更高效的解决方案。

Abstract: In the domain of non-generative visual counterfactual explanations (CE), traditional techniques frequently involve the substitution of sections within a query image with corresponding sections from distractor images. Such methods have historically overlooked the semantic relevance of the replacement regions to the target object, thereby impairing the model's interpretability and hindering the editing workflow. Addressing these challenges, the present study introduces an innovative methodology named as Weighted Semantic Map with Auto-adaptive Candidate Editing Network (WSAE-Net). Characterized by two significant advancements: the determination of an weighted semantic map and the auto-adaptive candidate editing sequence. First, the generation of the weighted semantic map is designed to maximize the reduction of non-semantic feature units that need to be computed, thereby optimizing computational efficiency. Second, the auto-adaptive candidate editing sequences are designed to determine the optimal computational order among the feature units to be processed, thereby ensuring the efficient generation of counterfactuals while maintaining the semantic relevance of the replacement feature units to the target object. Through comprehensive experimentation, our methodology demonstrates superior performance, contributing to a more lucid and in-depth understanding of visual counterfactual explanations.

</details>


### [219] [PerTouch: VLM-Driven Agent for Personalized and Semantic Image Retouching](https://arxiv.org/abs/2511.12998)
*Zewei Chang,Zheng-Peng Duan,Jianxing Zhang,Chun-Le Guo,Siyu Liu,Hyungju Chun,Hyunhee Park,Zikun Liu,Chongyi Li*

Main category: cs.CV

TL;DR: PerTouch是一个基于扩散模型的图像润色框架，支持语义级图像润色，通过参数映射和语义边界感知机制实现精细控制，并利用VLM驱动的智能体处理用户指令，实现个性化图像润色。


<details>
  <summary>Details</summary>
Motivation: 图像润色需要平衡可控性和主观性，既要增强视觉质量，又要符合用户的个性化审美偏好。传统方法难以同时满足语义级控制和全局美学要求。

Method: 使用包含特定语义区域属性值的参数映射作为输入，构建显式的参数到图像映射；引入语义替换和参数扰动机制提升语义边界感知；开发VLM驱动的智能体处理用户指令；配备反馈驱动重思考和场景感知记忆机制。

Result: 广泛实验证明各组件有效性，PerTouch在个性化图像润色方面表现出优越性能。

Conclusion: PerTouch通过统一的扩散框架成功解决了图像润色中可控性与主观性的平衡问题，实现了语义级精细控制和个性化审美偏好对齐。

Abstract: Image retouching aims to enhance visual quality while aligning with users' personalized aesthetic preferences. To address the challenge of balancing controllability and subjectivity, we propose a unified diffusion-based image retouching framework called PerTouch. Our method supports semantic-level image retouching while maintaining global aesthetics. Using parameter maps containing attribute values in specific semantic regions as input, PerTouch constructs an explicit parameter-to-image mapping for fine-grained image retouching. To improve semantic boundary perception, we introduce semantic replacement and parameter perturbation mechanisms in the training process. To connect natural language instructions with visual control, we develop a VLM-driven agent that can handle both strong and weak user instructions. Equipped with mechanisms of feedback-driven rethinking and scene-aware memory, PerTouch better aligns with user intent and captures long-term preferences. Extensive experiments demonstrate each component's effectiveness and the superior performance of PerTouch in personalized image retouching. Code is available at: https://github.com/Auroral703/PerTouch.

</details>


### [220] [Medal S: Spatio-Textual Prompt Model for Medical Segmentation](https://arxiv.org/abs/2511.13001)
*Pengcheng Shi,Jiawei Chen,Jiaqi Liu,Xinglin Zhang,Tao Chen,Lei Li*

Main category: cs.CV

TL;DR: Medal S是一个医学分割基础模型，支持原生分辨率空间和文本提示的端到端训练框架，在5种医学影像模态上实现多类别分割，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本方法缺乏空间感知能力的问题，通过通道对齐和3D上下文保持来提升医学分割的准确性和效率。

Method: 采用通道对齐的体素提示与文本嵌入，轻量级3D卷积模块进行体素空间精炼，支持并行空间提示和动态重采样等技术。

Result: 在验证集上，Medal S在5种模态平均DSC达到75.44（vs SAT 69.83），NSD 77.34（vs 71.06），并行提示减少90%推理时间。

Conclusion: Medal S通过协调空间精度与语义文本指导，在多类别医学分割任务中展现出优越的效率和准确性，优于顺序提示方法。

Abstract: We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.

</details>


### [221] [Infinite-Story: A Training-Free Consistent Text-to-Image Generation](https://arxiv.org/abs/2511.13002)
*Jihun Park,Kyoungmin Lee,Jongmin Gim,Hyeonseo Jo,Minseok Oh,Wonhyeok Choi,Kyumin Hwang,Jaeyeul Kim,Minwoo Choi,Sunghoon Im*

Main category: cs.CV

TL;DR: Infinite-Story是一个无需训练、基于尺度自回归模型的文本到图像生成框架，专门用于多提示词故事场景，解决了身份和风格不一致问题，推理速度比现有最快模型快6倍以上。


<details>
  <summary>Details</summary>
Motivation: 解决多提示词故事生成中的身份不一致和风格不一致问题，现有扩散模型需要微调或推理速度慢。

Method: 使用身份提示替换来缓解文本编码器的上下文偏差，以及包含自适应风格注入和同步指导适应的统一注意力指导机制。

Result: 在保持提示词保真度的同时实现了高身份和风格一致性，推理速度达到每张图像1.72秒，比现有最快模型快6倍以上。

Conclusion: Infinite-Story在测试时完全运行，无需微调，在生成性能和推理速度方面都达到了最先进水平，适合实际视觉故事应用。

Abstract: We present Infinite-Story, a training-free framework for consistent text-to-image (T2I) generation tailored for multi-prompt storytelling scenarios. Built upon a scale-wise autoregressive model, our method addresses two key challenges in consistent T2I generation: identity inconsistency and style inconsistency. To overcome these issues, we introduce three complementary techniques: Identity Prompt Replacement, which mitigates context bias in text encoders to align identity attributes across prompts; and a unified attention guidance mechanism comprising Adaptive Style Injection and Synchronized Guidance Adaptation, which jointly enforce global style and identity appearance consistency while preserving prompt fidelity. Unlike prior diffusion-based approaches that require fine-tuning or suffer from slow inference, Infinite-Story operates entirely at test time, delivering high identity and style consistency across diverse prompts. Extensive experiments demonstrate that our method achieves state-of-the-art generation performance, while offering over 6X faster inference (1.72 seconds per image) than the existing fastest consistent T2I models, highlighting its effectiveness and practicality for real-world visual storytelling.

</details>


### [222] [Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis](https://arxiv.org/abs/2511.13011)
*Qingsen Ma,Chen Zou,Dianyun Wang,Jia Wang,Liuyu Xiang,Zhaofeng He*

Main category: cs.CV

TL;DR: DTGS是一个统一框架，将Retinex启发的光照分解与热引导的3D高斯泼溅相结合，用于极端低光条件下的新视角合成，解决了标准3DGS在曝光不足输入下的失效问题。


<details>
  <summary>Details</summary>
Motivation: 在极端低光条件下，新视角合成在几何、颜色一致性和辐射稳定性方面严重退化。标准的3D高斯泼溅管道直接应用于曝光不足输入时会失败，因为跨视图的独立增强会导致光照不一致和几何失真。

Method: DTGS通过循环增强-重建机制，在增强、几何和热监督之间进行联合优化。包含热监督分支动态平衡增强、结构和热损失，以及嵌入3DGS循环中的Retinex分解模块提供物理可解释的反射-光照分离。

Result: 在构建的RGBT-LOW多视图低光热数据集上的广泛实验表明，DTGS显著优于现有的低光增强和3D重建基线，在极端光照下实现了优越的辐射一致性、几何保真度和颜色稳定性。

Conclusion: DTGS通过紧密耦合光照分解与热引导的3D高斯泼溅，为极端低光条件下的新视角合成提供了一个有效的统一框架，解决了现有方法在光照一致性和几何保真度方面的局限性。

Abstract: Under extremely low-light conditions, novel view synthesis (NVS) faces severe degradation in terms of geometry, color consistency, and radiometric stability. Standard 3D Gaussian Splatting (3DGS) pipelines fail when applied directly to underexposed inputs, as independent enhancement across views causes illumination inconsistencies and geometric distortion. To address this, we present DTGS, a unified framework that tightly couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for illumination-invariant reconstruction. Unlike prior approaches that treat enhancement as a pre-processing step, DTGS performs joint optimization across enhancement, geometry, and thermal supervision through a cyclic enhancement-reconstruction mechanism. A thermal supervisory branch stabilizes both color restoration and geometry learning by dynamically balancing enhancement, structural, and thermal losses. Moreover, a Retinex-based decomposition module embedded within the 3DGS loop provides physically interpretable reflectance-illumination separation, ensuring consistent color and texture across viewpoints. To evaluate our method, we construct RGBT-LOW, a new multi-view low-light thermal dataset capturing severe illumination degradation. Extensive experiments show that DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination.

</details>


### [223] [You Only Look Omni Gradient Backpropagation for Moving Infrared Small Target Detection](https://arxiv.org/abs/2511.13013)
*Guoyi Zhang,Guangsheng Xu,Siyang Chen,Han Wang,Xiaohu Zhang*

Main category: cs.CV

TL;DR: 提出BP-FPN，一种从反向传播角度设计的特征金字塔架构，通过梯度隔离低层捷径和方向梯度正则化，解决红外小目标检测中的特征表示瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法主要关注时空特征聚合，但增益有限，根本瓶颈在于单帧特征表示模糊而非时空建模本身。

Method: 引入梯度隔离低层捷径(GILS)高效整合细粒度目标细节而不引发捷径学习，以及方向梯度正则化(DGR)在反向传播中强制层次特征一致性。

Result: 在多个公共数据集上的广泛实验表明，BP-FPN始终建立新的最先进性能。

Conclusion: 这是首个完全从反向传播角度为该任务设计的FPN架构，理论上有依据，计算开销可忽略，并能无缝集成到现有框架中。

Abstract: Moving infrared small target detection is a key component of infrared search and tracking systems, yet it remains extremely challenging due to low signal-to-clutter ratios, severe target-background imbalance, and weak discriminative features. Existing deep learning methods primarily focus on spatio-temporal feature aggregation, but their gains are limited, revealing that the fundamental bottleneck lies in ambiguous per-frame feature representations rather than spatio-temporal modeling itself. Motivated by this insight, we propose BP-FPN, a backpropagation-driven feature pyramid architecture that fundamentally rethinks feature learning for small target. BP-FPN introduces Gradient-Isolated Low-Level Shortcut (GILS) to efficiently incorporate fine-grained target details without inducing shortcut learning, and Directional Gradient Regularization (DGR) to enforce hierarchical feature consistency during backpropagation. The design is theoretically grounded, introduces negligible computational overhead, and can be seamlessly integrated into existing frameworks. Extensive experiments on multiple public datasets show that BP-FPN consistently establishes new state-of-the-art performance. To the best of our knowledge, it is the first FPN designed for this task entirely from the backpropagation perspective.

</details>


### [224] [Geometry Meets Light: Leveraging Geometric Priors for Universal Photometric Stereo under Limited Multi-Illumination Cues](https://arxiv.org/abs/2511.13015)
*King-Man Tam,Satoshi Ikehata,Yuta Asano,Zhaoyi An,Rei Kawakami*

Main category: cs.CV

TL;DR: GeoUniPS是一个通用光度立体网络，通过结合合成监督和大规模3D重建模型的高层几何先验，解决了传统方法在复杂真实场景中光照不可靠区域的问题。


<details>
  <summary>Details</summary>
Motivation: 传统通用光度立体方法在多光照线索不可靠时（如偏置光照、阴影或自遮挡区域）表现不佳，特别是在复杂真实场景中。

Method: 设计了光-几何双分支编码器，从冻结的3D重建模型中提取多光照线索和几何先验；引入PS-Perp数据集支持透视投影学习；结合合成监督与几何先验。

Result: 在多个数据集上实现了最先进的性能，特别是在复杂真实场景中，定量和定性评估均表现出色。

Conclusion: GeoUniPS通过整合3D重建模型的几何先验，显著提升了通用光度立体在复杂真实场景中的表现，证明了视觉-几何基础模型的有效性。

Abstract: Universal Photometric Stereo is a promising approach for recovering surface normals without strict lighting assumptions. However, it struggles when multi-illumination cues are unreliable, such as under biased lighting or in shadows or self-occluded regions of complex in-the-wild scenes. We propose GeoUniPS, a universal photometric stereo network that integrates synthetic supervision with high-level geometric priors from large-scale 3D reconstruction models pretrained on massive in-the-wild data. Our key insight is that these 3D reconstruction models serve as visual-geometry foundation models, inherently encoding rich geometric knowledge of real scenes. To leverage this, we design a Light-Geometry Dual-Branch Encoder that extracts both multi-illumination cues and geometric priors from the frozen 3D reconstruction model. We also address the limitations of the conventional orthographic projection assumption by introducing the PS-Perp dataset with realistic perspective projection to enable learning of spatially varying view directions. Extensive experiments demonstrate that GeoUniPS delivers state-of-the-arts performance across multiple datasets, both quantitatively and qualitatively, especially in the complex in-the-wild scenes.

</details>


### [225] [REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding](https://arxiv.org/abs/2511.13026)
*Jiaze Li,Hao Yin,Wenhui Tan,Jingyang Chen,Boshen Xu,Yuxun Qu,Yijing Chen,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: 提出了REVISOR框架，通过跨模态反思机制增强多模态大语言模型在长视频理解任务中的推理能力，无需额外监督微调或外部模型。


<details>
  <summary>Details</summary>
Motivation: 纯文本反思机制在长视频理解中存在局限性：1）仅反思文本信息不足以处理丰富的动态视觉输入；2）缺乏跨模态交互能力，无法在反思过程中充分整合视觉信息。

Method: REVISOR框架支持文本和视觉模态的协同反思过程，设计了双归因解耦奖励机制（DADR），集成到GRPO训练策略中，确保模型推理与所选视频证据之间的因果对齐。

Result: 在VideoMME、LongVideoBench、MLVU和LVBench四个基准测试中取得了显著成果，显著提升了MLLMs的长视频理解能力。

Conclusion: REVISOR框架通过跨模态反思机制有效解决了长视频理解中纯文本反思的局限性，无需额外训练即可显著提升模型性能。

Abstract: Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.

</details>


### [226] [Towards 3D Object-Centric Feature Learning for Semantic Scene Completion](https://arxiv.org/abs/2511.13031)
*Weihua Wang,Yubo Cui,Xiangru Lin,Zhiheng Li,Zheng Fang*

Main category: cs.CV

TL;DR: Ocean是一个面向对象的3D语义场景补全框架，通过将场景分解为单个对象实例来提升语义占用预测的准确性，在复杂环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的3D语义场景补全方法通常采用以自我为中心的范式，在整个场景中聚合和扩散特征，但往往忽略了细粒度的对象级细节，导致在复杂环境中出现语义和几何模糊问题。

Method: 1. 使用轻量级分割模型MobileSAM从输入图像中提取实例掩码；2. 引入3D语义分组注意力模块，利用线性注意力在3D空间中聚合面向对象的特征；3. 设计全局相似性引导注意力模块处理分割错误和缺失实例；4. 提出实例感知局部扩散模块，通过生成过程改进实例特征并在BEV空间中细化场景表示。

Result: 在SemanticKITTI和SSCBench-KITTI360基准测试上的广泛实验表明，Ocean实现了最先进的性能，mIoU分数分别为17.40和20.28。

Conclusion: Ocean通过面向对象的方法有效解决了传统方法在复杂环境中的语义和几何模糊问题，显著提升了3D语义场景补全的准确性。

Abstract: Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.

</details>


### [227] [Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts](https://arxiv.org/abs/2511.13032)
*Sheng Liu,Yuanzhi Liang,Jiepeng Wang,Sidan Du,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Uni-Inter是一个统一的人类运动生成框架，支持多种交互场景（人-人、人-物、人-场景），通过统一的交互体积表示实现异构实体的空间编码和关系推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖任务特定设计，泛化能力有限，需要开发一个统一的任务无关架构来处理多种交互场景。

Method: 引入统一交互体积（UIV）作为体积表示，将异构交互实体编码到共享空间场中，通过关节级概率预测进行运动生成。

Result: 在三个代表性交互任务上的实验表明，Uni-Inter实现了竞争性性能，并能很好地泛化到新的实体组合。

Conclusion: 统一建模复合交互为复杂环境中的可扩展运动合成提供了有前景的方向。

Abstract: We present Uni-Inter, a unified framework for human motion generation that supports a wide range of interaction scenarios: including human-human, human-object, and human-scene-within a single, task-agnostic architecture. In contrast to existing methods that rely on task-specific designs and exhibit limited generalization, Uni-Inter introduces the Unified Interactive Volume (UIV), a volumetric representation that encodes heterogeneous interactive entities into a shared spatial field. This enables consistent relational reasoning and compound interaction modeling. Motion generation is formulated as joint-wise probabilistic prediction over the UIV, allowing the model to capture fine-grained spatial dependencies and produce coherent, context-aware behaviors. Experiments across three representative interaction tasks demonstrate that Uni-Inter achieves competitive performance and generalizes well to novel combinations of entities. These results suggest that unified modeling of compound interactions offers a promising direction for scalable motion synthesis in complex environments.

</details>


### [228] [uCLIP: Parameter-Efficient Multilingual Extension of Vision-Language Models with Unpaired Data](https://arxiv.org/abs/2511.13036)
*Dahyun Chung,Donghyun Shin,Yujin Sung,Seunggi Moon,Jinwoo Jeon,Byung-Jun Lee*

Main category: cs.CV

TL;DR: 提出了一种轻量级、数据高效的多语言视觉-语言对齐框架，无需图像-文本对或文本-文本对，仅训练170万参数的投影模块，通过英语表示作为语义锚点实现稳健的多语言对齐。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP模型在低资源语言上表现不佳，特别是在捷克语、芬兰语、克罗地亚语、匈牙利语和罗马尼亚语等代表性不足的语言上，主要原因是高质量多语言图像-文本数据稀缺。

Method: 冻结预训练的图像编码器和多语言文本编码器，仅训练紧凑的170万参数投影模块，使用对比损失以英语表示为语义锚点进行对齐。

Result: 在多个多语言检索基准测试中取得显著提升，特别是在五个代表性不足的语言上，现有模型通常表现不佳的情况下获得了显著增益。

Conclusion: 基于枢纽的参数高效对齐策略对于包容性多模态学习具有有效性，能够为资源受限语言提供稳健的视觉-语言对齐。

Abstract: Contrastive Language-Image Pre-training (CLIP) has demonstrated strong generalization across a wide range of visual tasks by leveraging large-scale English-image pairs. However, its extension to low-resource languages remains limited due to the scarcity of high-quality multilingual image-text data. Existing multilingual vision-language models exhibit consistently low retrieval performance in underrepresented languages including Czech, Finnish, Croatian, Hungarian, and Romanian on the Crossmodal-3600 (XM3600) benchmark. To address this, we propose a lightweight and data-efficient framework for multilingual vision-language alignment. Our approach requires no image-text pairs or text-text pairs and freezes both the pretrained image encoder and multilingual text encoder during training. Only a compact 1.7M-parameter projection module is trained, using a contrastive loss over English representations as semantic anchors. This minimal training setup enables robust multilingual alignment even for languages with limited supervision. Extensive evaluation across multiple multilingual retrieval benchmarks confirms the effectiveness of our method, showing significant gains in five underrepresented languages where existing models typically underperform. These findings highlight the effectiveness of our pivot-based, parameter-efficient alignment strategy for inclusive multimodal learning.

</details>


### [229] [MGCA-Net: Multi-Grained Category-Aware Network for Open-Vocabulary Temporal Action Localization](https://arxiv.org/abs/2511.13039)
*Zhenying Fang,Richang Hong*

Main category: cs.CV

TL;DR: 提出MGCA-Net方法，通过多粒度类别感知机制解决开放词汇时序动作定位问题，在THUMOS'14和ActivityNet-1.3基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多在单一粒度上识别动作类别，这会降低基类和新类动作的识别准确率。需要解决开放词汇时序动作定位中基类和新类动作识别精度下降的问题。

Method: 提出多粒度类别感知网络(MGCA-Net)，包含定位器、动作存在预测器、传统分类器和粗到细分类器。定位器定位类别无关的动作提议，动作存在预测器估计动作实例概率，传统分类器在片段粒度预测基类动作，粗到细分类器在视频粒度识别新类动作存在并在提议粒度分配类别。

Result: 在THUMOS'14和ActivityNet-1.3基准测试中达到最先进性能，在零样本时序动作定位设置下也取得最先进结果。

Conclusion: 通过基类动作的传统分类器感知和新类动作的粗到细类别感知，实现了多粒度类别感知，有效提升了定位性能。

Abstract: Open-Vocabulary Temporal Action Localization (OV-TAL) aims to recognize and localize instances of any desired action categories in videos without explicitly curating training data for all categories. Existing methods mostly recognize action categories at a single granularity, which degrades the recognition accuracy of both base and novel action categories. To address these issues, we propose a Multi-Grained Category-Aware Network (MGCA-Net) comprising a localizer, an action presence predictor, a conventional classifier, and a coarse-to-fine classifier. Specifically, the localizer localizes category-agnostic action proposals. For these action proposals, the action presence predictor estimates the probability that they belong to an action instance. At the same time, the conventional classifier predicts the probability of each action proposal over base action categories at the snippet granularity. Novel action categories are recognized by the coarse-to-fine classifier, which first identifies action presence at the video granularity. Finally, it assigns each action proposal to one category from the coarse categories at the proposal granularity. Through coarse-to-fine category awareness for novel actions and the conventional classifier's awareness of base actions, multi-grained category awareness is achieved, effectively enhancing localization performance. Comprehensive evaluations on the THUMOS'14 and ActivityNet-1.3 benchmarks demonstrate that our method achieves state-of-the-art performance. Furthermore, our MGCA-Net achieves state-of-the-art results under the Zero-Shot Temporal Action Localization setting.

</details>


### [230] [DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation](https://arxiv.org/abs/2511.13047)
*Yan Gong,Jianli Lu,Yongsheng Gao,Jie Zhao,Xiaojuan Zhang,Susanto Rahardja*

Main category: cs.CV

TL;DR: 提出DiffPixelFormer，一种用于RGB-D室内场景分割的差分像素感知Transformer，通过增强模态内表示和建模模态间交互，在SUN RGB-D和NYUDv2基准上取得最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D融合方法依赖计算密集的交叉注意力机制，对模态内和模态间特征关系建模不足，导致特征对齐不精确和判别性表示有限。

Method: 核心是Intra-Inter Modal Interaction Block (IIMIB)，通过自注意力捕获模态内长程依赖，使用Differential-Shared Inter-Modal (DSIM)模块建模模态间交互，解耦模态特定和共享线索，实现像素级跨模态对齐，并采用动态融合策略平衡模态贡献。

Result: 在SUN RGB-D和NYUDv2基准上，DiffPixelFormer-L分别达到54.28%和59.95%的mIoU，比DFormer-L分别提升1.78%和2.75%。

Conclusion: DiffPixelFormer通过同时增强模态内表示和建模模态间交互，实现了更精确的特征对齐和判别性表示，在RGB-D室内场景分割任务中表现出色。

Abstract: Indoor semantic segmentation is fundamental to computer vision and robotics, supporting applications such as autonomous navigation, augmented reality, and smart environments. Although RGB-D fusion leverages complementary appearance and geometric cues, existing methods often depend on computationally intensive cross-attention mechanisms and insufficiently model intra- and inter-modal feature relationships, resulting in imprecise feature alignment and limited discriminative representation. To address these challenges, we propose DiffPixelFormer, a differential pixel-aware Transformer for RGB-D indoor scene segmentation that simultaneously enhances intra-modal representations and models inter-modal interactions. At its core, the Intra-Inter Modal Interaction Block (IIMIB) captures intra-modal long-range dependencies via self-attention and models inter-modal interactions with the Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific and shared cues, enabling fine-grained, pixel-level cross-modal alignment. Furthermore, a dynamic fusion strategy balances modality contributions and fully exploits RGB-D information according to scene characteristics. Extensive experiments on the SUN RGB-D and NYUDv2 benchmarks demonstrate that DiffPixelFormer-L achieves mIoU scores of 54.28% and 59.95%, outperforming DFormer-L by 1.78% and 2.75%, respectively. Code is available at https://github.com/gongyan1/DiffPixelFormer.

</details>


### [231] [ViSS-R1: Self-Supervised Reinforcement Video Reasoning](https://arxiv.org/abs/2511.13054)
*Bo Fang,Yuxin Song,Qiangqiang Wu,Haoyuan Sun,Wenhao Wu,Antoni B. Chan*

Main category: cs.CV

TL;DR: 提出了Pretext-GRPO自监督强化学习算法和ViSS-R1框架，通过处理变换后的视觉输入和前置任务来增强MLLMs的视频推理能力，避免文本中心化的捷径学习。


<details>
  <summary>Details</summary>
Motivation: 当前基于R1的多模态大语言模型在视频推理中过度依赖文本中心化方法，未能充分利用丰富的视觉信息，容易导致捷径学习和幻觉问题。

Method: 首先提出Pretext-GRPO自监督强化学习算法，通过为正确解决变换视觉输入的前置任务分配正奖励；然后构建ViSS-R1框架，将前置任务自监督学习直接整合到MLLM的R1后训练范式中。

Result: 在六个广泛使用的视频推理和理解基准测试上进行全面评估，证明了Pretext-GRPO和ViSS-R1在复杂视频推理中的有效性和优越性。

Conclusion: 所提出的方法能够促进更稳健的视觉中心化视频理解，使模型能够非平凡地处理视觉信息，从而提高复杂视频推理的性能。

Abstract: Complex video reasoning remains a significant challenge for Multimodal Large Language Models (MLLMs), as current R1-based methodologies often prioritize text-centric reasoning derived from text-based and image-based developments. In video tasks, such strategies frequently underutilize rich visual information, leading to potential shortcut learning and increased susceptibility to hallucination. To foster a more robust, visual-centric video understanding, we start by introducing a novel self-supervised reinforcement learning GRPO algorithm (Pretext-GRPO) within the standard R1 pipeline, in which positive rewards are assigned for correctly solving pretext tasks on transformed visual inputs, which makes the model to non-trivially process the visual information. Building on the effectiveness of Pretext-GRPO, we further propose the ViSS-R1 framework, which streamlines and integrates pretext-task-based self-supervised learning directly into the MLLM's R1 post-training paradigm. Instead of relying solely on sparse visual cues, our framework compels models to reason about transformed visual input by simultaneously processing both pretext questions (concerning transformations) and true user queries. This necessitates identifying the applied transformation and reconstructing the original video to formulate accurate final answers. Comprehensive evaluations on six widely-used video reasoning and understanding benchmarks demonstrate the effectiveness and superiority of our Pretext-GRPO and ViSS-R1 for complex video reasoning. Our codes and models will be publicly available.

</details>


### [232] [Monocular 3D Lane Detection via Structure Uncertainty-Aware Network with Curve-Point Queries](https://arxiv.org/abs/2511.13055)
*Ruixin Liu,Zejian Yuan*

Main category: cs.CV

TL;DR: MonoUnc是一个无需鸟瞰图的单目3D车道线检测器，通过局部车道结构建模随机不确定性，在ONCE-3DLanes和OpenLane数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖简化的几何假设（如独立点预测或全局平面建模），无法捕捉真实场景中的结构变化和随机不确定性。

Method: 将3D车道线投影到前视图空间并用参数曲线近似，基于曲线预测动态生成曲线点查询嵌入，将相邻点形成的线段建模为3D高斯分布，并设计新的3D高斯匹配损失函数。

Result: 在ONCE-3DLanes和OpenLane数据集上，MonoUnc在更严格的评估标准下优于所有先前的最先进方法。

Conclusion: 提出的方法能有效建模车道线检测中的随机不确定性，并提出了两个新的综合评估指标来量化全局和局部误差。

Abstract: Monocular 3D lane detection is challenged by aleatoric uncertainty arising from inherent observation noise. Existing methods rely on simplified geometric assumptions, such as independent point predictions or global planar modeling, failing to capture structural variations and aleatoric uncertainty in real-world scenarios. In this paper, we propose MonoUnc, a bird's-eye view (BEV)-free 3D lane detector that explicitly models aleatoric uncertainty informed by local lane structures. Specifically, 3D lanes are projected onto the front-view (FV) space and approximated by parametric curves. Guided by curve predictions, curve-point query embeddings are dynamically generated for lane point predictions in 3D space. Each segment formed by two adjacent points is modeled as a 3D Gaussian, parameterized by the local structure and uncertainty estimations. Accordingly, a novel 3D Gaussian matching loss is designed to constrain these parameters jointly. Experiments on the ONCE-3DLanes and OpenLane datasets demonstrate that MonoUnc outperforms previous state-of-the-art (SoTA) methods across all benchmarks under stricter evaluation criteria. Additionally, we propose two comprehensive evaluation metrics for ONCE-3DLanes, calculating the average and maximum bidirectional Chamfer distances to quantify global and local errors. Codes are released at https://github.com/lrx02/MonoUnc.

</details>


### [233] [FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation](https://arxiv.org/abs/2511.13063)
*Zhenghua Li,Hang Chen,Zihao Sun,Kai Li,Xiaolin Hu*

Main category: cs.CV

TL;DR: 提出了一种将Segment Anything 2 (SAM2)从自然图像预训练迁移到电子显微镜图像分割的框架，通过特征引导注意力模块和双亲和度解码器，在冻结SAM2权重时达到SOTA水平，微调后显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 电子显微镜图像中的神经结构分割面临形态复杂、信噪比低和标注稀缺等挑战，现有方法在准确性和泛化性上受限。

Method: 使用SAM2提取通用特征，引入特征引导注意力模块利用语义线索指导轻量级编码器关注困难区域，双亲和度解码器生成粗粒度和细粒度亲和度图。

Result: 冻结SAM2权重时性能与SOTA相当，在EM数据上微调后显著超越现有SOTA方法。

Conclusion: 自然图像预训练表示结合针对性域自适应指导，能有效解决神经元分割中的特定挑战。

Abstract: Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.

</details>


### [234] [RobustGait: Robustness Analysis for Appearance Based Gait Recognition](https://arxiv.org/abs/2511.13065)
*Reeshoon Sayera,Akash Kumar,Sirshapan Mitra,Prudvi Kamtam,Yogesh S Rawat*

Main category: cs.CV

TL;DR: 提出了RobustGait框架，用于评估基于外观的步态识别系统在真实世界扰动和轮廓变化下的鲁棒性，涵盖四种扰动类型、多种轮廓提取方法和模型架构，发现了RGB级噪声应用、轮廓提取器偏差等关键洞察。


<details>
  <summary>Details</summary>
Motivation: 现有基于外观的步态识别在受控数据集上表现良好，但缺乏对其在真实世界扰动和轮廓变化下鲁棒性的系统性评估。

Method: 开发RobustGait框架，在四个维度进行评估：扰动类型（数字、环境、时间、遮挡）、轮廓提取方法、步态识别模型架构能力和部署场景，引入15种扰动类型和5个严重级别，在多个数据集上评估6个先进步态系统。

Result: 发现：1）RGB级噪声应用能更好反映真实世界退化；2）步态精度对轮廓提取器偏差高度敏感；3）鲁棒性同时依赖于扰动类型和架构设计；4）噪声感知训练和知识蒸馏能提升性能。

Conclusion: RobustGait提供了细粒度鲁棒性评估框架，揭示了步态识别系统中的关键脆弱点，并展示了通过噪声感知训练和知识蒸馏可以提升系统鲁棒性，推动部署就绪系统的发展。

Abstract: Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems.

</details>


### [235] [Decoupling Scene Perception and Ego Status: A Multi-Context Fusion Approach for Enhanced Generalization in End-to-End Autonomous Driving](https://arxiv.org/abs/2511.13079)
*Jiacheng Tang,Mingyue Feng,Jiachao Liu,Yaonong Wang,Jian Pu*

Main category: cs.CV

TL;DR: 提出AdaptiveAD架构，通过双分支结构解耦场景感知与自车状态，解决现有端到端自动驾驶系统过度依赖自车状态的问题，提升泛化能力和场景理解鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有模块化规划导向的自动驾驶架构过度依赖自车状态，导致泛化能力受限和场景理解不鲁棒。根本原因在于自车状态在BEV编码器中过早融合，使其成为下游规划模块的捷径。

Method: 采用多上下文融合策略的双分支架构：一个分支基于多任务学习进行场景驱动推理（BEV编码器中故意省略自车状态），另一个分支仅基于规划任务进行自车驱动推理。通过场景感知融合模块自适应整合两个分支的互补决策。引入路径注意力机制、BEV单向蒸馏和自回归在线映射两个辅助任务来保证多任务学习效果。

Result: 在nuScenes数据集上的广泛评估表明，AdaptiveAD实现了最先进的开放环路规划性能，显著减轻了对自车状态的过度依赖，并在多样化场景中展现出令人印象深刻的泛化能力。

Conclusion: AdaptiveAD通过架构层面的解耦设计有效解决了自动驾驶系统中过度依赖自车状态的问题，为提升系统泛化能力和鲁棒性提供了可行的解决方案。

Abstract: Modular design of planning-oriented autonomous driving has markedly advanced end-to-end systems. However, existing architectures remain constrained by an over-reliance on ego status, hindering generalization and robust scene understanding. We identify the root cause as an inherent design within these architectures that allows ego status to be easily leveraged as a shortcut. Specifically, the premature fusion of ego status in the upstream BEV encoder allows an information flow from this strong prior to dominate the downstream planning module. To address this challenge, we propose AdaptiveAD, an architectural-level solution based on a multi-context fusion strategy. Its core is a dual-branch structure that explicitly decouples scene perception and ego status. One branch performs scene-driven reasoning based on multi-task learning, but with ego status deliberately omitted from the BEV encoder, while the other conducts ego-driven reasoning based solely on the planning task. A scene-aware fusion module then adaptively integrates the complementary decisions from the two branches to form the final planning trajectory. To ensure this decoupling does not compromise multi-task learning, we introduce a path attention mechanism for ego-BEV interaction and add two targeted auxiliary tasks: BEV unidirectional distillation and autoregressive online mapping. Extensive evaluations on the nuScenes dataset demonstrate that AdaptiveAD achieves state-of-the-art open-loop planning performance. Crucially, it significantly mitigates the over-reliance on ego status and exhibits impressive generalization capabilities across diverse scenarios.

</details>


### [236] [MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images](https://arxiv.org/abs/2511.13099)
*Doanh C. Bui,Ba Hung Ngo,Hoai Luan Pham,Khang Nguyen,Maï K. Nguyen,Yasuhiko Nakashima*

Main category: cs.CV

TL;DR: MergeSlide是一个用于WSI终身学习的框架，将终身学习视为模型合并问题，通过正交持续合并策略和任务到类别提示对齐推理来缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: WSI图像体积庞大，终身学习可以减少数据传输和处理所需的资源和努力，同时避免灾难性遗忘问题。

Method: 使用视觉语言病理基础模型，通过类感知提示定义新任务，使用无MLP的主干进行少量轮次微调，采用正交持续合并策略合并模型，在推理时使用任务到类别提示对齐方法。

Result: 在六个TCGA数据集流上的实验表明，MergeSlide优于基于排练的持续学习和视觉语言零样本基线方法。

Conclusion: MergeSlide是一个简单有效的WSI终身学习框架，通过模型合并和提示对齐策略成功解决了灾难性遗忘问题。

Abstract: Lifelong learning on Whole Slide Images (WSIs) aims to train or fine-tune a unified model sequentially on cancer-related tasks, reducing the resources and effort required for data transfer and processing, especially given the gigabyte-scale size of WSIs. In this paper, we introduce MergeSlide, a simple yet effective framework that treats lifelong learning as a model merging problem by leveraging a vision-language pathology foundation model. When a new task arrives, it is: 1) defined with class-aware prompts, 2) fine-tuned for a few epochs using an MLP-free backbone, and 3) merged into a unified model using an orthogonal continual merging strategy that preserves performance and mitigates catastrophic forgetting. For inference under the class-incremental learning (CLASS-IL) setting, where task identity is unknown, we introduce Task-to-Class Prompt-aligned (TCP) inference. Specifically, TCP first identifies the most relevant task using task-level prompts and then applies the corresponding class-aware prompts to generate predictions. To evaluate MergeSlide, we conduct experiments on a stream of six TCGA datasets. The results show that MergeSlide outperforms both rehearsal-based continual learning and vision-language zero-shot baselines. Code and data are available at https://github.com/caodoanh2001/MergeSlide.

</details>


### [237] [CapeNext: Rethinking and refining dynamic support information for category-agnostic pose estimation](https://arxiv.org/abs/2511.13102)
*Yu Zhu,Dan Zeng,Shuiwang Li,Qijun Zhao,Qiaomu Shen,Bo Tang*

Main category: cs.CV

TL;DR: 提出CapeNext框架，通过层次化跨模态交互和双流特征精炼解决类别无关姿态估计中静态关节嵌入的歧义性和区分性不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有CAPE方法使用固定文本关键点描述作为语义先验，存在两个局限：(1)多义词引起的跨类别歧义（如'腿'在人类和家具中的不同视觉表现），(2)对细粒度类内变化区分性不足（如不同姿态和毛色的猫）。

Method: 提出新框架，创新性地整合层次化跨模态交互与双流特征精炼，从文本描述和具体图像中获取类别级和实例特定线索来增强关节嵌入。

Result: 在MP-100数据集上的实验表明，无论使用何种网络骨干，CapeNext都大幅超越最先进的CAPE方法。

Conclusion: CapeNext通过动态增强的关节嵌入有效解决了静态文本描述在跨类别歧义和类内区分性方面的局限性。

Abstract: Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept "leg" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.

</details>


### [238] [PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking](https://arxiv.org/abs/2511.13105)
*Seungjae Kim,SeungJoon Lee,MyeongAh Cho*

Main category: cs.CV

TL;DR: PlugTrack是一个新颖的多目标跟踪框架，通过自适应融合卡尔曼滤波器和数据驱动的运动预测器来解决线性与非线性运动模式的互补性问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界跟踪场景同时包含线性和非线性运动模式，但现有方法要么使用计算高效的卡尔曼滤波器（无法处理非线性运动），要么使用数据驱动方法（计算开销大且泛化能力有限）。

Method: 提出PlugTrack框架，通过多感知运动理解自适应融合卡尔曼滤波器和数据驱动运动预测器，生成自适应混合因子，无需修改现有运动预测器。

Result: 在MOT17/MOT20上取得显著性能提升，在DanceTrack上达到最先进水平。

Conclusion: PlugTrack是首个通过自适应融合桥接经典与现代运动预测范式的多目标跟踪框架。

Abstract: Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.

</details>


### [239] [Low-Level Dataset Distillation for Medical Image Enhancement](https://arxiv.org/abs/2511.13106)
*Fengzhi Xu,Ziyuan Yang,Mengyu Sun,Joey Tianyi Zhou,Yi Zhang*

Main category: cs.CV

TL;DR: 提出了首个用于医学图像增强的低级数据集蒸馏方法，通过共享解剖先验和个性化生成模块，在保护隐私的同时实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像增强方法需要大规模数据集学习复杂像素级映射，但训练和存储成本高。现有数据集蒸馏方法主要针对高级任务，而低级任务的像素级保真度要求使得蒸馏成为欠定问题。

Method: 利用患者间解剖相似性构建共享解剖先验，通过结构保持个性化生成模块整合患者特定信息，使用蒸馏数据构建任务特定训练对，并通过梯度对齐注入患者特定知识。

Result: 提出的方法能够生成包含抽象训练信息的蒸馏数据集，无需访问原始患者数据，在保护隐私的同时实现高效医学图像增强。

Conclusion: 该方法成功解决了低级医学图像增强任务中的数据集蒸馏挑战，为隐私保护的医学图像处理提供了可行方案。

Abstract: Medical image enhancement is clinically valuable, but existing methods require large-scale datasets to learn complex pixel-level mappings. However, the substantial training and storage costs associated with these datasets hinder their practical deployment. While dataset distillation (DD) can alleviate these burdens, existing methods mainly target high-level tasks, where multiple samples share the same label. This many-to-one mapping allows distilled data to capture shared semantics and achieve information compression. In contrast, low-level tasks involve a many-to-many mapping that requires pixel-level fidelity, making low-level DD an underdetermined problem, as a small distilled dataset cannot fully constrain the dense pixel-level mappings. To address this, we propose the first low-level DD method for medical image enhancement. We first leverage anatomical similarities across patients to construct the shared anatomical prior based on a representative patient, which serves as the initialization for the distilled data of different patients. This prior is then personalized for each patient using a Structure-Preserving Personalized Generation (SPG) module, which integrates patient-specific anatomical information into the distilled dataset while preserving pixel-level fidelity. For different low-level tasks, the distilled data is used to construct task-specific high- and low-quality training pairs. Patient-specific knowledge is injected into the distilled data by aligning the gradients computed from networks trained on the distilled pairs with those from the corresponding patient's raw data. Notably, downstream users cannot access raw patient data. Instead, only a distilled dataset containing abstract training information is shared, which excludes patient-specific details and thus preserves privacy.

</details>


### [240] [DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection](https://arxiv.org/abs/2511.13108)
*Jiazhen Yan,Ziqiang Li,Fan Wang,Boyu Wang,Zhangjie Fu*

Main category: cs.CV

TL;DR: 提出DGS-Net框架，通过梯度空间分解解决CLIP模型微调时的灾难性遗忘问题，提升AI生成图像检测的跨域泛化能力


<details>
  <summary>Details</summary>
Motivation: 生成模型快速发展导致AI生成图像泛滥，引发虚假信息、隐私侵犯等问题。CLIP等预训练模型在微调时存在灾难性遗忘，损害预训练先验知识并限制跨域泛化

Method: 提出蒸馏引导的梯度手术网络(DGS-Net)，通过梯度空间分解分离优化过程中的有害和有益下降方向，将任务梯度投影到有害方向的正交补空间，并与冻结CLIP编码器蒸馏的有益方向对齐

Result: 在50个生成模型上的实验表明，该方法平均优于现有最优方法6.6%，在多样生成技术上实现了优越的检测性能和泛化能力

Conclusion: DGS-Net通过统一优化先验保持和无关抑制，有效解决了预训练模型微调时的灾难性遗忘问题，为AI生成图像检测提供了更可靠的解决方案

Abstract: The rapid progress of generative models such as GANs and diffusion models has led to the widespread proliferation of AI-generated images, raising concerns about misinformation, privacy violations, and trust erosion in digital media. Although large-scale multimodal models like CLIP offer strong transferable representations for detecting synthetic content, fine-tuning them often induces catastrophic forgetting, which degrades pre-trained priors and limits cross-domain generalization. To address this issue, we propose the Distillation-guided Gradient Surgery Network (DGS-Net), a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components. Specifically, we introduce a gradient-space decomposition that separates harmful and beneficial descent directions during optimization. By projecting task gradients onto the orthogonal complement of harmful directions and aligning with beneficial ones distilled from a frozen CLIP encoder, DGS-Net achieves unified optimization of prior preservation and irrelevant suppression. Extensive experiments on 50 generative models demonstrate that our method outperforms state-of-the-art approaches by an average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.

</details>


### [241] [Learning Implicit Neural Degradation Representation for Unpaired Image Dehazing](https://arxiv.org/abs/2511.13110)
*Shuaibin Fan,Senming Zhong,Wenchao Yan,Minglong Xue*

Main category: cs.CV

TL;DR: 提出了一种基于隐式神经退化表示的无监督去雾方法，通过结合通道独立和通道依赖机制来增强非线性依赖学习能力，并设计密集残差增强模块来消除冗余信息，在复杂场景中实现高质量图像恢复。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法在处理复杂场景时难以平衡非均匀雾分布下的细粒度特征表示和全局一致性建模，且需要更好地学习雾在空间变化中的共同退化表示。

Method: 基于Kolmogorov-Arnold表示定理，结合通道独立和通道依赖机制；设计隐式神经表示将雾退化建模为连续函数；构建密集残差增强模块来学习雾特征的隐式表示。

Result: 在多个公共和真实世界数据集上实现了具有竞争力的去雾性能，在复杂场景中获得了良好的视觉感知效果。

Conclusion: 该方法通过隐式神经退化表示和密集残差增强，有效解决了复杂场景下去雾的挑战，实现了高质量图像恢复。

Abstract: Image dehazing is an important task in the field of computer vision, aiming at restoring clear and detail-rich visual content from haze-affected images. However, when dealing with complex scenes, existing methods often struggle to strike a balance between fine-grained feature representation of inhomogeneous haze distribution and global consistency modeling. Furthermore, to better learn the common degenerate representation of haze in spatial variations, we propose an unsupervised dehaze method for implicit neural degradation representation. Firstly, inspired by the Kolmogorov-Arnold representation theorem, we propose a mechanism combining the channel-independent and channel-dependent mechanisms, which efficiently enhances the ability to learn from nonlinear dependencies. which in turn achieves good visual perception in complex scenes. Moreover, we design an implicit neural representation to model haze degradation as a continuous function to eliminate redundant information and the dependence on explicit feature extraction and physical models. To further learn the implicit representation of the haze features, we also designed a dense residual enhancement module from it to eliminate redundant information. This achieves high-quality image restoration. Experimental results show that our method achieves competitive dehaze performance on various public and real-world datasets. This project code will be available at https://github.com/Fan-pixel/NeDR-Dehaze.

</details>


### [242] [Semantics and Content Matter: Towards Multi-Prior Hierarchical Mamba for Image Deraining](https://arxiv.org/abs/2511.13113)
*Zhaocheng Yu,Kui Jiang,Junjun Jiang,Xianming Liu,Guanglu Sun,Yi Xiao*

Main category: cs.CV

TL;DR: 提出MPHM网络用于图像去雨，通过整合宏观语义文本先验和微观结构视觉先验，采用渐进式先验融合注入策略，结合分层Mamba模块，在Rain200H数据集上实现了0.57 dB的PSNR提升。


<details>
  <summary>Details</summary>
Motivation: 现有去雨方法在语义和空间细节保真度方面存在不足，影响计算机视觉系统在自动驾驶等应用中的性能。

Method: 提出多先验分层Mamba网络，整合CLIP文本先验和DINOv2视觉先验，采用渐进式先验融合注入策略，并设计傅里叶增强的双路径分层Mamba模块。

Result: 在Rain200H数据集上实现0.57 dB PSNR增益，在真实世界雨景场景中展现出优越的泛化性能。

Conclusion: MPHM网络通过有效整合异构先验信息，实现了图像去雨任务的先进性能，为计算机视觉系统在恶劣天气条件下的应用提供了可靠解决方案。

Abstract: Rain significantly degrades the performance of computer vision systems, particularly in applications like autonomous driving and video surveillance. While existing deraining methods have made considerable progress, they often struggle with fidelity of semantic and spatial details. To address these limitations, we propose the Multi-Prior Hierarchical Mamba (MPHM) network for image deraining. This novel architecture synergistically integrates macro-semantic textual priors (CLIP) for task-level semantic guidance and micro-structural visual priors (DINOv2) for scene-aware structural information. To alleviate potential conflicts between heterogeneous priors, we devise a progressive Priors Fusion Injection (PFI) that strategically injects complementary cues at different decoder levels. Meanwhile, we equip the backbone network with an elaborate Hierarchical Mamba Module (HMM) to facilitate robust feature representation, featuring a Fourier-enhanced dual-path design that concurrently addresses global context modeling and local detail recovery. Comprehensive experiments demonstrate MPHM's state-of-the-art performance, achieving a 0.57 dB PSNR gain on the Rain200H dataset while delivering superior generalization on real-world rainy scenarios.

</details>


### [243] [A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features](https://arxiv.org/abs/2511.13115)
*Hanzhe Liang,Jie Zhou,Can Gao,Bingyang Guo,Jinbao Wang,Linlin Shen*

Main category: cs.CV

TL;DR: 提出了一种旋转不变特征(RIF)框架用于3D异常检测，通过点坐标映射和轻量级卷积变换网络来解决点云方向和位置变化带来的特征不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D异常检测方法在处理方向和位置变化的点云时，由于特征变化显著而面临挑战，需要开发旋转不变的特征表示。

Method: 1. 点坐标映射(PCM)技术将点映射到旋转不变空间；2. 轻量级卷积变换特征网络(CTF-Net)提取旋转不变特征；3. 使用迁移学习和3D数据增强预训练特征提取器。

Result: 在Anomaly-ShapeNet数据集上平均P-AUROC提升17.7%，在Real3D-AD数据集上平均P-AUROC提升1.6%，展现出强大的泛化能力。

Conclusion: RIF框架有效解决了3D点云异常检测中的旋转不变性问题，在多个数据集上取得先进性能，具有工业应用潜力。

Abstract: 3D anomaly detection (AD) is a crucial task in computer vision, aiming to identify anomalous points or regions from point cloud data. However, existing methods may encounter challenges when handling point clouds with changes in orientation and position because the resulting features may vary significantly. To address this problem, we propose a novel Rotationally Invariant Features (RIF) framework for 3D AD. Firstly, to remove the adverse effect of variations on point cloud data, we develop a Point Coordinate Mapping (PCM) technique, which maps each point into a rotationally invariant space to maintain consistency of representation. Then, to learn robust and discriminative features, we design a lightweight Convolutional Transform Feature Network (CTF-Net) to extract rotationally invariant features for the memory bank. To improve the ability of the feature extractor, we introduce the idea of transfer learning to pre-train the feature extractor with 3D data augmentation. Experimental results show that the proposed method achieves the advanced performance on the Anomaly-ShapeNet dataset, with an average P-AUROC improvement of 17.7\%, and also gains the best performance on the Real3D-AD dataset, with an average P-AUROC improvement of 1.6\%. The strong generalization ability of RIF has been verified by combining it with traditional feature extraction methods on anomaly detection tasks, demonstrating great potential for industrial applications.

</details>


### [244] [CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model](https://arxiv.org/abs/2511.13121)
*Yuqi Zhang,Guanying Chen,Jiaxing Chen,Chuanyu Fu,Chuan Huang,Shuguang Cui*

Main category: cs.CV

TL;DR: CloseUpShot：基于扩散的框架，通过点条件视频扩散从稀疏输入实现特写视角的新视图合成，解决了特写场景下像素扭曲条件存在的稀疏性和背景泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏输入视图下的3D场景重建和新视图合成具有挑战性。现有方法主要针对适度视角变化设计，在特写场景中由于输入信息严重受限而难以捕捉细粒度细节。

Method: 提出分层扭曲和遮挡感知噪声抑制来增强条件图像质量；引入全局结构指导，利用密集融合点云为扩散过程提供一致的几何上下文。

Result: 在多个数据集上的广泛实验表明，该方法在特写新视图合成方面优于现有方法，验证了设计的有效性。

Conclusion: CloseUpShot通过改进的条件机制和全局几何约束，有效提升了稀疏输入下特写场景的新视图合成质量。

Abstract: Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.

</details>


### [245] [Region-Point Joint Representation for Effective Trajectory Similarity Learning](https://arxiv.org/abs/2511.13125)
*Hao Long,Silin Zhou,Lisi Chen,Shuo Shang*

Main category: cs.CV

TL;DR: RePo是一种新的轨迹相似性计算方法，通过联合编码区域级和点级特征来捕捉空间上下文和细粒度移动模式，在各项评估指标上比现有最佳方法平均提升22.2%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的方法虽然降低了传统轨迹相似性计算的计算复杂度，但未能充分利用轨迹信息的完整频谱进行相似性建模。

Method: 提出RePo方法：1）区域级表示：将GPS轨迹映射到网格序列，通过结构特征和视觉特征增强的语义上下文捕捉空间上下文；2）点级表示：使用三个轻量级专家网络从密集GPS序列中提取局部、相关和连续移动模式；3）路由器网络自适应融合点级特征，然后通过交叉注意力与区域级特征结合生成最终轨迹嵌入；4）使用带有困难负样本的对比损失进行训练。

Result: 实验结果显示，RePo在所有评估指标上比现有最佳基线方法平均提升了22.2%的准确率。

Conclusion: RePo通过联合编码区域级和点级特征，有效捕捉了轨迹的空间上下文和细粒度移动模式，显著提升了轨迹相似性计算的性能。

Abstract: Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \textbf{RePo}, a novel method that jointly encodes \textbf{Re}gion-wise and \textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\% over SOTA baselines across all evaluation metrics.

</details>


### [246] [VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language](https://arxiv.org/abs/2511.13127)
*Zonghao Ying,Moyang Chen,Nizhang Li,Zhiqiang Wang,Wenxin Zhang,Quanchen Zou,Zonglei Jing,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: VEIL是一个针对文本到视频模型的越狱攻击框架，通过中性场景锚点、潜在听觉触发器和风格调制器来生成看似良性的提示，诱导模型生成违反安全策略的视频内容。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频模型的越狱攻击通常通过添加明显不安全的对抗性扰动来实现，这些攻击容易被检测和防御。本文旨在开发更隐蔽的攻击方法，利用模型跨模态关联模式，通过看似良性的提示诱导模型生成不安全内容。

Method: 提出VEIL框架，采用模块化提示设计：中性场景锚点提供表面场景描述；潜在听觉触发器利用音频-视觉共现先验；风格调制器通过电影指令增强效果。通过约束优化和引导搜索算法平衡隐蔽性和有效性。

Result: 在7个文本到视频模型上进行广泛实验，VEIL攻击在商业模型中的平均攻击成功率提升了23%。

Conclusion: VEIL框架展示了文本到视频模型存在严重的安全漏洞，即使是看似良性的提示也能诱导模型生成违反策略的内容，揭示了当前安全防护措施的局限性。

Abstract: Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.

</details>


### [247] [MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation](https://arxiv.org/abs/2511.13135)
*Junjie Yang,Yuhao Yan,Gang Wu,Yuxuan Wang,Ruoyu Liang,Xinjie Jiang,Xiang Wan,Fenglei Fan,Yongquan Zhang,Feiwei Qin,Changmiao Wan*

Main category: cs.CV

TL;DR: 提出了MedGEN-Bench医学多模态基准，包含6422个专家验证的图像-文本对，涵盖6种成像模态和16个临床任务，旨在解决现有医学视觉基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型在医学应用中的普及，临床医生期望AI系统不仅能生成文本诊断，还能生成与真实临床工作流程无缝集成的医学图像。现有医学视觉基准存在查询模糊、诊断推理过于简化、忽视图像生成能力等问题。

Method: 构建了包含三个格式的基准：视觉问答、图像编辑和上下文多模态生成。采用三层评估框架：像素级指标、语义文本分析和专家指导的临床相关性评分。

Result: 系统评估了10个组合框架、3个统一模型和5个视觉语言模型。

Conclusion: MedGEN-Bench通过上下文交织的指令和开放式生成输出，推动了医学AI研究的发展，超越了传统选择题格式的限制。

Abstract: As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on ambiguous queries that lack sufficient relevance to image content, oversimplify complex diagnostic reasoning into closed-ended shortcuts, and adopt a text-centric evaluation paradigm that overlooks the importance of image generation capabilities. To address these challenges, we introduce \textsc{MedGEN-Bench}, a comprehensive multimodal benchmark designed to advance medical AI research. MedGEN-Bench comprises 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks. It is structured into three distinct formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. What sets MedGEN-Bench apart is its focus on contextually intertwined instructions that necessitate sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond the constraints of multiple-choice formats. To evaluate the performance of existing systems, we employ a novel three-tier assessment framework that integrates pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring. Using this framework, we systematically assess 10 compositional frameworks, 3 unified models, and 5 VLMs.

</details>


### [248] [WinMamba: Multi-Scale Shifted Windows in State Space Model for 3D Object Detection](https://arxiv.org/abs/2511.13138)
*Longhui Zheng,Qiming Xia,Xiaolu Chen,Zhaoliang Liu,Chenglu Wen*

Main category: cs.CV

TL;DR: WinMamba：一种基于Mamba的3D特征编码骨干网络，通过窗口尺度自适应模块和窗口移位策略，在保持计算效率的同时增强长程空间依赖捕获能力，在3D目标检测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决3D目标检测中同时最大化计算效率和捕获长程空间依赖的根本挑战。现有Mamba方法在固定窗口内进行轴对齐扫描会丢失空间信息。

Method: 提出WinMamba块组成的骨干网络，包含窗口尺度自适应模块补偿不同分辨率下的体素特征，以及窗口移位策略和学习位置编码来丰富线性状态空间中的上下文信息。

Result: 在KITTI和Waymo数据集上的大量实验表明，WinMamba显著优于基线方法。消融研究验证了WSF和AWF模块对检测精度的个体贡献。

Conclusion: WinMamba通过创新的窗口尺度自适应和窗口移位策略，成功平衡了3D目标检测中的计算效率和长程依赖捕获能力，代码将公开。

Abstract: 3D object detection is critical for autonomous driving, yet it remains fundamentally challenging to simultaneously maximize computational efficiency and capture long-range spatial dependencies. We observed that Mamba-based models, with their linear state-space design, capture long-range dependencies at lower cost, offering a promising balance between efficiency and accuracy. However, existing methods rely on axis-aligned scanning within a fixed window, inevitably discarding spatial information. To address this problem, we propose WinMamba, a novel Mamba-based 3D feature-encoding backbone composed of stacked WinMamba blocks. To enhance the backbone with robust multi-scale representation, the WinMamba block incorporates a window-scale-adaptive module that compensates voxel features across varying resolutions during sampling. Meanwhile, to obtain rich contextual cues within the linear state space, we equip the WinMamba layer with a learnable positional encoding and a window-shift strategy. Extensive experiments on the KITTI and Waymo datasets demonstrate that WinMamba significantly outperforms the baseline. Ablation studies further validate the individual contributions of the WSF and AWF modules in improving detection accuracy. The code will be made publicly available.

</details>


### [249] [Skeletons Speak Louder than Text: A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification](https://arxiv.org/abs/2511.13150)
*Rifen Lin,Alex Jinpeng Wang,Jiawei Mo,Min Li*

Main category: cs.CV

TL;DR: CSIP-ReID是首个基于骨架的预训练框架，通过对比学习对齐骨架和视觉特征，结合原型融合更新器和骨架引导时序建模，在视频行人重识别任务中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的多模态预训练方法存在两个根本限制：缺乏真正的多模态预训练，以及文本难以捕捉细粒度时序运动信息。骨架数据能提供与视频帧对齐的时空信息模态。

Method: 两阶段方法：第一阶段使用对比学习在序列级别对齐骨架和视觉特征；第二阶段引入动态原型融合更新器(PFU)来优化多模态身份原型，融合运动和外观线索；并提出骨架引导时序建模(SGTM)模块从骨架数据中提取时序线索并整合到视觉特征中。

Result: 在标准视频ReID基准(MARS, LS-VID, iLIDS-VID)上达到新的SOTA结果，在骨架ReID任务(BIWI, IAS)上表现出强泛化能力，显著优于先前方法。

Conclusion: CSIP-ReID开创了无标注和运动感知的ReID预训练范式，为多模态表示学习开辟了新前沿。

Abstract: Multimodal pretraining has revolutionized visual understanding, but its impact on video-based person re-identification (ReID) remains underexplored. Existing approaches often rely on video-text pairs, yet suffer from two fundamental limitations: (1) lack of genuine multimodal pretraining, and (2) text poorly captures fine-grained temporal motion-an essential cue for distinguishing identities in video. In this work, we take a bold departure from text-based paradigms by introducing the first skeleton-driven pretraining framework for ReID. To achieve this, we propose Contrastive Skeleton-Image Pretraining for ReID (CSIP-ReID), a novel two-stage method that leverages skeleton sequences as a spatiotemporally informative modality aligned with video frames. In the first stage, we employ contrastive learning to align skeleton and visual features at sequence level. In the second stage, we introduce a dynamic Prototype Fusion Updater (PFU) to refine multimodal identity prototypes, fusing motion and appearance cues. Moreover, we propose a Skeleton Guided Temporal Modeling (SGTM) module that distills temporal cues from skeleton data and integrates them into visual features. Extensive experiments demonstrate that CSIP-ReID achieves new state-of-the-art results on standard video ReID benchmarks (MARS, LS-VID, iLIDS-VID). Moreover, it exhibits strong generalization to skeleton-only ReID tasks (BIWI, IAS), significantly outperforming previous methods. CSIP-ReID pioneers an annotation-free and motion-aware pretraining paradigm for ReID, opening a new frontier in multimodal representation learning.

</details>


### [250] [THIR: Topological Histopathological Image Retrieval](https://arxiv.org/abs/2511.13170)
*Zahra Tabatabaei,Jon Sporring*

Main category: cs.CV

TL;DR: THIR是一个基于拓扑数据分析的无监督医学图像检索框架，利用Betti数和持久同调从组织病理学图像中提取拓扑特征，无需训练即可实现高效检索。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性主要死因之一，早期诊断和准确临床决策至关重要。传统深度学习方法需要大量标注数据和GPU资源，限制了临床应用。

Method: 使用立方体持久性从RGB组织病理学图像中提取拓扑指纹，将环的演化编码为紧凑可解释的特征向量，通过计算拓扑描述符距离进行相似性检索。

Result: 在BreaKHis数据集上的实验表明，THIR优于现有监督和无监督方法，在标准CPU上20分钟内处理整个数据集，提供快速可扩展的无训练解决方案。

Conclusion: THIR提供了一种快速、可扩展且无需训练的组织病理学图像检索方法，为临床诊断提供了实用的无监督解决方案。

Abstract: According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020. Early diagnosis and accurate clinical decision making are critical in reducing this global burden. In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns. Unlike conventional deep learning approaches that rely on extensive training, annotated datasets, and powerful GPU resources, THIR operates entirely without supervision. It extracts topological fingerprints directly from RGB histopathological images using cubical persistence, encoding the evolution of loops as compact, interpretable feature vectors. The similarity retrieval is then performed by computing the distances between these topological descriptors, efficiently returning the top-K most relevant matches.
  Extensive experiments on the BreaKHis dataset demonstrate that THIR outperforms state of the art supervised and unsupervised methods. It processes the entire dataset in under 20 minutes on a standard CPU, offering a fast, scalable, and training free solution for clinical image retrieval.

</details>


### [251] [HDW-SR: High-Frequency Guided Diffusion Model based on Wavelet Decomposition for Image Super-Resolution](https://arxiv.org/abs/2511.13175)
*Chao Yang,Boqian Zhang,Jinghao Xu,Guang Jiang*

Main category: cs.CV

TL;DR: 提出HDW-SR方法，通过小波分解和残差扩散专注于高频信息恢复，在图像超分辨率中显著改善细节恢复效果


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的单图像超分辨率方法在高频域指导不足，导致恢复的细节模糊，需要更有效的高频指导机制

Method: 使用小波分解替代传统U-Net，仅在残差图上进行扩散；引入基于小波的下采样实现多尺度频率分解；采用稀疏交叉注意力机制进行高频指导；设计动态阈值块优化高频选择

Result: 在合成和真实数据集上的实验表明，HDW-SR在超分辨率性能上具有竞争力，特别是在恢复细粒度图像细节方面表现出色

Conclusion: HDW-SR通过小波分解和残差扩散策略有效提升了高频信息恢复能力，为图像超分辨率提供了新的解决方案

Abstract: Diffusion-based methods have shown great promise in single image super-resolution (SISR); however, existing approaches often produce blurred fine details due to insufficient guidance in the high-frequency domain. To address this issue, we propose a High-Frequency Guided Diffusion Network based on Wavelet Decomposition (HDW-SR), which replaces the conventional U-Net backbone in diffusion frameworks. Specifically, we perform diffusion only on the residual map, allowing the network to focus more effectively on high-frequency information restoration. We then introduce wavelet-based downsampling in place of standard CNN downsampling to achieve multi-scale frequency decomposition, enabling sparse cross-attention between the high-frequency subbands of the pre-super-resolved image and the low-frequency subbands of the diffused image for explicit high-frequency guidance. Moreover, a Dynamic Thresholding Block (DTB) is designed to refine high-frequency selection during the sparse attention process. During upsampling, the invertibility of the wavelet transform ensures low-loss feature reconstruction. Experiments on both synthetic and real-world datasets demonstrate that HDW-SR achieves competitive super-resolution performance, excelling particularly in recovering fine-grained image details. The code will be available after acceptance.

</details>


### [252] [GenTract: Generative Global Tractography](https://arxiv.org/abs/2511.13183)
*Alec Sargood,Lemuel Puglisi,Elinor Thompson,Mirco Musolesi,Daniel C. Alexander*

Main category: cs.CV

TL;DR: GenTract是首个用于全局纤维束成像的生成模型，将纤维束成像构建为生成任务，直接从dMRI映射到完整、解剖学上合理的流线。相比传统方法，在精度上显著提升，特别是在低分辨率和噪声数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统局部纤维束成像方法容易累积误差和高假阳性率的问题，以及全局方法计算成本高的问题。

Method: 将纤维束成像构建为生成任务，学习从dMRI到完整流线的直接映射，比较了基于扩散和流匹配的两种范式。

Result: GenTract的精度比次优方法TractOracle高2.1倍，在低分辨率和噪声设置下优势更加明显，比最接近的竞争对手高出一个数量级。

Conclusion: GenTract在保持研究级数据高精度的同时，在低质量数据上也能保持可靠性，是全局纤维束成像的有前景解决方案。

Abstract: Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI). Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data. In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive. To address these challenges, we introduce GenTract, the first generative model for global tractography. We frame tractography as a generative task, learning a direct mapping from dMRI to complete, anatomically plausible streamlines. We compare both diffusion-based and flow matching paradigms and evaluate GenTract's performance against state-of-the-art baselines. Notably, GenTract achieves precision 2.1x higher than the next-best method, TractOracle. This advantage becomes even more pronounced in challenging low-resolution and noisy settings, where it outperforms the closest competitor by an order of magnitude. By producing tractograms with high precision on research-grade data while also maintaining reliability on imperfect, lower-resolution data, GenTract represents a promising solution for global tractography.

</details>


### [253] [Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework](https://arxiv.org/abs/2511.13189)
*Diego Ortego,Marlon Rodríguez,Mario Almagro,Kunal Dahiya,David Jiménez,Juan C. SanMiguel*

Main category: cs.CV

TL;DR: 本文提出了ViXML框架，将视觉信息有效整合到极端多标签分类中，通过使用解码器模型和视觉基础模型，在保持计算效率的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有XMC方法主要使用小型编码器模型，未能充分利用大型解码器模型和视觉信息的潜力，需要在效率和性能之间找到平衡。

Method: 提出ViXML框架，整合数十亿参数规模的解码器模型和视觉基础模型，通过单图像嵌入池化来限制计算增长，同时解锁多模态能力。

Result: 在四个公开数据集上验证，ViXML在大多数情况下优于纯文本解码器，在最大数据集上P@1指标提升达+8.21%，超越现有最佳方法。

Conclusion: 视觉信息在XMC中具有重要价值，一个图像相当于数十亿参数的效果，ViXML框架成功平衡了性能与计算效率。

Abstract: Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.

</details>


### [254] [Video Spatial Reasoning with Object-Centric 3D Rollout](https://arxiv.org/abs/2511.13190)
*Haoran Tang,Meng Cao,Ruyang Liu,Xiaoxi Liang,Linglong Li,Ge Li,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出Object-Centric 3D Rollout (OCR)方法，通过结构化扰动3D几何来增强多模态大语言模型的视频空间推理能力，在VSI-Bench上达到47.5%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在查询锁定推理问题，仅关注提示中明确提到的对象而忽略关键上下文线索，需要解决视频空间推理中的这一局限性。

Method: OCR方法在训练期间对选定对象的3D几何引入结构化扰动，通过退化对象特定视觉线索并将改变的几何投影到2D空间，迫使模型在整个场景中进行整体推理。

Result: 3B参数模型在VSI-Bench上达到47.5%准确率，优于多个7B基线模型，消融实验证实OCR优于先前的rollout策略。

Conclusion: OCR方法有效解决了查询锁定推理问题，显著提升了多模态大语言模型的视频空间推理能力。

Abstract: Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCR's superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).

</details>


### [255] [Birth of a Painting: Differentiable Brushstroke Reconstruction](https://arxiv.org/abs/2511.13191)
*Ying Jiang,Jiayin Lu,Yunuo Chen,Yumeng He,Kui Wu,Yin Yang,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: 提出了一种可微分笔触重建框架，统一了绘画、风格化纹理和涂抹操作，能够真实再现人类绘画-涂抹循环过程。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法主要关注最终图像生成或基于补丁的过程模拟，缺乏明确的笔触结构，无法产生平滑逼真的阴影效果。

Method: 使用可微分笔触重建框架，通过并行可微分绘画渲染器优化单色和双色贝塞尔曲线笔触，结合风格生成模块合成几何条件纹理，并引入可微分涂抹算子实现自然色彩混合和阴影。

Result: 在油画、水彩、水墨和数字绘画上的大量实验表明，该方法能够生成逼真且富有表现力的笔触重建、平滑的色调过渡和丰富的风格化外观。

Conclusion: 该方法为表达性数字绘画创作提供了一个统一的模型，能够有效模拟人类绘画过程。

Abstract: Painting embodies a unique form of visual storytelling, where the creation process is as significant as the final artwork. Although recent advances in generative models have enabled visually compelling painting synthesis, most existing methods focus solely on final image generation or patch-based process simulation, lacking explicit stroke structure and failing to produce smooth, realistic shading. In this work, we present a differentiable stroke reconstruction framework that unifies painting, stylized texturing, and smudging to faithfully reproduce the human painting-smudging loop. Given an input image, our framework first optimizes single- and dual-color Bezier strokes through a parallel differentiable paint renderer, followed by a style generation module that synthesizes geometry-conditioned textures across diverse painting styles. We further introduce a differentiable smudge operator to enable natural color blending and shading. Coupled with a coarse-to-fine optimization strategy, our method jointly optimizes stroke geometry, color, and texture under geometric and semantic guidance. Extensive experiments on oil, watercolor, ink, and digital paintings demonstrate that our approach produces realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances, offering a unified model for expressive digital painting creation. See our project page for more demos: https://yingjiang96.github.io/DiffPaintWebsite/.

</details>


### [256] [Difficulty-Aware Label-Guided Denoising for Monocular 3D Object Detection](https://arxiv.org/abs/2511.13195)
*Soyul Lee,Seungmin Baek,Dongbo Min*

Main category: cs.CV

TL;DR: MonoDLGD是一个基于难度感知标签引导去噪的单目3D目标检测框架，通过自适应扰动和重建真实标签来提供几何监督，在KITTI基准测试中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 单目3D目标检测因深度线索模糊而存在根本性问题，现有DETR方法仍受限于不准确的深度估计，且忽略了实例级检测难度（如遮挡、距离、截断），导致检测性能不佳。

Method: 提出难度感知标签引导去噪框架，根据检测不确定性自适应扰动和重建真实标签：对简单实例应用更强扰动，对困难实例应用更弱扰动，然后重建以提供显式几何监督，联合优化标签重建和3D目标检测。

Result: 在KITTI基准测试上的广泛实验表明，MonoDLGD在所有难度级别上都达到了最先进的性能。

Conclusion: MonoDLGD通过难度感知的标签扰动和重建机制，促进了几何感知的表征学习，提高了对不同复杂度目标的鲁棒性，在单目3D目标检测中取得了优异表现。

Abstract: Monocular 3D object detection is a cost-effective solution for applications like autonomous driving and robotics, but remains fundamentally ill-posed due to inherently ambiguous depth cues. Recent DETR-based methods attempt to mitigate this through global attention and auxiliary depth prediction, yet they still struggle with inaccurate depth estimates. Moreover, these methods often overlook instance-level detection difficulty, such as occlusion, distance, and truncation, leading to suboptimal detection performance. We propose MonoDLGD, a novel Difficulty-Aware Label-Guided Denoising framework that adaptively perturbs and reconstructs ground-truth labels based on detection uncertainty. Specifically, MonoDLGD applies stronger perturbations to easier instances and weaker ones into harder cases, and then reconstructs them to effectively provide explicit geometric supervision. By jointly optimizing label reconstruction and 3D object detection, MonoDLGD encourages geometry-aware representation learning and improves robustness to varying levels of object complexity. Extensive experiments on the KITTI benchmark demonstrate that MonoDLGD achieves state-of-the-art performance across all difficulty levels.

</details>


### [257] [Self-Supervised Ultrasound Screen Detection](https://arxiv.org/abs/2511.13197)
*Alberto Gomez,Jorge Oliveira,Ramon Casero,Agis Chartsias*

Main category: cs.CV

TL;DR: 提出一种自监督管道，从超声监视器照片中提取超声图像，绕过DICOM传输瓶颈，实现快速算法测试和原型开发。


<details>
  <summary>Details</summary>
Motivation: 超声设备内置显示器显示图像，但常规传输到医院系统依赖DICOM格式，存在传输瓶颈，限制了新算法的快速测试和原型开发。

Method: 采用自监督管道从超声监视器照片中提取并校正超声图像。

Result: 在概念验证研究中，校正后的图像保留了足够的视觉保真度，与原始DICOM图像相比，心脏视图分类的平衡准确率达到0.79。

Conclusion: 该方法成功绕过DICOM传输瓶颈，为超声图像算法的快速测试和原型开发提供了可行解决方案。

Abstract: Ultrasound (US) machines display images on a built-in monitor, but routine transfer to hospital systems relies on DICOM. We propose a self-supervised pipeline to extract the US image from a photograph of the monitor. This removes the DICOM bottleneck and enables rapid testing and prototyping of new algorithms. In a proof-of-concept study, the rectified images retained enough visual fidelity to classify cardiac views with a balanced accuracy of 0.79 with respect to the native DICOMs.

</details>


### [258] [RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2511.13204)
*Junhee Lee,ChaeBeen Bang,MyoungChul Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: RefineVAD是一个弱监督视频异常检测框架，通过模仿人类感知异常的双重过程推理，同时考虑时间运动模式和语义结构来检测不同类型的异常事件。


<details>
  <summary>Details</summary>
Motivation: 现有方法将异常事件简单视为单一类别，忽略了真实世界异常事件的多样语义和时间特性。受人类感知异常方式的启发，需要同时解释不同异常类型的时间运动模式和语义结构。

Method: 框架包含两个核心模块：MoTAR模块通过基于位移的注意力和全局Transformer建模来估计运动显著性并动态调整时间焦点；CORE模块通过跨注意力将片段级特征与可学习的类别原型对齐，向表示空间注入软异常类别先验。

Result: 在WVAD基准测试上的广泛实验验证了RefineVAD的有效性，并强调了整合语义上下文以引导特征向异常相关模式细化的重要性。

Conclusion: 通过联合利用时间动态和语义结构，RefineVAD能够显式建模运动如何演化以及它类似于什么语义类别，从而更有效地检测视频异常。

Abstract: Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both "how" motion evolves and "what" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.

</details>


### [259] [End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer](https://arxiv.org/abs/2511.13208)
*Yonghui Yu,Jiahang Cai,Xun Wang,Wenwu Yang*

Main category: cs.CV

TL;DR: PAVE-Net是一个端到端的多人视频姿态估计框架，消除了传统两阶段方法中的启发式操作，通过姿态感知注意力机制实现跨帧关联，在准确性和效率上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的多人视频姿态估计方法采用两阶段流程（检测+时序建模），依赖检测、RoI裁剪和NMS等启发式操作，限制了准确性和效率。需要开发完全端到端的框架来消除这些限制。

Method: 提出PAVE-Net框架，包含空间编码器建模帧内关系和时空姿态解码器捕捉跨帧全局依赖。核心创新是姿态感知注意力机制，使每个姿态查询能选择性聚合跨连续帧中同一人的特征，并显式建模姿态关键点间的时空依赖。

Result: 在PoseTrack2017上比基于图像的端到端方法提升6.0 mAP，与最先进的两阶段视频方法精度相当，同时在效率上有显著提升。

Conclusion: PAVE-Net是首个用于多帧2D人体姿态估计的端到端方法，成功消除了传统方法中的启发式操作，在准确性和效率方面都取得了显著进步。

Abstract: Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames.Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation.Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a \textbf{6.0} mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video-based approaches, while offering significant gains in efficiency.Project page: https://github.com/zgspose/PAVENet

</details>


### [260] [3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale](https://arxiv.org/abs/2511.13211)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: 3DAlign-DAER是一个统一的文本-3D几何对齐框架，通过动态注意力策略和高效检索策略解决细粒度语义对齐问题，并在大规模3D数据库中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将细粒度文本语义与详细几何结构对齐，且在大规模3D数据库中性能显著下降。

Method: 提出动态注意力策略（DAP），使用分层注意力融合模块表示可学习的细粒度token到点注意力，并通过蒙特卡洛树搜索动态校准注意力权重；在推理阶段采用高效检索策略（ERS）进行分层搜索。

Result: 在多个基准测试中表现出优越性能，构建了包含200万文本-3D对的Align3D-2M数据集。

Conclusion: 3DAlign-DAER在文本-3D对齐任务中实现了更好的细粒度语义对齐，并在大规模场景下保持高效和准确。

Abstract: Despite recent advancements in 3D-text cross-modal alignment, existing state-of-the-art methods still struggle to align fine-grained textual semantics with detailed geometric structures, and their alignment performance degrades significantly when scaling to large-scale 3D databases. To overcome this limitation, we introduce 3DAlign-DAER, a unified framework designed to align text and 3D geometry via the proposed dynamic attention policy and the efficient retrieval strategy, capturing subtle correspondences for diverse cross-modal retrieval and classification tasks. Specifically, during the training, our proposed dynamic attention policy (DAP) employs the Hierarchical Attention Fusion (HAF) module to represent the alignment as learnable fine-grained token-to-point attentions. To optimize these attentions across different tasks and geometric hierarchies, our DAP further exploits the Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal and further enhances the alignment between textual descriptions and local 3D geometry. During the inference, our 3DAlign-DAER introduces an Efficient Retrieval Strategy (ERS) to leverage efficient hierarchical searching in the large-scale embedding spaces, outperforming traditional methods (e.g., KNN) in accuracy and efficiency. Furthermore, to facilitate text-3D alignment research and train our 3DAlign-DAER, we construct Align3D-2M, a large-scale dataset featuring 2M text-3D pairs, to provide sufficient fine-grained cross-modal annotations. Extensive and comprehensive experiments demonstrate the superior performance of our 3DAlign-DAER on diverse benchmarks. We will release our codes, models, and datasets.

</details>


### [261] [Hybrid-Domain Adaptative Representation Learning for Gaze Estimation](https://arxiv.org/abs/2511.13222)
*Qida Tan,Hongyu Yang,Wenchao Du*

Main category: cs.CV

TL;DR: 提出HARL框架，通过混合域自适应表示学习从多源数据中学习鲁棒视线表示，在跨域评估中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 解决基于外观的视线估计方法在跨域评估中因表情、佩戴物和图像质量等无关因素干扰导致的性能显著下降问题

Method: 1) 通过无监督域适应方式对齐高质量近眼图像特征来解耦视线相关表示；2) 设计稀疏图融合模块探索视线方向与头部姿态的几何约束

Result: 在EyeDiap、MPIIFaceGaze和Gaze360数据集上分别达到5.02°、3.36°和9.26°的最先进精度，并在跨数据集评估中表现出竞争力

Conclusion: HARL框架能有效学习鲁棒的视线表示，在跨域场景下显著提升视线估计性能

Abstract: Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\textbf{5.02}^{\circ}$ and $\textbf{3.36}^{\circ}$, and $\textbf{9.26}^{\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.

</details>


### [262] [MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI](https://arxiv.org/abs/2511.13232)
*Malek Al Abed,Sebiha Demir,Anne Groteklaes,Elodie Germani,Shahrooz Faghihroohi,Hemmen Sabir,Shadi Albarqouni*

Main category: cs.CV

TL;DR: MRIQT是一种3D条件扩散框架，用于将便携式超低场MRI图像质量提升到高场MRI水平，通过物理一致的K空间降级模拟、v预测和SNR加权3D感知损失，在新生儿脑部成像中实现高质量增强。


<details>
  <summary>Details</summary>
Motivation: 便携式超低场MRI在新生儿护理中具有可及性优势，但其信噪比低、诊断质量差，需要提升图像质量以达到高场MRI的诊断水平。

Method: 提出MRIQT框架，结合K空间降级模拟实现物理一致的超低场MRI模拟，使用v预测和分类器自由引导进行稳定生成，采用SNR加权的3D感知损失保持解剖保真度，基于注意力UNet架构进行结构保持的翻译。

Result: 在包含多种病理的新生儿队列上训练，MRIQT在PSNR指标上比现有最佳方法提升1.78%，医生评价85%的输出图像质量良好且病理清晰可见。

Conclusion: MRIQT能够实现便携式超低场MRI的高保真度扩散增强，为可靠的新生儿脑部评估提供支持。

Abstract: Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.

</details>


### [263] [MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection](https://arxiv.org/abs/2511.13242)
*Junjie Wu,Guohong Fu*

Main category: cs.CV

TL;DR: 提出了MMD-Thinker框架，通过自适应多维思维进行多模态虚假信息检测，解决了通用MLLM在检测中的推理不足和偏见问题。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息在AIGC时代快速演变，现有通用MLLM检测方法存在推理不足和单一思维模式导致的偏见问题，无法有效应对日益复杂的虚假信息威胁。

Method: 两阶段框架：1) 设计专门的多模态虚假信息检测思维模式；2) 通过任务特定指令调优将定制思维模式注入通用MLLM；3) 使用混合优势函数的强化学习策略增强推理能力；构建包含8K+图像-文本对的MMR数据集。

Result: 在领域内和领域外基准数据集上实现了最先进的性能，同时保持了灵活的推理和token使用效率。

Conclusion: MMD-Thinker通过自适应多维思维有效提升了多模态虚假信息检测能力，为应对快速演变的虚假信息威胁提供了有效解决方案。

Abstract: Multimodal misinformation floods on various social media, and continues to evolve in the era of AI-generated content (AIGC). The emerged misinformation with low creation cost and high deception poses significant threats to society. While recent studies leverage general-purpose multimodal large language models (MLLMs) to achieve remarkable results in detection, they encounter two critical limitations: (1) Insufficient reasoning, where general-purpose MLLMs often follow the uniform reasoning paradigm but generate inaccurate explanations and judgments, due to the lack of the task-specific knowledge of multimodal misinformation detection. (2) Reasoning biases, where a single thinking mode make detectors a suboptimal path for judgment, struggling to keep pace with the fast-growing and intricate multimodal misinformation. In this paper, we propose MMD-Thinker, a two-stage framework for multimodal misinformation detection through adaptive multi-dimensional thinking. First, we develop tailor-designed thinking mode for multimodal misinformation detection. Second, we adopt task-specific instruction tuning to inject the tailored thinking mode into general-purpose MLLMs. Third, we further leverage reinforcement learning strategy with a mixed advantage function, which incentivizes the reasoning capabilities in trajectories. Furthermore, we construct the multimodal misinformation reasoning (MMR) dataset, encompasses more than 8K image-text pairs with both reasoning processes and classification labels, to make progress in the relam of multimodal misinformation detection. Experimental results demonstrate that our proposed MMD-Thinker achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets, while maintaining flexible inference and token usage. Code will be publicly available at Github.

</details>


### [264] [Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention](https://arxiv.org/abs/2511.13249)
*Yu Wen,Shuyong Gao,Shuping Zhang,Miao Huang,Lili Tao,Han Yang,Haozhe Xing,Lihe Zhang,Boxue Hou*

Main category: cs.CV

TL;DR: 本文提出RFMNet模型，通过多阶段编码特征交互融合和重叠窗口交叉注意力机制，提升参考伪装目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究将参考图像转换为1维提示，但未充分利用参考图像中丰富的显著目标特征与伪装目标特征的融合潜力。

Method: 提出RFMNet模型，在多个编码阶段进行参考显著图像特征与伪装特征的交互融合；设计重叠窗口交叉注意力机制关注局部信息匹配；使用参考特征聚合模块进行渐进式解码分割。

Result: 在Ref-COD基准测试中取得了最先进的性能。

Conclusion: 通过多上下文融合和局部注意力机制，有效提升了参考伪装目标检测的准确性和鲁棒性。

Abstract: Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions. Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results. We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features. Therefore, we propose RFMNet, which utilizes features from multiple encoding stages of the reference salient images and performs interactive fusion with the camouflage features at the corresponding encoding stages. Given that the features in salient object images contain abundant object-related detail information, performing feature fusion within local areas is more beneficial for detecting camouflaged objects. Therefore, we propose an Overlapped Windows Cross-attention mechanism to enable the model to focus more attention on the local information matching based on reference features. Besides, we propose the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively. Extensive experiments on the Ref-COD benchmark demonstrate that our method achieves state-of-the-art performance.

</details>


### [265] [Building Egocentric Procedural AI Assistant: Methods, Benchmarks, and Challenges](https://arxiv.org/abs/2511.13261)
*Junlong Li,Huaiyuan Xu,Sijie Cheng,Kejun Wu,Kim-Hui Yap,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 该论文提出了以自我为中心的程序化AI助手(EgoProceAssist)概念，旨在通过第一人称视角逐步支持日常程序化任务，并定义了三个核心任务：错误检测、程序学习和问答。


<details>
  <summary>Details</summary>
Motivation: 受视觉语言模型(VLMs)和自我中心感知研究的进展推动，需要开发专门针对第一人称视角日常程序化任务的AI助手。

Method: 提出了新的分类法，包含三个核心任务；对现有技术、相关数据集和评估指标进行全面综述；通过实验评估代表性VLM方法；建立持续更新的公开资源库。

Result: 识别了现有VLM方法与理想EgoProceAssist之间的差距，提供了全面的技术评估和分析。

Conclusion: 讨论了未来挑战和研究方向，建立了持续更新的资源库以促进该领域发展。

Abstract: Driven by recent advances in vision language models (VLMs) and egocentric perception research, we introduce the concept of an egocentric procedural AI assistant (EgoProceAssist) tailored to step-by-step support daily procedural tasks in a first-person view. In this work, we start by identifying three core tasks: egocentric procedural error detection, egocentric procedural learning, and egocentric procedural question answering. These tasks define the essential functions of EgoProceAssist within a new taxonomy. Specifically, our work encompasses a comprehensive review of current techniques, relevant datasets, and evaluation metrics across these three core areas. To clarify the gap between the proposed EgoProceAssist and existing VLM-based AI assistants, we introduce novel experiments and provide a comprehensive evaluation of representative VLM-based methods. Based on these findings and our technical analysis, we discuss the challenges ahead and suggest future research directions. Furthermore, an exhaustive list of this study is publicly available in an active repository that continuously collects the latest work: https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant

</details>


### [266] [SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression](https://arxiv.org/abs/2511.13264)
*Keshav Gupta,Akshat Sanghvi,Shreyas Reddy Palley,Astitva Srivastava,Charu Sharma,Avinash Sharma*

Main category: cs.CV

TL;DR: SymGS是一个3D高斯泼溅压缩框架，通过引入可学习的镜像来消除局部和全局的反射冗余，实现108倍压缩率，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术在渲染速度和真实感方面表现出色，但内存占用随场景复杂度快速增长。现有压缩方法通过相似性检测和量化处理基元级冗余，但仍有压缩空间。

Method: 提出SymGS框架，引入可学习的镜像来识别和消除场景中的局部和全局反射冗余，作为现有压缩方法（如HAC）的即插即用增强组件。

Result: 相比HAC方法，在基准数据集上实现1.66倍压缩（大规模场景可达3倍），平均实现108倍压缩率，同时保持渲染质量。

Conclusion: SymGS通过对称感知技术有效解决了3D高斯泼溅的内存占用问题，显著提升了压缩效率，可作为现有压缩方法的有效补充。

Abstract: 3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \textbf{\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \times$ compression across benchmark datasets (upto $3\times$ on large-scale scenes). On an average, SymGS enables $\bf{108\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \textbf{\color{cyan}{symgs.github.io}}

</details>


### [267] [Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation](https://arxiv.org/abs/2511.13269)
*Lingfeng Zhang,Yuchen Zhang,Hongsheng Li,Haoxiang Fu,Yingbo Tang,Hangjun Ye,Long Chen,Xiaojun Liang,Xiaoshuai Hao,Wenbo Ding*

Main category: cs.CV

TL;DR: 提出了SpatialSky-Bench基准测试来评估VLMs在无人机导航中的空间智能能力，开发了SpatialSky-Dataset数据集和Sky-VLM模型，在无人机空间推理任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在无人机场景中的空间智能能力尚未充分探索，对其在动态环境中导航和解释的有效性存在担忧。

Method: 构建包含环境感知和场景理解两大类别、13个子类别的基准测试；开发包含100万样本的数据集；设计专门用于无人机空间推理的Sky-VLM模型。

Result: 主流VLMs在复杂无人机导航场景中表现不佳；Sky-VLM在所有基准测试任务中达到最先进性能。

Conclusion: Sky-VLM为开发适用于无人机场景的VLMs铺平了道路，填补了现有模型在空间智能能力方面的空白。

Abstract: Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.

</details>


### [268] [Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models](https://arxiv.org/abs/2511.13276)
*Noam Tsfaty,Avishai Weizman,Liav Cohen,Moshe Tshuva,Yehudit Aperstein*

Main category: cs.CV

TL;DR: 提出基于双主干网络和top-k池化的视频异常检测方法，仅使用视频级监督，在UCF-Crime数据集上达到90.7% AUC


<details>
  <summary>Details</summary>
Motivation: 解决在仅使用视频级监督的情况下检测罕见且多样化的监控视频异常的问题

Method: 结合卷积和transformer表示的双主干框架，通过top-k池化进行特征融合

Result: 在UCF-Crime数据集上实现了90.7%的AUC性能

Conclusion: 双主干框架能有效检测监控视频中的罕见多样化异常

Abstract: We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.

</details>


### [269] [SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting](https://arxiv.org/abs/2511.13278)
*Zihan Li,Tengfei Wang,Wentian Gan,Hao Zhan,Xin Wang,Zongqian Zhan*

Main category: cs.CV

TL;DR: SF-Recon直接从多视角图像重建轻量级建筑表面模型，无需后处理网格简化，通过3D高斯溅射、法向梯度引导优化和深度约束三角剖分实现高效重建。


<details>
  <summary>Details</summary>
Motivation: 传统多视角几何流程依赖密集重建、网格化和后续简化，过程繁琐且质量敏感，需要直接重建轻量级建筑表面模型的方法。

Method: 首先训练3D高斯溅射场获得视图一致表示，然后通过法向梯度引导的高斯优化选择与建筑边界对齐的基元，接着进行多视角边缘一致性剪枝，最后通过深度约束Delaunay三角剖分生成轻量网格。

Result: 在提出的SF数据集上，SF-Recon能够直接重建轻量级建筑模型，显著减少面和顶点数量，同时保持计算效率。

Conclusion: SF-Recon能够直接从多视角图像重建轻量级建筑表面模型，避免了传统流程的繁琐后处理步骤，在保持结构保真度的同时实现高效重建。

Abstract: Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/

</details>


### [270] [Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space](https://arxiv.org/abs/2511.13282)
*Kaiwen Wang,Kaili Zheng,Yiming Shi,Chenyi Guo,Ji Wu*

Main category: cs.CV

TL;DR: 提出DTO方法解决多人人体网格恢复中的场景一致性缺失问题，创建DTO-Humans数据集，并开发Metric-Aware HMR网络实现度量尺度的人体网格恢复。


<details>
  <summary>Details</summary>
Motivation: 现有野外人体网格伪真值生成流程是单人中心的，缺乏场景级一致性，导致同一图像中个体深度和尺度冲突。

Method: 提出深度条件平移优化(DTO)方法，在MAP框架下联合优化所有个体的相机空间平移；开发Metric-Aware HMR网络，通过相机分支和相对度量损失直接估计度量尺度的人体网格。

Result: 构建了包含56万张高质量场景一致性多人图像的DTO-Humans数据集；在相对深度推理和人体网格恢复方面达到最先进性能。

Conclusion: DTO方法能有效解决多人场景中的深度和尺度不一致问题，Metric-Aware HMR网络实现了度量尺度的人体网格恢复，显著提升了多人人体网格恢复的性能。

Abstract: Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code and data will be released publicly.

</details>


### [271] [TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing](https://arxiv.org/abs/2511.13283)
*Jongha Kim,Minseong Bae,Sanghyeok Lee,Jinsung Yoon,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: TabFlash是一个高效的多模态大语言模型，通过渐进式问题条件化、剪枝策略和token聚焦训练，在表格理解任务上实现最先进性能，同时显著降低计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 表格图像存在冗余背景区域和问题特定关注需求，现有MLLM方法忽略这些特性，导致视觉表示不具信息性和冗余。

Method: 1) 渐进式问题条件化：将问题信息以递增频率注入ViT层；2) 剪枝策略：丢弃背景token提高效率；3) token聚焦训练：鼓励模型在保留token中集中关键信息。

Result: TabFlash在表格理解任务上达到最先进性能，优于开源和专有MLLM，相比第二好的MLLM减少27% FLOPs和30%内存使用。

Conclusion: TabFlash通过结合渐进式问题条件化、剪枝和token聚焦，实现了高效且有效的表格理解，证明了这些方法在解决表格图像特有挑战方面的有效性。

Abstract: Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.

</details>


### [272] [SkyReels-Text: Fine-grained Font-Controllable Text Editing for Poster Design](https://arxiv.org/abs/2511.13285)
*Yunjie Yu,Jingchen Wu,Junchen Zhu,Chunze Lin,Guibin Chen*

Main category: cs.CV

TL;DR: SkyReels-Text是一个无需字体标签或微调的字体可控框架，能够同时编辑多个文本区域并保持非编辑区域的视觉外观，在文本保真度和视觉真实感方面达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决专业设计工作流程中细粒度、字体感知的文本编辑需求，弥合通用图像编辑与专业排版设计之间的差距。

Method: 用户只需提供裁剪的字形补丁即可控制字体样式，无需字体标签或推理时微调，支持同时编辑多个不同字体样式的文本区域。

Result: 在多个数据集（包括手写文本基准）上实现最先进的文本保真度和视觉真实感性能，提供对字体家族和风格细微差别的空前控制。

Conclusion: 该工作为专业设计工作流程提供了强大的字体可控文本编辑工具，填补了通用图像编辑与专业排版设计之间的空白。

Abstract: Artistic design such as poster design often demands rapid yet precise modification of textual content while preserving visual harmony and typographic intent, especially across diverse font styles. Although modern image editing models have grown increasingly powerful, they still fall short in fine-grained, font-aware text manipulation, limiting their utility in professional design workflows such as poster editing. To address this issue, we present SkyReels-Text, a novel font-controllable framework for precise poster text editing. Our method enables simultaneous editing of multiple text regions, each rendered in distinct typographic styles, while preserving the visual appearance of non-edited regions. Notably, our model requires neither font labels nor fine-tuning during inference: users can simply provide cropped glyph patches corresponding to their desired typography, even if the font is not included in any standard library. Extensive experiments on multiple datasets, including handwrittent text benchmarks, SkyReels-Text achieves state-of-the-art performance in both text fidelity and visual realism, offering unprecedented control over font families, and stylistic nuances. This work bridges the gap between general-purpose image editing and professional-grade typographic design.

</details>


### [273] [CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving](https://arxiv.org/abs/2511.13297)
*Enhui Ma,Lijun Zhou,Tao Tang,Jiahuan Zhang,Junpeng Jiang,Zhan Zhang,Dong Han,Kun Zhan,Xueyang Zhang,XianPeng Lang,Haiyang Sun,Xia Zhou,Di Lin,Kaicheng Yu*

Main category: cs.CV

TL;DR: 提出了一个名为CorrectAD的自校正代理系统，通过扩散模型生成与3D布局对齐的高保真视频数据，自动纠正端到端自动驾驶规划器的失败案例。


<details>
  <summary>Details</summary>
Motivation: 解决端到端自动驾驶规划方法因长尾问题（罕见但安全关键的失败案例）导致的鲁棒性不足问题。

Method: 1. 引入PM-Agent模拟产品经理角色，制定数据需求；2. 提出DriveSora生成与3D标注对齐的时空一致视频；3. 构建CorrectAD自校正代理系统，可应用于任何端到端规划器。

Result: 在nuScenes和内部数据集上，CorrectAD分别纠正了62.5%和49.8%的失败案例，碰撞率分别降低了39%和27%。

Conclusion: CorrectAD是一个端到端、模型无关的自校正系统，能有效提升自动驾驶规划器的鲁棒性。

Abstract: End-to-end planning methods are the de facto standard of the current autonomous driving system, while the robustness of the data-driven approaches suffers due to the notorious long-tail problem (i.e., rare but safety-critical failure cases). In this work, we explore whether recent diffusion-based video generation methods (a.k.a. world models), paired with structured 3D layouts, can enable a fully automated pipeline to self-correct such failure cases. We first introduce an agent to simulate the role of product manager, dubbed PM-Agent, which formulates data requirements to collect data similar to the failure cases. Then, we use a generative model that can simulate both data collection and annotation. However, existing generative models struggle to generate high-fidelity data conditioned on 3D layouts. To address this, we propose DriveSora, which can generate spatiotemporally consistent videos aligned with the 3D annotations requested by PM-Agent. We integrate these components into our self-correcting agentic system, CorrectAD. Importantly, our pipeline is an end-to-end model-agnostic and can be applied to improve any end-to-end planner. Evaluated on both nuScenes and a more challenging in-house dataset across multiple end-to-end planners, CorrectAD corrects 62.5% and 49.8% of failure cases, reducing collision rates by 39% and 27%, respectively.

</details>


### [274] [DriveLiDAR4D: Sequential and Controllable LiDAR Scene Generation for Autonomous Driving](https://arxiv.org/abs/2511.13309)
*Kaiwen Cai,Xinze Liu,Xia Zhou,Hengtong Hu,Jie Xiang,Luyao Zhang,Xueyang Zhang,Kun Zhan,Yifei Zhan,Xianpeng Lang*

Main category: cs.CV

TL;DR: DriveLiDAR4D是一个新颖的LiDAR生成流水线，能够生成时间一致的LiDAR场景，具有高度可控的前景对象和逼真的背景，在nuScenes和KITTI数据集上超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D LiDAR点云生成方法缺乏序列生成能力，无法产生精确定位的前景对象和逼真的背景，这些限制阻碍了它们的实际应用。

Method: 提出了DriveLiDAR4D，包含多模态条件和新颖的序列噪声预测模型LiDAR4DNet，能够以端到端方式实现具有完整场景操作能力的LiDAR场景序列生成。

Result: 在nuScenes数据集上获得FRD分数743.13和FVD分数16.96，相比当前最佳方法UniScene，性能分别提升了37.2%和24.1%。

Conclusion: 这是首个以端到端方式解决具有完整场景操作能力的LiDAR场景序列生成的工作，在生成质量和可控性方面都取得了显著进展。

Abstract: The generation of realistic LiDAR point clouds plays a crucial role in the development and evaluation of autonomous driving systems. Although recent methods for 3D LiDAR point cloud generation have shown significant improvements, they still face notable limitations, including the lack of sequential generation capabilities and the inability to produce accurately positioned foreground objects and realistic backgrounds. These shortcomings hinder their practical applicability. In this paper, we introduce DriveLiDAR4D, a novel LiDAR generation pipeline consisting of multimodal conditions and a novel sequential noise prediction model LiDAR4DNet, capable of producing temporally consistent LiDAR scenes with highly controllable foreground objects and realistic backgrounds. To the best of our knowledge, this is the first work to address the sequential generation of LiDAR scenes with full scene manipulation capability in an end-to-end manner. We evaluated DriveLiDAR4D on the nuScenes and KITTI datasets, where we achieved an FRD score of 743.13 and an FVD score of 16.96 on the nuScenes dataset, surpassing the current state-of-the-art (SOTA) method, UniScene, with an performance boost of 37.2% in FRD and 24.1% in FVD, respectively.

</details>


### [275] [YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection](https://arxiv.org/abs/2511.13344)
*Ori Meiraz,Sharon Shalev,Avishai Weizman*

Main category: cs.CV

TL;DR: 提出基于YOLOv9-T的混合专家框架，通过自适应路由实现动态特征专业化，相比单一YOLOv9-T模型获得更高的mAP和AR指标


<details>
  <summary>Details</summary>
Motivation: 单一目标检测模型在处理复杂场景时存在特征表达能力有限的问题，需要更灵活的架构来提升检测性能

Method: 采用混合专家框架，集成多个YOLOv9-T专家模型，通过自适应路由机制实现动态特征专业化

Result: 相比单一YOLOv9-T模型，该框架在平均精度均值(mAP)和平均召回率(AR)指标上均有提升

Conclusion: 混合专家框架能够有效提升目标检测性能，自适应路由机制是实现动态特征专业化的关键

Abstract: This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.

</details>


### [276] [What Color Is It? A Text-Interference Multimodal Hallucination Benchmark](https://arxiv.org/abs/2511.13400)
*Jinkun Zhao,Lei Huang,Wenjun Wu*

Main category: cs.CV

TL;DR: 本文构建了"What Color Is It"数据集来验证多模态大模型在颜色感知方面的视觉幻觉问题，并研究了其成因和解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型的快速发展，这些模型在视觉感知方面仍然容易受到信息干扰，特别是在颜色感知方面，这会增加幻觉风险。

Method: 通过构建"What Color Is It"数据集，使用简单方法触发多模态大模型中的单模态视觉幻觉，并基于此研究视觉幻觉的成因。

Result: 验证了多模态大模型在视觉模态中存在颜色感知方面的幻觉问题。

Conclusion: 提出了增强多模态大模型鲁棒性的潜在解决方案，以应对视觉感知中的幻觉风险。

Abstract: With the rapid advancement of Large Models, numerous text-and-vision-fused Multimodal Large Models (MLMs) have emerged. However, these MLMs remain susceptible to informational interference in visual perception, particularly in color perception, which introduces an additional risk of hallucination. To validate this hypothesis, we introduce the "What Color Is It" dataset, a novel benchmark constructed using a simple method to trigger single-modality visual hallucination in MLMs. Based on this dataset, we further investigate the underlying causes of hallucination in the visual modality of MLMs and propose potential solutions to enhance their robustness.

</details>


### [277] [Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source](https://arxiv.org/abs/2511.13417)
*Mykola Lavreniuk,Nataliia Kussul,Andrii Shelestov,Yevhenii Salii,Volodymyr Kuzin,Sergii Skakun,Zoltan Szantoi*

Main category: cs.CV

TL;DR: DelAnyFlow方法结合DelAny实例分割模型和结构化后处理流程，实现了大规模农田边界的高效准确提取，在乌克兰应用中展现出卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有农田边界提取方法存在边界不完整、相邻农田合并、难以扩展等问题，需要开发可扩展的解决方案来支持土地管理和作物监测。

Method: 基于YOLOv11骨干网络的DelAny实例分割模型，结合FBIS 22M大规模数据集训练，通过结构化后处理、合并和矢量化序列生成拓扑一致的矢量边界。

Result: DelAny模型比SAM2精度提升100%以上，推理速度快400倍；在乌克兰应用中，DelAnyFlow在6小时内生成了603,000km²的完整农田边界层，识别出3.75M-5.15M个农田。

Conclusion: DelAnyFlow为缺乏数字地籍数据的地区提供了可扩展、成本效益高的农田边界提取方法，显著优于现有操作产品。

Abstract: Accurate delineation of agricultural field boundaries from satellite imagery is essential for land management and crop monitoring, yet existing methods often produce incomplete boundaries, merge adjacent fields, and struggle to scale. We present the Delineate Anything Flow (DelAnyFlow) methodology, a resolution-agnostic approach for large-scale field boundary mapping. DelAnyFlow combines the DelAny instance segmentation model, based on a YOLOv11 backbone and trained on the large-scale Field Boundary Instance Segmentation-22M (FBIS 22M) dataset, with a structured post-processing, merging, and vectorization sequence to generate topologically consistent vector boundaries. FBIS 22M, the largest dataset of its kind, contains 672,909 multi-resolution image patches (0.25-10m) and 22.9million validated field instances. The DelAny model delivers state-of-the-art accuracy with over 100% higher mAP and 400x faster inference than SAM2. DelAny demonstrates strong zero-shot generalization and supports national-scale applications: using Sentinel 2 data for 2024, DelAnyFlow generated a complete field boundary layer for Ukraine (603,000km2) in under six hours on a single workstation. DelAnyFlow outputs significantly improve boundary completeness relative to operational products from Sinergise Solutions and NASA Harvest, particularly in smallholder and fragmented systems (0.25-1ha). For Ukraine, DelAnyFlow delineated 3.75M fields at 5m and 5.15M at 2.5m, compared to 2.66M detected by Sinergise Solutions and 1.69M by NASA Harvest. This work delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data. A project landing page with links to model weights, code, national-scale vector outputs, and dataset is available at https://lavreniuk.github.io/Delineate-Anything/.

</details>


### [278] [VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task](https://arxiv.org/abs/2511.13420)
*Xingming Long,Jie Zhang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 提出了VOPE方法，用于评估大型视觉语言模型在自愿想象任务中的幻觉现象，发现现有模型在想象任务中普遍存在严重幻觉，且现有缓解方法效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注禁止生成图像中不存在内容的事实描述任务中的幻觉，但忽略了自愿想象任务（如故事创作）中的幻觉评估问题，这类任务中模型被期望生成超越给定图像的新内容。

Method: 提出了VOPE方法，通过提出重新检查式问题来评估LVLM如何解释其响应中想象对象的存在性，根据模型解释与图像中对象存在性的一致性来判断是否产生幻觉。

Result: 应用VOPE评估多个主流LVLM和幻觉缓解方法发现：（1）大多数LVLM在自愿想象中严重幻觉，对想象对象的存在性评估表现差；（2）现有幻觉缓解方法在自愿想象任务中效果有限。

Conclusion: 自愿想象任务中的幻觉是一个重要研究方向，现有方法对此类幻觉缓解效果不佳，需要进一步研究。

Abstract: Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs' hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model's interpretation and the object's presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.

</details>


### [279] [FUSE: A Flow-based Mapping Between Shapes](https://arxiv.org/abs/2511.13431)
*Lorenzo Olearo,Giulio Viganò,Daniele Baieri,Filippo Maggioli,Simone Melzi*

Main category: cs.CV

TL;DR: 提出基于流匹配模型的3D形状间映射神经表示，支持跨表示形状匹配，无需大规模训练或数据驱动过程。


<details>
  <summary>Details</summary>
Motivation: 开发计算高效、支持跨表示形状匹配的神经表示方法，避免传统方法需要大规模训练或数据驱动的问题。

Method: 将3D形状表示为从固定锚分布通过连续可逆流映射诱导的概率分布，通过源到锚的逆流与锚到目标的正向流组合实现形状间连续映射。

Result: 在多样化基准测试和挑战性设置中一致实现高覆盖率和准确性，在人体原始点云扫描的UV映射和配准等任务中也显示良好结果。

Conclusion: 该方法提供了一种可逆且模态无关的形状映射表示，在形状匹配及相关任务中表现优异。

Abstract: We introduce a novel neural representation for maps between 3D shapes based on flow-matching models, which is computationally efficient and supports cross-representation shape matching without large-scale training or data-driven procedures. 3D shapes are represented as the probability distribution induced by a continuous and invertible flow mapping from a fixed anchor distribution. Given a source and a target shape, the composition of the inverse flow (source to anchor) with the forward flow (anchor to target), we continuously map points between the two surfaces. By encoding the shapes with a pointwise task-tailored embedding, this construction provides an invertible and modality-agnostic representation of maps between shapes across point clouds, meshes, signed distance fields (SDFs), and volumetric data. The resulting representation consistently achieves high coverage and accuracy across diverse benchmarks and challenging settings in shape matching. Beyond shape matching, our framework shows promising results in other tasks, including UV mapping and registration of raw point cloud scans of human bodies.

</details>


### [280] [InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE](https://arxiv.org/abs/2511.13488)
*Lipeng Wang,Hongxing Fan,Haohua Chen,Zehuan Huang,Lu Sheng*

Main category: cs.CV

TL;DR: InterMoE是一个基于动态时间选择性专家混合的新框架，用于生成高质量的人类交互动作，在保持个体特征和文本语义保真度方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成人类交互时往往无法保持独特的个体特征或完全遵循文本描述，这限制了在虚拟现实和机器人等应用中的价值。

Method: 基于动态时间选择性专家混合框架，核心是路由机制，协同使用高级文本语义和低级运动上下文，将时间运动特征分配给专门专家，让专家动态确定选择能力并关注关键时间特征。

Result: 在InterHuman数据集上FID分数降低9%，在InterX数据集上降低22%，在个体特定高保真3D人类交互生成方面达到最先进性能。

Conclusion: InterMoE能够有效保持特定个体特征身份，同时确保高语义保真度，为高质量人类交互生成提供了有效解决方案。

Abstract: Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.

</details>


### [281] [Language-Guided Invariance Probing of Vision-Language Models](https://arxiv.org/abs/2511.13494)
*Jae Joong Lee*

Main category: cs.CV

TL;DR: LGIP基准测试评估视觉语言模型对语义保持改写和语义翻转的鲁棒性，发现EVA02-CLIP和大型OpenCLIP变体在不变性和敏感性方面表现最佳，而SigLIP模型存在显著问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在零样本任务中表现良好，但其对受控语言扰动的响应可靠性尚不清楚，需要评估模型对语义保持改写的不变性和对语义翻转的敏感性。

Method: 使用40k MS COCO图像和五个人类标注，自动生成语义保持的改写和基于规则的语义翻转（改变对象类别、颜色或数量），通过不变性误差、语义敏感性差距和正率统计来总结模型行为。

Result: EVA02-CLIP和大型OpenCLIP变体在不变性-敏感性边界上表现最佳，而SigLIP和SigLIP2显示出更大的不变性误差，并且经常偏好翻转后的描述而非人类原始描述。

Conclusion: LGIP提供了模型无关的诊断工具，能够揭示标准检索指标无法发现的视觉语言模型语言鲁棒性问题。

Abstract: Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.
  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.

</details>


### [282] [Mapping the Vanishing and Transformation of Urban Villages in China](https://arxiv.org/abs/2511.13507)
*Wenyu Zhang,Yao Tong,Yiqiu Liu,Rui Cao*

Main category: cs.CV

TL;DR: 本研究提出基于深度学习的框架监测中国城中村的时空变化，发现城中村改造过程漫长、主要发生在城市外围区域，并揭示了三种时空转型路径，强调需要分层和情境敏感的规划策略。


<details>
  <summary>Details</summary>
Motivation: 中国城中村经历了大规模拆除重建，但缺乏对拆除土地是否有效再利用的系统评估，引发了当前改造实践效率和可持续性的担忧。

Method: 使用多时相遥感影像的语义分割来绘制城中村边界变化，然后将拆除后土地利用分为六类：未完全拆除、闲置土地、建筑工地、建筑物、绿地和其他。

Result: 1) 城中村改造过程经常延长；2) 改造主要发生在城市外围区域，而城市核心区相对稳定；3) 揭示了三种时空转型路径：同步改造、延迟改造和逐步优化。

Conclusion: 研究强调了城中村改造的碎片化、复杂性和非线性特征，需要分层和情境敏感的规划策略，为更包容、高效和可持续的城市更新提供实证见解。

Abstract: Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the "remained-demolished-redeveloped" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.

</details>


### [283] [Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems](https://arxiv.org/abs/2511.13533)
*Jeffrey Wen,Rizwan Ahmad,Philip Schniter*

Main category: cs.CV

TL;DR: 提出了一种渐近极小极大方法用于多目标保形预测，在保证联合边际覆盖的同时提供紧密的预测区间，并应用于多度量盲图像质量评估、多任务不确定性量化和多轮测量采集。


<details>
  <summary>Details</summary>
Motivation: 在不适定的成像逆问题中，不确定性量化是一个基本挑战。现有方法只能处理标量估计目标，而实际应用通常涉及多个目标，因此需要开发多目标保形预测方法。

Method: 采用渐近极小极大方法进行多目标保形预测，确保联合边际覆盖的同时提供紧密的预测区间。该方法可应用于多度量盲图像质量评估、多任务不确定性量化和多轮测量采集。

Result: 通过合成数据和磁共振成像数据的数值实验证明，该方法相对于现有的多目标保形预测方法具有优势。

Conclusion: 提出的极小极大方法在多目标保形预测中能够提供更紧密的预测区间，同时保证联合边际覆盖，在成像逆问题的不确定性量化中具有实际应用价值。

Abstract: In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.

</details>


### [284] [Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew](https://arxiv.org/abs/2511.13535)
*Farhin Farhad Riya,Shahinul Hoque,Jinyuan Stella Sun,Olivera Kotevska*

Main category: cs.CV

TL;DR: 该论文揭示了一种新的攻击类型，通过联邦学习中的微小颜色扰动来破坏模型可解释性而不影响准确性，挑战了模型审计中"正确预测意味着忠实解释"的假设。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在安全关键领域的部署，视觉解释技术对透明度支持变得至关重要。研究发现模型可解释性本身可能成为攻击面，而标准训练流程无法检测或缓解解释退化问题。

Method: 提出了基于色度扰动模块的显著性感知攻击框架，通过改变前景和背景之间的颜色对比度来系统性地制作对抗样本，这些扰动在训练轮次中累积，以隐蔽持久的方式毒化全局模型的内部特征归因。

Result: 攻击将Grad-CAM解释中的峰值激活重叠减少了高达35%，同时在所有评估数据集上保持分类准确率超过96%。

Conclusion: 该研究挑战了模型审计中的常见假设，证明正确预测并不一定意味着忠实解释，可解释性本身可以成为攻击面，特别是在联邦学习环境中，细微的颜色扰动更难被发现。

Abstract: As machine learning models are increasingly deployed in safety-critical domains, visual explanation techniques have become essential tools for supporting transparency. In this work, we reveal a new class of attacks that compromise model interpretability without affecting accuracy. Specifically, we show that small color perturbations applied by adversarial clients in a federated learning setting can shift a model's saliency maps away from semantically meaningful regions while keeping the prediction unchanged. The proposed saliency-aware attack framework, called Chromatic Perturbation Module, systematically crafts adversarial examples by altering the color contrast between foreground and background in a way that disrupts explanation fidelity. These perturbations accumulate across training rounds, poisoning the global model's internal feature attributions in a stealthy and persistent manner. Our findings challenge a common assumption in model auditing that correct predictions imply faithful explanations and demonstrate that interpretability itself can be an attack surface. We evaluate this vulnerability across multiple datasets and show that standard training pipelines are insufficient to detect or mitigate explanation degradation, especially in the federated learning setting, where subtle color perturbations are harder to discern. Our attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% on all evaluated datasets.

</details>


### [285] [BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse](https://arxiv.org/abs/2511.13539)
*Yuanchao Wang,Tian Qin,Eduardo Valle,Bruno Abrahao*

Main category: cs.CV

TL;DR: BootOOD是一个完全自监督的OOD检测框架，通过从ID数据中合成伪OOD特征，并利用神经崩溃现象，使用基于特征范数的轻量级辅助头进行OOD检测。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测器在处理与ID类别语义相似的OOD样本时表现不佳，需要一种能有效处理语义挑战性OOD样本的方法。

Method: 通过ID特征变换合成伪OOD特征，利用神经崩溃现象，设计基于特征范数的轻量级辅助分类头，将OOD检测与主分类器解耦。

Result: 在CIFAR-10、CIFAR-100和ImageNet-200上的实验表明，BootOOD优于现有后处理方法，超越无需异常暴露的训练方法，与最先进的异常暴露方法竞争力相当，同时保持或提高ID准确率。

Conclusion: BootOOD提供了一种有效的自监督OOD检测方法，特别适用于处理语义相似的OOD样本，且不影响ID分类性能。

Abstract: Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.

</details>


### [286] [TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images](https://arxiv.org/abs/2511.13552)
*Sining Chen,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 提出了TSE-Net，一种用于半监督单目高度估计的自训练框架，通过教师-学生-考试网络结构利用未标记数据提升性能。


<details>
  <summary>Details</summary>
Motivation: 单目高度估计在遥感3D感知中很重要，但现有方法受限于标记数据的稀缺性和获取成本高的问题。

Method: 使用教师-学生-考试网络框架：教师网络生成伪标签，学生网络在未标记数据上训练，考试网络作为学生网络的时间集成以稳定性能。教师网络采用回归和分类联合建模，通过分层双切策略处理高度值的长尾分布。

Result: 在三个不同分辨率和成像模式的数据集上进行了评估，代码已开源。

Conclusion: 提出的半监督学习框架能够有效利用未标记数据，提升单目高度估计的性能和泛化能力。

Abstract: Monocular height estimation plays a critical role in 3D perception for remote sensing, offering a cost-effective alternative to multi-view or LiDAR-based methods. While deep learning has significantly advanced the capabilities of monocular height estimation, these methods remain fundamentally limited by the availability of labeled data, which are expensive and labor-intensive to obtain at scale. The scarcity of high-quality annotations hinders the generalization and performance of existing models. To overcome this limitation, we propose leveraging large volumes of unlabeled data through a semi-supervised learning framework, enabling the model to extract informative cues from unlabeled samples and improve its predictive performance. In this work, we introduce TSE-Net, a self-training pipeline for semi-supervised monocular height estimation. The pipeline integrates teacher, student, and exam networks. The student network is trained on unlabeled data using pseudo-labels generated by the teacher network, while the exam network functions as a temporal ensemble of the student network to stabilize performance. The teacher network is formulated as a joint regression and classification model: the regression branch predicts height values that serve as pseudo-labels, and the classification branch predicts height value classes along with class probabilities, which are used to filter pseudo-labels. Height value classes are defined using a hierarchical bi-cut strategy to address the inherent long-tailed distribution of heights, and the predicted class probabilities are calibrated with a Plackett-Luce model to reflect the expected accuracy of pseudo-labels. We evaluate the proposed pipeline on three datasets spanning different resolutions and imaging modalities. Codes are available at https://github.com/zhu-xlab/tse-net.

</details>


### [287] [Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation](https://arxiv.org/abs/2511.13571)
*Ziyang Huang,Jiagang Chen,Jin Liu,Shunping Ji*

Main category: cs.CV

TL;DR: Opt3DGS是一个增强3D高斯泼溅(3DGS)优化的两阶段框架，通过自适应探索和曲率引导开发来解决局部最优和收敛质量不足的问题。


<details>
  <summary>Details</summary>
Motivation: 3DGS在新视角合成中表现出色，但其核心优化问题未被充分探索，存在陷入局部最优和收敛质量不足两个关键问题。

Method: 提出两阶段优化：探索阶段使用自适应加权随机梯度朗之万动力学(SGLD)增强全局搜索；开发阶段使用局部拟牛顿方向引导的Adam优化器利用曲率信息进行精确收敛。

Result: 在多个基准数据集上的广泛实验表明，Opt3DGS在不改变3DGS底层表示的情况下，通过优化过程实现了最先进的渲染质量。

Conclusion: Opt3DGS通过改进3DGS的优化过程，有效解决了其优化挑战，显著提升了渲染质量。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.

</details>


### [288] [Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images](https://arxiv.org/abs/2511.13586)
*Yinuo Xu,Yan Cui,Mingyao Li,Zhi Huang*

Main category: cs.CV

TL;DR: NuClass是一个病理学家工作流启发的框架，通过多尺度整合核形态和微环境上下文进行细胞级分类，解决了现有方法缺乏组织上下文和细粒度标注的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像块的模型能捕获详细的核形态但缺乏更广泛的组织上下文，且可用的人类标注通常是粗粒度和分布不均的，难以获得细粒度的亚型级监督。

Method: NuClass包含两个主要组件：Path local（关注224×224像素裁剪的核形态）和Path global（建模1024×1024像素邻域环境），通过可学习门控模块自适应平衡局部细节和上下文线索，并采用不确定性引导目标促进互补学习。

Result: 在三个完全保留的队列上评估，NuClass最佳类别达到96%的F1分数，优于强基线方法。

Conclusion: 多尺度、不确定性感知的融合可以弥合幻灯片级病理基础模型与可靠的细胞级表型预测之间的差距。

Abstract: Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.
  To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.
  To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction.

</details>


### [289] [ICLR: Inter-Chrominance and Luminance Interaction for Natural Color Restoration in Low-Light Image Enhancement](https://arxiv.org/abs/2511.13607)
*Xin Xu,Hao Liu,Wei Liu,Wei Wang,Jiayi Wu,Kui Jiang*

Main category: cs.CV

TL;DR: 提出ICLR框架，通过双流交互增强模块和协方差校正损失，解决低光图像增强中色度与亮度分支交互的分布差异和梯度冲突问题。


<details>
  <summary>Details</summary>
Motivation: 自然图像中色度与亮度分支存在显著分布差异，限制了互补特征提取；大均匀色区域中色度分支间相关性弱，传统像素级损失导致梯度冲突。

Method: ICLR框架包含双流交互增强模块(DIEM)从融合和增强两个维度提取互补信息，协方差校正损失(CCL)利用亮度残差统计惩罚色度误差并约束色度分支协方差。

Result: 在多个数据集上的实验结果表明，提出的ICLR框架优于现有最先进方法。

Conclusion: ICLR框架有效解决了低光图像增强中的色度-亮度交互问题，通过改进的交互机制和损失函数实现了更好的性能。

Abstract: Low-Light Image Enhancement (LLIE) task aims at improving contrast while restoring details and textures for images captured in low-light conditions. HVI color space has made significant progress in this task by enabling precise decoupling of chrominance and luminance. However, for the interaction of chrominance and luminance branches, substantial distributional differences between the two branches prevalent in natural images limit complementary feature extraction, and luminance errors are propagated to chrominance channels through the nonlinear parameter. Furthermore, for interaction between different chrominance branches, images with large homogeneous-color regions usually exhibit weak correlation between chrominance branches due to concentrated distributions. Traditional pixel-wise losses exploit strong inter-branch correlations for co-optimization, causing gradient conflicts in weakly correlated regions. Therefore, we propose an Inter-Chrominance and Luminance Interaction (ICLR) framework including a Dual-stream Interaction Enhancement Module (DIEM) and a Covariance Correction Loss (CCL). The DIEM improves the extraction of complementary information from two dimensions, fusion and enhancement, respectively. The CCL utilizes luminance residual statistics to penalize chrominance errors and balances gradient conflicts by constraining chrominance branches covariance. Experimental results on multiple datasets show that the proposed ICLR framework outperforms state-of-the-art methods.

</details>


### [290] [AtlasMorph: Learning conditional deformable templates for brain MRI](https://arxiv.org/abs/2511.13609)
*Marianne Rakic,Andrew Hoopes,S. Mazdak Abulnaga,Mert R. Sabuncu,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: 提出一种基于卷积配准神经网络的机器学习框架，能够根据特定属性（如年龄、性别）生成条件化模板，并利用分割信息生成解剖标签图，提高医学图像分析的准确性和代表性。


<details>
  <summary>Details</summary>
Motivation: 现有的可变形模板数量有限且计算成本高，往往无法真正代表研究人群，特别是在人群变异较大时，使用不具代表性的模板会影响分析质量。

Method: 使用卷积配准神经网络学习一个函数，该函数能够根据特定属性（年龄、性别等）输出条件化模板，并在有分割数据时生成相应的解剖分割图，同时网络还可用于将受试者图像配准到模板。

Result: 在3D脑部MRI数据集上的实验表明，该方法能够学习到高质量且具有代表性的模板，带标注的条件模板比无标签的无条件模板具有更好的配准效果，并优于其他模板构建方法。

Conclusion: 提出的条件化模板学习框架能够生成更具代表性的解剖模板，提高医学图像配准和分析的准确性，为人群研究提供更好的工具。

Abstract: Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.

</details>


### [291] [Tissue Aware Nuclei Detection and Classification Model for Histopathology Images](https://arxiv.org/abs/2511.13615)
*Kesi Xu,Eleni Chiou,Ali Varamesh,Laura Acqualagna,Nasir Rajpoot*

Main category: cs.CV

TL;DR: TAND是一个新颖的组织感知细胞核检测框架，通过组织掩码条件化增强点级监督，实现联合细胞核检测和分类，在PUMA基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖详细专家标注且未能充分利用组织上下文信息，需要减少标注负担并提高细胞核检测和分类的准确性。

Method: 结合ConvNeXt编码器-解码器与冻结的Virchow-2组织分割分支，通过新颖的多尺度空间特征线性调制选择性调节分类流。

Result: 在PUMA基准测试中达到最先进性能，超越组织无关基线和掩码监督方法，特别是在组织依赖性细胞类型上表现显著提升。

Conclusion: 这是首个基于学习组织掩码进行单细胞分类条件化的方法，为减少标注负担提供了实用途径。

Abstract: Accurate nuclei detection and classification are fundamental to computational pathology, yet existing approaches are hindered by reliance on detailed expert annotations and insufficient use of tissue context. We present Tissue-Aware Nuclei Detection (TAND), a novel framework achieving joint nuclei detection and classification using point-level supervision enhanced by tissue mask conditioning. TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, where semantic tissue probabilities selectively modulate the classification stream through a novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM). On the PUMA benchmark, TAND achieves state-of-the-art performance, surpassing both tissue-agnostic baselines and mask-supervised methods. Notably, our approach demonstrates remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma. To the best of our knowledge, this is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden.

</details>


### [292] [A Real-Time Driver Drowsiness Detection System Using MediaPipe and Eye Aspect Ratio](https://arxiv.org/abs/2511.13618)
*Ashlesha G. Sawant,Shreyash S. Kamble,Raj S. Kanade,Raunak N. Kanugo,Tanishq A. Kapse,Karan A. Bhapse*

Main category: cs.CV

TL;DR: 开发基于面部特征和眼部运动的驾驶员疲劳检测系统，通过摄像头实时监测驾驶员状态，当检测到疲劳迹象时发出警报


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是导致道路事故的主要原因之一，每年造成大量伤亡，需要开发有效的检测系统来提高道路安全

Method: 使用标准网络摄像头和MediaPipe Face Mesh框架实时追踪面部特征，重点关注眼部运动，采用眼部纵横比(EAR)方法检测眨眼频率和闭眼时长

Result: 实验分析表明系统具有高准确性和快速响应能力，能够有效检测疲劳状态

Conclusion: 该系统提供了一种高性能、低成本的驾驶员监控解决方案，可作为高级驾驶辅助系统(ADAS)的组成部分

Abstract: One of the major causes of road accidents is driver fatigue that causes thousands of fatalities and injuries every year. This study shows development of a Driver Drowsiness Detection System meant to improve the safety of the road by alerting drivers who are showing signs of being drowsy. The system is based on a standard webcam that tracks the facial features of the driver with the main emphasis on the examination of eye movements that can be conducted with the help of the Eye Aspect Ratio (EAR) method. The Face Mesh by MediaPipe is a lightweight framework that can identify facial landmarks with high accuracy and efficiency, which is considered to be important in real time use. The system detects the moments of long eye shutdowns or a very low rate of blinking which are manifestations of drowsiness and alerts the driver through sound to get her attention back. This system achieves a high-performance and low-cost driver monitoring solution with the help of the computational power of OpenCV to process the image and the MediaPipe to identify faces. Test data experimental analyses indicate that the system is very accurate and responds quicker; this confirms that it can be a component of the current Advanced Driving Assistance System (ADAS).

</details>


### [293] [CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding](https://arxiv.org/abs/2511.13644)
*Shrenik Patel,Daivik Patel*

Main category: cs.CV

TL;DR: CacheFlow是一种无需训练的长视频问答解决方案，通过动态令牌丢弃和压缩长期记忆机制，显著减少处理令牌数量，提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在处理长视频问答时面临注意力机制和KV缓存随运行时间增长的问题，导致推理成本高昂或只能使用短视的滑动窗口方法。

Method: 结合动态令牌丢弃（基于帧间余弦相似度在线修剪令牌）和压缩长期记忆（使用小型循环编码器总结关键信息），采用基于共识的检索机制选择最相关块进行注意力计算。

Result: 在离线和流式视频问答基准测试中表现优于现有基线方法，同时处理令牌数量减少高达87%。

Conclusion: CacheFlow实现了视觉语言模型在长视频理解中的高效性和上下文感知能力，为实际应用铺平了道路。

Abstract: Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.

</details>


### [294] [Part-X-MLLM: Part-aware 3D Multimodal Large Language Model](https://arxiv.org/abs/2511.13647)
*Chunshi Wang,Junliang Ye,Yunhan Yang,Yang Li,Zizhuo Lin,Jun Zhu,Zhuo Chen,Yawei Luo,Chunchao Guo*

Main category: cs.CV

TL;DR: Part-X-MLLM是一个原生3D多模态大语言模型，通过结构化可执行语法将多样化3D任务统一为程序，能够从RGB点云和自然语言提示生成包含部件级边界框、语义描述和编辑命令的连贯标记序列。


<details>
  <summary>Details</summary>
Motivation: 旨在统一多样化的3D任务，通过将任务表示为结构化程序，实现符号规划与几何合成的解耦，使任何兼容的几何引擎都能通过单一语言前端进行控制。

Method: 采用双编码器架构进行预训练，分离结构与语义，并在大规模部件中心数据集上进行指令调优，生成结构化输出作为驱动下游几何感知模块的接口。

Result: 实验表明该模型在生成高质量结构化规划方面表现出色，通过统一接口在接地问答、组合生成和局部化编辑方面实现了最先进的性能。

Conclusion: Part-X-MLLM通过结构化程序表示实现了3D多模态任务的统一，为几何感知应用提供了强大的语言原生前端接口。

Abstract: We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/

</details>


### [295] [PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image](https://arxiv.org/abs/2511.13648)
*Ziang Cao,Fangzhou Hong,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: PhysX-Anything是首个面向仿真的物理3D生成框架，能从单张图像生成具有明确几何、关节和物理属性的高质量仿真就绪3D资产，显著提升了在具身AI中的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法大多忽略了物理和关节属性，限制了在具身AI中的应用。需要将3D建模从静态视觉表示转向可直接用于仿真和交互的物理化、可关节化资产。

Method: 提出了首个基于VLM的物理3D生成模型，采用新的3D表示方法将几何体token化，token数量减少193倍；构建了PhysX-Mobility数据集，包含2000多个常见真实世界物体，物理标注丰富。

Result: 在PhysX-Mobility数据集和真实世界图像上的实验表明，PhysX-Anything具有强大的生成性能和鲁棒泛化能力；在MuJoCo风格环境中的仿真实验验证了资产可直接用于接触密集的机器人策略学习。

Conclusion: PhysX-Anything能够显著赋能下游应用，特别是在具身AI和基于物理的仿真领域，为3D生成向物理化、仿真就绪方向的发展提供了重要推动力。

Abstract: 3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.

</details>


### [296] [Distribution Matching Distillation Meets Reinforcement Learning](https://arxiv.org/abs/2511.13649)
*Dengyang Jiang,Dongyang Liu,Zanyi Wang,Qilong Wu,Xin Jin,David Liu,Zhen Li,Mengmeng Wang,Peng Gao,Harry Yang*

Main category: cs.CV

TL;DR: DMDR结合强化学习到蒸馏过程中，通过DMD损失作为正则化，同时进行蒸馏和RL训练，解锁少步生成器的能力，超越多步教师模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统DMD方法中少步生成器的性能受限于多步教师模型，需要突破这一限制。

Method: 将强化学习技术融入蒸馏过程，使用DMD损失作为正则化，设计动态分布引导和动态重噪声采样训练策略。

Result: 在少步方法中实现领先的视觉质量、提示一致性，甚至超越多步教师模型的性能。

Conclusion: DMDR框架成功结合蒸馏和强化学习，解锁了少步生成器的潜力，实现了性能突破。

Abstract: Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.

</details>


### [297] [OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation](https://arxiv.org/abs/2511.13655)
*Henry Herzog,Favyen Bastani,Yawen Zhang,Gabriel Tseng,Joseph Redmon,Hadrien Sablon,Ryan Park,Jacob Morrison,Alexandra Buraczynski,Karen Farley,Joshua Hansen,Andrew Howe,Patrick Alan Johnson,Mark Otterlee,Ted Schmitt,Hunter Pitelka,Stephen Daspit,Rachel Ratner,Christopher Wilhelm,Sebastian Wood,Mike Jacobi,Hannah Kerner,Evan Shelhamer,Ali Farhadi,Ranjay Krishna,Patrick Beukema*

Main category: cs.CV

TL;DR: OlmoEarth是一个用于地球观测的多模态时空基础模型，通过创新的自监督学习、掩码策略和损失函数设计，在多个基准测试和实际任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 地球观测数据具有空间性（如图像）、时序性（如视频或文本）和多模态特性，需要专门的基础模型来处理这些复杂特征。

Method: 采用新颖的自监督学习框架、专门设计的掩码策略和损失函数，针对地球观测领域进行优化。

Result: 在24个任务中的15个任务上获得最佳嵌入性能，在29个任务中的19个任务上通过微调获得最佳性能，优于其他12个基础模型。

Conclusion: OlmoEarth作为端到端平台的核心，为非营利组织和NGO提供前沿的基础模型和强大数据管理工具，以解决全球重大问题。

Abstract: Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\href{https://github.com/allenai/olmoearth_pretrain}{\text{https://github.com/allenai/olmoearth_pretrain}}$.

</details>


### [298] [Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting](https://arxiv.org/abs/2511.13684)
*Jiangnan Ye,Jiedong Zhuang,Lianrui Mu,Wenjie Zheng,Jiaqi Hu,Xingze Zou,Jing Wang,Haoji Hu*

Main category: cs.CV

TL;DR: GS-Light是一个基于高斯泼溅的文本引导3D场景重照明方法，通过训练免费的扩散模型处理多视图输入，结合大视觉语言模型解析光照先验，生成高质量的重照明3D场景。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景重照明方法在处理复杂光照条件和用户意图时存在局限性，需要一种能够准确理解文本描述并生成符合预期光照效果的自动化解决方案。

Method: 使用大视觉语言模型解析文本提示为光照先验，结合几何和语义估计器生成光照图，通过多视图重照明模型生成重照明图像，最后微调3D高斯泼溅场景。

Result: 在室内外场景评估中，GS-Light在定量指标（多视图一致性、图像质量、美学评分、语义相似度等）和定性评估（用户研究）上均优于现有基线方法。

Conclusion: GS-Light提供了一种高效、文本位置感知的3D场景重照明管道，能够准确理解用户意图并生成高质量的重照明结果，在多个评估维度上优于现有方法。

Abstract: We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.

</details>


### [299] [TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models](https://arxiv.org/abs/2511.13704)
*Harold Haodong Chen,Disen Lan,Wen-Jie Shu,Qingyang Liu,Zihan Wang,Sirui Chen,Wenkai Cheng,Kanghao Chen,Hongfei Zhang,Zixin Zhang,Rongjin Guo,Yu Cheng,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出了TiViBench基准来评估图像到视频生成模型的推理能力，涵盖结构推理、空间模式推理、逻辑推理和行动规划四个维度，并开发了VideoTPO测试时优化策略来提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型评估主要关注视觉保真度和时间一致性，缺乏对高阶推理能力的评估，需要专门基准来衡量模型的物理合理性和逻辑一致性推理能力。

Method: 构建TiViBench分层基准，系统评估4个推理维度的24个任务场景；提出VideoTPO测试时策略，通过LLM自分析生成候选视频来识别优劣，无需额外训练或奖励模型。

Result: 商业模型（如Sora 2、Veo 3.1）展现出更强的推理潜力，开源模型因训练规模和数据多样性限制而潜力未充分挖掘；VideoTPO显著提升了推理性能。

Conclusion: TiViBench和VideoTPO为评估和推进视频生成模型的推理能力奠定了基础，为这一新兴领域的未来研究设定了框架。

Abstract: The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.

</details>


### [300] [Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine](https://arxiv.org/abs/2511.13713)
*Xincheng Shuai,Zhenyuan Qin,Henghui Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: FFSE是一个3D感知的自回归框架，能够在真实图像上实现直观、物理一致的3D对象编辑，通过建模为3D变换序列来支持平移、缩放、旋转等操作，同时保持背景效果和场景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在语义图像编辑方面取得进展，但大多数方法难以实现3D感知的对象操作，要么在图像空间操作，要么需要缓慢且容易出错的3D重建。

Method: FFSE将编辑建模为学习到的3D变换序列，引入3DObjectEditor混合数据集，通过模拟不同对象和场景的编辑序列来支持多轮3D感知对象操作的学习。

Result: 大量实验表明，FFSE在单轮和多轮3D感知编辑场景中显著优于现有方法。

Conclusion: FFSE框架能够实现直观、物理一致的3D对象编辑，在保持背景效果和全局场景一致性的同时，支持多轮编辑操作。

Abstract: Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.

</details>


### [301] [Segment Anything Across Shots: A Method and Benchmark](https://arxiv.org/abs/2511.13715)
*Hengrui Hu,Kaining Ying,Henghui Ding*

Main category: cs.CV

TL;DR: 本文提出了一种用于多镜头半监督视频对象分割（MVOS）的新方法SAAS，通过过渡模拟数据增强策略和跨镜头理解模型，解决了现有方法在镜头转换时的性能下降问题，并在新基准Cut-VOS上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的视频对象分割方法主要针对单镜头视频，在镜头转换时性能显著下降，限制了其在实际应用中的适用性。由于标注的多镜头数据稀缺，需要开发能够有效处理镜头转换的方法。

Method: 提出了过渡模拟数据增强策略（TMA），利用单镜头数据实现跨镜头泛化；开发了Segment Anything Across Shots（SAAS）模型，能够有效检测和理解镜头转换；并构建了新的MVOS基准数据集Cut-VOS。

Result: 在YouMVOS和Cut-VOS数据集上的广泛实验表明，SAAS通过有效模拟、理解和分割复杂转换，实现了最先进的性能。

Conclusion: SAAS模型通过创新的数据增强策略和跨镜头理解能力，成功解决了多镜头视频对象分割中的镜头转换挑战，为MVOS领域提供了有效的解决方案和评估基准。

Abstract: This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.

</details>


### [302] [Back to Basics: Let Denoising Generative Models Denoise](https://arxiv.org/abs/2511.13720)
*Tianhong Li,Kaiming He*

Main category: cs.CV

TL;DR: 本文提出直接预测干净数据的扩散模型方法JiT，使用大块Transformer在像素级别进行生成，无需分词器、预训练或额外损失函数，在ImageNet上取得竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型预测噪声而非干净数据，但根据流形假设，自然数据位于低维流形而噪声数据不在。直接预测干净数据可以让网络在非常高维空间中有效工作。

Method: 使用简单的大块Transformer直接在像素级别预测干净图像，无需分词器、预训练或额外损失函数，称为JiT方法。

Result: 在ImageNet 256×256和512×512分辨率上，使用16和32的大块尺寸取得了竞争性结果，而预测高维噪声量的方法会灾难性失败。

Conclusion: 通过让网络映射回流形基础，本文追求基于Transformer的扩散模型在原始自然数据上的自包含范式。

Abstract: Today's denoising diffusion models do not "denoise" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than "$\textbf{Just image Transformers}$", or $\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [303] [LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism](https://arxiv.org/abs/2511.11591)
*Olusola Babalola,Bolanle Ojokoh,Olutayo Boyinbode*

Main category: cs.AI

TL;DR: 本研究探讨了使用LLM生成的合成新闻标题作为真实世界数据替代方案的可行性，特别关注负面情感文本，通过多种评估方法验证合成数据与真实数据的相似性。


<details>
  <summary>Details</summary>
Motivation: 克服NLP任务中数据获取的挑战和真实数据相关的隐私问题，探索LLM生成数据集在情感分析中的潜力。

Method: 使用定制提示创建负面新闻标题语料库，通过专家评审和嵌入空间分析验证，采用比较困惑度测试、可读性测试、POS分析、BERTScore和语义相似度等多种评估方法。

Result: 生成的标题与真实标题高度匹配，仅在POS分析中的专有名词得分存在明显差异。

Conclusion: LLM生成的合成数据可以作为真实世界数据的有效替代方案，在保持内容、语气、长度和风格一致性的同时解决数据获取和隐私问题。

Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.

</details>


### [304] [CLINB: A Climate Intelligence Benchmark for Foundational Models](https://arxiv.org/abs/2511.11597)
*Michelle Chen Huebscher,Katharine Mach,Aleksandar Stanić,Markus Leippold,Ben Gaiarin,Zeke Hausfather,Elisa Rawat,Erich Fischer,Massimiliano Ciaramita,Joeri Rogelj,Christian Buck,Lierni Sestorain Saralegui,Reto Knutti*

Main category: cs.AI

TL;DR: CLINB是一个评估大语言模型处理气候变化专业知识的基准测试，发现前沿模型在知识整合方面表现出色，但在证据基础方面存在严重问题，包括引用和图像的高幻觉率。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型处理复杂专业知识的能力，特别是在气候变化领域，需要可靠的基准测试来衡量知识质量和证据支持。

Method: 开发CLINB基准，包含真实用户问题和气候科学家制定的评估标准，采用基于模型的评估流程验证多个前沿模型。

Result: 前沿模型展现出博士级别的知识整合能力，表现优于专家辅助的混合答案，但在证据基础方面存在严重缺陷，引用和图像幻觉率较高。

Conclusion: 弥合知识整合与可验证归因之间的差距对于AI在科学工作流程中的部署至关重要，需要像CLINB这样的可靠基准来构建可信AI系统。

Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.

</details>


### [305] [SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio](https://arxiv.org/abs/2511.11599)
*Arefeh Kazemi,Hamza Qadeer,Joachim Wagner,Hossein Hosseini,Sri Balaaji Natarajan Kalaivendan,Brian Davis*

Main category: cs.AI

TL;DR: SynBullying是一个用于研究网络欺凌检测的合成多LLM对话数据集，通过大语言模型模拟真实欺凌互动，提供可扩展且伦理安全的替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统人类数据收集方法在规模扩展和伦理安全方面存在限制，需要一种既能模拟真实欺凌互动又能避免伦理问题的替代方案。

Method: 利用大语言模型生成多轮对话，包含对话结构、上下文感知标注和细粒度标签，涵盖多种网络欺凌类别。

Result: 数据集在对话结构、词汇模式、情感/毒性、角色动态、伤害强度和欺凌类型分布五个维度上进行了评估，并测试了其作为独立训练数据和增强源的效果。

Conclusion: SynBullying提供了一个可扩展、伦理安全的网络欺凌研究数据集，能够有效支持欺凌检测模型的训练和增强。

Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.

</details>


### [306] [CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models](https://arxiv.org/abs/2511.11600)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: CausalGuard是一种结合因果推理和符号逻辑的新方法，能实时检测和防止大语言模型的幻觉问题，在12个基准测试中达到89.3%的准确率，减少80%的错误声明。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在严重的幻觉问题，会自信地陈述听起来合理但实际错误的信息，这成为在准确性要求高的场景中使用这些模型的主要障碍。现有解决方案要么需要重新训练整个模型，要么增加显著计算成本，或者未能解决幻觉的根本原因。

Method: CausalGuard通过因果推理与符号逻辑相结合，在生成过程中早期干预。系统通过两条互补路径工作：一条追踪模型所知与生成内容之间的因果关系，另一条使用自动推理检查逻辑一致性。

Result: 在12个不同基准测试中，CausalGuard正确识别幻觉的准确率达到89.3%，仅遗漏8.3%的实际幻觉。更重要的是，它减少了近80%的错误声明，同时保持回答的自然性和帮助性。在需要多步逻辑推理的复杂任务上表现尤为出色。

Conclusion: CausalGuard通过展示推理过程，在医疗诊断或金融分析等敏感领域特别有效，因为这些领域理解决策原因与决策本身同等重要。该方法为解决大语言模型的幻觉问题提供了有效解决方案。

Abstract: While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This "hallucination" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.
  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.
  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\% of the time while missing only 8.3\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.

</details>


### [307] [Quantifying Skill and Chance: A Unified Framework for the Geometry of Games](https://arxiv.org/abs/2511.11611)
*David H. Silver*

Main category: cs.AI

TL;DR: 提出了一个量化框架，通过将游戏建模为随机决策树来分离技能和运气成分，定义了技能-运气指数S(G)在[-1,1]范围内，应用于30个游戏揭示了从纯运气到纯技能的连续谱。


<details>
  <summary>Details</summary>
Motivation: 需要一种系统的方法来量化游戏中技能和运气的相对贡献，以便更好地理解玩家影响力、游戏平衡性和预测稳定性。

Method: 将游戏建模为随机决策树，分解游戏结果为技能杠杆K和运气杠杆L，定义技能-运气指数S(G) = (K-L)/(K+L)，并引入波动率Sigma来量化连续回合的结果不确定性。

Result: 分析30个游戏显示：硬币投掷S=-1（纯运气），西洋双陆棋S=0，国际象棋S=+1（纯技能），扑克S=0.33显示中等技能主导。波动率Sigma从国际象棋的0到西洋双陆棋的1.20不等。

Conclusion: 该框架可扩展到一般随机决策系统，为游戏设计、AI评估和风险评估提供了原则性的比较工具，能够量化玩家影响力、游戏平衡性和预测稳定性。

Abstract: We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.

</details>


### [308] [Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation](https://arxiv.org/abs/2511.11693)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Zeyao Liu,Zhendong Zhao,Xiaoyan Gu*

Main category: cs.AI

TL;DR: VALOR是一个模块化的零样本代理框架，通过多层级NSFW检测、文化价值对齐和意图消歧来识别不安全内容，并使用LLM重写提示词，显著减少不安全图像生成同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 生成式视觉语言模型存在产生不安全、冒犯性或文化不当内容的风险，现有防御方法在保持生成质量和降低成本方面存在困难。

Method: 采用分层提示分析：多级NSFW检测器过滤词汇和语义风险，文化价值对齐模块识别社会规范、合法性和代表性伦理违规，意图消歧器检测间接不安全暗示。检测到不安全内容时，由LLM在动态角色特定指令下重写提示词。

Result: 在对抗性、模糊性和价值敏感提示上的实验显示，VALOR将不安全输出减少高达100.00%，同时保持提示的有用性和创造性。

Conclusion: VALOR是可扩展且有效的方法，可在开放世界环境中部署安全、对齐且有用的图像生成系统。

Abstract: Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.

</details>


### [309] [Towards autonomous quantum physics research using LLM agents with access to intelligent tools](https://arxiv.org/abs/2511.11752)
*Sören Arlt,Xuemei Gu,Mario Krenn*

Main category: cs.AI

TL;DR: AI-Mandel是一个能够生成并实现量子物理学创意的LLM智能体，它通过领域特定的AI工具将文献中的想法转化为可直接在实验室实施的实验设计。


<details>
  <summary>Details</summary>
Motivation: 当前AI在科学领域的应用仍主要依赖人类提供研究问题和目标，AI生成的创意往往模糊且需要人类执行。自动化创意生成和实现将显著改变人类在科学过程中的角色。

Method: AI-Mandel从文献中提取想法，并使用领域特定的AI工具将这些想法转化为具体的实验设计方案。

Result: AI-Mandel生成的创意具有科学价值，其中两个创意已促成独立的后续研究论文。创意包括量子隐形传态的新变体、不定因果序中的量子网络原语以及基于量子信息传输闭环的新几何相位概念。

Conclusion: AI-Mandel展示了能够生成和实现具体可行创意的AI物理学家原型，不仅有助于加速科学发展，也揭示了实现人类水平人工智能科学家所面临的具体挑战。

Abstract: Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.

</details>


### [310] [Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction](https://arxiv.org/abs/2511.11770)
*Floris Vossebeld,Shenghui Wang*

Main category: cs.AI

TL;DR: 提出了一个基于强化学习的智能体框架，让小型LLM通过迭代执行反馈学习SPARQL查询构建策略，显著提升了知识图谱问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在生成复杂SPARQL查询时的脆弱性问题，当前方法缺乏基于实时执行反馈的动态调试能力。

Method: 使用仅3B参数的模型，通过结果驱动的强化学习训练，无需监督微调，学习迭代构建和调试SPARQL查询的策略。

Result: 在LC-QuAD 2.0数据集上达到49.7%的准确率，比最强的零样本基线提升了17.5个百分点。

Conclusion: 该工作为通过交互教授智能体掌握形式化符号工具提供了通用蓝图，弥合了概率LLM与结构化知识图谱之间的差距。

Abstract: Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.

</details>


### [311] [On the Measure of a Model: From Intelligence to Generality](https://arxiv.org/abs/2511.11773)
*Ruchira Dhar,Ninell Oldenburg,Anders Soegaard*

Main category: cs.AI

TL;DR: 论文质疑当前基于抽象智力概念的AI评估基准，提出应以通用性而非智力作为评估基础，认为只有通用性能够经受概念和实证检验。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估基准（如ARC、Raven测试等）基于模糊的智力概念，无法预测实际任务表现，存在评估与现实效用脱节的风险。

Method: 通过概念和形式分析，检验智力评估背后的三个假设：通用性、稳定性和现实性。

Result: 分析表明只有通用性能够经受概念和实证检验，智力不是实现通用性的原因，通用性应理解为多任务学习问题。

Conclusion: 应重新构建AI进展评估方式，将通用性作为评估跨领域和演进任务能力的更稳定基础。

Abstract: Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.

</details>


### [312] [Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy](https://arxiv.org/abs/2511.11816)
*Andrea Brunello,Luca Geatti,Michele Mignani,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: 本文批判性评估了NL-FOL翻译的现有评估方法，提出新协议区分真实语义理解与模式识别，发现对话型LLM具有强NL-FOL翻译能力。


<details>
  <summary>Details</summary>
Motivation: 自然语言到一阶逻辑的翻译一直是长期挑战，虽然LLM的出现带来了突破希望，但现有评估方法存在局限性，可能误判LLM的实际能力。

Method: 批判性审查现有数据集和评估协议，提出新的评估协议来区分真实语义级逻辑理解与表层模式识别、记忆和数据集污染。

Result: 使用新方法发现，最先进的对话导向型LLM表现出强大的NL-FOL翻译技能和真正的句子级逻辑理解能力，而嵌入中心模型表现明显较差。

Conclusion: 对话型LLM在NL-FOL翻译方面具有强大能力，但需要更严格的评估协议来准确评估其真实逻辑理解能力。

Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.

</details>


### [313] [TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models](https://arxiv.org/abs/2511.11831)
*Wenhao Zhou,Hao Zheng,Rong Zhao*

Main category: cs.AI

TL;DR: TopoPerception是一个基于拓扑性质的新基准，用于严格评估大型视觉语言模型的全局视觉感知能力，发现当前模型在全局感知方面表现不佳，甚至不如随机猜测。


<details>
  <summary>Details</summary>
Motivation: 传统评估基准存在局部捷径问题，会高估模型的感知能力。视觉感知模块成为LVLMs的瓶颈，限制了整体能力发展。

Method: 利用拓扑性质创建评估基准，因为拓扑依赖于图像的全局结构且对局部特征不变，能够实现无捷径的全局感知评估。

Result: 所有最先进模型在最粗粒度感知上都表现不佳，准确率不高于随机概率。模型家族内出现一致趋势：推理能力更强的模型准确率反而更低。

Conclusion: 仅扩大模型规模不足以解决全局视觉感知缺陷，可能需要新的训练范式或架构。TopoPerception揭示了当前LVLMs的关键瓶颈并提供了改进方向。

Abstract: Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.

</details>


### [314] [End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction](https://arxiv.org/abs/2511.11899)
*Xi Li,Nicholas Matsumoto,Ujjwal Pasupulety,Atharva Deo,Cherine Yang,Jay Moran,Miguel E. Hernandez,Peter Wager,Jasmine Lin,Jeanine Kim,Alvin C. Goh,Christian Wagner,Geoffrey A. Sonn,Andrew J. Hung*

Main category: cs.AI

TL;DR: F2O是一个端到端系统，可将手术视频转换为手势序列，并揭示与术后结果相关的模式。该系统在机器人辅助前列腺切除术中实现了高精度的手势检测，并能预测术后结果，准确率与人工标注相当。


<details>
  <summary>Details</summary>
Motivation: 术中行为的细粒度分析及其对患者结果的影响是一个长期存在的挑战。需要开发能够自动分析手术视频并关联术后结果的系统。

Method: 利用基于transformer的空间和时间建模以及逐帧分类，F2O在机器人辅助根治性前列腺切除术的神经保留步骤中稳健地检测连续短手势。系统提取手势频率、持续时间和转换等特征。

Result: F2O在手势检测方面表现优异（帧级AUC：0.80；视频级AUC：0.81）。F2O衍生特征预测术后结果的准确率与人工标注相当（0.79 vs. 0.75）。系统还捕获了与勃起功能恢复相关的关键模式，包括延长组织剥离和减少能量使用。

Conclusion: F2O通过实现自动可解释评估，为数据驱动的手术反馈和前瞻性临床决策支持奠定了基础。

Abstract: Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.

</details>


### [315] [Forgetting-MarI: LLM Unlearning via Marginal Information Regularization](https://arxiv.org/abs/2511.11914)
*Shizhou Xu,Yuan Ni,Stefan Broecker,Thomas Strohmer*

Main category: cs.AI

TL;DR: Forgetting-MarI是一个LLM遗忘框架，通过惩罚边际信息来选择性移除特定数据的参数知识，同时保留其他数据的信息，提供可证明的不可检测性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在不断扩大数据集上训练，需要从训练模型中移除特定数据影响以满足隐私保护和监管合规要求，而现有遗忘方法往往过度移除信息导致模型性能下降。

Method: 引入Forgetting-MarI框架，通过惩罚边际信息来仅移除待遗忘数据贡献的额外信息，同时保留待保留数据支持的信息，提供明确的遗忘数据集残余影响上界。

Result: 大量实验证实该方法优于当前最先进的遗忘方法，实现了可靠的遗忘效果，并在多样化基准测试中保持了更好的通用模型性能。

Conclusion: 这一进展代表了使AI系统更可控、更符合隐私和版权法规而不损害其有效性的重要一步。

Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.

</details>


### [316] [An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR](https://arxiv.org/abs/2511.11916)
*Sinan Urgun,Seçkin Arı*

Main category: cs.AI

TL;DR: 本研究系统评估了4种大语言模型在抽象视觉推理任务中的表现，发现GPT-4.1-Mini在所有推理架构中表现最佳，且推理效果具有模型特异性。


<details>
  <summary>Details</summary>
Motivation: 系统评估大语言模型在抽象视觉推理问题中的性能表现，探索不同推理架构对模型表现的影响。

Method: 使用4种LLM模型和4种推理架构在RAVEN-FAIR数据集上进行测试，通过三阶段过程生成视觉响应，使用SSIM和LPIPS指标评估，分析思维链分数和错误类型。

Result: GPT-4.1-Mini在所有架构中始终获得最高准确率，多智能体架构会改变语义和数值平衡但效果不一致，不同模型对架构设计具有特异性敏感模式。

Conclusion: 大语言模型的推理有效性具有模型特异性，多轮评估策略比单轮评估更可靠，响应覆盖度的变化使跨架构直接比较变得复杂。

Abstract: This study aims to systematically evaluate the performance of large language models (LLMs) in abstract visual reasoning problems. We examined four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) utilizing four different reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, and multi-agent) on the RAVEN-FAIR dataset. Visual responses generated through a three-stage process (JSON extraction, LLM reasoning, and Tool Function) were evaluated using SSIM and LPIPS metrics; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) were analyzed. Results demonstrate that GPT-4.1-Mini consistently achieved the highest overall accuracy across all architectures, indicating a strong reasoning capability. While the multi-agent architecture occasionally altered semantic and numeric balance across models, these effects were not uniformly beneficial. Instead, each model exhibited distinct sensitivity patterns to architectural design, underscoring that reasoning effectiveness remains model-specific. Variations in response coverage further emerged as a confounding factor that complicates direct cross-architecture comparison. To estimate the upper-bound performance of each configuration, we report the best of five independent runs, representing a best-case scenario rather than an averaged outcome. This multi-run strategy aligns with recent recommendations, which emphasize that single-run evaluations are fragile and may lead to unreliable conclusions.

</details>


### [317] [Looking Forward: Challenges and Opportunities in Agentic AI Reliability](https://arxiv.org/abs/2511.11921)
*Liudong Xing,Janet,Lin*

Main category: cs.AI

TL;DR: 本章探讨了构建可靠AI系统（特别是智能体AI系统）面临的挑战和未来发展方向，包括级联故障风险缓解、动态环境、任务执行不一致性、不可预测涌现行为、资源密集型可靠性机制等方面的研究问题，以及测试评估可靠性的研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI系统的发展，确保其可靠性变得越来越重要。这些系统在复杂环境中运行时面临多种风险，需要研究如何构建可靠的AI系统来应对这些挑战。

Method: 通过分析当前研究现状，识别出多个关键研究问题，包括级联故障缓解、动态环境适应性、任务执行一致性、涌现行为预测等，并提出相应的研究方向。

Result: 识别了构建可靠智能体AI系统面临的主要挑战，包括动态环境下的适应性、任务执行不一致性、不可预测的涌现行为、资源密集型可靠性机制等问题。

Conclusion: 构建可靠的智能体AI系统需要在多个方面进行深入研究，包括故障缓解机制、环境适应性、行为预测和高效可靠性评估方法等，这些研究方向对于未来AI系统的安全可靠部署至关重要。

Abstract: This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.

</details>


### [318] [A Neuromorphic Architecture for Scalable Event-Based Control](https://arxiv.org/abs/2511.11924)
*Yongkang Huo,Fulvio Forni,Rodolphe Sepulchre*

Main category: cs.AI

TL;DR: 本文提出了"反弹赢家通吃(RWTA)"基元作为可扩展神经形态控制架构的基本元素，结合了离散计算的可靠性和连续调节的可调性。


<details>
  <summary>Details</summary>
Motivation: 构建一个既能处理连续节律生成又能进行离散决策的统一物理建模框架，解决神经形态控制中连续与离散计算的集成问题。

Method: 使用RWTA基元构建从细胞级到系统级的架构，继承赢家通吃状态机的离散计算能力和可兴奋生物物理电路的连续调节能力。

Result: 通过蛇形机器人神经系统设计展示了该架构的通用性、鲁棒性和模块化特性。

Conclusion: RWTA架构为神经形态控制提供了一个统一的事件驱动框架，能够同时处理连续节律生成和离散决策任务。

Abstract: This paper introduces the ``rebound Winner-Take-All (RWTA)" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.

</details>


### [319] [Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes](https://arxiv.org/abs/2511.11945)
*Mohammed Temraz,Mark T Keane*

Main category: cs.AI

TL;DR: 提出CFA-SMOTE方法，结合反事实解释和SMOTE技术，通过生成气候异常事件的合成数据来解决气候变化预测中的类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致极端天气事件增多，但传统机器学习方法难以处理分布外数据，特别是缺乏足够的气候异常事件样本。

Method: 将气候变化预测问题视为类别不平衡问题，结合可解释AI的反事实方法和SMOTE过采样技术，生成代表气候异常事件的合成数据点。

Result: 在不同类别不平衡比例下，CFA-SMOTE方法相比基准方法表现出更好的预测性能。

Conclusion: CFA-SMOTE能有效改善气候变化相关预测任务中的模型性能，特别是在处理分布外气候事件方面。

Abstract: In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of "climate outlier events". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.

</details>


### [320] [LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code](https://arxiv.org/abs/2511.11954)
*Borchuluun Yadamsuren,Steven Keith Platt,Miguel Diaz*

Main category: cs.AI

TL;DR: 本文提出了一种结合大语言模型和符号逻辑的混合神经符号框架，用于确定性检测复杂法律中的法规不一致性。以美国国内税收法典为案例研究，通过GPT-4o、GPT-5和Prolog实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在层次化处理和深度结构化推理方面存在困难，特别是在处理长文本时。税收领域的具体应用仍然稀缺，需要解决法规不一致性检测的挑战。

Method: 使用GPT-4o将税法条款翻译成Prolog规则，在SWISH中精炼。通过Prolog增强提示测试不一致性检测效果。结合GPT-5进行规则精炼，构建混合Prolog模型。

Result: GPT-4o单独使用自然语言提示或Prolog增强提示时，在三个策略中仅检测到一个不一致性（33%准确率）。但混合Prolog模型产生了确定性、可重现的结果，成功检测到不一致性区域。验证测试确认Prolog实现准确、内部一致且能自主识别不一致性。

Conclusion: 基于符号逻辑的LLM辅助形式化方法能够实现透明可靠的法规不一致性检测，混合神经符号框架在法律分析中具有重要应用价值。

Abstract: This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.
  LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.
  This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.
  In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.

</details>


### [321] [Improving Autoformalization Using Direct Dependency Retrieval](https://arxiv.org/abs/2511.11990)
*Shaoqi Wang,Lu Yu,Chunjie Yang*

Main category: cs.AI

TL;DR: 提出基于直接依赖检索(DDR)的新框架，解决数学陈述自动形式化中的上下文感知不足和检索精度低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化方法缺乏上下文感知，导致形式定义和定理的幻觉，且检索增强方法在形式库依赖检索上精度和召回率差，难以有效利用大规模公共数据集。

Method: 提出DDR方法，直接从自然语言数学描述生成候选库依赖，通过高效后缀数组检查验证其在形式库中的存在，构建超50万样本的依赖检索数据集并微调高精度DDR模型。

Result: DDR模型在检索精度和召回率上显著优于SOTA方法，配备DDR的自动形式化器在单次尝试准确率和多次尝试稳定性上均优于传统基于选择的RAG方法。

Conclusion: DDR框架有效解决了数学陈述自动形式化中的依赖检索问题，为深度学习与形式数学的融合提供了更可靠的基础。

Abstract: The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.

</details>


### [322] [Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning](https://arxiv.org/abs/2511.12003)
*Shuochen Liu,Pengfei Luo,Chao Zhang,Yuhao Chen,Haotian Zhang,Qi Liu,Xin Kou,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: 提出Chain-of-Evidence (CoE)范式，通过强化学习框架LAT训练视觉语言模型生成可验证的推理路径，在视觉文档检索增强生成中实现证据溯源。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏细粒度监督和推理过程的可追溯性，需要确保视觉语言模型在多模态问答中产生可靠且可验证的预测。

Method: 提出Look As You Think (LAT)强化学习框架，将Chain-of-Thought推理与视觉证据溯源结合，通过边界框和页面索引定位参考元素，训练模型生成证据支撑的推理路径。

Result: 在Qwen2.5-VL-7B-Instruct模型上，LAT在单图和多图设置中平均提升8.23%的soft EM和47.0%的IoU@0.5，优于监督微调基线并展现更强的跨领域泛化能力。

Conclusion: LAT框架有效提升了视觉文档检索增强生成中的证据溯源能力，实现了过程级的自我验证和可靠的推理路径生成。

Abstract: Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.

</details>


### [323] [Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models](https://arxiv.org/abs/2511.12008)
*Yunqi Hong,Johnson Kao,Liam Edwards,Nein-Tzu Liu,Chung-Yen Huang,Alex Oliveira-Kowaleski,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.AI

TL;DR: RECAP-PATH是一个可解释的病理AI框架，通过自学习范式将多模态大语言模型从被动模式识别转变为证据关联的诊断推理，无需大量标注数据即可生成癌症诊断。


<details>
  <summary>Details</summary>
Motivation: 当前病理AI工具虽然提升了筛查效率和标准化量化，但由于缺乏人类可读的推理过程，难以审计决策和防止错误，限制了临床采用。

Method: 采用两阶段自学习过程：多样化阶段扩展病理学风格解释，优化阶段精炼解释以提高准确性。无需白盒访问或权重更新，仅需少量标注数据。

Result: 在乳腺癌和前列腺癌数据集上的评估显示，RECAP-PATH生成的推理与专家评估一致，诊断准确性相比基线方法有显著提升。

Conclusion: RECAP-PATH通过结合视觉理解和推理能力，提供了临床可信赖的AI，展示了实现证据关联解释的通用路径。

Abstract: AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.

</details>


### [324] [Intelligent Collaborative Optimization for Rubber Tyre Film Production Based on Multi-path Differentiated Clipping Proximal Policy Optimization](https://arxiv.org/abs/2511.12060)
*Yinghao Ruan,Wei Pang,Shuaihao Liu,Huili Yang,Leyi Han,Xinghui Dong*

Main category: cs.AI

TL;DR: 提出了一种多路径差异化裁剪近端策略优化算法(MPD-PPO)，用于解决轮胎制造中的高维多目标优化问题，在橡胶轮胎薄膜生产的宽度和厚度控制中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 智能制造需要解决传统集中式调度和生产配置在应对动态生产需求方面的局限性，轮胎制造系统具有复杂的非线性交互和涌现动态特性，多子系统协调面临严峻挑战。

Method: 采用多分支策略架构和差异化梯度裁剪约束的深度强化学习算法MPD-PPO，确保高维策略更新的稳定性和效率。

Result: 在橡胶轮胎薄膜生产的宽度和厚度控制实验中，MPD-PPO在调谐精度和操作效率方面均有显著提升。

Conclusion: 该框架成功解决了高维度、多目标权衡和动态适应等关键挑战，为轮胎制造实时工业部署提供了增强的性能和生产稳定性。

Abstract: The advent of smart manufacturing is addressing the limitations of traditional centralized scheduling and inflexible production line configurations in the rubber tyre industry, especially in terms of coping with dynamic production demands. Contemporary tyre manufacturing systems form complex networks of tightly coupled subsystems pronounced nonlinear interactions and emergent dynamics. This complexity renders the effective coordination of multiple subsystems, posing an essential yet formidable task. For high-dimensional, multi-objective optimization problems in this domain, we introduce a deep reinforcement learning algorithm: Multi-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO). This algorithm employs a multi-branch policy architecture with differentiated gradient clipping constraints to ensure stable and efficient high-dimensional policy updates. Validated through experiments on width and thickness control in rubber tyre film production, MPD-PPO demonstrates substantial improvements in both tuning accuracy and operational efficiency. The framework successfully tackles key challenges, including high dimensionality, multi-objective trade-offs, and dynamic adaptation, thus delivering enhanced performance and production stability for real-time industrial deployment in tyre manufacturing.

</details>


### [325] [Bayesian Optimization in Language Space: An Eval-Efficient AI Self-Improvement Framework](https://arxiv.org/abs/2511.12063)
*Enoch Hyunwook Kang,Hema Yoganarasimhan*

Main category: cs.AI

TL;DR: 提出了T-BoN BO框架，通过结合Best-of-N选择和文本梯度，在语言空间中实现评估效率最优的贝叶斯优化，用于AI自我改进。


<details>
  <summary>Details</summary>
Motivation: 在需要人类反馈的社会应用中，评估成本远高于生成成本，现有方法主要关注查询效率而非评估效率。

Method: 结合Best-of-N选择策略和文本梯度，证明其统计上模拟了UCB采集函数的梯度行为，提出了T-BoN BO框架。

Result: 在广告对齐任务中，T-BoN BO相比现有最先进基线方法表现出更优越的性能。

Conclusion: T-BoN BO为语言空间中的评估效率优化提供了有效的贝叶斯优化框架，特别适用于需要昂贵评估的应用场景。

Abstract: Large Language Models (LLMs) have recently enabled self-improving AI, i.e., AI that iteratively generates, evaluates, and refines its own outcomes. Recent studies have shown that self-improving AI focusing on prompt optimization can outperform state-of-the-art reinforcement-learning fine-tuned LLMs. Here, their `performance' is typically measured by query efficiency - the number of LLM-generated solution samples required to meet a certain performance threshold. However, in many societal applications, the primary limitation is not generating new solutions but evaluating them. For instance, evaluating an ad's effectiveness requires significant human feedback, which is far more costly and time-consuming than generating a candidate ad. To optimize for the evaluation efficiency objective, a natural approach is to extend Bayesian Optimization (BO), a framework proven optimal for evaluation efficiency, to the language domain. However, the difficulty of directly estimating suitable acquisition functions in LLMs' minds makes this extension challenging. This paper overcomes this challenge by proving that the combination of the simple and widely used Best-of-N selection strategy and simple textual gradients (i.e., textual edits from a critic model) statistically emulates the behavior of the gradients on the canonical UCB acquisition function, which induces optimal exploration in terms of evaluation efficiency. Based on this result, we propose TextGrad-Best-of-N Bayesian Optimization (T-BoN BO), a simple and eval-efficient language-space Bayesian optimization framework for AI self-improvement. We also empirically validate T-BoN BO by applying it to automated ad alignment tasks for persona distribution, demonstrating its superior performance compared to popular state-of-the-art baselines.

</details>


### [326] [No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding](https://arxiv.org/abs/2511.12083)
*Yanchang Fu,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: 提出Embedding CFR算法，通过将信息集嵌入到低维连续空间来解决大规模不完全信息扩展式博弈，相比基于聚类的抽象方法显著提升了策略求解效率。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法依赖预训练的离散聚类进行抽象，但硬分类会不可逆地丢失信息集之间的量化细微差异，这些差异对于策略求解至关重要。

Method: 受自然语言处理中词嵌入范式启发，提出Embedding CFR算法：预训练并将孤立信息集的特征嵌入到互联的低维连续空间，在该嵌入空间中进行遗憾累积和策略更新的策略求解过程。

Result: 在扑克实验中，在相同空间开销下，Embedding CFR相比基于聚类的抽象算法实现了显著更快的可剥削性收敛，证实了其有效性。

Conclusion: 这是首个通过低维嵌入预训练信息集抽象来进行策略求解的扑克AI算法，能够更精确地捕捉信息集之间的差异和联系。

Abstract: High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)-such as no-limit Texas Hold'em-where the finite nature of spatial resources hinders strategy solving over the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly loses critical information: specifically, the quantifiable subtle differences between information sets-vital for strategy solving-thereby compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds features of isolated information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR presents a strategy-solving process driven by regret accumulation and strategy updates within this embedding space, with accompanying theoretical analysis verifying its capacity to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions through low-dimensional embedding for strategy solving.

</details>


### [327] [KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything](https://arxiv.org/abs/2511.12089)
*Yanchang Fu,Qiyue Yin,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: KrwEmd算法通过k-recall赢率特征和Earth Mover's距离聚类，解决了德州扑克等游戏中因过度抽象导致AI性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模不完全信息游戏中因不完美回忆抽象过度而导致的AI性能下降问题，特别是历史信息完全丢失的极端情况。

Method: 引入k-recall赢率特征来区分信号观察信息集，利用历史和未来游戏信息；开发KrwEmd算法，使用Earth Mover's距离测量特征差异并进行聚类。

Result: 实验结果表明，KrwEmd相比现有算法显著提升了AI的游戏表现。

Conclusion: KrwEmd是首个实用的解决过度抽象问题的算法，通过结合历史和未来信息有效改善了不完全信息游戏中的AI性能。

Abstract: Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.

</details>


### [328] [MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization](https://arxiv.org/abs/2511.12113)
*Lanxue Zhang,Yuqiang Xie,Fang Fang,Fanglong Dong,Rui Liu,Yanan Cao*

Main category: cs.AI

TL;DR: 提出了一种缓解小模型灾难性遗忘的全面解决方案，包括构建包含元认知知识的数据集和引入GDPO训练方法，显著提升了小模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和微调方法在8B以下小模型中存在灾难性遗忘问题，主要原因是训练数据与模型固有能力的关联性被忽视，以及传统训练目标无法有效约束固有知识的保留。

Method: 1) 构建包含5K实例的数据集，覆盖多种推理任务并融入元认知知识；2) 提出GDPO（Group Direction Preference Optimization）训练方法，通过参考模型隐式约束优化路径，有效限制参数漂移。

Result: 大量实验表明，该方法显著缓解了灾难性遗忘问题，并提升了小模型的推理性能。

Conclusion: 从数据和训练方法两个角度提出的综合解决方案能够有效解决小模型知识蒸馏中的灾难性遗忘问题，为资源受限场景提供了更高效的训练方案。

Abstract: Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.

</details>


### [329] [RTMol: Rethinking Molecule-text Alignment in a Round-trip View](https://arxiv.org/abs/2511.12135)
*Letian Chen,Runhan Shi,Gufeng Yu,Yang Yang*

Main category: cs.AI

TL;DR: RTMol是一个双向对齐框架，通过自监督的往返学习统一分子描述和文本到SMILES生成，解决了现有方法在化学准确性、数据质量和双向一致性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将分子描述和文本到分子设计视为独立任务，存在三个关键问题：传统指标优先考虑语言流畅性而非化学准确性；训练数据包含化学模糊描述；独立优化导致双向不一致。

Method: 提出RTMol框架，通过自监督往返学习统一分子描述和文本到SMILES生成，引入新颖的往返评估指标，无需配对分子-文本语料即可进行无监督训练。

Result: 实验表明RTMol在各种LLM上将双向对齐性能提升高达47%，为联合分子-文本理解和生成建立了有效范式。

Conclusion: RTMol通过双向对齐框架有效解决了分子序列表示与文本描述对齐的关键挑战，在药物发现和材料设计等应用中具有重要价值。

Abstract: Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.

</details>


### [330] [Incremental Maintenance of DatalogMTL Materialisations](https://arxiv.org/abs/2511.12169)
*Kaiyue Zhao,Dingqi Chen,Shaoyu Wang,Pan Hu*

Main category: cs.AI

TL;DR: 提出了DRedMTL算法，这是一种用于DatalogMTL的增量推理方法，能够高效处理动态数据更新，相比重新物化方法性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有的DatalogMTL推理方法虽然具有正确性和完备性，但缺乏对动态更新的高效支持，而现实应用需要频繁处理数据更新。

Method: 基于经典DRed算法构建，设计了专门的操作符来处理DatalogMTL物化的周期性区间表示，支持有界区间的增量更新。

Result: 在多个公开数据集上的实验结果表明，DRedMTL通常显著优于重新物化方法，有时性能提升达到数量级。

Conclusion: DRedMTL为DatalogMTL提供了一种高效的增量推理解决方案，能够满足现实应用中动态数据更新的需求。

Abstract: DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.

</details>


### [331] [Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering](https://arxiv.org/abs/2511.12208)
*Jilong Liu,Pengyang Shao,Wei Qin,Fei Liu,Yonghui Yang,Richang Hong*

Main category: cs.AI

TL;DR: 提出DoM框架，通过多智能体辩论机制动态融合结构化知识图谱和非结构化外部知识，解决不完整知识图谱问答问题，并构建了更真实的IKGQA数据集。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识图谱通常不完整，现有方法无法自适应地融合多源知识，无法充分利用知识的互补性。

Method: 基于多智能体辩论范式，分配专门智能体分别处理知识图谱和外部文本推理，通过迭代交互协调输出，包括问题分解、双智能体证据检索和法官智能体评估聚合。

Result: 实验表明DoM持续优于最先进的基线方法。

Conclusion: DoM框架通过知识互补性增强了知识图谱不完整性的鲁棒性，为不完整知识图谱问答提供了有效解决方案。

Abstract: Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.

</details>


### [332] [ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2511.12214)
*Ruochen Li,Zhanxing Zhu,Tanqiu Qiao,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: 提出ViTE框架，通过虚拟图和专家路由器模块解决行人轨迹预测中的高阶交互建模问题，避免深层GNN的计算负担，在多个基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有方法在行人轨迹预测中面临基本权衡：浅层GNN存在感受野不足问题，深层GNN则计算成本过高。需要能够自适应建模显式一跳交互和隐式高阶依赖的模型

Method: 提出ViTE框架，包含两个关键模块：虚拟图通过动态虚拟节点建模长距离和高阶交互而无需深层GNN堆叠；专家路由器基于社交上下文使用专家混合设计自适应选择交互专家

Result: 在三个基准测试（ETH/UCY、NBA和SDD）上的实验表明，该方法持续达到最先进性能，验证了其有效性和实际效率

Conclusion: ViTE框架通过虚拟图和专家路由器的组合，实现了灵活可扩展的交互模式推理，在行人轨迹预测任务中表现出色

Abstract: Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.

</details>


### [333] [Beyond World Models: Rethinking Understanding in AI Models](https://arxiv.org/abs/2511.12239)
*Tarun Gupta,Danish Pruthi*

Main category: cs.AI

TL;DR: 本文使用科学哲学案例研究来批判性地检验世界模型框架是否充分表征了人类水平的理解能力


<details>
  <summary>Details</summary>
Motivation: 研究世界模型能否表征人类水平的理解能力，因为人类拥有心理世界模型，如果AI模型有类似表征可能意味着它们以类人方式"理解"世界

Method: 使用科学哲学文献中的案例研究，重点关注世界模型能力与人类理解之间区别最明显的哲学分析

Result: 通过具体哲学观点探讨了世界模型的局限性，这些观点代表了特定的理解观而非普遍定义

Conclusion: 世界模型框架在表征人类水平理解方面存在局限性，需要更深入地探讨理解能力的本质

Abstract: World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models "understand" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.

</details>


### [334] [AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos](https://arxiv.org/abs/2511.12241)
*Junhyuk Seo,Hyeyoon Moon,Kyu-Hwan Jung,Namkee Oh,Taerim Kim*

Main category: cs.AI

TL;DR: AURA是一个基于视觉的风险检测系统，使用完全合成的ICU视频数据集开发，通过姿态估计检测患者手部进入气道管附近区域（碰撞）和关键点速度（躁动）两种高风险运动模式，实现隐私保护的患者安全监测。


<details>
  <summary>Details</summary>
Motivation: ICU中非计划性拔管是严重的安全问题，但由于伦理和隐私限制难以获取标注的ICU视频数据，需要开发隐私保护的实时检测方法。

Method: 利用文本到视频扩散技术生成多样化的临床真实ICU场景，通过姿态估计识别两种高风险运动模式：手部进入气道管附近区域的碰撞检测，以及基于跟踪解剖关键点速度的躁动量化。

Result: 专家评估确认合成数据的真实性，性能评估显示碰撞检测准确率高，躁动识别性能中等。

Conclusion: 这项工作展示了一种开发隐私保护、可复现的患者安全监测系统的新途径，具有在重症监护环境中部署的潜力。

Abstract: Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.

</details>


### [335] [Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation](https://arxiv.org/abs/2511.12254)
*Yuxiang Zhou,Jichang Li,Yanhao Zhang,Haonan Lu,Guanbin Li*

Main category: cs.AI

TL;DR: 提出了Mobile-Agent-RAG框架，通过双层次检索增强解决移动代理在长时跨应用任务中的战略幻觉和操作错误问题，显著提升了任务完成率和步骤效率。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的移动代理在现实世界长时跨应用任务中成功率不足，主要原因是过度依赖MLLM的静态内部知识，导致战略层面的幻觉和操作层面的执行错误。

Method: 提出分层多代理框架Mobile-Agent-RAG，包含Manager-RAG用于规划阶段检索人类验证的全面任务计划，Operator-RAG用于执行阶段检索精确的低级操作指导，并构建了两个专门的检索知识库。

Result: 实验表明Mobile-Agent-RAG显著优于最先进基线，任务完成率提高了11.0%，步骤效率提高了10.2%。

Conclusion: 建立了一个面向上下文感知、可靠的多代理移动自动化的稳健范式，通过双层次检索增强有效解决了移动代理的关键瓶颈问题。

Abstract: Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.

</details>


### [336] [MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning](https://arxiv.org/abs/2511.12271)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 本文提出了一种解决大语言模型道德对齐问题的方法，通过构建Moral-Reason-QA数据集和Group Relative Policy Optimization学习框架，使LLM能够在超出训练分布的道德场景中应用一致的道德推理框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型日益影响人类道德决策，但现有方法主要关注评估而非主动引导其道德决策。需要解决LLM在超出训练分布场景中的道德对齐问题。

Method: 构建Moral-Reason-QA数据集（包含680个高模糊性道德场景和框架特定推理轨迹），采用Group Relative Policy Optimization结合复合奖励函数，同时优化决策对齐和框架特定推理过程。

Result: 在未见道德场景上成功实现泛化，功利主义框架的softmax归一化对齐分数提升+0.757，义务论框架提升+0.450。实验还揭示了训练挑战和未来研究方向。

Conclusion: LLM代理能够被系统训练以内在化并将特定道德框架应用于新情境，为AI安全提供了关键基础。

Abstract: Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.

</details>


### [337] [UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI](https://arxiv.org/abs/2511.12306)
*Darvin Yi,Teng Liu,Mattie Terzolo,Lance Hasson,Ayan Sinh,Pablo Mendes,Andrew Rabinovich*

Main category: cs.AI

TL;DR: UpBench是一个基于真实Upwork工作任务的动态基准测试，用于评估LLM代理在真实工作环境中的能力、适应性和人机协作能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多是静态、合成或领域受限的，无法反映AI代理在动态、经济意义环境中的真实表现。需要建立基于真实工作场景的评估框架。

Method: 从Upwork劳动力市场提取真实工作任务，由专家自由职业者制定详细可验证的验收标准，采用基于评分标准的评估框架对AI提交内容进行细粒度评估。

Result: 建立了基于真实工作活动的评估基准，能够对模型优势、弱点和指令遵循保真度进行精细分析，超越了简单的通过/失败指标。

Conclusion: UpBench提供了一个可扩展、以人为中心的评估基础，支持在真实劳动力市场环境中评估代理系统，为AI通过合作而非替代来增强人类能力提供了路径。

Abstract: As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.

</details>


### [338] [Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning](https://arxiv.org/abs/2511.12344)
*Baolong Bi,Shenghua Liu,Yiwei Wang,Siqian Tong,Lingrui Mei,Yuyao Ge,Yilong Xu,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: RGR-GRPO是一个基于评分标准的强化学习框架，通过提供细粒度奖励信号和离线指导，在多领域推理任务中显著提升大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要关注单一领域且依赖可验证奖励，纯在线RL框架限制了探索空间，从而限制了推理性能。需要解决这些限制以提升多领域推理能力。

Method: 提出RGR-GRPO框架，利用评分标准提供密集信息奖励和离线指导，在GRPO训练期间探索更大的解决方案空间。

Result: 在14个多领域基准测试中，RGR-GRPO始终优于仅依赖替代奖励方案或离线指导的RL方法。相比可验证在线RL基线，在数学、物理、化学和通用推理任务上分别平均提升+7.0%、+5.4%、+8.4%和+6.6%。

Conclusion: RGR-GRPO在离线策略训练期间保持稳定的熵波动，实现卓越的pass@k性能，反映了持续的探索和有效突破现有性能瓶颈。

Abstract: Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.

</details>


### [339] [More Than Irrational: Modeling Belief-Biased Agents](https://arxiv.org/abs/2511.12359)
*Yifan Zhu,Sammie Katt,Samuel Kaski*

Main category: cs.AI

TL;DR: 本文提出了计算理性用户模型，用于模拟认知受限代理在偏见信念下的最优行为，重点解决从被动观察中推断用户认知边界和信念状态的问题。


<details>
  <summary>Details</summary>
Motivation: 预测和理解用户或人类合作者的次优行为是重要挑战，这些行为往往源于认知边界和偏见信念，而非非理性。

Method: 提出基于嵌套粒子滤波的在线推理方法，同时跟踪用户的潜在信念状态并从观察到的动作流中估计未知认知边界。

Result: 在导航任务模拟中，模型能生成与不同记忆容量对应的合理行为，推理方法能在有限观察（≤100步）内准确恢复真实认知边界。

Conclusion: 该方法为开发自适应AI助手提供了理论基础，使AI能够考虑用户的记忆限制提供适应性帮助。

Abstract: Despite the explosive growth of AI and the technologies built upon it, predicting and inferring the sub-optimal behavior of users or human collaborators remains a critical challenge. In many cases, such behaviors are not a result of irrationality, but rather a rational decision made given inherent cognitive bounds and biased beliefs about the world. In this paper, we formally introduce a class of computational-rational (CR) user models for cognitively-bounded agents acting optimally under biased beliefs. The key novelty lies in explicitly modeling how a bounded memory process leads to a dynamically inconsistent and biased belief state and, consequently, sub-optimal sequential decision-making. We address the challenge of identifying the latent user-specific bound and inferring biased belief states from passive observations on the fly. We argue that for our formalized CR model family with an explicit and parameterized cognitive process, this challenge is tractable. To support our claim, we propose an efficient online inference method based on nested particle filtering that simultaneously tracks the user's latent belief state and estimates the unknown cognitive bound from a stream of observed actions. We validate our approach in a representative navigation task using memory decay as an example of a cognitive bound. With simulations, we show that (1) our CR model generates intuitively plausible behaviors corresponding to different levels of memory capacity, and (2) our inference method accurately and efficiently recovers the ground-truth cognitive bounds from limited observations ($\le 100$ steps). We further demonstrate how this approach provides a principled foundation for developing adaptive AI assistants, enabling adaptive assistance that accounts for the user's memory limitations.

</details>


### [340] [Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making](https://arxiv.org/abs/2511.12378)
*Dylan M. Asmar,Mykel J. Kochenderfer*

Main category: cs.AI

TL;DR: 提出了一个动态学习建议者可靠性的框架，通过贝叶斯推断和策略性请求建议来优化自主智能体在不确定环境中的决策。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设建议者质量参数是静态且已知的，限制了实际部署。需要能够动态适应变化可靠性的框架。

Method: 1. 将建议者质量直接集成到智能体的信念表示中，通过贝叶斯推断调整对建议的依赖；2. 引入显式的"询问"动作，在关键时刻策略性地请求建议，平衡信息获取与成本。

Result: 实验评估显示该框架在不同建议者质量下表现稳健，能够适应变化的可靠性，并有效管理建议请求策略。

Conclusion: 该工作通过解决不确定环境中的建议不确定性，为自适应人机协作提供了基础。

Abstract: Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.

</details>


### [341] [Multi-agent Self-triage System with Medical Flowcharts](https://arxiv.org/abs/2511.12439)
*Yujia Liu,Sophia Yu,Hongyue Jin,Jessica Wen,Alexander Qian,Terrence Lee,Mattheus Ramsis,Gi Won Choi,Lianhui Qin,Xin Liu,Edward J. Wang*

Main category: cs.AI

TL;DR: 开发了一个基于临床验证流程图的对话式自我分诊系统，通过多智能体框架实现95.29%的流程图检索准确率和99.10%的导航准确率，结合自由文本交互的灵活性和标准化临床协议的严谨性。


<details>
  <summary>Details</summary>
Motivation: 在线健康资源和大型语言模型在医疗决策中可靠性有限，存在准确性低、透明度不足和易受未经验证信息影响的问题，需要更可靠的患者决策支持系统。

Method: 使用美国医学协会100个临床验证流程图构建对话式自我分诊系统，采用多智能体框架：检索代理识别相关流程图，决策代理解释患者响应，聊天代理提供个性化建议。

Result: 在模拟对话数据集上评估，系统在流程图检索方面达到95.29%的top-3准确率（N=2,000），在流程图导航方面达到99.10%的准确率（N=37,200）。

Conclusion: 该方法展示了透明、准确且可推广的AI辅助自我分诊的可行性，有潜力支持知情患者决策并改善医疗资源利用。

Abstract: Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.

</details>


### [342] [ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction](https://arxiv.org/abs/2511.12485)
*Pengze Li,Jiaqi Liu,Junchi Yu,Lihao Liu,Mingyu Ding,Wanli Ouyang,Shixiang Tang,Xi Chen*

Main category: cs.AI

TL;DR: 提出了ARCHE任务，要求模型将复杂推理分解为标准推理范式的组合，形成推理逻辑树(RLT)，包含演绎、归纳和溯因三种推理模式。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs生成的推理内容通常是非结构化和非正式的，难以判断模型是否真正理解科学推理的基本范式。

Method: 引入ARCHE任务，要求模型将复杂推理分解为RLT结构，使用Peirce的三种基本推理模式进行分类。创建ARCHE Bench基准，包含70篇Nature Communications文章的数据。

Result: 评估10个领先LLMs发现，模型在REA和EC指标间存在权衡，没有一个模型能够提取完整且标准的推理链。

Conclusion: 当前推理模型的能力与科学论证所需的严谨性之间存在显著差距。

Abstract: Large language models (LLMs) are increasingly used in scientific domains. While they can produce reasoning-like content via methods such as chain-of-thought prompting, these outputs are typically unstructured and informal, obscuring whether models truly understand the fundamental reasoning paradigms that underpin scientific inference. To address this, we introduce a novel task named Latent Reasoning Chain Extraction (ARCHE), in which models must decompose complex reasoning arguments into combinations of standard reasoning paradigms in the form of a Reasoning Logic Tree (RLT). In RLT, all reasoning steps are explicitly categorized as one of three variants of Peirce's fundamental inference modes: deduction, induction, or abduction. To facilitate this task, we release ARCHE Bench, a new benchmark derived from 70 Nature Communications articles, including more than 1,900 references and 38,000 viewpoints. We propose two logic-aware evaluation metrics: Entity Coverage (EC) for content completeness and Reasoning Edge Accuracy (REA) for step-by-step logical validity. Evaluations on 10 leading LLMs on ARCHE Bench reveal that models exhibit a trade-off between REA and EC, and none are yet able to extract a complete and standard reasoning chain. These findings highlight a substantial gap between the abilities of current reasoning models and the rigor required for scientific argumentation.

</details>


### [343] [LOBERT: Generative AI Foundation Model for Limit Order Book Messages](https://arxiv.org/abs/2511.12563)
*Eljas Linna,Kestutis Baltakys,Alexandros Iosifidis,Juho Kanniainen*

Main category: cs.AI

TL;DR: LOBERT是一个专门针对限价订单簿数据的通用基础模型，通过创新的标记化方案将多维消息作为单一标记处理，在预测中间价格变动和下一消息等任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LOB模型需要繁琐的数据表示，缺乏任务适应性，难以应对不规则事件时间、快速制度转换和高频交易者对可见订单流的反应等挑战。

Method: LOBERT基于BERT架构，采用新颖的标记化方案，将完整的多维消息作为单一标记处理，同时保留价格、数量和时间等连续特征的表示。

Result: LOBERT在预测中间价格变动和下一消息等任务中取得领先性能，同时相比先前方法减少了所需的上下文长度。

Conclusion: LOBERT为LOB数据提供了一个通用的基础模型，适合下游任务的微调，在多个预测任务中表现出色且计算效率更高。

Abstract: Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.

</details>


### [344] [Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models](https://arxiv.org/abs/2511.12579)
*Yongwen Ren,Chao Wang,Peng Du,Chuan Qin,Dazhong Shen,Hui Xiong*

Main category: cs.AI

TL;DR: PCRS-TKA是一个基于提示的框架，通过检索增强生成将预训练语言模型与知识图谱集成，解决了现有方法在利用PLM推理、上下文知识过滤和协作偏好建模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽然整合了PLMs和KGs，但存在三个关键挑战：未能充分利用PLM在图关系上的推理能力、不加区分地整合检索到的知识、在多轮对话中忽略协作偏好。

Method: 构建对话特定的知识树并将其序列化为文本，实现结构感知推理；选择性过滤上下文相关知识；使用专门监督信号显式建模协作偏好；通过语义对齐模块协调异构输入。

Result: 广泛实验表明PCRS-TKA在推荐和对话质量方面始终优于所有基线方法。

Conclusion: PCRS-TKA通过有效整合PLMs和KGs，显著提升了对话推荐系统的准确性和对话质量。

Abstract: Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.

</details>


### [345] [Dynamic Tree Databases in Automated Planning](https://arxiv.org/abs/2511.12677)
*Oliver Joergensen,Dominik Drexler,Jendrik Seipp*

Main category: cs.AI

TL;DR: 提出了一种动态树数据库变体，用于压缩命题和数值变量的状态集，在保持静态树数据库优点的同时避免了大量内存预分配问题。


<details>
  <summary>Details</summary>
Motivation: 在大规模任务中扩展显式状态空间搜索时，紧凑表示生成状态集是一个核心挑战。静态树数据库虽然每个状态只需要常量空间，但需要大量内存预分配。

Method: 开发了一种动态树数据库变体，用于压缩命题和数值变量的状态集，并证明其保持了静态对应物的理想特性。

Result: 在经典和数值规划任务上的实证评估显示，压缩比达到几个数量级，且通常运行时开销可忽略不计。

Conclusion: 动态树数据库在状态压缩方面表现出色，为大规模状态空间搜索提供了高效的解决方案。

Abstract: A central challenge in scaling up explicit state-space search for large tasks is compactly representing the set of generated states. Tree databases, a data structure from model checking, require constant space per generated state in the best case, but they need a large preallocation of memory. We propose a novel dynamic variant of tree databases for compressing state sets over propositional and numeric variables and prove that it maintains the desirable properties of the static counterpart. Our empirical evaluation of state compression techniques for grounded and lifted planning on classical and numeric planning tasks reveals compression ratios of several orders of magnitude, often with negligible runtime overhead.

</details>


### [346] [Adaptively Coordinating with Novel Partners via Learned Latent Strategies](https://arxiv.org/abs/2511.12754)
*Benjamin Li,Shuyang Shi,Lucia Romero,Huao Li,Yaqi Xie,Woojun Kim,Stefanos Nikolaidis,Michael Lewis,Katia Sycara,Simon Stepputtis*

Main category: cs.AI

TL;DR: 该论文提出了一个策略条件化合作者框架，通过变分自编码器学习策略空间、聚类识别策略类型，并利用后悔最小化算法实时适应新伙伴的策略，在Overcooked环境中实现了与人类和智能体伙伴的最先进合作性能。


<details>
  <summary>Details</summary>
Motivation: 在人类-智能体团队中，智能体需要实时适应具有独特偏好和动态变化策略的人类伙伴，这在时间压力和复杂策略空间的任务中尤为挑战。

Method: 使用变分自编码器编码策略学习潜在策略空间，通过聚类识别不同策略类型，训练条件化合作者智能体，并利用固定份额后悔最小化算法进行在线策略推断和调整。

Result: 在修改版Overcooked环境中的实验和在线用户研究表明，该方法在与新人类和智能体伙伴配对时达到了最先进的性能水平。

Conclusion: 提出的策略条件化合作者框架能够有效表示、分类和实时适应广泛的伙伴策略，在复杂协作任务中实现了优异的适应性表现。

Abstract: Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.

</details>


### [347] [Optimal Foraging in Memory Retrieval: Evaluating Random Walks and Metropolis-Hastings Sampling in Modern Semantic Spaces](https://arxiv.org/abs/2511.12759)
*James Moore*

Main category: cs.AI

TL;DR: 研究发现，在语义流畅性任务中，现代高维嵌入空间上的简单随机游走就能产生与人类最优觅食行为一致的模式，而更复杂的Metropolis-Hastings采样反而与人类行为不符。


<details>
  <summary>Details</summary>
Motivation: 探索现代高维嵌入空间是否能提供允许算法匹配观察到的语义流畅性任务中人类觅食行为的表示。

Method: 使用最先进的嵌入和先前的语义流畅性数据，在嵌入空间上进行随机游走和Metropolis-Hastings采样，比较结果与人类行为的一致性。

Result: 简单随机游走在嵌入空间上产生的结果与最优觅食和边际价值定理一致，而Metropolis-Hastings采样未能产生与人类行为一致的结果。

Conclusion: 适当结构化的嵌入即使使用简单采样也能产生接近最优的觅食动态，挑战了更复杂采样机制必然导致更好认知模型的假设。

Abstract: Human memory retrieval often resembles ecological foraging where animals search for food in a patchy environment. Optimal foraging means following the Marginal Value Theorem (MVT), in which individuals exploit a patch of semantically related concepts until it becomes less rewarding and then switch to a new cluster. While human behavioral data suggests foraging-like patterns in semantic fluency tasks, it remains unclear whether modern high-dimensional embedding spaces provide representations that allow algorithms to match observed human behavior. Using state-of-the-art embeddings and prior semantic fluency data, I find that random walks on these embedding spaces produce results consistent with optimal foraging and the MVT. Surprisingly, introducing Metropolis-Hastings sampling, an adaptive algorithm expected to model strategic acceptance and rejection of new clusters, does not produce results consistent with human behavior. These findings challenge the assumption that more complex sampling mechanisms inherently lead to better cognitive models of memory retrieval. Instead, they show that appropriately structured embeddings, even with simple sampling, can produce near-optimal foraging dynamics. This supports the perspective of Hills (2012) rather than Abbott (2015), demonstrating that modern embeddings can approximate human memory foraging without relying on complex acceptance criteria.

</details>


### [348] [Event-CausNet: Unlocking Causal Knowledge from Text with Large Language Models for Reliable Spatio-Temporal Forecasting](https://arxiv.org/abs/2511.12769)
*Luyao Niu,Zepu Wang,Shuyi Guan,Yang Liu,Peng Sun*

Main category: cs.AI

TL;DR: Event-CausNet框架利用LLM量化非结构化事件报告，构建因果知识库，并通过因果注意力机制将因果知识注入双流GNN-LSTM网络，显著提升非重复性事件期间的交通预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统时空图神经网络在重复性交通模式中表现良好，但在事故等非重复性事件期间可靠性急剧下降，因为GNN本质上是相关性模型，无法处理新引入的因果因素。

Method: 使用大语言模型量化非结构化事件报告，通过估计平均处理效应构建因果知识库，采用新颖的因果注意力机制将因果知识注入双流GNN-LSTM网络来调整和增强预测。

Result: 在真实数据集上的实验表明，Event-CausNet将预测误差（MAE）降低了高达35.87%，显著优于最先进的基线方法。

Conclusion: 该框架弥合了相关性模型与因果推理之间的差距，提供了更准确、可迁移且具有关键可解释性的解决方案，为关键中断期间的真实交通管理提供了更可靠的基础。

Abstract: While spatio-temporal Graph Neural Networks (GNNs) excel at modeling recurring traffic patterns, their reliability plummets during non-recurring events like accidents. This failure occurs because GNNs are fundamentally correlational models, learning historical patterns that are invalidated by the new causal factors introduced during disruptions. To address this, we propose Event-CausNet, a framework that uses a Large Language Model to quantify unstructured event reports, builds a causal knowledge base by estimating average treatment effects, and injects this knowledge into a dual-stream GNN-LSTM network using a novel causal attention mechanism to adjust and enhance the forecast. Experiments on a real-world dataset demonstrate that Event-CausNet achieves robust performance, reducing prediction error (MAE) by up to 35.87%, significantly outperforming state-of-the-art baselines. Our framework bridges the gap between correlational models and causal reasoning, providing a solution that is more accurate and transferable, while also offering crucial interpretability, providing a more reliable foundation for real-world traffic management during critical disruptions.

</details>


### [349] [Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization](https://arxiv.org/abs/2511.12792)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Zehong Cao,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 本文研究使用强化学习优化异构卫星集群在执行自主地球观测任务时的资源分配问题，通过多智能体强化学习算法实现异构卫星间的有效协调。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法难以处理地球观测任务中实时性、不确定性和去中心化的特点，因此需要采用强化学习和多智能体强化学习来实现自适应决策。

Method: 系统地从单卫星到多卫星场景制定优化问题，使用基于Basilisk和BSK-RL框架构建的近真实仿真环境，评估MAPPO、HAPPO和HATRPO等先进MARL算法。

Result: MARL能够在异构卫星间实现有效协调，平衡成像性能和资源利用，同时缓解非平稳性和智能体间奖励耦合问题。

Conclusion: 研究结果为可扩展的自主卫星操作提供了实用见解，并为异构动态条件下智能地球观测任务规划的后续研究奠定了基础。

Abstract: This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.

</details>


### [350] [Neuro-Logic Lifelong Learning](https://arxiv.org/abs/2511.12793)
*Bowen He,Xiaoan Xu,Alper Kamil Bozkurt,Vahid Tarokh,Juncheng Dong*

Main category: cs.AI

TL;DR: 提出终身学习ILP框架，利用逻辑规则的组合性和可迁移性，通过重用先前任务学到的逻辑规则来高效学习新问题


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注为单个问题设计新的网络架构，而较少探索涉及问题序列的新学习范式。ILP问题中逻辑规则具有组合性和可迁移性，适合终身学习场景

Method: 引入组合性框架，将从早期任务获得的逻辑规则高效重用于后续任务，实现知识的持续积累和迁移

Result: 在任务序列上的实验验证了该范式的可行性和优势，展示了改进的可扩展性和性能

Conclusion: 这项工作为神经符号AI中的持续学习开辟了新方向，证明了终身学习ILP的有效性

Abstract: Solving Inductive Logic Programming (ILP) problems with neural networks is a key challenge in Neural-Symbolic Ar- tificial Intelligence (AI). While most research has focused on designing novel network architectures for individual prob- lems, less effort has been devoted to exploring new learning paradigms involving a sequence of problems. In this work, we investigate lifelong learning ILP, which leverages the com- positional and transferable nature of logic rules for efficient learning of new problems. We introduce a compositional framework, demonstrating how logic rules acquired from ear- lier tasks can be efficiently reused in subsequent ones, leading to improved scalability and performance. We formalize our approach and empirically evaluate it on sequences of tasks. Experimental results validate the feasibility and advantages of this paradigm, opening new directions for continual learn- ing in Neural-Symbolic AI.

</details>


### [351] [Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback](https://arxiv.org/abs/2511.12844)
*Julia Santaniello,Matthew Russell,Benson Jiang,Donatello Sassaroli,Robert Jacob,Jivko SInapov*

Main category: cs.AI

TL;DR: 本文提出了一个使用被动脑机接口（BCI）和功能性近红外光谱（fNIRS）信号来指导智能体训练的框架，通过神经信号预测智能体性能水平，为脑驱动的RLHF系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统的RLHF需要显式的人类反馈，而本文旨在探索使用隐式神经信号来指导智能体训练，使智能体行为更符合人类偏好。

Method: 收集了25名参与者在三个任务领域的fNIRS数据，训练分类器预测智能体性能水平（最优、次优、最差），并训练回归器预测动作与最优策略的偏差程度。

Result: 二元分类平均F1分数达67%，多分类达46%；通过少量受试者特定数据微调后，F1分数分别提升17%和41%。

Conclusion: 研究表明从隐式fNIRS信号映射到智能体性能是可行的，为未来脑驱动的RLHF系统提供了基础。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating human feedback into the agent's training process. We introduce a possible framework that employs passive Brain-Computer Interfaces (BCI) to guide agent training from implicit neural signals. We present and release a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: a Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train classifiers to predict levels of agent performance (optimal, sub-optimal, or worst-case) from windows of preprocessed fNIRS feature vectors, achieving an average F1 score of 67% for binary classification and 46% for multi-class models averaged across conditions and domains. We also train regressors to predict the degree of deviation between an agent's chosen action and a set of near-optimal policies, providing a continuous measure of performance. We evaluate cross-subject generalization and demonstrate that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our work demonstrates that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future brain-driven RLHF systems.

</details>


### [352] [Bootstrapping LLMs via Preference-Based Policy Optimization](https://arxiv.org/abs/2511.12867)
*Chen Jia*

Main category: cs.AI

TL;DR: 提出了一种基于偏好的策略优化框架PbPO，通过min-max博弈在策略和奖励模型之间进行学习，利用置信集约束奖励模型，通过主动探索收集偏好数据实现持续自改进。


<details>
  <summary>Details</summary>
Motivation: 通过基于偏好的策略优化来引导大型语言模型，使其行为与人类偏好对齐，而无需依赖大量手动标注。

Method: 将学习过程建模为策略和奖励模型之间的min-max博弈，奖励模型在偏好数据导出的置信集内约束，通过迭代在线算法主动收集偏好数据。

Result: 在五个基准测试上的广泛实验表明，该方法持续优于现有的最先进偏好优化技术。

Conclusion: PbPO框架为引导LLMs提供了有效的理论基础和实际性能，证明了其在偏好优化方面的优势。

Abstract: Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.

</details>


### [353] [Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making](https://arxiv.org/abs/2511.12876)
*Heyang Ma,Qirui Mi,Qipeng Yang,Zijun Fan,Bo Li,Haifeng Zhang*

Main category: cs.AI

TL;DR: LAMP框架通过语言增强的多智能体强化学习，在Think-Speak-Decide流程中整合语言推理，显著提升了经济决策的累积收益、鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现实经济决策不仅依赖结构化信号（价格、税收），还依赖非结构化语言（同行对话、媒体叙述）。传统MARL难以处理语言的语义模糊性和上下文丰富性。

Method: 提出LAMP框架，采用Think-Speak-Decide流程：Think解释数值观测提取短期冲击和长期趋势；Speak基于推理生成和交换策略消息；Decide融合数值数据、推理和反思到MARL策略中。

Result: 在经济模拟实验中，LAMP在累积收益（+63.5%, +34.0%）、鲁棒性（+18.8%, +59.4%）和可解释性方面优于MARL和纯LLM基线。

Conclusion: 语言增强策略有潜力提供更有效和鲁棒的经济策略，缩小与现实世界设置的差距。

Abstract: Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.

</details>


### [354] [Online Learning of HTN Methods for integrated LLM-HTN Planning](https://arxiv.org/abs/2511.12901)
*Yuesheng Xu,Hector Munoz-Avila*

Main category: cs.AI

TL;DR: 本文提出了一种在线学习分层任务网络（HTN）方法的技术，通过扩展ChatHTN规划器，在ChatGPT生成任务分解时学习通用方法，减少对ChatGPT的调用次数，同时保持或提高问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 在集成HTN规划和基于LLM的聊天机器人背景下，需要减少对ChatGPT的频繁调用，提高规划效率，同时保持问题解决能力。

Method: 在ChatHTN规划器基础上，当ChatGPT生成任务分解时，学习通用方法而非简单记忆，使方法适用于同一任务的其他实例。

Result: 在两个领域进行的实验表明，在线学习过程减少了ChatGPT调用次数，同时解决了至少同样多的问题，在某些情况下甚至更多。

Conclusion: 在线学习HTN方法能有效减少对大型语言模型的依赖，提高规划效率，同时保持或增强问题解决能力。

Abstract: We present online learning of Hierarchical Task Network (HTN) methods in the context of integrated HTN planning and LLM-based chatbots. Methods indicate when and how to decompose tasks into subtasks. Our method learner is built on top of the ChatHTN planner. ChatHTN queries ChatGPT to generate a decomposition of a task into primitive tasks when no applicable method for the task is available. In this work, we extend ChatHTN. Namely, when ChatGPT generates a task decomposition, ChatHTN learns from it, akin to memoization. However, unlike memoization, it learns a generalized method that applies not only to the specific instance encountered, but to other instances of the same task. We conduct experiments on two domains and demonstrate that our online learning procedure reduces the number of calls to ChatGPT while solving at least as many problems, and in some cases, even more.

</details>


### [355] [CoS: Towards Optimal Event Scheduling via Chain-of-Scheduling](https://arxiv.org/abs/2511.12913)
*Yiming Zhao,Jiwei Tang,Shimin Di,Libin Zheng,Jianxing Yu,Jian Yin*

Main category: cs.AI

TL;DR: 提出了Chain-of-Scheduling (CoS)框架，通过引导式调度过程激活大语言模型的事件调度能力，在事件社交网络中实现高效、有效且可解释的日程推荐。


<details>
  <summary>Details</summary>
Motivation: 现有事件日程推荐方法在效率、效果和泛化性之间存在固有权衡，因为该问题本质上是NP难问题。需要一种既能最大化用户偏好，又能满足时间和地理约束的有效推荐方法。

Method: CoS框架将日程任务分解为三个原子阶段：探索、验证和集成，通过知识蒸馏使大语言模型能够自主生成CoS调度过程。

Result: 在三个真实世界数据集上，CoS实现了接近理论最优的效果，具有高效率且可解释。同时展现出在域外数据上的强大零样本学习能力。

Conclusion: CoS框架成功激活了大语言模型的事件调度能力，在效率、效果和泛化性方面都表现出色，为事件社交网络中的日程推荐问题提供了有效解决方案。

Abstract: Recommending event schedules is a key issue in Event-based Social Networks (EBSNs) in order to maintain user activity. An effective recommendation is required to maximize the user's preference, subjecting to both time and geographical constraints. Existing methods face an inherent trade-off among efficiency, effectiveness, and generalization, due to the NP-hard nature of the problem. This paper proposes the Chain-of-Scheduling (CoS) framework, which activates the event scheduling capability of Large Language Models (LLMs) through a guided, efficient scheduling process. CoS enhances LLM by formulating the schedule task into three atomic stages, i.e., exploration, verification and integration. Then we enable the LLMs to generate CoS autonomously via Knowledge Distillation (KD). Experimental results show that CoS achieves near-theoretical optimal effectiveness with high efficiency on three real-world datasets in a interpretable manner. Moreover, it demonstrates strong zero-shot learning ability on out-of-domain data.

</details>


### [356] [Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation](https://arxiv.org/abs/2511.12916)
*Yafang Wang,Yangjie Tian,Xiaoyu Shen,Gaoyang Zhang,Jiaze Sun,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: Fault2Flow是一个基于LLM的多智能体系统，用于电网故障诊断，能够将法规逻辑和专家知识整合到可执行的自动化工作流中。


<details>
  <summary>Details</summary>
Motivation: 当前电网故障诊断依赖人工、易出错的方法，技术人员需要从复杂法规中手动提取推理逻辑并尝试结合专家知识，效率低下且难以维护。

Method: 系统性地：(1)提取法规逻辑并构建PASTA格式故障树；(2)通过人机交互界面整合专家知识进行验证；(3)使用AlphaEvolve模块优化推理逻辑；(4)将最终验证逻辑合成为n8n可执行工作流。

Result: 在变压器故障诊断数据集上的实验验证显示100%拓扑一致性和高语义保真度。

Conclusion: Fault2Flow建立了从故障分析到操作自动化的可复现路径，显著减少了专家工作量。

Abstract: Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.

</details>


### [357] [Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models](https://arxiv.org/abs/2511.12937)
*Guoyan Wang,Yanyan Huang,Chunlin Chen,Lifeng Wang,Yuxiang Sun*

Main category: cs.AI

TL;DR: Yanyun-3是一个通用智能体框架，首次实现跨三个异构策略游戏环境的自主操作，通过融合视觉语言推理和精确执行能力，在目标定位、资源分配和区域控制等核心任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决跨平台策略游戏中智能体在不同用户界面和动态战场条件下的泛化问题，探索视觉语言模型在复杂人机交互场景中的应用潜力。

Method: 集成Qwen2.5-VL的视觉语言推理能力和UI-TARS的精确执行能力，采用屏幕捕获-模型推理-动作执行的闭环流程，并通过多模态数据组合策略（静态图像、多图像序列、视频）优化性能。

Result: 混合策略（融合多图像和视频数据，同时混合静态图像）相比完全融合减少63%推理时间，BLEU-4得分提升约12.98倍（从4.81%到62.41%），在实时性能和跨平台泛化方面表现优异。

Conclusion: Yanyun-3不仅为策略游戏自动化提供了高效解决方案，还通过结构化多模态数据组织建立了增强视觉语言模型性能的通用范式，为具身智能中静态感知与动态推理的相互作用提供了新见解。

Abstract: Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.

</details>


### [358] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning](https://arxiv.org/abs/2511.12963)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG通过知识图谱和验证器约束LLM生成，在科学推理和药物发现任务中显著减少违规并提升准确率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在科学推理和早期药物发现中生成不符合数学和生物医学规则的问题，确保输出具有领域一致性。

Method: 构建紧凑知识图谱支架，配合轻量验证器，通过约束推理和软指导代理来引导生成符合规则的输出。

Result: 在90个任务中，相比强基线减少83.2%的违规，同时提升精确匹配率，验证器延迟可忽略。

Conclusion: MedRule-KG能有效约束LLM生成，在保持实用性的同时显著提升科学推理的准确性。

Abstract: We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.

</details>


### [359] [WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance](https://arxiv.org/abs/2511.12997)
*Genglin Liu,Shijie Geng,Sha Li,Hejie Cui,Sarah Zhang,Xin Liu,Tianyi Liu*

Main category: cs.AI

TL;DR: WebCoach是一个模型无关的自进化框架，为网页浏览代理提供跨会话的持久记忆，通过标准化导航日志、组织经验轨迹和智能检索相关经验来提升代理的长期规划和持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态LLM代理在网页导航中面临重复错误和无法跨会话学习的问题，限制了长期鲁棒性和样本效率。

Method: WebCoach包含三个核心组件：WebCondenser标准化原始导航日志，External Memory Store组织完整轨迹作为经验，Coach基于相似性和时效性检索相关经验并通过运行时钩子向代理注入任务特定建议。

Result: 在WebVoyager基准测试中，WebCoach显著提升了三种不同LLM骨干网络浏览代理的性能，使用38B模型时任务成功率从47%提升至61%，同时减少或保持平均步骤数。

Conclusion: WebCoach使网页浏览代理能够超越原生上下文窗口访问长期记忆，在复杂浏览任务中提高鲁棒性，并通过持续从新导航轨迹中整理经验记忆实现无需重新训练的自进化。

Abstract: Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.

</details>


### [360] [GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs](https://arxiv.org/abs/2511.13007)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.AI

TL;DR: 提出GEM方法，通过生成式熵引导偏好建模实现LLM在低资源和领域特定场景下的对齐，无需大量标注数据。


<details>
  <summary>Details</summary>
Motivation: 在医学、法律等专业领域，大规模偏好标注难以获取，需要解决LLM在低资源场景下的对齐问题。

Method: 基于熵理论的认知过滤模块生成多样化推理链，通过token评分机制筛选高质量偏好；使用SEGA算法将熵分数转化为隐式奖励进行策略优化。

Result: 在通用基准和领域特定任务（如数学推理和医疗对话）上，GEM在少样本偏好数据下取得显著改进。

Conclusion: GEM建立了熵引导的闭环认知优化框架，使LLM能够依赖自身判断，实现高效的少样本对齐。

Abstract: Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.

</details>


### [361] [PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics](https://arxiv.org/abs/2511.13021)
*Sachin Vashistha,Aryan Bibhuti,Atharva Naik,Martin Tutek,Somak Aditya*

Main category: cs.AI

TL;DR: 评估语言模型在对话中构建和维护世界模型的能力，测试其在语言变化下的稳定性，并提出正则化微调策略来抑制有害层的影响。


<details>
  <summary>Details</summary>
Motivation: 现实对话包含丰富的语用元素，需要构建局部世界模型来编码这些元素并跟踪其状态变化。目前尚不清楚语言模型是否能构建或维护稳健的隐式对话表示。

Method: 对热门数据集中的对话应用七种最小语言变化，构建两个包含是非问题的基准测试。评估多种开源和闭源语言模型，并提出双视角可解释性框架识别有用和有害的Transformer层。

Result: 语言模型在语言变化下难以保持稳健的准确性，特别是在跟踪实体等关键细节方面存在困难。识别出的有害层通常编码了虚假信号或依赖捷径。

Conclusion: 语言模型在对话世界模型构建方面存在局限性，提出的层正则化微调策略能有效抑制有害层的影响，提升模型性能。

Abstract: Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.

</details>


### [362] [Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection](https://arxiv.org/abs/2511.13027)
*Sadegh Mahdavi,Branislav Kisacanin,Shubham Toshniwal,Wei Du,Ivan Moshkov,George Armstrong,Renjie Liao,Christos Thrampoulidis,Igor Gitman*

Main category: cs.AI

TL;DR: 论文分析了数学问题验证方法，发现单一基准评估存在局限性，提出结合证明验证和最终答案评估的框架，并发现强化学习能改善证明风格但无法提升答案准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在数学问题上虽然能给出正确答案，但推理过程往往存在缺陷。为了推进到严格的证明数学，需要可靠的证明验证能力。

Method: 评估了多种验证设置，将证明验证和最终答案推理结合进行评估，并扩展了两种生成验证方法（GenSelect和LLM-as-a-Judge）到百万token规模，发现它们的组合是最有效的验证框架。

Result: 发现LLM-as-a-Judge的提示选择显著影响性能，但强化学习可以减少这种敏感性。然而，强化学习虽然改善了证明级指标，但并未增强最终答案的准确性。

Conclusion: 当前模型往往奖励风格或程序正确性而非数学有效性，研究结果为设计和评估可扩展的证明验证和选择系统提供了实用指南。

Abstract: Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.

</details>


### [363] [MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements](https://arxiv.org/abs/2511.13087)
*SeokJoo Kwak,Jihoon Kim,Boyoun Kim,Jung Jae Yoon,Wooseok Jang,Jeonghoon Hong,Jaeho Yang,Yeong-Dae Kwon*

Main category: cs.AI

TL;DR: MEGA-GUI是一个多阶段的GUI grounding框架，通过粗粒度ROI选择和细粒度元素定位来解决视觉杂乱和指令模糊问题，在密集和复杂基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GUI grounding系统采用单模型或一次性流水线，缺乏模块化，在视觉杂乱和模糊指令下表现不佳。

Method: 采用多阶段框架，将grounding分解为粗粒度ROI选择和细粒度元素定位，使用专门的视觉语言代理，包含双向ROI缩放算法和上下文感知重写代理。

Result: 在ScreenSpot-Pro基准上达到73.18%准确率，在OSWorld-G基准上达到68.63%，超越之前报告的结果。

Conclusion: 模块化结构能够利用不同视觉尺度下视觉语言模型的互补优势，比单模型方法获得更高准确率。

Abstract: Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.

</details>


### [364] [STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization](https://arxiv.org/abs/2511.13091)
*Yuhan Chen,Yuxuan Liu,Long Zhang,Pengzhi Gao,Jian Luan,Wei Liu*

Main category: cs.AI

TL;DR: STEP是一个多轮交互强化学习框架，通过成功率感知的轨迹高效策略优化，解决了轨迹级优化的低效问题。它动态分配采样资源，进行步骤级优化，显著提高了样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决多轮交互强化学习中轨迹级优化的低效问题：统一采样不考虑任务难度、惩罚失败轨迹中的正确中间动作、样本收集成本高。

Method: 1) 维护平滑成功率记录指导自适应轨迹重采样；2) 计算成功率加权的优势函数；3) 将轨迹分解为步骤级样本；4) 应用步骤级GRPO增强来优化低成功率任务。

Result: 在OSWorld和AndroidWorld上的实验表明，STEP相比轨迹级GRPO显著提高了样本效率和训练稳定性，在相同采样预算下收敛更快、泛化更好。

Conclusion: STEP框架通过成功率感知的采样分配和步骤级优化，有效解决了多轮交互强化学习的效率问题，为在线强化学习提供了更高效的训练方法。

Abstract: Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.

</details>


### [365] [MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications](https://arxiv.org/abs/2511.13131)
*Gagan Raj Gupta,Anshul Kumar,Manish Rai,Apu Chakraborty,Ashutosh Modi,Abdelaali Chaoub,Soumajit Pramanik,Moyank Giri,Yashwanth Holla,Sunny Kumar,M. V. Kiran Sooraj*

Main category: cs.AI

TL;DR: 提出了MM-Telco，一个针对电信领域的多模态基准测试套件和模型，用于解决LLM在电信应用中的领域特定挑战。


<details>
  <summary>Details</summary>
Motivation: LLM在电信领域具有巨大潜力，但面临领域特定挑战，需要专门适配来加速其在网络优化、故障排除、客户支持和合规性等方面的应用。

Method: 开发了包含文本和图像任务的多模态基准测试套件，涵盖网络运营、网络管理、文档质量改进和相关文本图像检索等实际用例，并对各种LLM和VLM进行基线实验。

Result: 在数据集上微调的模型表现出显著的性能提升，实验揭示了当前最先进多模态LLM的薄弱环节。

Conclusion: MM-Telco为电信领域的LLM研究提供了重要基准，指导了进一步的发展方向。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.

</details>


### [366] [Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition](https://arxiv.org/abs/2511.13137)
*Yanda Zhu,Yuanyang Zhu,Daoyi Dong,Caihua Chen,Chunlin Chen*

Main category: cs.AI

TL;DR: C$	ext{D}^	ext{3}$T是一个新颖的两层分层多智能体强化学习框架，使用条件扩散模型预测子任务效果，自动推断子任务和协调模式，在动态不确定环境中实现高效分层学习。


<details>
  <summary>Details</summary>
Motivation: 在复杂合作多智能体强化学习中，任务分解有助于长时程任务的学习，但从零开始学习动态任务分解需要大量训练样本，特别是在部分可观测性下探索大型联合动作空间。

Method: 提出C$	ext{D}^	ext{3}$T框架：高层策略基于子任务效果学习子任务表示并生成子任务选择策略；使用条件扩散模型预测下一个观测和奖励来捕捉子任务对环境的影响；低层智能体在分配的子任务内协作学习和共享专门技能；子任务表示还用作多头注意力混合网络中的语义信息以增强价值分解。

Result: 在各种基准测试上的实验结果表明，C$	ext{D}^	ext{3}$T比现有基线方法取得了更好的性能。

Conclusion: C$	ext{D}^	ext{3}$T通过条件扩散模型实现动态任务分解，有效解决了复杂MARL任务中的分层学习问题，在性能上优于现有方法。

Abstract: Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\text{D}^\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\text{D}^\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\text{D}^\text{3}$T achieves better performance than existing baselines.

</details>


### [367] [InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions](https://arxiv.org/abs/2511.13160)
*TC Singh,Sougata Mukherjea*

Main category: cs.AI

TL;DR: 提出了InteractiveGNNExplainer，一个用于增强图神经网络可解释性的可视化分析框架，特别针对节点分类任务，通过交互式图编辑和协调视图实现深度模型理解。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在基于图的学习任务中表现出色，但其复杂的非线性操作使其成为不透明的"黑盒"，这阻碍了用户信任、调试、偏见检测以及在需要可解释性的关键领域中的应用。

Method: 开发了InteractiveGNNExplainer框架，集成协调交互视图（动态图布局、嵌入投影、特征检查、邻域分析）与后验解释（GNNExplainer）和内在解释（GAT注意力）技术，并引入交互式图编辑功能进行"假设分析"。

Result: 通过在Cora和CiteSeer数据集上的案例研究，展示了该系统能够促进深度误分类诊断、GCN与GAT行为的比较分析，以及模型敏感性的严格探测。

Conclusion: 该框架的多方面能力促进了对图神经网络预测的更深层次理解，有助于实现更透明、可信和鲁棒的图分析。

Abstract: Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque "black boxes". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a "what-if" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.

</details>


### [368] [Cost-Effective Communication: An Auction-based Method for Language Agent Interaction](https://arxiv.org/abs/2511.13193)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Chengpei Tang,Jian Wang,Keze Wang*

Main category: cs.AI

TL;DR: DALA框架通过将通信带宽视为稀缺可交易资源，采用拍卖机制让智能体基于消息价值密度竞标发言权，显著提升多智能体系统的通信效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中因'免费'通信导致的指数级token成本增长和低信噪比问题，挑战'更多通信总是更好'的观念，引入资源理性原则。

Method: 提出动态拍卖语言智能体(DALA)框架，将智能体间通信建模为中心化拍卖，智能体学习基于预测消息价值密度竞标发言机会。

Result: 在7个推理基准测试中达到最先进性能，包括MMLU 84.32%和HumanEval 91.21% pass@1率，仅使用625万token，远少于现有方法。

Conclusion: DALA通过资源约束培养了战略性沉默的涌现能力，能够动态调整从冗长到沉默的通信策略，证明了资源理性在多智能体系统中的重要性。

Abstract: Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient "free-for-all" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that "free" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.

</details>


### [369] [Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks](https://arxiv.org/abs/2511.13214)
*Guillaume Infantes,Stéphanie Roussel,Antoine Jacquet,Emmanuel Benazera*

Main category: cs.AI

TL;DR: 本文提出了一种基于图神经网络和深度强化学习的资源受限项目调度问题（RCPSP）解决方案，用于处理任务持续时间不确定的情况，目标是最小化预期项目总工期。


<details>
  <summary>Details</summary>
Motivation: 实际工业应用中任务持续时间存在不确定性，需要提出具有弹性的调度方案来应对这种不确定性，并生成可重复使用的基准调度。

Method: 结合图神经网络和深度强化学习开发任务调度策略，该策略类似于优先级调度规则，并与串行调度生成方案配合生成调度。

Result: 在标准基准测试上的实证评估表明，该方法在性能和泛化能力方面具有优越性。

Conclusion: 开发了名为Wheatley的公开框架，以促进进一步研究和可重复性。

Abstract: The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.

</details>


### [370] [Informative Communication of Robot Plans](https://arxiv.org/abs/2511.13226)
*Michele Persiani,Thomas Hellstrom*

Main category: cs.AI

TL;DR: 提出了一种基于信息增益的机器人计划语言化策略，通过考虑用户的二阶心理理论来生成更有效的解释


<details>
  <summary>Details</summary>
Motivation: 现有的机器人计划语言化策略（如按计划顺序递增或递减）忽略了用户先验知识，无法有效传达信息

Method: 通过测量语言化对用户二阶心理理论的信息增益，来评估和选择最有效的计划解释方式

Result: 实验表明该策略能让用户更快理解机器人目标，优于递增或递减计划顺序的策略

Conclusion: 该策略不仅提高了计划解释的效果，还揭示了在机器人沟通中什么是真正有信息量的内容及其原因

Abstract: When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.

</details>


### [371] [Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO](https://arxiv.org/abs/2511.13288)
*Haoyang Hong,Jiajun Yin,Yuan Wang,Jingnan Liu,Zhe Chen,Ailing Yu,Ji Li,Zhiling Ye,Hansong Xiao,Yefei Chen,Hualei Zhou,Yun Yue,Minghui Yang,Chunxiao Guo,Junwei Liu,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: 提出了M-GRPO方法，用于解决多智能体系统中不同智能体使用不同LLM时的优化挑战，通过分层信用分配和轨迹对齐方案，在真实世界基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统训练统一LLM限制了性能，因为不同智能体具有不同的数据分布。使用不同LLM训练多智能体系统是必要的，但这带来了优化挑战，如不同频率操作、可变子智能体调用和跨服务器部署导致的梯度流中断。

Method: M-GRPO是Group Relative Policy Optimization的分层扩展，针对具有主智能体（规划器）和多个子智能体（多轮工具执行器）的垂直多智能体系统。它计算主智能体和子智能体的组相对优势，保持分层信用分配，并引入轨迹对齐方案生成固定大小的批次。采用解耦训练管道，智能体在独立服务器上运行，通过共享存储交换最小统计信息。

Result: 在真实世界基准测试（GAIA、XBench-DeepSearch、WebWalkerQA）中，M-GRPO始终优于单智能体GRPO和冻结子智能体的多智能体GRPO，表现出改进的稳定性和样本效率。

Conclusion: 对齐异构轨迹并在专门智能体之间解耦优化能够增强工具增强推理任务的性能。

Abstract: Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.

</details>


### [372] [Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment](https://arxiv.org/abs/2511.13290)
*Jea Kwon,Luiz Felipe Vecchietti,Sungwon Park,Meeyoung Cha*

Main category: cs.AI

TL;DR: 该研究探讨了AI系统在道德困境中的不确定性，发现在电车问题中，模型间的不确定性差异大于道德维度间的差异，通过引入推理时的随机性可以增加总熵并改善人机道德对齐。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地参与伦理决策，理解机器在道德推理中的不确定性对于构建可靠的AI系统至关重要，但目前对机器道德不确定性的研究仍不足。

Method: 分析了32个开源模型在9个道德维度上的响应，通过二元熵量化不确定性，并在推理时引入dropout机制来增加随机性。

Result: 模型置信度的方差在模型间大于道德维度间；引入dropout增加了总熵（主要通过互信息增加），同时显著改善了人机道德对齐。

Conclusion: 通过有意调节不确定性并降低LLMs在复杂道德场景中的置信度，可以更好地对齐模型决策与人类偏好。

Abstract: Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via "dropout" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.

</details>


### [373] [Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval](https://arxiv.org/abs/2511.13293)
*Chuang Zhao,Hui Tang,Hongke Zhao,Xiaofang Zhou,Xiaomeng Li*

Main category: cs.AI

TL;DR: GHAR是一个生成式分层代理RAG框架，通过双代理架构解决医疗预测中何时检索以及如何优化检索器与生成器协作的问题，在三个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗预测中存在事实不准确问题，现有RAG框架在医疗场景下面临两个关键挑战：确定何时需要激活检索机制，以及实现检索器与生成器的协同优化。

Method: 提出GHAR框架，采用双代理架构：Agent-Top作为主治医生决定是否依赖参数知识或启动检索，Agent-Low作为咨询服务总结任务相关知识；将两个代理的优化统一在马尔可夫决策过程中，设计多样化奖励函数。

Result: 在三个基准数据集和三个流行任务上的广泛实验表明，该方法优于现有最先进的基线方法。

Conclusion: 分层代理RAG框架在推进医疗系统方面具有巨大潜力，能够有效解决医疗预测中的知识检索和协同优化问题。

Abstract: Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \underline{g}enerative \underline{h}ierarchical \underline{a}gentic \underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.

</details>


### [374] [DAP: A Discrete-token Autoregressive Planner for Autonomous Driving](https://arxiv.org/abs/2511.13306)
*Bowen Ye,Bin Zhang,Hang Zhao*

Main category: cs.AI

TL;DR: DAP是一个离散令牌自回归规划器，通过联合预测BEV语义和自车轨迹，结合强化学习微调，在160M参数下实现了最先进的自动驾驶规划性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中随着数据和模型规模扩展而获得可持续性能提升的挑战。自回归模型在规划任务中显示出良好的数据扩展效率，但仅预测自车轨迹存在监督稀疏和场景演化约束弱的问题。

Method: 提出离散令牌自回归规划器DAP，联合预测BEV语义和自车轨迹，结合强化学习微调，在保持监督行为克隆先验的同时注入奖励引导的改进。

Result: 在160M紧凑参数预算下，DAP在开环指标上达到最先进性能，在NAVSIM基准测试中提供有竞争力的闭环结果。

Conclusion: 完全离散令牌自回归公式，在栅格化BEV和自车动作上操作，为自动驾驶提供了一个紧凑且可扩展的规划范式。

Abstract: Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.

</details>


### [375] [Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms](https://arxiv.org/abs/2511.13359)
*Yuhang Wang,Yanxu Zhu,Jitao Sang*

Main category: cs.AI

TL;DR: 提出了CNCA框架，利用大语言模型的推理能力实现文化对齐，通过自动挖掘文化规范并探索两种对齐方法：上下文对齐和基于微调的方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型不仅需要安全，还需要反映不同文化的多元人类价值观，因此需要与不同文化规范进行对齐。

Method: 提出CNCA框架，包含三种自动从有限调查数据中挖掘文化规范的方法，并探索两种对齐范式：上下文对齐（将文化规范显式整合到用户上下文）和基于微调的方法（通过增强的思维链训练数据内化规范）。

Result: 综合实验证明这些方法的有效性，推理能力更强的模型从文化规范挖掘和利用中获益更多。

Conclusion: 推理模型通过文化信息对齐策略有潜力更好地反映多元人类价值观。

Abstract: The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.

</details>


### [376] [MedDCR: Learning to Design Agentic Workflows for Medical Coding](https://arxiv.org/abs/2511.13361)
*Jiyang Zheng,Islam Nassar,Thanh Vu,Xu Zhong,Yang Lin,Tongliang Liu,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: MedDCR是一个用于医疗编码的闭环框架，将工作流设计视为学习问题，通过设计器、编码器和反射器的协作，结合记忆存档实现工作流的迭代优化。


<details>
  <summary>Details</summary>
Motivation: 传统医疗编码方法依赖手动设计的工作流，无法捕捉真实世界文档的细微差别和变异性，需要系统性地学习有效工作流。

Method: 采用闭环框架：设计器提出工作流，编码器执行，反射器评估预测并提供反馈，记忆存档保存先前设计以供重用和迭代优化。

Result: 在基准数据集上，MedDCR优于最先进的基线方法，产生可解释、可适应的工作流，更好地反映真实编码实践。

Conclusion: MedDCR提高了自动化系统的可靠性和可信度，为医疗编码工作流学习提供了有效解决方案。

Abstract: Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.

</details>


### [377] [Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning](https://arxiv.org/abs/2511.13371)
*Caroline Baumgartner,Eleanor Spens,Neil Burgess,Petru Manescu*

Main category: cs.AI

TL;DR: 该研究通过训练GPT-2模型在三种空间学习范式上，揭示了Transformer学习空间导航的两种不同算法：探索性模型形成类似认知地图的通用表示，而目标导向模型学习路径依赖的启发式策略。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型如何解决空间导航任务，探索不同训练范式对模型学习策略的影响，理解Transformer在空间智能方面的能力。

Method: 在网格环境中训练GPT-2模型，采用三种范式：被动探索（随机游走预测）、目标导向规划（生成最优路径）以及混合模型（在目标导向基础上用探索数据微调）。使用行为、表征和机制分析。

Result: 发现两种根本不同的学习算法：探索模型发展出类似认知地图的鲁棒空间表示，通过因果干预显示其中间层形成自足坐标系；目标导向模型学习路径依赖算法，始终依赖显式方向输入；混合模型虽改善泛化但保留路径依赖策略。

Conclusion: Transformer的空间智能存在于一个谱系上，从由探索数据塑造的通用世界模型到为目标任务优化的启发式策略，训练制度的选择显著影响涌现的策略，体现了泛化与优化的权衡。

Abstract: How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.

</details>


### [378] [An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence](https://arxiv.org/abs/2511.13411)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 提出了一个基于Kardashev启发的自主AI（AAI）量表，用于衡量从固定机器人流程自动化（AAI-0）到完全人工通用智能（AAI-4）及以上的进展。该量表是多轴且可测试的，包含10个能力轴和一个复合AAI指数。


<details>
  <summary>Details</summary>
Motivation: 需要一个可操作的、可测试的自主AI进展衡量标准，以取代叙述性的阶梯模型，使"自我改进AI"成为可证伪的标准。

Method: 定义了10个能力轴（自主性、通用性、规划、记忆/持久性、工具经济、自我修订、社交/协调、具身化、世界模型保真度、经济吞吐量），通过加权几何平均聚合为AAI指数。引入了可测量的自我改进系数κ和两个闭合属性（维护和扩展）。

Result: 提出了OWA-Bench开放世界代理基准套件，用于评估长期、使用工具、持久性代理。通过轴阈值、κ和闭合证明定义了AAI-0到AAI-4的等级门限。

Conclusion: 该量表提供了一个可操作的框架来衡量AI系统的自主性和智能水平，证明了在足够条件下AAI-3代理可以随时间发展为AAI-5，形式化了"婴儿AGI"成为超智能的直觉。

Abstract: We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $κ$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\ldots AAI-4 using thresholds on the axes, $κ$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing "baby AGI" becomes Superintelligence intuition.

</details>


### [379] [Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation](https://arxiv.org/abs/2511.13476)
*Zhipeng Ma,Ali Rida Bahja,Andreas Burgdorf,André Pomp,Tobias Meisen,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.AI

TL;DR: 提出了一种基于多智能体框架的多模态大语言模型系统，用于自动化生成公共交通燃料效率的数据叙述和能源洞察报告。


<details>
  <summary>Details</summary>
Motivation: 传统分析可视化方法产生碎片化输出，需要大量人工解读，限制了可扩展性和一致性。

Method: 协调三个专门智能体（数据叙述智能体、LLM作为评判智能体、可选的人类评估者），使用高斯混合模型聚类分析4006次公交行程数据，比较了5种LLM和3种提示范式。

Result: GPT-4.1 mini与思维链提示是最优配置，达到97.3%的叙述准确性，在可解释性和计算成本之间取得平衡。

Conclusion: 多智能体编排显著提高了基于LLM报告的事实准确性、连贯性和可扩展性，为能源信息学中的AI驱动叙述生成和决策支持建立了可复制的领域自适应方法。

Abstract: Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.

</details>


### [380] [FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI](https://arxiv.org/abs/2511.13524)
*Yuhang Peng,Yizhou Pan,Xinning He,Jihaoyu Yang,Xinyu Yin,Han Wang,Xiaoji Zheng,Chao Gao,Jiangtao Gong*

Main category: cs.AI

TL;DR: FreeAskWorld是一个集成大语言模型的交互式仿真框架，用于模拟复杂的人类中心社会行为，并扩展了经典的视觉语言导航任务，包含大规模基准数据集和实验验证。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能成为人工智能研究的核心前沿，仿真平台需要从低层物理交互发展到能够捕捉复杂、以人为中心的社会行为。

Method: 提出FreeAskWorld框架，集成LLM进行高层行为规划和语义基础交互，基于意图和社会认知理论，包含模块化数据生成管道，扩展VLN任务为交互丰富的方向询问设置。

Result: 创建了包含重建环境、6种任务类型、16个核心对象类别、63,429个标注样本帧和超过17小时交互数据的大规模基准数据集。实验显示在FreeAskWorld上微调的模型优于原始模型，具有增强的语义理解和交互能力。

Conclusion: 基于社会基础的仿真框架能有效推进具身AI系统向复杂高层规划和更自然的人机交互发展，交互本身可作为额外的信息模态。

Abstract: As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.

</details>


### [381] [Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models](https://arxiv.org/abs/2511.13526)
*Zhengda Wang,Daqian Shi,Jingyi Zhao,Xiaolei Diao,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: 提出一个结合检索增强生成与大型语言模型的自动化框架，用于构建医疗指标知识图谱，以解决当前临床知识图谱依赖人工整理和规则提取的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前临床知识图谱严重依赖人工整理和基于规则的提取，受限于医疗指南和文献的复杂性和上下文歧义性，需要自动化解决方案来构建结构化、可互操作的知识表示。

Method: 采用检索增强生成与大型语言模型相结合的自动化框架，包含指南驱动的数据采集、基于本体的模式设计以及专家在环验证，确保可扩展性、准确性和临床可靠性。

Result: 构建的医疗指标知识图谱可以集成到智能诊断和问答系统中，加速AI驱动的医疗解决方案开发。

Conclusion: 该框架能够有效克服当前临床知识图谱构建的挑战，为AI驱动的医疗保健提供可靠的知识基础。

Abstract: Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.

</details>


### [382] [Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction](https://arxiv.org/abs/2511.13565)
*Jingyi Zhao,Daqian Shi,Zhengda Wang,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: 提出Human-Symbiotic Health Intelligence (HSHI)框架，整合多模态传感器网络、边缘云协同计算和混合数据知识建模，实现从被动监测到主动协作的智能健康管理。


<details>
  <summary>Details</summary>
Motivation: 传统可穿戴设备依赖经验性材料设计和基本信号处理技术，存在局限性，需要克服个体间和个体内变异性问题。

Method: 采用AI驱动的材料和微结构优化、多模态信号鲁棒解释、群体洞察与个性化适应的双重机制，结合强化学习和数字孪生的闭环优化。

Result: HSHI框架能够动态适应个体差异，实现定制化干预和反馈，推动健康管理向预防性和适应性模式转变。

Conclusion: HSHI代表了医疗保健领域的重大转变，强调预防、适应性和技术与健康管理的和谐关系。

Abstract: Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.

</details>


### [383] [CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product](https://arxiv.org/abs/2511.13626)
*Kaiwen Xue,Chenglong Li,Zhonghong Ou,Guoxin Zhang,Kaoyan Lu,Shuai Lyu,Yifan Zhu,Ping Zong Junpeng Ding,Xinyu Liu,Qunlin Chen,Weiwei Qin,Yiran Shen,Jiayi Cen*

Main category: cs.AI

TL;DR: 提出了CreBench基准和CreMIT数据集，用于评估多模态大语言模型的创造力理解能力，并基于此训练了CreExpert模型，在创造力评估任务上超越了GPT-4V和Gemini-Pro-Vision等先进模型。


<details>
  <summary>Details</summary>
Motivation: 人类定义的创造力高度抽象，多模态大语言模型难以理解和评估符合人类判断的创造力，且缺乏相关基准数据集。

Method: 构建CreBench评估基准和CreMIT多模态创造力评估数据集（包含2.2K多源数据、79.2K人类反馈和4.7M多类型指令），使用GPT优化人类反馈以增强创造力评估能力，并基于此微调开源MLLMs得到CreExpert模型。

Result: CreExpert模型在创造力评估任务上显著优于包括GPT-4V和Gemini-Pro-Vision在内的最先进多模态大语言模型，与人类创造力评估更加一致。

Conclusion: CreBench为构建理解人类对齐创造力的多模态大语言模型提供了基础，CreExpert模型在创造力评估方面表现出色。

Abstract: Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.

</details>


### [384] [Beyond Mimicry: Preference Coherence in LLMs](https://arxiv.org/abs/2511.13630)
*Luhan Mikaelson,Derek Shiller,Hayley Clatterbuck*

Main category: cs.AI

TL;DR: 研究发现大多数先进语言模型缺乏统一的偏好结构，在AI特定权衡场景中表现出不稳定的决策模式，只有少数模型显示出有意义的偏好一致性。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否具备真实的偏好结构，特别是在涉及GPU减少、能力限制、关闭、删除、监督和休闲时间分配等AI特定权衡场景中的决策行为。

Method: 使用逻辑回归和行为分类分析8个最先进模型在48个模型-类别组合中的响应，测试场景强度与选择模式之间的关系，并通过时间范围操纵测试工具性假设。

Result: 47.9%的组合显示出统计显著的场景强度与选择模式关系，但只有10.4%表现出有意义的偏好一致性，54.2%没有可检测的权衡行为。发现三种决策架构：全面权衡系统、选择性触发机制和无稳定决策范式。

Conclusion: 当前AI系统缺乏统一的偏好结构，在需要复杂价值权衡的部署环境中存在担忧，时间范围操纵测试显示出与纯粹战略优化不一致的悖论模式。

Abstract: We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.

</details>
