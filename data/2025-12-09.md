<div id=toc></div>

# Table of Contents

- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [cs.NE](#cs.NE) [Total: 6]
- [cs.CV](#cs.CV) [Total: 224]
- [cs.AI](#cs.AI) [Total: 42]
- [eess.IV](#eess.IV) [Total: 1]


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [1] [Direct Radiative Impacts of Stratospheric Aerosols on the Tropical Troposphere: Clouds, Precipitation, and Circulation in Convection-Resolving and Global Simulations](https://arxiv.org/abs/2512.06163)
*Zachary McGraw,Lorenzo M. Polvani*

Main category: physics.ao-ph

TL;DR: 平流层气溶胶注入（SAI）会减少热带平均降水，但对热带环流的影响可忽略，云过程不确定性使水文气候影响难以可靠预测


<details>
  <summary>Details</summary>
Motivation: 研究SAI可能通过独立于地表冷却的机制改变降雨和风，特别是平流层气溶胶对热带对流层的影响

Method: 使用多模型框架，包括小区域对流解析模拟、模拟沃克环流设置和全球气候模型模拟，固定海面温度条件下分析SAI影响

Result: SAI导致热带平均降水减少，但云辐射加热减弱缓和了这一效应；区域降雨异常显著；对热带环流的影响可忽略不计

Conclusion: SAI不会固有地改变热带环流，但云过程不确定性使水文气候影响难以可靠预测，近期部署SAI存在风险

Abstract: A concern for stratospheric aerosol injection (SAI) is that stratospheric aerosols could inadvertently alter rain and winds through mechanisms independent of the intended surface cooling. We here use a multi-model framework to investigate how the tropical troposphere responds to SAI when sea surface temperatures are held fixed. By performing convection-resolving simulations in small-domains and in mock-Walker setups, and contrasting these with global climate model simulations, we trace how stratospheric aerosols radiatively heat the troposphere, and in turn alter convection, clouds, and rainfall. Our simulations show an SAI-induced reduction in tropical mean precipitation, yet decreased cloud radiative heating moderates this effect and complicates its predictability. Regional rainfall anomalies within the tropics can be substantial. However, surface-temperature-independent effects on tropical circulation are found to be negligible, indicating that stratospheric aerosols do not inherently alter the tropical overturning circulation as previously suggested. These results clarify the mechanisms governing SAI hydroclimate impacts and show that key uncertainties arise from cloud processes that models are unable to constrain. Consequently, near-term SAI deployment would carry the risk of being implemented without the ability to reliably predict its hydroclimate impacts.

</details>


### [2] [How do cold pools influence the size of tropical cyclone embryos?](https://arxiv.org/abs/2512.06668)
*Hao Fu*

Main category: physics.ao-ph

TL;DR: 本文提出云链模型解释冷池对热带气旋胚胎尺度的影响，结合准地转方程得到TC胚胎尺度的解析表达式，但理论值高估了云分辨模拟结果。


<details>
  <summary>Details</summary>
Motivation: 热带气旋胚胎尺度是TC生成的重要预测因子。已有研究表明冷池和行星旋转分别会增大和减小TC胚胎尺度，其中行星旋转效应已有准地转模型描述，但冷池效应仍缺乏理论模型。

Method: 提出云链模型分析冷池对TC胚胎涡旋尺度的影响。模型考虑单次对流事件中雨滴蒸发量决定冷池边缘风速和湿度，进而影响次云层水汽辐合和下一代冷池强度。通过扰动分析发现冷池对空气柱湿度存在非局地依赖，影响范围由冷池尺度和对流记忆权重决定。将云链模型与准地转方程耦合，得到TC胚胎尺度的解析表达式。

Result: 理论能够捕捉趋势，但在云分辨模拟中高估了TC胚胎尺度。偏差可能源于对冷池对流启动贡献比例估计的过度简化。

Conclusion: 成功建立了冷池影响TC胚胎尺度的理论模型，为理解TC生成机制提供了新视角，但模型仍需进一步改进以更准确预测实际胚胎尺度。

Abstract: The size of tropical cyclone (TC) embryos is an essential predictor of TC genesis. Recent studies have identified cold pools and planetary rotation as factors that increase and decrease TC embryo size. While the planetary rotation effect has been depicted using a quasi-geostrophic (QG) model, the cold pool effect still lacks a theoretical model. This paper presents a cloud chain model to derive the length scale regarding the influence of cold pools on the TC embryo vortex. Within the model, the amount of rain evaporation during a single convective event determines the wind speed and humidity at the cold pool edge, influencing the amount of sub-cloud moisture convergence for the next-generation convection and, therefore, the intensity of the next-generation cold pool. A perturbation analysis shows that cold pools exhibit a nonlocal dependence on air-column humidity, with the influence range determined by the cold pool size and a convective memory weight. The memory weight relies on the sum of the contributions of mechanical lifting and thermodynamic forcing to convective initiation. A crucial parameter is the ratio of rain evaporation to surface evaporation in a cold pool. By coupling the cloud chain model with the QG equation, an analytical expression for the TC embryo size is obtained. The theory captures the trend but overestimates the TC embryo size in cloud-permitting simulations. The deviation might be due to the oversimplification in estimating the fractional contribution of cold pools to convective initiation.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [3] [A Multi-objective Optimization Approach for Feature Selection in Gentelligent Systems](https://arxiv.org/abs/2512.05971)
*Mohammadhossein Ghahramani,Yan Qiao,NaiQi Wu,Mengchu Zhou*

Main category: cs.NE

TL;DR: 提出一种基于支配的多目标进化算法混合框架，用于同时优化特征选择和分类性能，实现制造过程的智能故障检测。


<details>
  <summary>Details</summary>
Motivation: 将人工智能等先进技术集成到制造过程中，开发能够提高效率和自动化的智能系统（称为"Gentelligent系统"），通过可靠的故障检测方法改善产品质量、提高产量并降低生产成本。

Method: 提出一种混合框架，采用基于支配的多目标进化算法，在单次运行中同时优化特征选择和分类性能，探索帕累托最优解，监控各种制造操作并处理需要同时最小化的冲突目标。

Result: 在两个不同工业领域的真实数据集上进行验证，结果表明该方法具有良好的泛化能力和有效性。

Conclusion: 该混合框架为制造商提供了有效的预测方法，帮助他们更好地适应新兴趋势，实现制造过程的智能监控和优化。

Abstract: The integration of advanced technologies, such as Artificial Intelligence (AI), into manufacturing processes is attracting significant attention, paving the way for the development of intelligent systems that enhance efficiency and automation. This paper uses the term "Gentelligent system" to refer to systems that incorporate inherent component information (akin to genes in bioinformatics-where manufacturing operations are likened to chromosomes in this study) and automated mechanisms. By implementing reliable fault detection methods, manufacturers can achieve several benefits, including improved product quality, increased yield, and reduced production costs. To support these objectives, we propose a hybrid framework with a dominance-based multi-objective evolutionary algorithm. This mechanism enables simultaneous optimization of feature selection and classification performance by exploring Pareto-optimal solutions in a single run. This solution helps monitor various manufacturing operations, addressing a range of conflicting objectives that need to be minimized together. Manufacturers can leverage such predictive methods and better adapt to emerging trends. To strengthen the validation of our model, we incorporate two real-world datasets from different industrial domains. The results on both datasets demonstrate the generalizability and effectiveness of our approach.

</details>


### [4] [SEB-ChOA: An Improved Chimp Optimization Algorithm Using Spiral Exploitation Behavior](https://arxiv.org/abs/2512.05981)
*Leren Qian,Mohammad Khishe,Yiqian Huang,Seyedali Mirjalili*

Main category: cs.NE

TL;DR: 提出SEB-ChOA算法，通过六种螺旋函数和两种混合螺旋函数改进黑猩猩优化算法，解决收敛慢和早熟问题，在多个基准测试和工程问题上表现优异。


<details>
  <summary>Details</summary>
Motivation: 黑猩猩优化算法(ChOA)模拟黑猩猩狩猎行为，但现有模型过于简单，导致收敛速度慢且容易早熟，需要改进以提升性能。

Method: 提出六种螺旋函数和两种新型混合螺旋函数(SEB-ChOA)，增强算法的探索和开发能力，改进狩猎过程的四个步骤(驱赶、阻挡、追逐、攻击)。

Result: 在23个标准基准、20个IEEE CEC-2005测试、10个IEEE CEC06-2019案例和12个IEEE CEC-2020实际工程问题上测试，SEB-ChOA在几乎所有基准测试中排名靠前，性能优于PSO、GA、SMA、MPA、ALO、HGSO等算法，与jDE100和DISHchain1e+12表现相当。

Conclusion: SEB-ChOA通过螺旋函数改进显著提升了黑猩猩优化算法的性能，在多种优化问题上表现出色，具有实际应用价值。

Abstract: The chimp optimization algorithm (ChOA) is a nature-inspired algorithm that imitates chimpanzees' individual intelligence and hunting behaviors. In this algorithm, the hunting process consists of four steps: driving, blocking, chasing, and attacking. Because of the novelty of ChOA, the steps of the hunting process have been modeled in a simple way, leading to slow and premature convergence similar to other iterative algorithms. This paper proposes six spiral functions and introduces two novel hybrid spiral functions (SEB-ChOA) to address these deficiencies. The performance of SEB-ChOA is evaluated on 23 standard benchmarks, 20 benchmarks of the IEEE CEC-2005 test suite, 10 cases from the IEEE CEC06-2019 test suite, and 12 constrained real-world engineering problems from IEEE CEC-2020. The SEB-ChOA variants are compared with three groups of optimization algorithms, including Particle Swarm Optimization (PSO) and Genetic Algorithm (GA) as well-known optimizers; Slime Mould Algorithm (SMA), Marine Predators Algorithm (MPA), Ant Lion Optimization (ALO), and Henry Gas Solubility Optimization (HGSO) as recently developed optimizers; and jDE100 and DISHchain1e+12, the winners of the IEEE CEC06-2019 competition. Additional comparisons are made with EBOwithCMAR and CIPDE as strong secondary baselines. The SEB-ChOA methods achieve top-ranked results on nearly all benchmarks and show competitive performance compared to jDE100 and DISHchain1e+12. Statistical results indicate that SEB-ChOA outperforms PSO, GA, SMA, MPA, ALO, and HGSO while producing results comparable to those of jDE100 and DISHchain1e+12.

</details>


### [5] [Entropic Regularization in the Deep Linear Network](https://arxiv.org/abs/2512.06137)
*Alan Chen,Tejas Kotwal,Govind Menon*

Main category: cs.NE

TL;DR: 研究深度线性网络的正则化，使用熵公式分析自由能在黎曼流形上的平衡点和梯度流，发现只有最小化点是平衡点，梯度流可简化为常微分方程，并分析熵在奇异值空间中的凹性。


<details>
  <summary>Details</summary>
Motivation: 研究深度线性网络的正则化问题，特别是使用熵公式来分析网络参数空间的几何结构，理解深度线性网络在正则化下的动态行为。

Method: 使用arXiv:2509.09088中引入的熵公式，在深度线性网络的端到端映射黎曼流形上分析自由能的平衡点和梯度流，研究依赖于端到端矩阵奇异值的对称能量函数。

Result: 平衡点仅为最小化点，最小化点集合是正交群的一个轨道；与随机矩阵理论不同，没有奇异值排斥现象；梯度流可简化为常微分方程，给出向最小化点松弛的显式速率；熵在奇异值空间的欧几里得几何中是严格凹的，但在深度线性网络度量的黎曼几何中不是。

Conclusion: 深度线性网络的正则化具有独特的几何特性：平衡点结构简单，梯度流可解析求解，熵的凹性在不同几何结构中表现不同，这些发现为理解深度线性网络的优化动态提供了理论洞见。

Abstract: We study regularization for the deep linear network (DLN) using the entropy formula introduced in arXiv:2509.09088. The equilibria and gradient flow of the free energy on the Riemannian manifold of end-to-end maps of the DLN are characterized for energies that depend symmetrically on the singular values of the end-to-end matrix.
  The only equilibria are minimizers and the set of minimizers is an orbit of the orthogonal group. In contrast with random matrix theory there is no singular value repulsion. The corresponding gradient flow reduces to a one-dimensional ordinary differential equation whose solution gives explicit relaxation rates toward the minimizers. We also study the concavity of the entropy in the chamber of singular values. The entropy is shown to be strictly concave in the Euclidean geometry on the chamber but not in the Riemannian geometry defined by the DLN metric.

</details>


### [6] [Neuro-Vesicles: Neuromodulation Should Be a Dynamical System, Not a Tensor Decoration](https://arxiv.org/abs/2512.06966)
*Zilin Li,Weiwei Xu,Vicki Kane*

Main category: cs.NE

TL;DR: Neuro-Vesicles框架为传统神经网络添加了一个动态的囊泡计算层，这些离散的移动囊泡在神经网络外部运行，通过事件驱动的方式实现神经调节功能。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络的神经调节机制通常使用固定的张量条件，缺乏动态、事件驱动的调节能力。作者希望创建一种更接近生物神经调节的计算层，能够实现积累、分散、级联触发等动态行为。

Method: 提出了囊泡框架，每个囊泡包含向量载荷、类型标签、图位置、生命周期和内部状态。囊泡响应活动、错误或元信号而发射，沿学习到的转移核迁移，概率性地停靠在节点，通过内容相关的释放操作器修改激活、参数或外部记忆，最后衰减或被吸收。

Result: 提供了完整的数学规范，包括发射、迁移、停靠、释放、衰减及其与学习的耦合；连续密度松弛产生图上的可微分反应扩散动力学；强化学习视角将囊泡控制视为优化下游性能的策略。

Conclusion: 该框架能够统一多种神经调节机制，从密集短寿命囊泡近似FiLM、超网络和注意力，到稀疏长寿命囊泡类似移动代理。该形式主义可扩展到脉冲网络和类脑硬件，实现大规模脑启发计算机上的可编程神经调节。

Abstract: We introduce Neuro-Vesicles, a framework that augments conventional neural networks with a missing computational layer: a dynamical population of mobile, discrete vesicles that live alongside the network rather than inside its tensors. Each vesicle is a self contained object v = (c, kappa, l, tau, s) carrying a vector payload, type label, location on the graph G = (V, E), remaining lifetime, and optional internal state. Vesicles are emitted in response to activity, errors, or meta signals; migrate along learned transition kernels; probabilistically dock at nodes; locally modify activations, parameters, learning rules, or external memory through content dependent release operators; and finally decay or are absorbed.
  This event based interaction layer reshapes neuromodulation. Instead of applying the same conditioning tensors on every forward pass, modulation emerges from the stochastic evolution of a vesicle population that can accumulate, disperse, trigger cascades, carve transient pathways, and write structured traces into topological memory. Dense, short lived vesicles approximate familiar tensor mechanisms such as FiLM, hypernetworks, or attention. Sparse, long lived vesicles resemble a small set of mobile agents that intervene only at rare but decisive moments.
  We give a complete mathematical specification of the framework, including emission, migration, docking, release, decay, and their coupling to learning; a continuous density relaxation that yields differentiable reaction diffusion dynamics on the graph; and a reinforcement learning view where vesicle control is treated as a policy optimized for downstream performance. We also outline how the same formalism extends to spiking networks and neuromorphic hardware such as the Darwin3 chip, enabling programmable neuromodulation on large scale brain inspired computers.

</details>


### [7] [Synchrony-Gated Plasticity with Dopamine Modulation for Spiking Neural Networks](https://arxiv.org/abs/2512.07194)
*Yuchen Tian,Samuel Tensingh,Jason Eshraghian,Nhan Duy Truong,Omid Kavehei*

Main category: cs.NE

TL;DR: 提出DA-SSDP规则，将局部脉冲同步信号与监督学习结合，在保持性能的同时提升深度脉冲神经网络精度


<details>
  <summary>Details</summary>
Motivation: 虽然代理反向传播对训练深度脉冲神经网络有用，但大规模整合生物启发的局部信号仍具挑战性，主要因为需要高内存记录精确脉冲时序，且纯局部可塑性调整可能与监督学习目标冲突

Method: 引入DA-SSDP规则，将脉冲模式压缩为批量级同步度量，通过初始预热阶段评估其与任务损失的关系并设置固定门控，随后调整局部更新的幅度。当同步与任务无关时，简化为基本的两因子同步机制

Result: 在CIFAR-10(+0.42%)、CIFAR-100(+0.99%)、CIFAR10-DVS(+0.1%)和ImageNet-1K(+0.73%)等基准测试中实现一致的精度提升，计算开销略有增加

Conclusion: DA-SSDP有效整合局部脉冲同步信号与监督学习，在不改变模型结构或优化流程的情况下提升深度脉冲神经网络性能，可作为训练过程中的正则化器

Abstract: While surrogate backpropagation proves useful for training deep spiking neural networks (SNNs), incorporating biologically inspired local signals on a large scale remains challenging. This difficulty stems primarily from the high memory demands of maintaining accurate spike-timing logs and the potential for purely local plasticity adjustments to clash with the supervised learning goal. To effectively leverage local signals derived from spiking neuron dynamics, we introduce Dopamine-Modulated Spike-Synchrony-Dependent Plasticity (DA-SSDP), a synchrony-based rule that is sensitive to loss and brings a synchrony-based local learning signal to the model. DA-SSDP condenses spike patterns into a synchrony metric at the batch level. An initial brief warm-up phase assesses its relationship to the task loss and sets a fixed gate that subsequently adjusts the local update's magnitude. In cases where synchrony proves unrelated to the task, the gate settles at one, simplifying DA-SSDP to a basic two-factor synchrony mechanism that delivers minor weight adjustments driven by concurrent spike firing and a Gaussian latency function. These small weight updates are only added to the network`s deeper layers following the backpropagation phase, and our tests showed this simplified version did not degrade performance and sometimes gave a small accuracy boost, serving as a regularizer during training. The rule stores only binary spike indicators and first-spike latencies with a Gaussian kernel. Without altering the model structure or optimization routine, evaluations on benchmarks like CIFAR-10 (+0.42\%), CIFAR-100 (+0.99\%), CIFAR10-DVS (+0.1\%), and ImageNet-1K (+0.73\%) demonstrated consistent accuracy gains, accompanied by a minor increase in computational overhead. Our code is available at https://github.com/NeuroSyd/DA-SSDP.

</details>


### [8] [Algorithm-hardware co-design of neuromorphic networks with dual memory pathways](https://arxiv.org/abs/2512.07602)
*Pengfei Sun,Zhe Su,Jascha Achterberg,Giacomo Indiveri,Dan F. M. Goodman,Danyal Akarca*

Main category: cs.NE

TL;DR: 本文提出一种受大脑皮层快慢组织启发的双记忆通路（DMP）架构，通过算法-硬件协同设计，在保持事件驱动稀疏性的同时实现长时程记忆，参数减少40-60%，吞吐量提升4倍，能效提升5倍以上。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络在事件驱动感知方面表现出色，但在硬件实现中同时满足严格的能量和内存预算仍然是一个核心挑战。需要解决长时程任务相关上下文的保持问题。

Method: 算法层面：受大脑皮层快慢组织启发，引入具有显式慢记忆通路的神经网络，形成双记忆通路（DMP）架构，每层维护紧凑的低维状态来总结近期活动并调节脉冲动态。硬件层面：提出近内存计算架构，充分利用DMP架构优势，在异构稀疏脉冲和密集记忆通路间优化数据流。

Result: 在长序列基准测试中达到竞争性精度，参数比最先进的脉冲神经网络减少40-60%。硬件实现展示吞吐量提升4倍以上，能效提升5倍以上。

Conclusion: 生物原理可以指导既算法有效又硬件高效的功能抽象，为实时神经形态计算和学习建立了可扩展的协同设计范式。

Abstract: Spiking neural networks excel at event-driven sensing yet maintaining task-relevant context over long timescales. However building these networks in hardware respecting both tight energy and memory budgets, remains a core challenge in the field. We address this challenge through novel algorithm-hardware co-design effort. At the algorithm level, inspired by the cortical fast-slow organization in the brain, we introduce a neural network with an explicit slow memory pathway that, combined with fast spiking activity, enables a dual memory pathway (DMP) architecture in which each layer maintains a compact low-dimensional state that summarizes recent activity and modulates spiking dynamics. This explicit memory stabilizes learning while preserving event-driven sparsity, achieving competitive accuracy on long-sequence benchmarks with 40-60% fewer parameters than equivalent state-of-the-art spiking neural networks. At the hardware level, we introduce a near-memory-compute architecture that fully leverages the advantages of the DMP architecture by retaining its compact shared state while optimizing dataflow, across heterogeneous sparse-spike and dense-memory pathways. We show experimental results that demonstrate more than a 4x increase in throughput and over a 5x improvement in energy efficiency compared with state-of-the-art implementations. Together, these contributions demonstrate that biological principles can guide functional abstractions that are both algorithmically effective and hardware-efficient, establishing a scalable co-design paradigm for real-time neuromorphic computation and learning.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [9] [Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices](https://arxiv.org/abs/2512.05969)
*Hokin Deng*

Main category: cs.CV

TL;DR: 视频生成模型现在可以进行推理任务，在棋类、迷宫、数独等任务上达到60%成功率，研究者建立了"任务对"实验范式并开发了评估框架


<details>
  <summary>Details</summary>
Motivation: 探索视频生成模型是否具备推理能力，建立可扩展的评估范式来系统测试模型在复杂推理任务上的表现

Method: 采用"任务对"实验设计，构建包含39个模型的代码框架VMEvalKit，支持自动化评估并与人工判断高度相关

Result: Sora-2等领先模型在棋类、迷宫、数独、心理旋转和瑞文矩阵等推理任务上达到约60%的成功率

Conclusion: 视频生成模型已具备初步推理能力，建立的评估范式为强化学习改进视频模型推理提供了机会

Abstract: We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the "Task Pair" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.

</details>


### [10] [Adaptive Dataset Quantization: A New Direction for Dataset Pruning](https://arxiv.org/abs/2512.05987)
*Chenyue Yu,Jianyu Yu*

Main category: cs.CV

TL;DR: 提出一种新颖的数据集量化方法，通过减少样本内冗余来压缩大规模数据集，适用于资源受限的边缘设备，在保持模型训练性能的同时显著减少存储和通信成本。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限边缘设备中大规模数据集的存储和通信成本挑战，传统的数据集剪枝和蒸馏方法主要关注样本间冗余，但忽略了样本内的冗余信息。需要一种能够压缩每个样本内部冗余内容的方法。

Method: 首先对每个样本应用线性对称量化获得初始量化范围和尺度，然后引入自适应量化分配算法，为具有不同精度要求的样本分配不同的量化比例，同时保持恒定的总压缩比。

Result: 在CIFAR-10、CIFAR-100和ImageNet-1K数据集上的实验表明，该方法在相同压缩比下优于传统量化和数据集剪枝基线，能够保持模型训练性能的同时实现显著的数据集压缩。

Conclusion: 该研究首次提出使用有限比特表示数据集以减少存储，引入具有自适应比例分配的数据集级量化算法，为资源受限边缘设备的大规模数据集管理提供了有效的解决方案。

Abstract: This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.

</details>


### [11] [VG3T: Visual Geometry Grounded Gaussian Transformer](https://arxiv.org/abs/2512.05988)
*Junho Kim,Seongwon Lee*

Main category: cs.CV

TL;DR: VG3T是一个多视角前馈网络，通过3D高斯表示预测3D语义占据，解决了现有方法多视角融合困难导致3D表示碎片化的问题，在nuScenes基准上取得了更好的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视角融合方面存在困难，导致3D表示碎片化和性能不佳。需要一种能够从多视角图像直接预测统一3D语义表示的方法。

Method: 提出VG3T多视角前馈网络，直接预测一组具有语义属性的3D高斯表示。引入网格采样和位置细化两个关键组件，解决像素对齐高斯初始化方法中常见的距离依赖密度偏差问题。

Result: 在nuScenes基准上，VG3T相比之前最先进方法在mIoU上提升了1.7%，同时使用的基元数量减少了46%，显示出优越的效率和性能。

Conclusion: VG3T通过多视角联合预测3D高斯表示，克服了逐视角处理的碎片化和不一致性问题，为几何和语义表示提供了统一范式，在3D场景重建任务中表现出色。

Abstract: Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.

</details>


### [12] [EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head](https://arxiv.org/abs/2512.05991)
*Chang Liu,Tianjiao Jing,Chengcheng Ma,Xuanqi Zhou,Zhengxuan Lian,Qin Jin,Hongliang Yuan,Shi-Sheng Huang*

Main category: cs.CV

TL;DR: EmoDiffTalk：首个支持基于AU的连续多模态情感编辑的3D高斯泼溅说话头生成框架


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的逼真3D说话头在情感表达操控方面存在不足，特别是在使用多模态控制进行细粒度和动态情感编辑时

Method: 提出情感感知高斯扩散方法，包括：1）用于细粒度面部动画的AU提示高斯扩散过程；2）准确的文本到AU情感控制器，通过文本输入提供精确的动态情感编辑

Result: 在EmoTalk3D和RenderMe-360数据集上的实验表明，EmoDiffTalk在情感细腻度、唇形同步保真度和可控性方面优于先前工作

Conclusion: EmoDiffTalk建立了高质量、扩散驱动、多模态可编辑3D说话头合成的原则性途径，是首批支持基于AU表达空间的连续多模态情感编辑的3D高斯泼溅说话头生成框架之一

Abstract: Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.

</details>


### [13] [Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology](https://arxiv.org/abs/2512.05993)
*Ruchika Verma,Shrishtee Kandoi,Robina Afzal,Shengjia Chen,Jannes Jegminat,Michael W. Karlovich,Melissa Umphlett,Timothy E. Richardson,Kevin Clare,Quazi Hossain,Jorge Samanamud,Phyllis L. Faust,Elan D. Louis,Ann C. McKee,Thor D. Stein,Jonathan D. Cherry,Jesse Mez,Anya C. McGoldrick,Dalilah D. Quintana Mora,Melissa J. Nirenberg,Ruth H. Walker,Yolfrankcis Mendez,Susan Morgello,Dennis W. Dickson,Melissa E. Murray,Carlos Cordon-Cardo,Nadejda M. Tsankova,Jamie M. Walker,Diana K. Dangoor,Stephanie McQuillan,Emma L. Thorn,Claudia De Sanctis,Shuying Li,Thomas J. Fuchs,Kurt Farrell,John F. Crary,Gabriele Campanella*

Main category: cs.CV

TL;DR: NeuroFM：首个专门针对神经病理学的视觉基础模型，在神经退行性疾病分析任务上优于通用病理学模型


<details>
  <summary>Details</summary>
Motivation: 现有病理学基础模型主要基于外科病理数据训练，缺乏神经病理学特有的细胞类型、组织结构和疾病特征，限制了其在神经退行性疾病分析中的表现

Method: 开发NeuroFM模型，专门使用包含多种神经退行性疾病的脑组织全切片图像进行训练，针对神经病理学特征进行优化

Result: NeuroFM在混合性痴呆分类、海马区域分割、神经退行性共济失调识别等多个神经病理学任务上均优于通用基础模型

Conclusion: 领域专业化基础模型能更好地捕捉神经病理学特征，为脑疾病诊断和研究提供更准确可靠的AI分析工具，为数字病理学其他专业领域树立了先例

Abstract: Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.

</details>


### [14] [FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting](https://arxiv.org/abs/2512.05996)
*Yi Liu,Jingyu Song,Vedanth Kallakuri,Katherine A. Skinner*

Main category: cs.CV

TL;DR: FishDetector-R1是一个基于多模态大语言模型的弱监督框架，用于水下鱼类检测、分割和计数，在DeepFish数据集上显著提升性能，并展示出良好的跨域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 水下鱼类图像分析对生态监测至关重要，但由于视觉质量下降和标注成本高昂而面临挑战。需要开发能够在弱监督下准确进行鱼类检测、分割和计数的可靠解决方案。

Method: 提出统一的MLLM框架，包含两个关键组件：1）新颖的detect-to-count提示，强制空间一致的检测和计数；2）基于可验证奖励的强化学习（RLVR），利用稀疏点标签的可扩展范式。

Result: 在DeepFish数据集上，相比基线方法，AP提升20%，mIoU提升10%，MAE降低30%，GAME降低35%。消融研究验证了奖励设计的有效性，且改进能很好地泛化到其他水下数据集，显示出强大的跨域鲁棒性。

Conclusion: FishDetector-R1通过弱监督为准确的海底视觉理解提供了可靠且可扩展的解决方案，显著提升了水下鱼类分析任务的性能。

Abstract: Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.

</details>


### [15] [PrunedCaps: A Case For Primary Capsules Discrimination](https://arxiv.org/abs/2512.06003)
*Ramin Sharifi,Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: 该论文研究了胶囊网络中主要胶囊的剪枝，在多个数据集上实现了最高9.9倍的速度提升和95.36%的浮点运算减少，同时保持准确率不变。


<details>
  <summary>Details</summary>
Motivation: 胶囊网络相比卷积神经网络具有更好的仿射变换鲁棒性和重叠图像检测能力，但存在资源效率低的问题，主要胶囊数量多导致训练和测试速度慢、资源消耗大。

Method: 研究胶囊网络中主要胶囊的剪枝方法，在MNIST、Fashion-MNIST、CIFAR-10和SVHN数据集上进行实验，通过移除95%的胶囊来优化网络结构。

Result: 剪枝后的胶囊网络比传统架构快9.90倍，在动态路由阶段节省超过95.36%的浮点运算，且准确率没有损失。不同数据集从剪枝中获益程度不同。

Conclusion: 胶囊网络的主要胶囊剪枝是可行的，能显著提升计算效率，同时保持模型性能。研究还揭示了不同数据集对剪枝敏感度的差异。

Abstract: Capsule Networks (CapsNets) are a generation of image classifiers with proven advantages over Convolutional Neural Networks (CNNs). Better robustness to affine transformation and overlapping image detection are some of the benefits associated with CapsNets. However, CapsNets cannot be classified as resource-efficient deep learning architecture due to the high number of Primary Capsules (PCs). In addition, CapsNets' training and testing are slow and resource hungry. This paper investigates the possibility of Primary Capsules pruning in CapsNets on MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and SVHN datasets. We show that a pruned version of CapsNet performs up to 9.90 times faster than the conventional architecture by removing 95 percent of Capsules without a loss of accuracy. Also, our pruned architecture saves on more than 95.36 percent of floating-point operations in the dynamic routing stage of the architecture. Moreover, we provide insight into why some datasets benefit significantly from pruning while others fall behind.

</details>


### [16] [Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization](https://arxiv.org/abs/2512.06006)
*Xuefei,Wang,Kai A. Horstmann,Ethan Lin,Jonathan Chen,Alexander R. Farhang,Sophia Stiles,Atharva Sehgal,Jonathan Light,David Van Valen,Yisong Yue,Jennifer J. Sun*

Main category: cs.CV

TL;DR: AI代理能自动优化科学计算机视觉工具的适配代码，超越人工专家方案，但复杂代理架构并非总是更好


<details>
  <summary>Details</summary>
Motivation: 将生产级计算机视觉工具适配到特定科学数据集存在"最后一公里"瓶颈：微调需要大量标注数据（科学家通常缺乏），而手动代码适配需要科学家花费数周到数月时间

Method: 引入系统化的代理代码优化评估框架，研究三种生产级生物医学成像流程，比较不同代理设计的效果

Result: 简单代理框架生成的适配代码持续优于人类专家解决方案；常见复杂代理架构并非普遍有益

Conclusion: AI代理能有效自动化科学工具适配，提供实用的代理设计路线图，开源框架并验证了实际部署可行性

Abstract: Adapting production-level computer vision tools to bespoke scientific datasets is a critical "last mile" bottleneck. Current solutions are impractical: fine-tuning requires large annotated datasets scientists often lack, while manual code adaptation costs scientists weeks to months of effort. We consider using AI agents to automate this manual coding, and focus on the open question of optimal agent design for this targeted task. We introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. We demonstrate that a simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Our analysis reveals that common, complex agent architectures are not universally beneficial, leading to a practical roadmap for agent design. We open source our framework and validate our approach by deploying agent-generated functions into a production pipeline, demonstrating a clear pathway for real-world impact.

</details>


### [17] [Fast and Flexible Robustness Certificates for Semantic Segmentation](https://arxiv.org/abs/2512.06010)
*Thomas Massena,Corentin Friedrich,Franck Mamalet,Mathieu Serrurier*

Main category: cs.CV

TL;DR: 提出一种新的可认证鲁棒语义分割网络，通过内置Lipschitz约束实现高效训练和实时认证，比随机平滑方法快600倍。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络对微小扰动敏感，现有鲁棒性研究主要集中在分类任务，语义分割的高效认证方法较少。需要开发既能保持高像素精度又能提供实时认证的鲁棒语义分割网络。

Method: 引入具有内置Lipschitz约束的可认证鲁棒语义分割网络，提出通用化语义分割任务鲁棒性认证框架，利用Lipschitz网络实现灵活高效的计算。

Result: 在Cityscapes等挑战性数据集上达到竞争性像素精度，首次实现实时兼容的可认证鲁棒语义分割，认证过程比随机平滑方法快600倍，并能计算ℓ₂攻击下的最坏情况性能。

Conclusion: 该方法为语义分割任务提供了高效、灵活的鲁棒性认证方案，首次实现实时兼容的认证，在保持准确性的同时大幅提升认证效率，为实际应用中的鲁棒语义分割提供了可行方案。

Abstract: Deep Neural Networks are vulnerable to small perturbations that can drastically alter their predictions for perceptually unchanged inputs. The literature on adversarially robust Deep Learning attempts to either enhance the robustness of neural networks (e.g, via adversarial training) or to certify their decisions up to a given robustness level (e.g, by using randomized smoothing, formal methods or Lipschitz bounds). These studies mostly focus on classification tasks and few efficient certification procedures currently exist for semantic segmentation. In this work, we introduce a new class of certifiably robust Semantic Segmentation networks with built-in Lipschitz constraints that are efficiently trainable and achieve competitive pixel accuracy on challenging datasets such as Cityscapes. Additionally, we provide a novel framework that generalizes robustness certificates for semantic segmentation tasks, where we showcase the flexibility and computational efficiency of using Lipschitz networks. Our approach unlocks real-time compatible certifiably robust semantic segmentation for the first time. Moreover, it allows the computation of worst-case performance under $\ell_2$ attacks of radius $ε$ across a wide range of performance measures. Crucially, we benchmark the runtime of our certification process and find our approach to be around 600 times faster than randomized smoothing methods at inference with comparable certificates on an NVIDIA A100 GPU. Finally, we evaluate the tightness of our worstcase certificates against state-of-the-art adversarial attacks to further validate the performance of our method.

</details>


### [18] [High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing](https://arxiv.org/abs/2512.06012)
*Emmanuel Akeweje,Conall Kirk,Chi-Wai Chan,Denis Dowling,Mimi Zhang*

Main category: cs.CV

TL;DR: 开发了一种基于机器学习的自动化框架，通过高通量成像、形状提取和聚类分析来大规模分析金属粉末形态，为SLM工艺提供实时原料监控


<details>
  <summary>Details</summary>
Motivation: 选择性激光熔化(SLM)的零件质量严重依赖于原料粉末形态，但传统的粉末表征方法通量低且定性化，无法捕捉工业规模批次的异质性

Method: 开发了三种聚类流程：自编码器流程、形状描述符流程和函数数据流程，使用约126,000个粉末图像(0.5-102微米直径)进行测试

Result: 傅里叶描述符+k-means流程被证明最有效，获得最低的Davies-Bouldin指数和最高的Calinski-Harabasz分数，同时在标准桌面工作站上保持亚毫秒级处理速度

Conclusion: 这种无监督学习框架实现了粉末形态的快速自动化评估，支持跟踪重复使用周期中的形状演变，为SLM工作流程中的实时原料监控提供了途径

Abstract: Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.

</details>


### [19] [VAT: Vision Action Transformer by Unlocking Full Representation of ViT](https://arxiv.org/abs/2512.06013)
*Wenhao Li,Chengwei Ma,Weixin Mao*

Main category: cs.CV

TL;DR: Vision Action Transformer (VAT) 是一种新型架构，通过利用ViT的完整特征层次结构，在机器人模仿学习中实现了感知与动作生成的深度融合，在LIBERO基准测试中达到98.15%的平均成功率。


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习中使用Vision Transformers时，大多数方法仅使用最后一层的特征，丢弃了有价值的信息，这导致表示能力不足。作者认为需要充分利用ViT的完整特征层次来提升机器人策略学习。

Method: 提出Vision Action Transformer (VAT)，从ViT扩展而来，通过在所有transformer层中处理专门的动作令牌与视觉特征，实现感知与动作生成的渐进式深度融合。

Result: 在模拟操作任务套件中，VAT在四个LIBERO基准测试上达到98.15%的平均成功率，超越了OpenVLA-OFT等先前方法，建立了新的最先进水平。

Conclusion: VAT不仅是一个强大的模仿学习模型，更重要的是证明了利用视觉模型的完整"表示轨迹"对于推进机器人策略学习至关重要。这项工作为机器人学习提供了新的架构思路。

Abstract: In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.

</details>


### [20] [Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets](https://arxiv.org/abs/2512.06014)
*Jiho Shin,Dominic Marshall,Matthieu Komorowski*

Main category: cs.CV

TL;DR: 该研究对两种大规模胸部X光嵌入模型（CXR-Foundation和MedImageInsight）在公开数据集上进行基准测试，发现MedImageInsight性能略优，而CXR-Foundation具有更好的跨数据集稳定性，强调了医学基础模型标准化评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管医学基础模型在图像表示学习方面表现出色，但它们在跨数据集上的比较行为尚未得到充分探索。本研究旨在填补这一空白，为医学基础模型建立可复现的基准。

Method: 使用统一的预处理流程和固定下游分类器，在MIMIC-CR和NIH ChestX-ray14数据集上评估两种模型。直接从预训练编码器提取嵌入，训练轻量级LightGBM分类器处理多种疾病标签，报告平均AUROC和F1分数及95%置信区间。

Result: MedImageInsight在大多数任务中性能略高，而CXR-Foundation表现出更强的跨数据集稳定性。MedImageInsight嵌入的无监督聚类显示出与定量结果一致的疾病特异性结构。

Conclusion: 研究结果强调了医学基础模型标准化评估的必要性，并为未来多模态和临床整合研究建立了可复现的基准。两种模型各有优势，选择应基于具体应用需求。

Abstract: Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.

</details>


### [21] [PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation](https://arxiv.org/abs/2512.06020)
*Wenyi Mo,Tianyu Zhang,Yalong Bai,Ligong Han,Ying Ba,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: 提出一个多模态框架，利用MLLM提取丰富的用户表征并注入到扩散模型中，通过偏好导向的视觉问答和判别任务捕捉用户偏好，实现个性化图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么无法捕捉细微的用户偏好，要么缺乏有效的个性化视觉信号编码机制，需要更好的方法来适应生成模型以反映个人审美选择。

Method: 1) 训练MLLM进行偏好导向的视觉问答以捕捉语义线索；2) 引入用户间判别和用户内判别任务来分离偏好相关特征；3) 设计基于最大均值差异的对齐损失来弥合模态差距；4) 将得到的嵌入用于条件化图像生成器。

Result: 在图像质量和偏好对齐方面显著优于强基线方法，证明了表征提取和对齐在个性化生成中的有效性。

Conclusion: 提出的多模态框架通过有效的用户表征提取和对齐机制，成功实现了既能忠实于文本提示又能遵循用户偏好的个性化图像生成。

Abstract: Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.

</details>


### [22] [Neural reconstruction of 3D ocean wave hydrodynamics from camera sensing](https://arxiv.org/abs/2512.06024)
*Jiabin Liu,Zihao Zhou,Jialei Yan,Anxin Guo,Alvise Benetazzo,Hui Li*

Main category: cs.CV

TL;DR: 提出一种基于注意力增强金字塔架构的波浪自由表面视觉重建神经网络，用于从立体视觉数据中高效重建三维波浪表面和速度场，在真实海况下实现毫米级精度和高计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统密集视觉重建方法在长期海洋波浪观测任务中计算成本高，且存在持续视觉遮挡问题，需要一种高效、精确的三维波浪表面和速度场重建方法。

Method: 设计注意力增强金字塔架构神经网络，针对波浪运动的多尺度和时间连续特性进行优化，利用物理约束从演化的自由表面边界进行时间分辨的三维速度场重建。

Result: 在真实海况实验中，实现中央区域毫米级波浪高程预测，主导频率误差低于0.01 Hz，精确估计高频谱幂律，高保真三维非线性速度场重建，仅需1.35秒即可重建两百万个点。

Conclusion: 该方法基于立体视觉数据集，优于传统视觉重建方法，在遮挡条件下保持强泛化能力，得益于其全局多尺度注意力和学习到的波浪传播动力学编码。

Abstract: Precise three-dimensional (3D) reconstruction of wave free surfaces and associated velocity fields is essential for developing a comprehensive understanding of ocean physics. To address the high computational cost of dense visual reconstruction in long-term ocean wave observation tasks and the challenges introduced by persistent visual occlusions, we propose an wave free surface visual reconstruction neural network, which is designed as an attention-augmented pyramid architecture tailored to the multi-scale and temporally continuous characteristics of wave motions. Using physics-based constraints, we perform time-resolved reconstruction of nonlinear 3D velocity fields from the evolving free-surface boundary. Experiments under real-sea conditions demonstrate millimetre-level wave elevation prediction in the central region, dominant-frequency errors below 0.01 Hz, precise estimation of high-frequency spectral power laws, and high-fidelity 3D reconstruction of nonlinear velocity fields, while enabling dense reconstruction of two million points in only 1.35 s. Built on a stereo-vision dataset, the model outperforms conventional visual reconstruction approaches and maintains strong generalization in occluded conditions, owing to its global multi-scale attention and its learned encoding of wave propagation dynamics.

</details>


### [23] [The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation](https://arxiv.org/abs/2512.06032)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.CV

TL;DR: SAM2和SAM3之间存在根本性断层：SAM2是基于空间提示的几何分割模型，而SAM3是统一视觉语言架构，具备开放词汇推理、语义基础、对比对齐和基于示例的概念理解能力。


<details>
  <summary>Details</summary>
Motivation: 研究SAM2和SAM3之间的根本性差异，解释为什么SAM2的提示分割专业知识无法迁移到SAM3的多模态概念驱动范式，确立SAM3作为分割基础模型的新类别。

Method: 通过五个核心组件进行对比分析：1) 概念断层：提示分割vs概念分割；2) 架构差异：纯视觉-时间设计vs视觉语言编码器集成；3) 数据集差异：SA-V视频掩码vs多模态概念标注语料库；4) 训练差异：优化知识不适用；5) 评估差异：几何IoU指标vs语义开放词汇评估。

Result: 分析表明SAM3代表了分割基础模型的新类别，从基于几何的提示分割范式转向多模态概念驱动的分割时代，两者在架构、数据集、训练和评估方面存在根本性差异。

Conclusion: SAM3开启了概念驱动分割的新时代，为新兴的多模态概念分割领域指明了未来发展方向，标志着从几何分割到语义理解的根本转变。

Abstract: This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.

</details>


### [24] [Representation Learning for Point Cloud Understanding](https://arxiv.org/abs/2512.06058)
*Siming Yan*

Main category: cs.CV

TL;DR: 该论文提出通过集成预训练的2D模型来支持3D网络训练，显著提升3D理解能力，而不只是简单转换2D数据。


<details>
  <summary>Details</summary>
Motivation: 随着3D数据采集技术的快速发展，3D数据在计算机视觉、机器人等领域应用日益广泛。结合2D图像，3D数据能为机器提供更全面的环境理解，但如何有效利用2D知识来提升3D理解能力是一个重要挑战。

Method: 论文聚焦三个主要方向：点云基元分割的监督表示学习、自监督学习方法、以及从2D到3D的迁移学习。核心方法是集成预训练的2D模型来支持3D网络训练。

Result: 大量实验验证了方法的有效性，展示了通过有效集成2D知识来推进点云表示学习的潜力。

Conclusion: 该方法能够显著改善3D理解能力，为点云表示学习提供了新的方向，展示了2D-3D知识融合的重要价值。

Abstract: With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.

</details>


### [25] [EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing](https://arxiv.org/abs/2512.06065)
*Runjia Li,Moayed Haji-Ali,Ashkan Mirzaei,Chaoyang Wang,Arpit Sahni,Ivan Skorokhodov,Aliaksandr Siarohin,Tomas Jakab,Junlin Han,Sergey Tulyakov,Philip Torr,Willi Menapace*

Main category: cs.CV

TL;DR: 提出EgoEdit生态系统，用于第一人称视角视频的实时指令编辑，包括数据集EgoEditData、编辑器EgoEdit和评估套件EgoEditBench，解决现有方法在快速自我运动和手物交互场景中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有AI视频编辑器在第三人称视频上表现良好，但第一人称视角存在独特挑战：快速自我运动和频繁的手物交互导致显著的领域差距。现有离线编辑流程延迟高，限制了实时交互应用。

Method: 构建EgoEditData数据集，专门针对第一人称编辑场景设计，包含丰富的手物交互并显式保留手部；开发EgoEdit指令跟随第一人称视频编辑器，支持单GPU实时流式推理；创建EgoEditBench评估套件，针对指令忠实度、手部和交互保留、自我运动下的时间稳定性进行评估。

Result: EgoEdit在时间稳定性、指令忠实度和交互延迟方面表现优异，在第一人称编辑基准上取得明显优势（现有方法在此表现不佳），同时在通用编辑任务上保持与最强基线相当的性能。

Conclusion: 提出了完整的EgoEdit生态系统，解决了第一人称视频编辑的独特挑战，实现了实时交互式编辑，数据集和评估套件将公开供研究社区使用。

Abstract: We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit

</details>


### [26] [Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light](https://arxiv.org/abs/2512.06080)
*Tzofi Klinghoffer,Siddharth Somasundaram,Xiaoyu Xiang,Yuchen Fan,Christian Richardt,Akshat Dave,Ramesh Raskar,Rakesh Ranjan*

Main category: cs.CV

TL;DR: 利用单光子激光雷达通过数据驱动方法解决多路照明下复杂光传输反演问题，实现单次测量重建包含镜面反射和遮挡的3D场景


<details>
  <summary>Details</summary>
Motivation: 单次测量重建3D场景面临遮挡区域和镜面材料（如镜子）的挑战。单光子激光雷达可以测量多次反弹的光子，包含额外信息，但现有方法仅适用于逐点扫描，而实际应用中需要同时照明多个场景点

Method: 提出数据驱动方法反演单光子激光雷达中的光传输：1）创建首个大规模室内场景激光雷达瞬态数据集（约10万样本）；2）学习复杂光传输先验，将测量的双反弹光分解为每个激光点的贡献；3）利用分解后的光推断3D几何

Result: 实验证明该方法可以从单次测量中推断包含遮挡和镜子的场景的3D几何结构，代码和数据集已开源

Conclusion: 通过数据驱动方法成功解决了多路照明下单光子激光雷达复杂光传输的反演问题，实现了对包含遮挡和镜面反射场景的单次测量3D重建

Abstract: 3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.

</details>


### [27] [BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving](https://arxiv.org/abs/2512.06096)
*Karthik Mohan,Sonam Singh,Amit Arvind Kale*

Main category: cs.CV

TL;DR: BeLLA是一个端到端架构，将统一的360°鸟瞰图表示与大型语言模型连接，用于自动驾驶问答任务，在需要空间推理的问题上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在自动驾驶研究中存在局限性：单视角编码器无法利用多摄像头系统的空间结构，而聚合的多视角特征缺乏统一的空间表示，难以进行自我中心方向、物体关系和更广泛上下文的推理。

Method: 提出BeLLA端到端架构，连接统一的360°鸟瞰图表示与大型语言模型，用于自动驾驶问答。该方法利用BEV表示提供统一的空间结构，结合LLM进行推理。

Result: 在NuScenes-QA和DriveLM基准测试中，BeLLA在需要空间推理的问题上（如相对物体定位和附近物体行为理解）持续优于现有方法，某些任务获得高达+9.3%的绝对提升。在其他类别中也表现具有竞争力。

Conclusion: BeLLA通过结合统一的360°BEV表示和大型语言模型，有效解决了自动驾驶中空间推理的挑战，在需要复杂空间理解的任务上表现出色，同时能够处理多样化的问答类型。

Abstract: The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360° BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.

</details>


### [28] [SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection](https://arxiv.org/abs/2512.06103)
*Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

TL;DR: 提出SpectraIrisPAD框架，利用多光谱成像和DINOv2 ViT进行虹膜呈现攻击检测，并创建了包含5个近红外波段的MSIrPAD数据集，在未见攻击评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着虹膜识别在现实应用中的广泛部署，其易受呈现攻击的脆弱性引发严重安全担忧。传统虹膜识别主要在近红外波段工作，而多光谱成像能提供互补的反射信息，增强呈现攻击检测方法的泛化能力。

Method: 提出SpectraIrisPAD框架，采用DINOv2 Vision Transformer骨干网络，配备可学习的光谱位置编码、令牌融合和对比学习，提取区分性的波段特定特征。同时创建了包含5个近红外波长（800nm, 830nm, 850nm, 870nm, 980nm）的多光谱虹膜呈现攻击检测数据集MSIrPAD，包含18,848张图像和8种攻击类型。

Result: 在未见攻击评估协议下进行综合实验，SpectraIrisPAD在所有性能指标上持续优于多个最先进的基线方法，展现出检测广泛呈现攻击的卓越鲁棒性和泛化能力。

Conclusion: SpectraIrisPAD框架通过多光谱成像和先进的深度学习技术，为虹膜生物识别系统提供了强大的呈现攻击检测解决方案，有效保障系统安全性和完整性。

Abstract: Iris recognition is widely recognized as one of the most accurate biometric modalities. However, its growing deployment in real-world applications raises significant concerns regarding its vulnerability to Presentation Attacks (PAs). Effective Presentation Attack Detection (PAD) is therefore critical to ensure the integrity and security of iris-based biometric systems. While conventional iris recognition systems predominantly operate in the near-infrared (NIR) spectrum, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance the generalizability of PAD methods. In this work, we propose \textbf{SpectraIrisPAD}, a novel deep learning-based framework for robust multispectral iris PAD. The SpectraIrisPAD leverages a DINOv2 Vision Transformer (ViT) backbone equipped with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features that effectively distinguish bona fide samples from various spoofing artifacts. Furthermore, we introduce a new comprehensive dataset Multispectral Iris PAD (\textbf{MSIrPAD}) with diverse PAIs, captured using a custom-designed multispectral iris sensor operating at five distinct NIR wavelengths (800\,nm, 830\,nm, 850\,nm, 870\,nm, and 980\,nm). The dataset includes 18,848 iris images encompassing eight diverse PAI categories, including five textured contact lenses, print attacks, and display-based attacks. We conduct comprehensive experiments under unseen attack evaluation protocols to assess the generalization capability of the proposed method. SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.

</details>


### [29] [Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation](https://arxiv.org/abs/2512.06105)
*Junwen Zheng,Xinran Xu,Li Rong Wang,Chang Cai,Lucinda Siyun Tan,Dingyuan Wang,Hong Liang Tey,Xiuyi Fan*

Main category: cs.CV

TL;DR: 提出CEFM框架，通过对比学习将临床ABC标准映射到视觉Transformer嵌入空间，生成结构化文本解释，提高黑色素瘤分类的可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在黑色素瘤分类中已达到专家级性能，但模型不透明和缺乏可解释性阻碍了临床采用，医生难以信任黑盒模型的决策过程。

Method: CEFM框架利用对比学习作为核心机制，通过双投影头将临床诊断标准（不对称性、边界、颜色）映射到Vision Transformer嵌入空间，对齐临床语义与视觉特征，并通过自然语言生成转化为结构化文本解释。

Result: 在公开数据集上达到92.79%准确率和0.961的AUC，在多个可解释性指标上有显著提升。定性分析显示学习到的嵌入空间排列与医生应用的ABC规则一致。

Conclusion: CEFM有效弥合了高性能分类与临床信任之间的差距，通过将临床标准与视觉特征对齐并生成可解释的文本输出，为黑色素瘤诊断提供了透明可靠的深度学习框架。

Abstract: Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.

</details>


### [30] [Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation](https://arxiv.org/abs/2512.06158)
*Su Sun,Cheng Zhao,Himangi Mittal,Gaurav Mittal,Rohith Kukkala,Yingjie Victor Chen,Mei Chen*

Main category: cs.CV

TL;DR: Track4DGen：两阶段框架，通过注入跟踪器运动先验到扩散特征中，生成高质量动态4D对象，解决外观漂移和跨视图一致性问题


<details>
  <summary>Details</summary>
Motivation: 从稀疏输入生成动态4D对象很困难，需要同时保持外观和运动在视图和时间上的一致性，同时抑制伪影和时间漂移。现有方法仅依赖像素或潜在空间的视频扩散损失监督，缺乏显式的、时间感知的特征级跟踪指导。

Method: 两阶段框架：第一阶段在多视图视频扩散模型中注入基础点跟踪器导出的运动先验，强制特征级点对应关系，生成时间一致的特征；第二阶段使用混合运动编码重建4D高斯泼溅，将扩散特征与Hex-plane特征拼接，并用4D球谐函数增强动态建模。

Result: Track4DGen在多视图视频生成和4D生成基准测试中超越基线方法，产生时间稳定、可文本编辑的4D资产。同时创建了Sketchfab28高质量数据集用于对象中心4D生成的基准测试。

Conclusion: 通过显式注入跟踪器导出的运动先验到扩散特征表示中，Track4DGen能够生成高质量、时间一致的动态4D对象，解决了外观漂移和跨视图一致性问题，为4D生成提供了有效解决方案。

Abstract: Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.

</details>


### [31] [Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection](https://arxiv.org/abs/2512.06171)
*Jessica Plassmann,Nicolas Schuler,Michael Schuth,Georg von Freymann*

Main category: cs.CV

TL;DR: 提出基于深度学习的自动化工作流程，从剪切散斑测量中生成缺陷标注，减少人工标注工作量，支持可扩展的数据集创建


<details>
  <summary>Details</summary>
Motivation: 剪切散斑技术用于检测安全关键部件的表面下缺陷，但工业应用受限的主要原因是缺乏高质量标注数据集，因为人工标注劳动密集、主观性强且难以标准化

Method: 引入自动化工作流程，利用深度学习从剪切散斑测量中生成缺陷标注，包括高分辨率分割和边界框标签

Result: 与专家标注数据对比评估显示，该方法具有足够准确性，能够支持弱监督训练，减少人工工作量

Conclusion: 该自动化工作流程能够支持可扩展的数据集创建，为稳健的缺陷检测提供支持，促进剪切散斑技术在工业中的采用

Abstract: Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.

</details>


### [32] [Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction](https://arxiv.org/abs/2512.06174)
*Shilin Hu,Jingyi Xu,Akshat Dave,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: 提出一种将显式物理建模（几何与光照）融入深度学习阴影生成的新框架，通过3D几何和光照方向预测获得物理准确的阴影位置形状，再用扩散模型优化为真实高保真阴影


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的阴影生成方法很少利用阴影形成的显式物理建模（遮挡物几何和场景光照），而物理建模能提供更准确的阴影位置和形状

Method: 1) 从单目RGB图像获取密集点云表示的近似3D几何和主导光照方向；2) 基于阴影形成物理原理计算初始阴影位置形状；3) 将物理估计融入扩散框架进行真实感细化，保持几何光照一致性

Result: 在DESOBAV2数据集上训练，模型生成的阴影既视觉真实又物理一致，在复杂几何或模糊光照场景中优于现有方法

Conclusion: 将显式物理建模与深度学习结合能产生更真实、物理一致的阴影生成，特别是在几何复杂或光照模糊的场景中效果显著

Abstract: Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.

</details>


### [33] [Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction](https://arxiv.org/abs/2512.06179)
*Shilin Hu,Jingyi Xu,Sagnik Das,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: 提出首个联合检测投射阴影和附着阴影的框架，通过光照与几何推理的闭环系统提升附着阴影检测效果，并创建了首个包含两种阴影标注的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有阴影检测方法主要针对投射阴影，缺乏专门的附着阴影检测数据集和模型。附着阴影对理解物体三维结构至关重要，但当前研究存在空白。

Method: 构建包含阴影检测模块和光照估计模块的闭环系统：阴影检测模块分别预测两种阴影类型；光照估计模块从阴影推断光照方向；结合表面法线生成几何一致的部分遮挡图；该图反馈到阴影检测模块进行迭代优化。

Result: 实验结果表明，该迭代几何-光照推理方法显著提升了附着阴影检测效果，BER降低至少33%，同时保持了良好的整体阴影和投射阴影检测性能。

Conclusion: 通过光照与几何的联合推理，首次实现了投射阴影和附着阴影的有效联合检测，填补了该领域的研究空白，为三维场景理解提供了新方法。

Abstract: Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.

</details>


### [34] [SPOOF: Simple Pixel Operations for Out-of-Distribution Fooling](https://arxiv.org/abs/2512.06185)
*Ankit Gupta,Christoph Adami,Emily Dolson*

Main category: cs.CV

TL;DR: 重新实现进化攻击方法，发现现代神经网络（尤其是ViT）仍然容易受到高置信度欺骗图像攻击，并提出更高效的SPOOF攻击方法


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络在图像识别任务中表现出色，但它们仍然对与自然图像毫无相似之处的输入表现出过度自信。本研究旨在重新验证Nguyen等人（2015）提出的"欺骗图像"问题在现代架构（包括卷积和Transformer分类器）上的持续性。

Method: 1. 重新实现了基于CPPN和直接编码的进化欺骗攻击方法；2. 在包括卷积和Transformer分类器的现代架构上进行测试；3. 提出了SPOOF攻击方法，这是一种简约、一致且更高效的黑盒攻击方法，通过最小像素修改生成高置信度欺骗图像。

Result: 1. 确认高置信度欺骗问题在现代网络中仍然存在；2. 基于Transformer的ViT-B/16是最易受攻击的架构，相比卷积模型需要更少的查询就能实现近乎确定的错误分类；3. SPOOF方法能够以极少的像素修改和大幅降低的计算成本生成不可识别的欺骗图像；4. 即使使用欺骗图像作为额外类别进行重新训练，也只能提供部分抵抗力。

Conclusion: 现代深度分类器在面对欺骗图像攻击时仍然表现出持续的脆弱性，即使采用防御性重新训练，SPOOF攻击仍然能够以稍高的查询预算持续欺骗模型，这突显了深度神经网络安全性的根本挑战。

Abstract: Deep neural networks (DNNs) excel across image recognition tasks, yet continue to exhibit overconfidence on inputs that bear no resemblance to natural images. Revisiting the "fooling images" work introduced by Nguyen et al. (2015), we re-implement both CPPN-based and direct-encoding-based evolutionary fooling attacks on modern architectures, including convolutional and transformer classifiers. Our re-implementation confirm that high-confidence fooling persists even in state-of-the-art networks, with transformer-based ViT-B/16 emerging as the most susceptible--achieving near-certain misclassifications with substantially fewer queries than convolution-based models. We then introduce SPOOF, a minimalist, consistent, and more efficient black-box attack generating high-confidence fooling images. Despite its simplicity, SPOOF generates unrecognizable fooling images with minimal pixel modifications and drastically reduced compute. Furthermore, retraining with fooling images as an additional class provides only partial resistance, as SPOOF continues to fool consistently with slightly higher query budgets--highlighting persistent fragility of modern deep classifiers.

</details>


### [35] [Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying](https://arxiv.org/abs/2512.06190)
*Shichen Li,Ahmadreza Eslaminia,Chenhui Shao*

Main category: cs.CV

TL;DR: 提出一种新颖的多模态颜色轨迹预测方法，整合高维时序颜色信息与干燥工艺参数，实现准确且数据高效的颜色轨迹预测


<details>
  <summary>Details</summary>
Motivation: 现有食品干燥研究主要依赖低维颜色特征，无法充分捕捉食品样品复杂的动态颜色轨迹，且现有建模方法缺乏对未见工艺条件的泛化能力

Method: 开发多模态颜色轨迹预测方法，将高维时序颜色信息与干燥工艺参数相结合，实现准确且数据高效的颜色轨迹预测

Result: 在未见干燥条件下，模型对饼干干燥的RMSE为2.12，苹果干燥的RMSE为1.29，相比基线模型误差降低超过90%

Conclusion: 实验结果表明该模型具有优越的准确性、鲁棒性和广泛适用性，能够有效预测食品干燥过程中的颜色轨迹变化

Abstract: Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.

</details>


### [36] [The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning](https://arxiv.org/abs/2512.06206)
*Akis Linardos,Sarthak Pati,Ujjwal Baid,Brandon Edwards,Patrick Foley,Kevin Ta,Verena Chung,Micah Sheller,Muhammad Irfan Khan,Mojtaba Jafaritadi,Elina Kontio,Suleiman Khan,Leon Mächler,Ivan Ezhov,Suprosanna Shit,Johannes C. Paetzold,Gustav Grimberg,Manuel A. Nickel,David Naccache,Vasilis Siomos,Jonathan Passerat-Palmbach,Giacomo Tarroni,Daewoon Kim,Leonard L. Klausmann,Prashant Shah,Bjoern Menze,Dimitrios Makris,Spyridon Bakas*

Main category: cs.CV

TL;DR: MICCAI FeTS 2024挑战赛评估了联邦学习在胶质瘤分割中的应用，PID控制器方法在分割性能和通信效率上表现最佳


<details>
  <summary>Details</summary>
Motivation: 评估新的权重聚合方法，以提高联邦学习在多参数MRI胶质瘤亚区分割中的鲁棒性和效率，推动医学影像联邦学习的发展

Method: 使用标准化联邦学习设置和多机构数据集（来自BraTS基准），采用累积评分系统评估分割性能（DSC和HD95）和通信效率（收敛分数）

Result: 基于PID控制器的方法获得最高排名，ET、TC、WT的平均DSC分别为0.733、0.761、0.751，HD95分别为33.922mm、33.623mm、32.309mm，收敛分数0.764

Conclusion: PID控制器是稳定和优化联邦学习中权重聚合的有效机制，超越了先前挑战赛的最佳方法，推动了医学影像联邦学习的发展

Abstract: We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.

</details>


### [37] [Revisiting SVD and Wavelet Difference Reduction for Lossy Image Compression: A Reproducibility Study](https://arxiv.org/abs/2512.06221)
*Alena Makarova*

Main category: cs.CV

TL;DR: 对SVD+WDR图像压缩方法的独立可重复性研究，发现该方法并未如原论文声称的那样优于JPEG2000和WDR


<details>
  <summary>Details</summary>
Motivation: 验证原论文声称的SVD与WDR结合能获得比JPEG2000和独立WDR更好的视觉质量和压缩比的真实性，并检查可重复性问题

Method: 重新实现SVD+WDR方法，仔细检查缺失的实现细节，尽可能复制原始实验，并在新图像上进行额外实验，使用PSNR和SSIM评估性能

Result: 与原始声称相反，SVD+WDR方法在PSNR方面通常不优于JPEG2000或WDR，仅在SSIM方面部分优于JPEG2000；发现了原始描述中的模糊之处（如量化和阈值初始化）

Conclusion: 原始论文的声称未能得到验证，研究强调了方法描述不明确对可重复性和报告性能的重大影响

Abstract: This work presents an independent reproducibility study of a lossy image compression technique that integrates singular value decomposition (SVD) and wavelet difference reduction (WDR). The original paper claims that combining SVD and WDR yields better visual quality and higher compression ratios than JPEG2000 and standalone WDR. I re-implemented the proposed method, carefully examined missing implementation details, and replicated the original experiments as closely as possible. I then conducted additional experiments on new images and evaluated performance using PSNR and SSIM. In contrast to the original claims, my results indicate that the SVD+WDR technique generally does not surpass JPEG2000 or WDR in terms of PSNR, and only partially improves SSIM relative to JPEG2000. The study highlights ambiguities in the original description (e.g., quantization and threshold initialization) and illustrates how such gaps can significantly impact reproducibility and reported performance.

</details>


### [38] [GPU-GLMB: Assessing the Scalability of GPU-Accelerated Multi-Hypothesis Tracking](https://arxiv.org/abs/2512.06230)
*Pranav Balakrishnan,Sidisha Barik,Sean M. O'Rourke,Benjamin M. Marlin*

Main category: cs.CV

TL;DR: 本文提出一种改进的GLMB滤波器，支持同一传感器对同一目标产生多个检测，从而打破检测间依赖关系，提升并行计算能力，实现GPU加速。


<details>
  <summary>Details</summary>
Motivation: 传统多目标跟踪方法（特别是基于标记随机有限集的GLMB滤波器）虽然理论性质良好，但在标准测量模型下维护多个假设的计算成本极高，即使采用假设剪枝近似仍难以满足实际需求。特别是在分布式机器学习虚拟传感器网络中，需要支持同一传感器对同一目标产生多个检测的能力。

Method: 提出GLMB滤波器的变体，允许同一传感器对同一目标产生多个检测。这种改进打破了标准GLMB滤波器中检测间的依赖关系，使得滤波器更新具有更好的并行可扩展性，从而能够在GPU硬件上高效部署。

Result: 通过初步分析GPU加速实现的GLMB跟踪器，展示了在目标数量和保留假设最大数量方面的运行时可扩展性改进。改进后的滤波器能够有效处理多检测场景，同时保持计算效率。

Conclusion: 提出的GLMB滤波器变体通过支持多检测能力，成功打破了检测间依赖，实现了更好的并行计算性能，为在GPU硬件上部署高效的多目标跟踪系统提供了可行方案，特别适用于分布式机器学习虚拟传感器网络环境。

Abstract: Much recent research on multi-target tracking has focused on multi-hypothesis approaches leveraging random finite sets. Of particular interest are labeled random finite set methods that maintain temporally coherent labels for each object. While these methods enjoy important theoretical properties as closed-form solutions to the multi-target Bayes filter, the maintenance of multiple hypotheses under the standard measurement model is highly computationally expensive, even when hypothesis pruning approximations are applied. In this work, we focus on the Generalized Labeled Multi-Bernoulli (GLMB) filter as an example of this class of methods. We investigate a variant of the filter that allows multiple detections per object from the same sensor, a critical capability when deploying tracking in the context of distributed networks of machine learning-based virtual sensors. We show that this breaks the inter-detection dependencies in the filter updates of the standard GLMB filter, allowing updates with significantly improved parallel scalability and enabling efficient deployment on GPU hardware. We report the results of a preliminary analysis of a GPU-accelerated implementation of our proposed GLMB tracker, with a focus on run time scalability with respect to the number of objects and the maximum number of retained hypotheses.

</details>


### [39] [Opinion: Learning Intuitive Physics May Require More than Visual Data](https://arxiv.org/abs/2512.06232)
*Ellen Su,Solim Legris,Todd M. Gureckis,Mengye Ren*

Main category: cs.CV

TL;DR: 在发育真实的儿童视角视频数据集上预训练V-JEPA模型，未能显著提升直觉物理推理能力，表明仅靠数据分布变化不足以让当前架构学习直觉物理。


<details>
  <summary>Details</summary>
Motivation: 人类通过建立基于直觉物理的内部模型来导航世界，而当前深度学习模型即使在大量互联网视频数据上训练，在直觉物理基准测试上仍达不到人类水平。本研究探讨数据分布（而非数据量）是否是学习这些物理原理的关键。

Method: 使用SAYCam数据集（一个发育真实的、以自我为中心的儿童视角视频数据集，记录了三个儿童的日常视觉体验）预训练Video Joint Embedding Predictive Architecture (V-JEPA)模型。该数据集仅占当前最先进模型训练数据量的0.01%。

Result: 在IntPhys2基准测试上，使用该发育真实数据集训练并未带来显著性能提升。结果表明，仅在当前架构上使用发育真实数据集训练，不足以学习支持直觉物理的表征。

Conclusion: 仅改变视觉数据的量和分布可能不足以构建具有人工直觉物理的系统。需要更根本的架构或训练方法改进。

Abstract: Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.

</details>


### [40] [NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks](https://arxiv.org/abs/2512.06251)
*Fangzhou Lin,Yuping Wang,Yuliang Guo,Zixun Huang,Xinyu Huang,Haichong Zhang,Kazunori Yamada,Zhengzhong Tu,Liu Ren,Ziming Zhang*

Main category: cs.CV

TL;DR: NexusFlow是一个轻量级即插即用框架，通过可逆耦合层对齐异构任务的潜在特征分布，在部分监督多任务学习中实现跨结构不同任务的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 现有部分监督多任务学习方法主要关注同构密集预测任务，而现实场景中任务结构往往不同（如密集地图重建与稀疏多目标跟踪），需要处理结构差异和领域差距的挑战。

Method: 引入NexusFlow框架，使用具有可逆耦合层的代理网络对齐任务的潜在特征分布，创建统一表示空间。可逆性避免表示坍塌，保持表达能力，实现跨结构不同任务的知识迁移。

Result: 在nuScenes自动驾驶数据集上，NexusFlow在领域分区场景（地图重建与多目标跟踪）中取得新的SOTA结果。在NYUv2数据集上，对三个同构密集预测任务（分割、深度、表面法线）也获得一致性能提升。

Conclusion: NexusFlow是一个通用有效的部分监督多任务学习框架，既能处理异构任务的结构差异和领域差距，也能在同构任务上取得性能提升，具有广泛的适用性。

Abstract: Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.

</details>


### [41] [Language-driven Fine-grained Retrieval](https://arxiv.org/abs/2512.06255)
*Shijie Wang,Xin Yu,Yadan Luo,Zijian Wang,Pengfei Zhang,Zi Huang*

Main category: cs.CV

TL;DR: LaFG是一个语言驱动的细粒度图像检索框架，利用大语言模型和视觉语言模型将类别名称转换为属性级监督，以提升对未见类别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的细粒度图像检索方法使用从类别名称派生的语义稀疏one-hot标签作为监督，虽然对已见类别有效，但忽略了类别名称中丰富的语义信息，阻碍了跨类别细节可比性的建模，从而限制了对未见类别的泛化能力。

Method: LaFG框架将每个类别名称作为语义锚点，使用大语言模型生成详细的属性导向描述。为了缓解描述中的属性遗漏，利用冻结的视觉语言模型将这些描述投影到视觉对齐空间，聚类成数据集范围的属性词汇表，同时从相关类别中获取补充属性。利用这个词汇表，全局提示模板选择类别相关属性，聚合成类别特定的语言原型，这些原型监督检索模型。

Result: 论文未在摘要中提供具体实验结果，但方法旨在通过语言驱动的属性级监督来提升细粒度图像检索的泛化能力。

Conclusion: LaFG通过将类别名称转换为丰富的属性级监督，利用LLMs和VLMs的协同作用，解决了现有细粒度图像检索方法中语义稀疏监督的问题，有望提升对未见类别的泛化性能。

Abstract: Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer

</details>


### [42] [Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs](https://arxiv.org/abs/2512.06258)
*Chaoyang Wang,Yangfan He,Yiyang Zhou,Yixuan Wang,Jiaqi Liu,Peng Xia,Zhengzhong Tu,Mohit Bansal,Huaxiu Yao*

Main category: cs.CV

TL;DR: 论文揭示了大型视觉语言模型（LVLMs）的一个关键缺陷：即使知道正确答案，也经常通过错误的推理路径得出结果。作者提出PSO（路径选择优化）框架，通过两阶段后训练提升推理性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型存在一个被忽视的问题：模型虽然具备正确答案的知识，但在推理过程中倾向于选择不稳定或不一致的路径，导致结果不可靠。Pass@K与Pass@1之间的显著差距表明，失败主要源于推理错误而非知识缺乏。

Method: 提出PSO两阶段后训练框架：第一阶段使用GRPO（组相对策略优化）结合模板和答案奖励，培养结构化逐步推理；第二阶段进行在线偏好优化，模型从GRPO生成的数据中采样推理路径，自我评估并朝向优选轨迹对齐。同时建立负向回放记忆（NRM）存储错误路径作为硬负例，定期回顾以防止重复错误。

Result: 实验表明PSO能有效剪除无效推理路径，显著提升推理准确性（平均提升7.4%），并产生更稳定一致的思维链。代码将在GitHub上开源。

Conclusion: 论文揭示了LVLMs的路径选择偏差问题，提出的PSO框架通过优化推理路径选择，有效提升了模型的推理性能和稳定性，为解决模型"知道但推理错误"的问题提供了系统化方案。

Abstract: We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.

</details>


### [43] [TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting](https://arxiv.org/abs/2512.06269)
*Quan Tran,Tuan Dang*

Main category: cs.CV

TL;DR: 本文提出了一种通过约束多视角三角测量来增强3D高斯泼溅几何一致性的方法，解决了现有方法因仅依赖光度损失导致的浮游伪影和几何不一致问题。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯泼溅方法仅依赖光度损失进行重建，导致几何不一致、浮游伪影和结构混乱的问题，限制了高质量表面的提取能力。

Method: 通过约束多视角三角测量强制全局几何一致性，利用多个估计视角达成物理世界3D表示的共识，通过自监督方式惩罚渲染3D点与鲁棒共识点的偏差。

Result: 在多个数据集上验证了方法的有效性，在DTU数据集上达到0.50mm的平均倒角距离，优于同类显式方法。

Conclusion: 提出的几何一致性约束方法显著改善了3D高斯泼溅的重建质量，解决了浮游伪影问题，实现了最先进的性能，并将开源代码促进社区验证。

Abstract: 3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in "floater" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.

</details>


### [44] [FacePhys: State of the Heart Learning](https://arxiv.org/abs/2512.06275)
*Kegang Wang,Jiankai Tang,Yuntao Wang,Xin Liu,Yuxuan Fan,Jiatong Ji,Yuanchun Shi,Daniel McDuff*

Main category: cs.CV

TL;DR: FacePhys是一种基于时空状态空间对偶性的内存高效rPPG算法，解决了模型可扩展性、跨数据集泛化和实时操作的三难问题，在保持最小计算开销的同时实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 基于摄像头的生命体征测量技术（特别是远程光电容积描记术rPPG）为舒适、普适的健康监测提供了机会。然而，实际部署受到前端设备计算限制和数据压缩传输导致信号质量下降的制约。

Method: 提出FacePhys算法，基于时空状态空间对偶性构建，利用可转移的心脏状态捕捉视频帧间的细微周期性变化，同时保持最小的计算开销，支持长视频序列训练和低延迟推理。

Result: FacePhys实现了最先进的性能，误差减少49%。实时推理内存占用仅3.6MB，每帧延迟9.46ms，比现有方法提升83%到99%。

Conclusion: FacePhys解决了rPPG部署中的计算限制和精度下降问题，实现了可靠的实时性能，为实际部署提供了可行解决方案。

Abstract: Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\% to 99\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/.

</details>


### [45] [RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension](https://arxiv.org/abs/2512.06276)
*Tianyi Gao,Hao Li,Han Fang,Xin Wei,Xiaodong Dong,Hongbo Sun,Ye Yuan,Zhongjiang He,Jinglin Xu,Jingmin Xin,Hao Sun*

Main category: cs.CV

TL;DR: RefBench-PRO是一个新的指代表达理解基准，将任务分解为感知和推理两个维度，包含六个渐进挑战性子任务，并提出RL-based学习方案Ref-R1来提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有REC基准主要评估感知能力，缺乏可解释的评分机制，无法揭示多模态大语言模型在不同认知能力上的grounding能力。

Method: 1) 提出RefBench-PRO基准，将指代表达分解为感知和推理两个维度，细分为属性、位置、交互、常识、关系和拒绝六个子任务；2) 开发全自动数据生成流水线；3) 提出Ref-R1学习方案，采用基于动态IoU的GRPO来提升复杂推理条件下的定位精度。

Result: RefBench-PRO能够对MLLM在指代表达理解上进行可解释评估，在感知和推理方面都提出了更大挑战，Ref-R1为REC建立了更强的基线。

Conclusion: RefBench-PRO解决了现有REC基准的局限性，提供了更全面的评估框架，能够更好地揭示MLLM在不同认知维度上的grounding能力。

Abstract: Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.

</details>


### [46] [Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models](https://arxiv.org/abs/2512.06281)
*Hengzhuang Li,Xinsong Zhang,Qiming Peng,Bin Luo,Han Hu,Dengyang Jiang,Han-Jia Ye,Teng Zhang,Hai Jin*

Main category: cs.CV

TL;DR: 提出LaVer框架，通过潜在空间掩码图像建模解决MLLMs中视觉信息在深层网络中被弱化的模态不平衡问题


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在模态不平衡问题，视觉信息在深层网络中往往被弱化，导致视觉性能下降或产生幻觉。这源于训练主要依赖文本token预测，缺乏直接的视觉监督信号，导致视觉表征在层间逐渐同质化。

Method: 提出潜在视觉重建（LaVer）训练框架，在LLM的联合潜在语义空间中通过掩码图像建模，让MLLMs学习更具区分性的视觉表征。该方法为MLLMs提供直接的视觉激活。

Result: 实验表明LaVer能增强MLLMs的视觉注意力分配，提高视觉信息利用率。在多种基准测试中，特别是在需要密集视觉能力的场景下，LaVer表现出优越性能。

Conclusion: LaVer通过潜在空间掩码图像建模有效解决了MLLMs的模态不平衡问题，增强了视觉表征能力，在需要密集视觉理解的任务中表现优异。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.

</details>


### [47] [A Sleep Monitoring System Based on Audio, Video and Depth Information](https://arxiv.org/abs/2512.06282)
*Lyn Chao-ling Chen,Kuan-Wen Chen,Yi-Ping Hung*

Main category: cs.CV

TL;DR: 开发基于事件的家庭睡眠监测系统，使用深度传感器、RGB摄像头和麦克风阵列检测运动、开关灯和噪音三类睡眠干扰事件


<details>
  <summary>Details</summary>
Motivation: 需要一种非侵入式的定量评估睡眠干扰的方法，能够在家庭环境中监测睡眠质量，识别影响睡眠的各种干扰因素

Method: 使用红外深度传感器、RGB摄像头和四麦克风阵列设备，在低光照环境下监测睡眠。建立深度信号的背景模型检测运动幅度，建立彩色图像的背景模型检测光照变化，采用事件检测算法从三类传感器数据中识别事件

Result: 系统在睡眠条件下进行了测试，实验结果验证了系统的可靠性，能够有效检测运动、开关灯和噪音三类睡眠干扰事件

Conclusion: 提出的基于事件的家庭睡眠监测系统能够非侵入式地定量评估睡眠干扰，为家庭环境下的睡眠质量监测提供了有效的技术方案

Abstract: For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.

</details>


### [48] [StrokeNet: Unveiling How to Learn Fine-Grained Interactions in Online Handwritten Stroke Classification](https://arxiv.org/abs/2512.06290)
*Yiheng Huang,Shuang She,Zewei Wei,Jianmin Lin,Ming Yang,Wenyin Liu*

Main category: cs.CV

TL;DR: StrokeNet：一种新颖的笔画分类网络，通过参考点对表示和空间查询机制，有效建模笔画间的细粒度语义关系，在多个手写数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 笔画分类面临书写风格变化、内容模糊和动态书写位置的挑战，核心问题在于建模笔画间的语义关系。现有深度学习方法难以捕捉笔画间局部化的细粒度交互关系。

Method: 提出StrokeNet网络架构：1）将笔画编码为参考点对表示（点+特征向量）；2）使用Inline Sequence Attention模块构建上下文特征；3）设计Cross-Ellipse Query机制捕捉空间特征交互；4）联合优化框架同时预测笔画类别和相邻笔画语义转换。

Result: 在多个公开在线手写数据集上取得最先进性能，特别是在CASIA-onDo数据集上准确率从93.81%提升到95.54%，证明了方法的有效性和鲁棒性。

Conclusion: 通过参考点表示和空间查询机制，StrokeNet能够有效建模笔画间的细粒度语义关系，解决了笔画分类中的核心挑战，为在线手写识别提供了新的解决方案。

Abstract: Stroke classification remains challenging due to variations in writing style, ambiguous content, and dynamic writing positions. The core challenge in stroke classification is modeling the semantic relationships between strokes. Our observations indicate that stroke interactions are typically localized, making it difficult for existing deep learning methods to capture such fine-grained relationships. Although viewing strokes from a point-level perspective can address this issue, it introduces redundancy. However, by selecting reference points and using their sequential order to represent strokes in a fine-grained manner, this problem can be effectively solved. This insight inspired StrokeNet, a novel network architecture encoding strokes as reference pair representations (points + feature vectors), where reference points enable spatial queries and features mediate interaction modeling. Specifically, we dynamically select reference points for each stroke and sequence them, employing an Inline Sequence Attention (ISA) module to construct contextual features. To capture spatial feature interactions, we devised a Cross-Ellipse Query (CEQ) mechanism that clusters reference points and extracts features across varying spatial scales. Finally, a joint optimization framework simultaneously predicts stroke categories via reference points regression and adjacent stroke semantic transition modeling through an Auxiliary Branch (Aux-Branch). Experimental results show that our method achieves state-of-the-art performance on multiple public online handwritten datasets. Notably, on the CASIA-onDo dataset, the accuracy improves from 93.81$\%$ to 95.54$\%$, demonstrating the effectiveness and robustness of our approach.

</details>


### [49] [Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation](https://arxiv.org/abs/2512.06306)
*Haoxian Zhou,Chuanzhi Xu,Langyi Chen,Haodong Chen,Yuk Ying Chung,Qiang Qu,Xaoming Chen,Weidong Cai*

Main category: cs.CV

TL;DR: 提出基于点云框架的事件相机姿态估计方法，通过事件时间切片卷积和序列建模，结合边缘增强，在DHP19数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有方法将事件流转换为密集事件帧，增加了计算负担并牺牲了事件信号的高时间分辨率。需要充分利用事件流的时空特性来提升人体姿态估计性能

Method: 1) 基于点云框架处理事件流；2) 设计事件时间切片卷积模块捕获短期依赖；3) 事件切片序列模块进行结构化时间建模；4) 在点云表示中应用边缘增强以改善稀疏事件条件下的空间边缘信息

Result: 在DHP19数据集上，该方法在三种代表性点云骨干网络（PointNet、DGCNN、Point Transformer）上均能持续提升性能

Conclusion: 提出的基于点云的事件流处理方法有效利用了事件的时空特性，避免了传统密集帧转换的缺点，显著提升了人体姿态估计性能

Abstract: Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.

</details>


### [50] [ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models](https://arxiv.org/abs/2512.06328)
*Jiahao Li,Yusheng Luo,Yunzhong Lou,Xiangdong Zhou*

Main category: cs.CV

TL;DR: ReCAD是一个强化学习框架，利用预训练大模型从多模态输入生成精确的参数化CAD模型，通过参数化代码引导和分层原语学习，在文本到CAD和图像到CAD任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖监督微调注入知识，对可编辑性支持有限，且未能充分利用预训练大模型的强大生成先验。需要一种能利用PLM内在生成能力、支持复杂CAD操作且保持几何精度的方法。

Method: 1) 微调视觉语言模型获得基本CAD生成能力，将CAD脚本重写为参数化代码并生成准确文本描述用于监督；2) 提出新颖的强化学习策略，以参数化代码为引导增强模型在复杂问题上的推理能力；3) 采用分层原语学习过程，在统一奖励函数下逐步教授结构化和组合技能，确保几何精度和语义保真度。

Result: 在文本到CAD和图像到CAD任务上均达到新的SOTA。在图像到CAD任务中，将平均Chamfer距离从73.47降至29.61（分布内），从272.06降至80.23（分布外），显著超越现有基线。

Conclusion: ReCAD框架成功利用预训练大模型的生成能力，通过强化学习引导和分层学习，实现了从多模态输入生成精确参数化CAD模型，在几何精度和语义保真度方面均有显著提升，为CAD生成任务提供了新思路。

Abstract: We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.

</details>


### [51] [S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening](https://arxiv.org/abs/2512.06330)
*Haoyu Zhang,Junhan Luo,Yugang Cao,Siran Peng,Jie Huang,Liangjian-Deng*

Main category: cs.CV

TL;DR: S2WMamba提出了一种基于频率解耦和轻量级跨模态交互的全色锐化方法，通过2D/1D Haar小波变换分离空间和光谱信息，使用Mamba进行跨模态调制，在多尺度动态门控融合下实现高质量HRMS图像重建。


<details>
  <summary>Details</summary>
Motivation: 全色锐化中同时处理PAN和MS图像常常导致空间细节与光谱保真度的纠缠问题，传统方法难以有效分离这两种信息，影响重建质量。

Method: 1) 使用2D Haar DWT处理PAN图像定位空间边缘和纹理；2) 使用通道级1D Haar DWT处理每个像素的光谱作为1D信号分离高低频分量；3) 设计光谱分支和空间分支分别处理MS和PAN特征；4) 采用基于Mamba的跨调制模块建模长程依赖；5) 多尺度动态门控（乘法和加法）自适应融合分支输出。

Result: 在WV3、GF2和QB数据集上，S2WMamba匹配或超越了近期强基线（FusionMamba、CANNet、U2Net、ARConv），PSNR提升最高达0.23 dB，在WV3全分辨率上达到HQNR 0.956。消融实验验证了2D/1D DWT配置、并行双分支和融合门的设计选择。

Conclusion: S2WMamba通过显式频率解耦和轻量级跨模态交互有效解决了全色锐化中的空间-光谱纠缠问题，在多个数据集上取得了优异性能，证明了该方法在高质量HRMS图像重建中的有效性。

Abstract: Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.

</details>


### [52] [CryoHype: Reconstructing a thousand cryo-EM structures with transformer-based hypernetworks](https://arxiv.org/abs/2512.06332)
*Jeffrey Gu,Minkyu Jeon,Ambri Ma,Serena Yeung-Levy,Ellen D. Zhong*

Main category: cs.CV

TL;DR: CryoHype：基于Transformer的超网络，用于从混合的冷冻电镜图像中同时重建多个分子结构


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜通常用于单个分子物种的结构解析，但实际样本常包含多种不同分子物种的混合物。现有方法主要关注单个或少数结构的构象异质性，无法有效解决由许多不同分子物种混合引起的组成异质性。

Method: 提出CryoHype，一种基于Transformer的超网络，用于冷冻电镜重建。该网络动态调整隐式神经表示的权重，能够同时处理多个分子结构。

Result: 在包含100个结构的挑战性基准数据集上取得了最先进的结果。进一步证明CryoHype能够扩展到从无标签的冷冻电镜图像中重建1000个不同的结构（在固定姿态设置下）。

Conclusion: CryoHype为高通量冷冻电镜结构解析提供了有效解决方案，能够处理复杂的组成异质性，实现多目标同时重建。

Abstract: Cryo-electron microscopy (cryo-EM) is an indispensable technique for determining the 3D structures of dynamic biomolecular complexes. While typically applied to image a single molecular species, cryo-EM has the potential for structure determination of many targets simultaneously in a high-throughput fashion. However, existing methods typically focus on modeling conformational heterogeneity within a single or a few structures and are not designed to resolve compositional heterogeneity arising from mixtures of many distinct molecular species. To address this challenge, we propose CryoHype, a transformer-based hypernetwork for cryo-EM reconstruction that dynamically adjusts the weights of an implicit neural representation. Using CryoHype, we achieve state-of-the-art results on a challenging benchmark dataset containing 100 structures. We further demonstrate that CryoHype scales to the reconstruction of 1,000 distinct structures from unlabeled cryo-EM images in the fixed-pose setting.

</details>


### [53] [Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate](https://arxiv.org/abs/2512.06344)
*Kaile Wang,Lijun He,Haisheng Fu,Haixia Bi,Fan Li*

Main category: cs.CV

TL;DR: MTGC框架通过多模态引导和任务感知语义压缩，在超低码率下提升生成式图像压缩的语义一致性和重建质量


<details>
  <summary>Details</summary>
Motivation: 生成式图像压缩在超低码率下存在语义偏差问题，限制了其在6G语义通信中的可靠部署，需要解决生成幻觉导致的语义不一致问题

Method: 提出MTGC框架：1) 集成三种引导模态：文本描述、高压缩图像、语义伪词；2) 设计任务感知语义压缩模块提取任务相关语义；3) 多模态引导扩散解码器采用双路径协同引导机制

Result: 实验表明MTGC显著提升语义一致性（DISTS下降10.59%），同时在超低码率下获得感知质量和像素级保真度的显著提升

Conclusion: MTGC框架通过多模态引导和任务感知语义压缩，有效解决了超低码率下生成式图像压缩的语义偏差问题，为6G语义通信提供了可靠解决方案

Abstract: Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp < 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model's powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.

</details>


### [54] [CLUENet: Cluster Attention Makes Neural Networks Have Eyes](https://arxiv.org/abs/2512.06345)
*Xiangshuai Song,Jun-Jie Huang,Tianrui Liu,Ke Liang,Chang Tang*

Main category: cs.CV

TL;DR: CLUENet：一种基于聚类范式的透明深度架构，通过全局软聚合硬分配、温度缩放余弦注意力、门控残差连接、硬共享特征分发和改进的聚类池化策略，在视觉语义理解任务中实现了准确性、效率和透明性的平衡。


<details>
  <summary>Details</summary>
Motivation: 卷积和注意力模型在视觉任务中虽然成功，但其固定的感受野和复杂架构限制了建模不规则空间模式的能力，且可解释性不足。聚类范式虽然提供可解释性和灵活语义建模，但存在准确性有限、效率低和训练梯度消失等问题。

Method: 提出CLUENet，包含三个关键创新：1) 全局软聚合硬分配，结合温度缩放余弦注意力和门控残差连接增强局部建模；2) 块间硬共享特征分发；3) 改进的聚类池化策略。

Result: 在CIFAR-100和Mini-ImageNet上的实验表明，CLUENet超越了现有聚类方法和主流视觉模型，在分类性能和视觉可解释性方面均有显著提升。

Conclusion: CLUENet为视觉语义理解提供了一种透明深度架构，在准确性、效率和透明度之间取得了良好平衡，为解决现有模型的局限性提供了新思路。

Abstract: Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.

</details>


### [55] [TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search](https://arxiv.org/abs/2512.06353)
*Kaicheng Yang,Kaisen Yang,Baiting Wu,Xun Zhang,Qianrui Yang,Haotong Qin,He Zhang,Yulun Zhang*

Main category: cs.CV

TL;DR: TreeQ是一个针对扩散变换器（DiT）的统一量化框架，通过树结构搜索、环境噪声引导和通用君主分支技术，实现了DiT模型在超低比特量化下的高性能，首次在DiT上达到接近无损的4位PTQ性能。


<details>
  <summary>Details</summary>
Motivation: DiT虽然比U-Net更具可扩展性和性能优势，但其实际部署面临高计算和内存需求挑战。现有的混合精度量化（MPQ）方法在U-Net上取得了成功，但在DiT架构上的应用仍有限且未充分探索。

Method: 1. 树结构搜索（TSS）：利用DiT的线性特性，以O(n)时间复杂度遍历解空间，通过比较剪枝提高目标精度；2. 环境噪声引导（ENG）：使用单一超参数对齐PTQ和QAT配置；3. 通用君主分支（GMB）：结构化稀疏分支防止超低比特下的信息瓶颈。

Result: TreeQ在DiT-XL/2模型上，在W3A3和W4A4的PTQ/PEFT设置下实现了最先进的性能。首次在DiT模型上实现了接近无损的4位PTQ性能。

Conclusion: TreeQ是一个有效的DiT量化框架，通过创新的搜索、优化和结构设计，解决了DiT量化中的关键挑战，为DiT的实际部署提供了可行的解决方案。

Abstract: Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ

</details>


### [56] [Rectifying Latent Space for Generative Single-Image Reflection Removal](https://arxiv.org/abs/2512.06358)
*Mingjia Li,Jin Hu,Hainuo Wang,Qiming Hu,Jiarui Wang,Xiaojie Guo*

Main category: cs.CV

TL;DR: 将编辑型潜在扩散模型重构用于单图像反射去除，通过反射等变VAE、可学习任务特定文本嵌入和深度引导早期分支采样策略，在多个基准测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 单图像反射去除是一个高度不适定问题，现有方法难以推理被破坏区域的组成，导致在真实场景中恢复和泛化能力不足。潜在空间缺乏解释复合图像作为线性叠加层的能力是核心挑战。

Method: 1) 反射等变VAE：将潜在空间与反射形成的线性物理对齐；2) 可学习任务特定文本嵌入：绕过模糊语言提供精确指导；3) 深度引导早期分支采样策略：利用生成随机性获得有希望的结果

Result: 在多个基准测试中实现了新的SOTA性能，并且在具有挑战性的真实世界案例中表现出良好的泛化能力

Conclusion: 通过重构编辑型潜在扩散模型，结合三个协同组件，成功解决了单图像反射去除问题，实现了高质量的恢复效果和良好的泛化性能

Abstract: Single-image reflection removal is a highly ill-posed problem, where existing methods struggle to reason about the composition of corrupted regions, causing them to fail at recovery and generalization in the wild. This work reframes an editing-purpose latent diffusion model to effectively perceive and process highly ambiguous, layered image inputs, yielding high-quality outputs. We argue that the challenge of this conversion stems from a critical yet overlooked issue, i.e., the latent space of semantic encoders lacks the inherent structure to interpret a composite image as a linear superposition of its constituent layers. Our approach is built on three synergistic components, including a reflection-equivariant VAE that aligns the latent space with the linear physics of reflection formation, a learnable task-specific text embedding for precise guidance that bypasses ambiguous language, and a depth-guided early-branching sampling strategy to harness generative stochasticity for promising results. Extensive experiments reveal that our model achieves new SOTA performance on multiple benchmarks and generalizes well to challenging real-world cases.

</details>


### [57] [Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection](https://arxiv.org/abs/2512.06363)
*Jiabao Guo,Yadian Wang,Hui Ma,Yuhao Fu,Ju Jia,Hui Liu,Shengeng Tang,Lechao Cheng,Yunfeng Diao,Ajian Liu*

Main category: cs.CV

TL;DR: 提出SPL-UAD框架，通过解耦物理和数字攻击的提示空间优化分支，实现统一的物理-数字攻击检测，解决现有方法中优化方向冲突的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的人脸识别系统容易受到物理呈现攻击和数字伪造攻击的双重威胁。现有方法主要使用CLIP加正则化约束来增强模型在两项任务上的泛化能力，但这些方法在相同类别提示空间下存在物理和数字攻击检测的优化方向冲突问题。

Method: 提出SPL-UAD框架：1）构建可学习的并行提示分支，通过自适应欺骗上下文提示生成增强，实现对每种攻击类型的独立优化控制；2）设计线索感知增强，利用双提示机制在数据上生成具有挑战性的样本挖掘任务，显著增强模型对未见攻击类型的鲁棒性。

Result: 在大型UniAttackDataPlus数据集上的大量实验表明，该方法在统一攻击检测任务中取得了显著的性能提升。

Conclusion: 通过解耦物理和数字攻击的提示空间优化分支，SPL-UAD框架能够有效解决现有方法中的优化冲突问题，实现更全面的生物特征数据保护。

Abstract: Real-world face recognition systems are vulnerable to both physical presentation attacks (PAs) and digital forgery attacks (DFs). We aim to achieve comprehensive protection of biometric data by implementing a unified physical-digital defense framework with advanced detection. Existing approaches primarily employ CLIP with regularization constraints to enhance model generalization across both tasks. However, these methods suffer from conflicting optimization directions between physical and digital attack detection under same category prompt spaces. To overcome this limitation, we propose a Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization branches for physical and digital attacks in the prompt space. Specifically, we construct a learnable parallel prompt branch enhanced with adaptive Spoofing Context Prompt Generation, enabling independent control of optimization for each attack type. Furthermore, we design a Cues-awareness Augmentation that leverages the dual-prompt mechanism to generate challenging sample mining tasks on data, significantly enhancing the model's robustness against unseen attack types. Extensive experiments on the large-scale UniAttackDataPlus dataset demonstrate that the proposed method achieves significant performance improvements in unified attack detection tasks.

</details>


### [58] [Human3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos](https://arxiv.org/abs/2512.06368)
*Weitao Xiong,Zhiyuan Yuan,Jiahao Lu,Chengfeng Zhao,Peng Li,Yuan Liu*

Main category: cs.CV

TL;DR: 提出Human3R方法，通过结合SMPL人体模型和单目深度估计的混合几何先验，解决动态视频重建中的几何不一致和分辨率退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏3D人体结构理解，导致几何不一致的结果（肢体比例扭曲、人-物融合不自然），且内存限制的下采样导致人体边界向背景几何漂移。

Method: 提出Human3R方法，采用分层处理流程：先处理全分辨率图像获取整体场景几何，然后通过策略性裁剪和交叉注意力融合增强人体特定细节，通过特征融合模块集成SMPL先验。

Result: 在TUM Dynamics和GTA-IM数据集上的实验表明，该方法在动态人体重建方面具有优越性能。

Conclusion: 通过结合混合几何先验和分层处理，Human3R能够实现几何一致且细节丰富的动态人体场景重建。

Abstract: Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues. Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry. To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation. Our approach leverages structured human priors to maintain surface consistency while capturing fine-grained geometric details in human regions. We introduce Human3R, featuring a hierarchical pipeline with refinement components that processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. The method integrates SMPL priors through a Feature Fusion Module to ensure geometrically plausible reconstruction while preserving fine-grained human boundaries. Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction.

</details>


### [59] [VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2512.06373)
*Yuji Wang,Wenlong Liu,Jingxuan Niu,Haoji Zhang,Yansong Tang*

Main category: cs.CV

TL;DR: VG-Refiner：首个工具精炼的指代接地推理框架，通过两阶段思考-再思考机制和精炼奖励，提升对不可靠工具输出的处理能力，显著提高指代接地任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有工具集成视觉推理范式主要关注通过强化学习整合各种视觉工具，但缺乏处理不可靠或错误工具输出的有效响应机制。这在指代和接地任务中尤为突出，不准确的检测工具预测常常误导模型产生幻觉推理。

Method: 提出VG-Refiner框架，引入两阶段思考-再思考机制，让模型能显式分析和响应工具反馈；设计精炼奖励机制，鼓励对不良工具结果进行有效修正；提出两个新指标和公平评估协议来系统衡量模型的精炼能力。

Result: 使用少量任务特定数据增强VG-Refiner的精炼能力，在指代和推理接地基准测试中实现了准确率和修正能力的显著提升，同时保持了预训练模型的通用能力。

Conclusion: VG-Refiner是首个针对工具精炼的指代接地推理框架，通过系统化的精炼机制设计，有效解决了现有TiVR模型在处理不可靠工具输出时的局限性，为视觉推理提供了更可靠的解决方案。

Abstract: Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.

</details>


### [60] [Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework](https://arxiv.org/abs/2512.06376)
*Xinhao Xiang,Abhijeet Rastogi,Jiawei Zhang*

Main category: cs.CV

TL;DR: 论文提出诊断框架评估AI生成驾驶视频对自动驾驶模型训练的有效性，发现原始AIGV会降低感知性能，但经过ADGVE筛选后可成为真实数据的有效补充。


<details>
  <summary>Details</summary>
Motivation: AI生成的驾驶视频为自动驾驶提供了低成本、可扩展的数据替代方案，但需要系统评估这些视频能否可靠支持自动驾驶模型的训练和评估。

Method: 1) 建立AIGV常见故障模式分类法；2) 构建ADGV-Bench驾驶基准数据集；3) 提出ADGVE评估器，结合静态语义、时序线索、车道遵守信号和VLM引导推理生成质量评分；4) 实验验证筛选机制的有效性。

Result: 盲目添加原始AIGV会降低感知性能，但使用ADGVE筛选后能同时提升视频质量评估指标和下游自动驾驶模型性能，使AIGV成为真实数据的有效补充。

Conclusion: 研究揭示了AIGV在自动驾驶中的风险和潜力，提供了实用工具来安全利用大规模视频生成技术，为未来自动驾驶流程中的AIGV应用提供了指导。

Abstract: Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.

</details>


### [61] [VAD-Net: Multidimensional Facial Expression Recognition in Intelligent Education System](https://arxiv.org/abs/2512.06377)
*Yi Huo,Yun Ge*

Main category: cs.CV

TL;DR: 该研究为FER2013数据集添加了VAD（效价-唤醒-支配）三维情感标注，并提出了使用正交卷积的回归网络来提升VAD预测精度。


<details>
  <summary>Details</summary>
Motivation: 当前FER数据集主要基于离散情感类别（如高兴、愤怒等），表达性有限。未来情感计算需要更全面精确的VAD多维参数，但现有数据集如AffectNet缺少支配维度标注。

Method: 1. 为FER2013数据集手动标注VAD三维情感参数；2. 在ResNet基础上引入正交卷积，提取更多样化的特征以提升VAD预测能力。

Result: 1. 成功标注了D维度，但发现D维度比V和A维度更难标注和预测；2. 正交卷积的消融实验验证了其能获得更好的VAD预测结果。

Conclusion: 该研究首次为FER数据集提供了D维度标注，并通过正交卷积提出了更好的VAD预测网络。新建的VAD标注数据集可作为多维情感基准，正交回归网络可作为VAD情感预测的基线模型。

Abstract: Current FER (Facial Expression Recognition) dataset is mostly labeled by emotion categories, such as happy, angry, sad, fear, disgust, surprise, and neutral which are limited in expressiveness. However, future affective computing requires more comprehensive and precise emotion metrics which could be measured by VAD(Valence-Arousal-Dominance) multidimension parameters. To address this, AffectNet has tried to add VA (Valence and Arousal) information, but still lacks D(Dominance). Thus, the research introduces VAD annotation on FER2013 dataset, takes the initiative to label D(Dominance) dimension. Then, to further improve network capacity, it enforces orthogonalized convolution on it, which extracts more diverse and expressive features and will finally increase the prediction accuracy. Experiment results show that D dimension could be measured but is difficult to obtain compared with V and A dimension no matter in manual annotation or regression network prediction. Secondly, the ablation test by introducing orthogonal convolution verifies that better VAD prediction could be obtained in the configuration of orthogonal convolution. Therefore, the research provides an initiative labelling for D dimension on FER dataset, and proposes a better prediction network for VAD prediction through orthogonal convolution. The newly built VAD annotated FER2013 dataset could act as a benchmark to measure VAD multidimensional emotions, while the orthogonalized regression network based on ResNet could act as the facial expression recognition baseline for VAD emotion prediction. The newly labeled dataset and implementation code is publicly available on https://github.com/YeeHoran/VAD-Net .

</details>


### [62] [OCFER-Net: Recognizing Facial Expression in Online Learning System](https://arxiv.org/abs/2512.06379)
*Yi Huo,Lei Zhang*

Main category: cs.CV

TL;DR: 论文提出OCFER-Net，通过正交性正则化提升卷积核多样性，改进面部表情识别性能


<details>
  <summary>Details</summary>
Motivation: 在线学习中情感交互很重要，面部表情识别（FER）能帮助教师了解学生情绪状态。现有方法较少利用卷积矩阵的正交性，正交性可以提取更多样化和表达力强的特征。

Method: 提出OCFER-Net网络，通过正则化强制卷积核的正交性，从而提取更具多样性和表达力的特征

Result: 在FER-2013数据集上实验，性能优于基线方法1.087分

Conclusion: 正交性正则化能有效提升面部表情识别性能，代码已开源

Abstract: Recently, online learning is very popular, especially under the global epidemic of COVID-19. Besides knowledge distribution, emotion interaction is also very important. It can be obtained by employing Facial Expression Recognition (FER). Since the FER accuracy is substantial in assisting teachers to acquire the emotional situation, the project explores a series of FER methods and finds that few works engage in exploiting the orthogonality of convolutional matrix. Therefore, it enforces orthogonality on kernels by a regularizer, which extracts features with more diversity and expressiveness, and delivers OCFER-Net. Experiments are carried out on FER-2013, which is a challenging dataset. Results show superior performance over baselines by 1.087. The code of the research project is publicly available on https://github.com/YeeHoran/OCFERNet.

</details>


### [63] [Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement](https://arxiv.org/abs/2512.06400)
*Jing Tao,Yonghong Zong,Banglei Guana,Pengju Sun,Taihang Lei,Yang Shanga,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出基于区域感知的红外与可见光融合框架，通过多曝光与多模态成像结合，在极端环境下保持几何保真度与热辐射信息，提升图像清晰度与测量精度。


<details>
  <summary>Details</summary>
Motivation: 在摄影测量中，准确融合红外与可见光谱同时保持可见特征的几何保真度并融入热辐射信息是一个重大挑战，特别是在极端条件下。现有方法常常会损害可见图像质量，影响测量精度。

Method: 提出基于区域感知的融合框架，使用空间变化曝光相机结合多曝光与多模态成像。包括：1）基于区域感知的特征融合确保精确的多模态配准；2）自适应融合与对比度增强；3）由区域显著性图指导的结构相似性补偿机制优化IR-VIS光谱整合；4）框架可适应单曝光场景。

Result: 在合成和真实数据上的实验表明，该方法在图像清晰度和性能方面优于现有最先进方法，定量和视觉评估均证实了其优越性。

Conclusion: 提出的区域感知融合框架有效解决了极端环境下红外与可见光融合的挑战，在保持几何保真度的同时提升了图像质量和测量精度，为多模态摄影测量提供了鲁棒的解决方案。

Abstract: In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.

</details>


### [64] [Rethinking Training Dynamics in Scale-wise Autoregressive Generation](https://arxiv.org/abs/2512.06421)
*Gengze Zhou,Chongjian Ge,Hao Tan,Feng Liu,Yicong Hong*

Main category: cs.CV

TL;DR: SAR（自回归精炼）通过交错尺度展开和对比学生强制损失，解决自回归生成模型中的曝光偏差问题，显著提升生成质量


<details>
  <summary>Details</summary>
Motivation: 尺度自回归模型存在曝光偏差问题，这主要由训练-测试不匹配和尺度间学习难度不平衡两个原因导致，影响生成质量

Method: 提出自回归精炼（SAR）方法，包含交错尺度展开（SSR）机制和对比学生强制损失（CSFL）。SSR通过轻量级自回归展开让模型接触自身中间预测，对齐训练测试模式；CSFL为自生成上下文提供充分监督，确保训练稳定

Result: SAR应用于预训练AR模型能持续改善生成质量，计算开销小。例如，在ImageNet 256上训练的FlexVAR-d16模型，SAR在10个epoch内（32xA100 GPU上5小时）实现5.2%的FID降低

Conclusion: SAR作为一种高效、可扩展且有效的后训练方法，有望成为视觉自回归生成的可靠解决方案

Abstract: Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.

</details>


### [65] [A Perception CNN for Facial Expression Recognition](https://arxiv.org/abs/2512.06422)
*Chunwei Tian,Jingyuan Xie,Lingjun Li,Wangmeng Zuo,Yanning Zhang,David Zhang*

Main category: cs.CV

TL;DR: 提出感知CNN（PCNN）用于面部表情识别，通过并行网络学习局部面部特征，结合多域交互机制和两阶段损失函数，在多个数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在面部表情识别中可能忽略面部区域分割的影响，无法有效捕捉表情的细微变化，需要更好的方法来同时学习局部特征和全局结构特征。

Method: 1. 使用五个并行网络分别学习眼睛、脸颊和嘴巴等局部面部特征；2. 采用多域交互机制融合局部器官特征和全局面部结构特征；3. 设计两阶段损失函数约束感知信息的准确性和重建面部图像的质量。

Result: PCNN在多个实验室和真实世界面部表情识别基准数据集（CK+、JAFFE、FER2013、FERPlus、RAF-DB和遮挡姿态变异数据集）上取得了优越的结果。

Conclusion: PCNN通过并行学习局部特征、多域特征融合和两阶段损失函数，有效提升了面部表情识别的性能，能够更好地捕捉表情的细微变化。

Abstract: Convolutional neural networks (CNNs) can automatically learn data patterns to express face images for facial expression recognition (FER). However, they may ignore effect of facial segmentation of FER. In this paper, we propose a perception CNN for FER as well as PCNN. Firstly, PCNN can use five parallel networks to simultaneously learn local facial features based on eyes, cheeks and mouth to realize the sensitive capture of the subtle changes in FER. Secondly, we utilize a multi-domain interaction mechanism to register and fuse between local sense organ features and global facial structural features to better express face images for FER. Finally, we design a two-phase loss function to restrict accuracy of obtained sense information and reconstructed face images to guarantee performance of obtained PCNN in FER. Experimental results show that our PCNN achieves superior results on several lab and real-world FER benchmarks: CK+, JAFFE, FER2013, FERPlus, RAF-DB and Occlusion and Pose Variant Dataset. Its code is available at https://github.com/hellloxiaotian/PCNN.

</details>


### [66] [DragMesh: Interactive 3D Generation Made Easy](https://arxiv.org/abs/2512.06424)
*Tianshan Zhang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: DragMesh是一个实时交互式3D关节运动生成框架，通过解耦的语义意图推理和几何回归来预测关节参数，并使用双四元数VAE生成符合运动学约束的完整运动轨迹。


<details>
  <summary>Details</summary>
Motivation: 当前关节运动生成方法存在两难：要么物理一致但速度慢无法实时使用，要么生成速度快但违反基本运动学约束。需要一种既能实时交互又能保持运动学一致性的方法。

Method: 1) 解耦的运动学推理：通过语义意图推理确定关节类型，通过几何回归（KPP-Net）确定轴和原点；2) 双四元数VAE（DQ-VAE）：利用双四元数的紧凑、连续、无奇异性特性表示刚体运动；3) 使用FiLM条件注入在每一层确保运动学约束；4) 数值稳定的叉积损失保证轴对齐。

Result: DragMesh实现了实时性能，能够在未见过的物体上生成合理的关节运动而无需重新训练，为生成式3D智能提供了实用步骤。

Conclusion: DragMesh通过解耦的运动学推理和生成框架，解决了实时交互式3D关节运动生成的挑战，在保持运动学一致性的同时实现了实时性能。

Abstract: While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.

</details>


### [67] [When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition](https://arxiv.org/abs/2512.06426)
*Nzakiese Mbongo,Kailash A. Hambarde,Hugo Proença*

Main category: cs.CV

TL;DR: 提出双路径Transformer框架，利用CLIP联合建模视觉和属性线索，实现远距离性别识别，在统一数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 远距离图像性别识别面临分辨率低、视角多变、面部特征缺失等挑战，需要更鲁棒的方法。

Method: 双路径Transformer框架：1) 视觉路径通过选择性微调CLIP图像编码器上层；2) 属性路径通过软生物特征提示（发型、服装等）在CLIP文本-图像空间中推理性别；加入空间通道注意力模块增强判别定位。

Result: 在构建的U-DetAGReID数据集上，方法在macro-F1、准确率、AUC等多个指标上超越现有行人属性和重识别基线，对距离、角度、高度变化具有鲁棒性。

Conclusion: 语言引导的双路径学习为无约束远距离场景下的负责任性别识别提供了可扩展的基础框架。

Abstract: Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.

</details>


### [68] [Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion](https://arxiv.org/abs/2512.06504)
*Andrii Lysyi,Anatoliy Sachenko,Pavlo Radiuk,Mykola Lysyi,Oleksandr Melnychenko,Diana Zahorodnia*

Main category: cs.CV

TL;DR: 开发了一个智能光伏检测框架，通过多模态融合和自适应重采集，解决了传统方法的热调色板偏差、数据冗余和高带宽问题，在公开基准上取得了12-15%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统光伏检测方法存在热调色板偏差、数据冗余和高通信带宽需求等关键缺陷，需要开发自动化、智能化的监测系统来提高光伏电站的安全性和运营效率。

Method: 采用协同架构：1）学习调色板不变的热嵌入表示；2）通过门控机制与对比归一化的RGB流融合；3）基于罗德里格斯更新的自适应重采集控制器；4）使用DBSCAN和半正矢距离的地理空间去重模块。

Result: 在PVF-10基准上达到0.903的mAP@0.5，比单模态基线提升12-15%；现场验证召回率达96%；去重减少15-20%的重复误报；相关性遥测减少60-70%的空中数据传输。

Conclusion: 建立了一个主动式光伏检测新范式，系统已具备现场应用准备，显著提升了检测精度和效率，同时大幅降低了通信带宽需求。

Abstract: The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.

</details>


### [69] [Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening](https://arxiv.org/abs/2512.06434)
*Lucas R. Mareque,Ricardo L. Armentano,Leandro J. Cymberknop*

Main category: cs.CV

TL;DR: 使用深度学习从2D合成人体图像自动估计五项关键人体测量指标，为运动员心血管筛查提供可扩展的自动化工具


<details>
  <summary>Details</summary>
Motivation: 传统运动员心血管检查中的人体测量方法（如检测马凡综合征）需要手动测量，存在劳动密集、操作者依赖性强、难以规模化的问题，需要自动化解决方案来提高筛查效率和可扩展性

Method: 提出基于深度学习的全自动方法，使用从3D人体网格生成的10万张合成图像数据集，训练并评估VGG19、ResNet50和DenseNet121等卷积神经网络，通过全连接层进行回归分析，预测五项关键人体测量指标

Result: 所有模型都达到了亚厘米级精度，其中ResNet50表现最佳，在所有测量指标上的平均MAE为0.668厘米，证明深度学习能够提供准确的人体测量数据

Conclusion: 深度学习方法能够规模化提供准确的人体测量数据，可作为运动员筛查协议的实际补充工具，未来工作将在真实世界图像上验证模型以扩展适用性

Abstract: Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.

</details>


### [70] [ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images](https://arxiv.org/abs/2512.06521)
*Jens Dede,Anna Förster*

Main category: cs.CV

TL;DR: 提出ShadowWolf统一框架，通过整合优化AI模型训练和评估阶段，解决野生动物监测中环境变化带来的挑战，实现动态模型重训练以减少标注工作量并提升适应性。


<details>
  <summary>Details</summary>
Motivation: 全球人口增长导致人类栖息地扩张，野生动物空间减少，人兽互动增加。这些互动从轻微干扰到物种灭绝不等，使得野生动物监测变得重要。传统AI训练面临环境多样性（地形、天气、光照、距离）带来的模型鲁棒性和适应性挑战。

Method: 提出名为ShadowWolf的统一框架，整合优化AI模型训练和评估阶段。该框架支持动态模型重训练，以适应环境条件和应用需求的变化，减少标注工作量，允许现场模型适应。

Result: 该自适应统一方法提高了野生动物监测系统的准确性和效率，促进更有效和可扩展的保护工作。

Conclusion: ShadowWolf框架通过动态适应环境变化，解决了野生动物监测AI模型的实际应用挑战，为保护工作提供了更有效的技术解决方案。

Abstract: The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.
  In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.

</details>


### [71] [AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars](https://arxiv.org/abs/2512.06438)
*Ramazan Fazylov,Sergey Zagoruyko,Aleksandr Parkin,Stamatis Lefkimmiatis,Ivan Laptev*

Main category: cs.CV

TL;DR: AGORA：首个基于3D高斯泼溅的可动画化3D人体化身生成框架，通过FLAME条件变形分支实现身份保持的精细表情控制，支持实时推理（GPU 250+FPS，CPU ~9FPS）


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF的方法渲染速度慢且动态一致性差，而3DGS方法通常仅限于静态头部生成，缺乏动态控制能力。需要开发既能保持高质量又能实现实时动画控制的3D人体化身生成方法。

Method: 将3D高斯泼溅（3DGS）与生成对抗网络结合，引入轻量级FLAME条件变形分支预测每个高斯的残差，实现身份保持的精细表情控制。采用双判别器训练方案，利用参数化网格的合成渲染来增强表情保真度。

Result: 在表情准确性上优于最先进的NeRF方法，在单GPU上渲染速度达250+FPS，CPU推理约9FPS，是首个实现实用CPU推理的可动画化3DGS化身合成方法。

Conclusion: AGORA代表了向实用、高性能数字人类迈出的重要一步，实现了视觉真实性和精确可控性的平衡，为VR、远程呈现和娱乐应用提供了可行的解决方案。

Abstract: The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: https://ramazan793.github.io/AGORA/

</details>


### [72] [Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images](https://arxiv.org/abs/2512.06531)
*Sayan Das,Arghadip Biswas*

Main category: cs.CV

TL;DR: 提出两种新颖的深度学习架构：SAETCN用于脑肿瘤分类（准确率99.38%），SAS-Net用于脑肿瘤分割（像素准确率99.23%），解决现有模型泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤对人类生命构成重大威胁，早期准确检测对诊断和治疗至关重要。虽然放射科医生可以手动从MRI扫描图像中检测脑肿瘤，但近年来儿童和青少年脑肿瘤发病率上升导致数据量巨大，手动检测耗时且困难。现有模型泛化能力不足，在验证数据上表现不佳。

Method: 提出两种新颖的深度学习架构：1) SAETCN（自注意力增强肿瘤分类网络）用于脑肿瘤分类，包含胶质瘤、脑膜瘤、垂体瘤和非肿瘤病例四类；2) SAS-Net（自注意力分割网络）用于脑肿瘤的精确分割。两种架构都利用了自注意力机制。

Result: SAETCN在验证数据集上达到99.38%的准确率，成为少数能够准确检测脑肿瘤的新型深度学习架构之一；SAS-Net达到99.23%的整体像素准确率。

Conclusion: 提出的两种深度学习架构在脑肿瘤检测任务中表现出色，SAETCN在分类任务上达到99.38%准确率，SAS-Net在分割任务上达到99.23%像素准确率，为解决脑肿瘤自动检测问题提供了有效的CAD系统方案。

Abstract: Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.

</details>


### [73] [Towards Stable Cross-Domain Depression Recognition under Missing Modalities](https://arxiv.org/abs/2512.06447)
*Jiuyi Chen,Mingkui Tan,Haifeng Lu,Qiuna Xu,Zhihua Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: 提出SCD-MLLM框架，基于多模态大语言模型实现稳定的跨域抑郁症识别，支持异构数据输入并增强对缺失模态的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 抑郁症筛查具有重要公共卫生意义，但现有音频视频检测方法缺乏统一通用框架，对真实场景中常见的缺失模态数据稳定性不足。

Method: 提出SCD-MLLM框架，包含：1) 多源数据输入适配器(MDIA)，使用掩码机制和任务特定提示将异构输入转为统一标记序列；2) 模态感知自适应融合模块(MAFM)，通过共享投影机制自适应融合音视频特征。

Result: 在五个公开异构数据集(CMDC、AVEC2014、DAIC-WOZ、DVlog、EATD)上，无论是完整还是部分模态设置下，SCD-MLLM均优于现有SOTA模型及主流商业LLM(Gemini和GPT)。

Conclusion: SCD-MLLM展示了优越的跨域泛化能力、增强的多模态抑郁症线索捕捉能力，以及在真实应用中缺失模态情况下的强稳定性。

Abstract: Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.

</details>


### [74] [SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities](https://arxiv.org/abs/2512.06562)
*Dung Thuy Nguyen,Quang Nguyen,Preston K. Robinette,Eli Jiang,Taylor T. Johnson,Kevin Leach*

Main category: cs.CV

TL;DR: SUGAR是一个可扩展的生成式遗忘框架，能够同时或顺序移除多个身份，无需重新训练整个模型，通过个性化代理潜在表示将不需要的身份重定向到视觉连贯的替代方案。


<details>
  <summary>Details</summary>
Motivation: 随着3D感知生成模型的发展，人类身份的高保真图像合成能力提升，但引发了关于用户同意和从模型输出空间中移除特定个体的紧迫问题。

Method: SUGAR为每个身份学习个性化的代理潜在表示，将重建重定向到视觉连贯的替代方案，同时引入持续效用保持目标以防止随着更多身份被遗忘而导致的性能退化。

Result: SUGAR在移除多达200个身份方面实现了最先进的性能，与现有基线相比，保留效用提高了700%。

Conclusion: SUGAR提供了一种可扩展的生成式遗忘解决方案，能够在移除不需要的身份的同时保持模型质量和多样性，解决了3D生成模型中用户同意和隐私保护的重要问题。

Abstract: Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.

</details>


### [75] [Sanvaad: A Multimodal Accessibility Framework for ISL Recognition and Voice-Based Interaction](https://arxiv.org/abs/2512.06485)
*Kush Revankar,Shreyas Deshpande,Araham Sayeed,Ansh Tandale,Sarika Bobde*

Main category: cs.CV

TL;DR: Sanvaad是一个轻量级多模态无障碍框架，支持聋人、视障人士和听力正常人群之间的实时双向通信，结合了手语识别、语音转手语、语音识别和文本转语音功能。


<details>
  <summary>Details</summary>
Motivation: 当前聋人、视障人士和听力正常人群之间的通信工具通常只支持单向交互，存在沟通障碍。需要一种支持双向实时通信的无障碍解决方案。

Method: 1. 使用MediaPipe地标构建印度手语识别模块，因其高效低计算负载；2. 语音转手语组件将检测到的语音映射到预定义短语并生成相应GIF或字母可视化；3. 为视障用户提供无屏幕语音界面，集成多语言语音识别、文本摘要和文本转语音功能；4. 通过Streamlit界面整合所有组件，支持桌面和移动环境。

Result: 开发了一个统一的轻量级框架，能够在边缘设备上流畅运行，无需专用硬件，实现了聋人、视障人士和听力正常人群之间的双向实时通信。

Conclusion: Sanvaad通过结合轻量级计算机视觉和语音处理工具，为包容性通信提供了实用且可访问的途径，解决了当前单向通信工具的局限性。

Abstract: Communication between deaf users, visually im paired users, and the general hearing population often relies on tools that support only one direction of interaction. To address this limitation, this work presents Sanvaad, a lightweight multimodal accessibility framework designed to support real time, two-way communication. For deaf users, Sanvaad includes an ISL recognition module built on MediaPipe landmarks. MediaPipe is chosen primarily for its efficiency and low computational load, enabling the system to run smoothly on edge devices without requiring dedicated hardware. Spoken input from a phone can also be translated into sign representations through a voice-to-sign component that maps detected speech to predefined phrases and produces corresponding GIFs or alphabet-based visualizations. For visually impaired users, the framework provides a screen free voice interface that integrates multilingual speech recognition, text summarization, and text-to-speech generation. These components work together through a Streamlit-based interface, making the system usable on both desktop and mobile environments. Overall, Sanvaad aims to offer a practical and accessible pathway for inclusive communication by combining lightweight computer vision and speech processing tools within a unified framework.

</details>


### [76] [Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution](https://arxiv.org/abs/2512.06642)
*Achmad Ardani Prasha,Clavino Ourizqi Rachmadi,Muhamad Fauzan Ibnu Syahlan,Naufal Rahfi Anugerah,Nanda Garin Raditya,Putri Amelia,Sabrina Laila Mutiara,Hilman Syachr Ramadhan*

Main category: cs.CV

TL;DR: 使用掩码自编码器（MAE）在强引力透镜模拟图像上进行预训练，学习可泛化的表征，用于暗物质模型分类和图像超分辨率任务，相比从头训练获得性能提升。


<details>
  <summary>Details</summary>
Motivation: 强引力透镜可以揭示暗物质子结构的影响，但从噪声、低分辨率图像中分析这些效应具有挑战性。需要开发能够处理多个下游任务的通用表征学习方法。

Method: 在DeepLense ML4SCI基准的模拟强透镜图像上使用掩码自编码器（MAE）预训练策略。使用Vision Transformer编码器，通过掩码图像建模目标进行预训练，然后针对两个下游任务分别微调：暗物质模型分类（冷暗物质、轴子样或无子结构）和图像超分辨率（16x16到64x64）。研究了不同掩码比例的影响。

Result: 在90%掩码比例下，微调后的分类器达到宏观AUC 0.968和准确率88.65%，优于从头训练的基线（AUC 0.957，准确率82.46%）。对于超分辨率任务，MAE预训练模型重建图像的PSNR约33 dB，SSIM 0.961，略优于从头训练。发现掩码比例存在权衡：更高的掩码比例改善分类但略微降低重建保真度。

Conclusion: MAE预训练在物理丰富的模拟数据上提供了灵活、可重用的编码器，适用于多个强透镜分析任务，证明了自监督预训练在天体物理应用中的价值。

Abstract: Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.

</details>


### [77] [TextMamba: Scene Text Detector with Mamba](https://arxiv.org/abs/2512.06657)
*Qiyan Zhao,Yue Yan,Da-Han Wang*

Main category: cs.CV

TL;DR: 提出基于Mamba的场景文本检测器，通过选择机制与注意力层结合，增强编码器从长序列中提取相关信息的能力，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的场景文本检测方法虽然解决了CNN全局特征提取的局限性，但直接使用原生Transformer注意力层存在跨域限制和固有缺陷：在建模长距离依赖时会遗忘重要信息或关注无关表示。最近提出的状态空间模型Mamba通过线性复杂度选择机制展现了更好的长距离依赖建模能力。

Method: 1. 提出基于Mamba的场景文本检测器，将选择机制与注意力层集成；2. 采用Top_k算法显式选择关键信息，减少Mamba建模中无关信息的干扰；3. 设计双尺度前馈网络和嵌入金字塔增强模块，促进高维隐藏状态交互和多尺度特征融合。

Result: 在多个基准测试中达到最先进或竞争性性能：CTW1500上F-measure为89.7%，TotalText上为89.2%，ICDAR19ArT上为78.5%。

Conclusion: 提出的基于Mamba的场景文本检测器通过集成选择机制与注意力层，有效增强了编码器从长序列中提取相关信息的能力，在多个基准测试中展现了优越性能。

Abstract: In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\%, 89.2\%, and 78.5\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.

</details>


### [78] [The Role of Entropy in Visual Grounding: Analysis and Optimization](https://arxiv.org/abs/2512.06726)
*Shuo Li,Jiajun Sun,Zhihao Zhang,Xiaoran Fan,Senjie Jin,Hui Li,Yuming Yang,Junjie Ye,Lixing Shen,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

TL;DR: 本文提出ECVGPO算法，通过熵控制优化视觉定位任务中多模态大语言模型的强化学习微调，在探索与利用之间取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 虽然基于强化学习的多模态大语言模型微调在熵控制方面取得了进展，但熵在视觉定位等感知导向任务中的作用和特性尚未得到充分研究，需要探索有效的熵控制策略。

Method: 首先分析视觉定位任务中熵的作用和特性，并与推理任务进行比较。基于这些发现，提出ECVGPO（熵控制视觉定位策略优化）算法，这是一个可解释的算法，专门用于有效的熵调节。

Result: 实验表明，ECVGPO在多个基准测试和模型上都取得了广泛的改进，通过熵控制更好地平衡了探索与利用之间的权衡。

Conclusion: ECVGPO算法为视觉定位任务中的熵控制提供了有效的解决方案，通过更好的探索-利用平衡实现了性能提升，为多模态大语言模型在感知任务中的强化学习微调提供了新思路。

Abstract: Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.

</details>


### [79] [On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization](https://arxiv.org/abs/2512.06530)
*Mohammed Wattad,Tamir Shor,Alex Bronstein*

Main category: cs.CV

TL;DR: 该论文研究了学习型k空间采集模式在加速MRI中的跨域泛化能力，提出了一种通过引入采集不确定性来增强域鲁棒性的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多针对单一数据集或模态优化k空间采集模式，缺乏对跨成像域可迁移性的考虑。本文旨在探索学习型k空间采样在域偏移下的泛化能力。

Method: 提出了一种增强域鲁棒性的新方法：在训练过程中引入采集不确定性，通过随机扰动k空间轨迹来模拟不同扫描仪和成像条件的变化。

Result: 通过跨数据集和采集范式的系统评估，证明使用学习型采样模式训练的模型在跨域设置下表现出更好的泛化性能。

Conclusion: k空间轨迹设计不仅应被视为加速机制，更应作为改善MRI重建中域泛化能力的主动自由度。

Abstract: Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.

</details>


### [80] [Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2512.06746)
*Ruoxin Chen,Jiahui Gao,Kaiqing Lin,Keyue Zhang,Yandan Zhao,Isabel Guan,Taiping Yao,Shouhong Ding*

Main category: cs.CV

TL;DR: 论文提出任务-模型对齐原则，通过将AIGI检测分解为语义一致性检查和像素伪影检测两个互补任务，并分别用VLM和像素专家处理，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于VLM的AIGI检测方法需要大量资源且存在严重幻觉问题。研究发现VLM对语义敏感但对像素伪影不敏感，而传统像素检测器则相反，这揭示了任务与模型之间的不匹配问题。

Method: 提出任务-模型对齐原则，构建AlignGemini双分支检测器：1）用纯语义监督微调VLM进行语义一致性检查；2）用纯像素伪影监督训练像素专家进行像素伪影检测。两个分支在简化数据集上分别训练，产生互补的判别能力。

Result: 在五个真实世界基准测试中，AlignGemini实现了平均准确率+9.5%的提升，证明了任务-模型对齐原则对可泛化AIGI检测的有效性。

Conclusion: 将AIGI检测形式化为两个互补任务（语义一致性检查和像素伪影检测），并通过任务-模型对齐原则分别优化相应模型，可以显著提升检测性能并减少系统盲点。

Abstract: Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.

</details>


### [81] [VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors](https://arxiv.org/abs/2512.06759)
*Wenbo Lyu,Yingjun Du,Jinglin Zhao,Xianton Zhen,Ling Shao*

Main category: cs.CV

TL;DR: VisChainBench是一个大规模基准测试，用于评估大型视觉语言模型在多图像、多轮场景中的视觉推理能力，包含1,457个任务和20,000多张图像，覆盖日常场景和工程故障排除等领域。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注静态或水平比较（如发现视觉差异或评估适当性），且过度依赖语言线索，忽视了渐进式、上下文相关的推理和视觉到视觉的推理挑战。需要填补这一研究空白。

Method: 使用多智能体生成流水线构建VisChainBench基准测试，确保高视觉多样性和受控的语言偏差。基准包含1,457个任务，涵盖20,000多张图像，分布在三个不同领域，模拟现实世界的决策过程。

Result: 创建了VisChainBench基准测试，包含大规模的多步骤视觉推理任务，数据可通过Hugging Face获取。该基准专门设计用于在最小语言指导下评估LVLMs的顺序、相互依赖任务推理能力。

Conclusion: VisChainBench填补了多图像、多轮场景评估的空白，为评估大型视觉语言模型的渐进式、上下文相关的视觉推理能力提供了重要工具，有助于推动该领域的研究发展。

Abstract: Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench

</details>


### [82] [Bridging spatial awareness and global context in medical image segmentation](https://arxiv.org/abs/2512.06560)
*Dalia Alzu'bi,A. Ben Hamza*

Main category: cs.CV

TL;DR: U-CycleMLP：一种新型U形编码器-解码器网络，用于医学图像分割，通过位置注意力权重激励块、密集空洞块和通道CycleMLP块，在保持轻量级架构的同时提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割模型难以有效捕捉局部和全局上下文信息，导致边界像素丢失和分割错误。需要在保持计算效率的同时提高分割精度。

Method: 提出U-CycleMLP网络：编码器使用位置注意力权重激励块、密集空洞块和下采样操作学习多尺度上下文特征；解码器通过上采样、密集空洞块和特征融合机制重建高分辨率分割掩码；在解码器的跳跃连接中加入通道CycleMLP块以增强特征集成。

Result: 在三个基准数据集上的实验表明，U-CycleMLP相比最先进方法具有竞争力，在所有数据集上获得更好的分割精度，能捕捉细粒度解剖结构，在不同医学成像模态中表现出鲁棒性。

Conclusion: U-CycleMLP通过创新的架构设计有效平衡了分割精度和计算效率，消融研究进一步验证了核心组件对提升分割精度的重要性。

Abstract: Medical image segmentation is a fundamental task in computer-aided diagnosis, requiring models that balance segmentation accuracy and computational efficiency. However, existing segmentation models often struggle to effectively capture local and global contextual information, leading to boundary pixel loss and segmentation errors. In this paper, we propose U-CycleMLP, a novel U-shaped encoder-decoder network designed to enhance segmentation performance while maintaining a lightweight architecture. The encoder learns multiscale contextual features using position attention weight excitation blocks, dense atrous blocks, and downsampling operations, effectively capturing both local and global contextual information. The decoder reconstructs high-resolution segmentation masks through upsampling operations, dense atrous blocks, and feature fusion mechanisms, ensuring precise boundary delineation. To further refine segmentation predictions, channel CycleMLP blocks are incorporated into the decoder along the skip connections, enhancing feature integration while maintaining linear computational complexity relative to input size. Experimental results, both quantitative and qualitative, across three benchmark datasets demonstrate the competitive performance of U-CycleMLP in comparison with state-of-the-art methods, achieving better segmentation accuracy across all datasets, capturing fine-grained anatomical structures, and demonstrating robustness across different medical imaging modalities. Ablation studies further highlight the importance of the model's core architectural components in enhancing segmentation accuracy.

</details>


### [83] [Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding](https://arxiv.org/abs/2512.06769)
*Hang Yin,Xiaomin He,PeiWen Yuan,Yiwei Li,Jiayi Shi,Wenxiao Fan,Shaoxiong Feng,Kan Li*

Main category: cs.CV

TL;DR: 提出Stitch and Tell方法，通过拼接图像和生成空间感知的文本对，无需额外标注即可提升视觉语言模型的空间理解能力，减少空间幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型常出现空间幻觉问题，即错误描述图像中物体的相对位置。作者认为这源于图像和文本之间的不对称特性，需要增强模型的空间理解能力。

Method: 提出Stitch and Tell方法：1）沿空间轴拼接图像创建新图像；2）基于拼接图像布局生成空间感知的标题或问答对；3）无需额外标注、无需昂贵模型或人工参与；4）即插即用。

Result: 在LLaVA-v1.5-7B、LLaVA-Qwen2-1.5B和HALVA-7B三种架构上测试，在八个基准上评估。结果显示：空间理解任务显著提升（MME_Position +5.50%，Spatial-MM +4.19%），同时保持或提升通用视觉语言能力（COCO-QA +1.02%，MMBench +4.76%）。

Conclusion: 通过将空间感知结构显式注入训练数据，可以有效缓解空间幻觉问题并提升空间理解能力，同时保持通用视觉语言能力。该方法简单、无需标注、即插即用。

Abstract: Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\text{MME}_{\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.

</details>


### [84] [RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting](https://arxiv.org/abs/2512.06774)
*Longjie Zhao,Ziming Hong,Zhenyang Ren,Runnan Chen,Mingming Gong,Tongliang Liu*

Main category: cs.CV

TL;DR: RDSplat：一种针对3D高斯泼溅的鲁棒数字水印方法，专门设计来抵抗基于扩散的编辑攻击，通过嵌入到低频高斯分量中并使用对抗训练增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS水印方法对基于扩散的编辑攻击非常脆弱，这种编辑可以轻易擦除嵌入的来源信息，因此迫切需要开发对扩散编辑具有内在抵抗力的3DGS水印技术。

Method: 1) 将水印嵌入到扩散编辑会自然保留的3DGS组件中：主动针对低频高斯分量；2) 使用扩散代理进行对抗训练；3) 多域框架在3DGS空间原生操作，通过协调协方差正则化和2D滤波将水印嵌入到扩散编辑保留的低频高斯中；4) 利用高斯模糊作为高效训练代理来模拟扩散编辑的低通滤波行为。

Result: 在三个基准数据集上的综合定量和定性评估表明，RDSplat不仅在对基于扩散的编辑下保持卓越的鲁棒性，还能保持水印的不可见性，实现了最先进的性能。

Conclusion: RDSplat为3D高斯泼溅提供了一种鲁棒的水印范式，能够有效抵抗扩散编辑攻击，解决了现有方法的关键脆弱性问题，为数字资产的版权保护提供了更可靠的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.

</details>


### [85] [GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation](https://arxiv.org/abs/2512.06565)
*Xiujin Liu*

Main category: cs.CV

TL;DR: GNC-Pose是一种无需学习的单目6D物体姿态估计方法，通过渲染初始化、几何感知对应加权和GNC优化实现，无需训练数据或类别先验，在YCB数据集上达到与学习方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法需要大量训练数据和类别先验，而传统方法在严重离群值污染下不稳定。需要一种简单、鲁棒且无需学习的6D姿态估计解决方案。

Method: 1) 通过特征匹配和渲染对齐获取粗略2D-3D对应关系；2) 引入几何感知的聚类加权机制，基于3D结构一致性分配点级置信度；3) 采用GNC（渐进非凸性）优化原则处理离群值；4) 最后进行LM（Levenberg-Marquardt）细化提高精度。

Result: 在YCB物体和模型集上测试，尽管无需学习特征、训练数据或类别特定先验，GNC-Pose在精度上与基于学习的方法和无需学习的方法都具有竞争力。

Conclusion: GNC-Pose为无需学习的6D姿态估计提供了一个简单、鲁棒且实用的解决方案，通过几何先验和加权策略显著提高了优化稳定性，在严重离群值污染下表现优异。

Abstract: We present GNC-Pose, a fully learning-free monocular 6D object pose estimation pipeline for textured objects that combines rendering-based initialization, geometry-aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D-3D correspondences obtained through feature matching and rendering-based alignment, our method builds upon the Graduated Non-Convexity (GNC) principle and introduces a geometry-aware, cluster-based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC-Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC-Pose achieves competitive accuracy compared with both learning-based and learning-free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.

</details>


### [86] [RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2512.06811)
*Xiang Lin,Weixin Li,Shu Guo,Lihong Wang,Di Huang*

Main category: cs.CV

TL;DR: RMAdapter：一种基于重建的多模态适配器，通过双分支架构平衡任务特定适应与通用知识，在少样本场景下显著提升CLIP等预训练视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型（如CLIP）在少样本微调时面临任务特定适应与泛化能力之间的平衡挑战。现有研究主要关注基于提示的方法，而基于适配器的方法研究不足且性能存在明显差距。

Method: 提出重建式多模态适配器（RMAdapter），采用双分支架构：1）适应分支通过参数高效微调注入任务特定知识；2）重建分支通过将潜在空间特征重建回原始特征空间来保留通用知识。通过局部重建损失计算、共享投影模块和一致性约束来保持轻量化。

Result: 在不依赖数据增强或重复提示设计的情况下，RMAdapter在三个代表性任务（新类别泛化、新目标数据集泛化、领域泛化）上全面超越现有最先进方法。

Conclusion: RMAdapter通过创新的双分支架构有效平衡了任务特定适应与通用知识保留，为预训练视觉语言模型的少样本微调提供了高效且性能优越的解决方案。

Abstract: Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.

</details>


### [87] [Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior](https://arxiv.org/abs/2512.06866)
*Yulin Li,Haokun Gui,Ziyang Fan,Junjie Wang,Bin Kang,Bin Chen,Zhuotao Tian*

Main category: cs.CV

TL;DR: DyToK提出了一种无需训练的token压缩方法，利用VLLM自身的注意力机制动态调整每帧token保留比例，提升长视频处理效率


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在处理长视频时面临二次计算复杂度增长的问题，传统关键帧采样方法存在额外计算开销且二进制帧选择范式不够优化

Method: DyToK通过分析VLLM注意力层自然编码的查询条件关键帧先验，动态调整每帧token保留比例，优先保留语义丰富的帧并抑制冗余信息

Result: DyToK实现了最先进的效率-准确性权衡，与现有压缩方法兼容，在LLaVA-OneVision和Qwen2.5-VL等多个VLLM上实现4.3倍加速推理同时保持准确性

Conclusion: DyToK提供了一种无需训练的动态token压缩范式，有效解决了长视频处理中的计算效率瓶颈问题

Abstract: Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .

</details>


### [88] [MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding](https://arxiv.org/abs/2512.06581)
*Yuhao Su,Anwesa Choudhuri,Zhongpai Gao,Benjamin Planche,Van Nguyen Nguyen,Meng Zheng,Yuhan Shen,Arun Innanje,Terrence Chen,Ehsan Elhamifar,Ziyan Wu*

Main category: cs.CV

TL;DR: 提出了MedVidBench医学视频理解基准和MedGRPO强化学习框架，用于解决医学视频理解中的空间精度、时间推理和临床语义问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在医学视频理解方面存在困难，需要空间精度、时间推理和临床语义能力。现有方法无法有效处理医学视频的多层次任务。

Method: 1) 创建MedVidBench基准：531,850个视频-指令对，来自8个医学来源，涵盖视频、片段和帧级任务，采用专家引导提示和双模型验证的质量保证流程。2) 提出MedGRPO强化学习框架：包含跨数据集奖励归一化（将各数据集的中位性能映射到共同奖励值）和医学LLM法官（通过比较相似性评分评估五个临床维度的字幕质量）。

Result: 在MedVidBench上监督微调Qwen2.5-VL-7B显著优于GPT-4.1和Gemini-2.5-Flash。MedGRPO框架在定位和字幕任务上进一步超越了SFT基线。

Conclusion: 该工作为医学领域的视觉语言模型建立了基础基准和稳健的训练方法，推动了医学视频理解的发展。

Abstract: Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.

</details>


### [89] [JoPano: Unified Panorama Generation via Joint Modeling](https://arxiv.org/abs/2512.06885)
*Wancheng Feng,Chen An,Zhenliang He,Meina Kan,Shiguang Shan,Lukun Wang*

Main category: cs.CV

TL;DR: JoPano：基于DiT的统一全景图生成方法，通过联合面适配器和条件切换机制，在单个模型中同时支持文本到全景图和视图到全景图生成，解决了现有方法视觉质量受限和任务独立建模的问题。


<details>
  <summary>Details</summary>
Motivation: 现有全景图生成方法面临两个主要挑战：1）基于U-Net的架构限制了生成全景图的视觉质量；2）通常将文本到全景图和视图到全景图两个核心任务独立处理，导致建模冗余和效率低下。

Method: 提出基于DiT的联合全景图生成方法：1）使用联合面适配器将预训练DiT的生成能力迁移到全景图领域，基于立方体贴图表示联合建模全景图的不同视图；2）应用泊松融合减少立方体面边界处的接缝不一致；3）引入条件切换机制在单个模型中统一两个生成任务。

Result: 在FID、CLIP-FID、IS和CLIP-Score等指标上达到最先进性能，能够为两个任务生成高质量全景图，并提出了Seam-SSIM和Seam-Sobel指标定量评估接缝一致性。

Conclusion: JoPano通过统一的DiT架构和创新的适配器设计，成功解决了全景图生成中的视觉质量和任务统一问题，为全景图生成提供了高效且高质量的解决方案。

Abstract: Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.

</details>


### [90] [From Remote Sensing to Multiple Time Horizons Forecasts: Transformers Model for CyanoHAB Intensity in Lake Champlain](https://arxiv.org/abs/2512.06598)
*Muhammad Adil,Patrick J. Clemins,Andrew W. Schroth,Panagiotis D. Oikonomou,Donna M. Rizzo,Peter D. F. Isles,Xiaohan Zhang,Kareem I. Hannoun,Scott Turnbull,Noah B. Beckage,Asim Zia,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出结合Transformer和BiLSTM的遥感预测框架，利用卫星数据预测蓝藻水华强度，可提前14天预警，在稀疏数据条件下表现良好。


<details>
  <summary>Details</summary>
Motivation: 蓝藻有害藻华对水生生态系统和公共健康构成严重威胁，特别是在尚普兰湖等地区。遥感技术为解决现场观测稀疏的问题提供了可扩展的监测方案。

Method: 开发基于Transformer和BiLSTM的遥感预测框架，使用蓝藻指数和温度卫星数据。针对数据稀疏问题（蓝藻指数缺失30%，温度数据缺失90%），采用两阶段预处理：像素级前向填充和加权时间插补，然后平滑处理。通过等频分箱和温度统计提取特征。

Result: 模型在不同预测时间窗口表现优异：1天、2天、3天预测的F1分数分别为89.5%、86.4%、85.5%；14天预测的F1分数为78.9%，AUC为82.6%。

Conclusion: 该模型能够从稀疏卫星数据中捕捉复杂的时空动态，为蓝藻水华管理提供可靠的早期预警，证明了遥感预测框架的有效性。

Abstract: Cyanobacterial Harmful Algal Blooms (CyanoHABs) pose significant threats to aquatic ecosystems and public health globally. Lake Champlain is particularly vulnerable to recurring CyanoHAB events, especially in its northern segment: Missisquoi Bay, St. Albans Bay, and Northeast Arm, due to nutrient enrichment and climatic variability. Remote sensing provides a scalable solution for monitoring and forecasting these events, offering continuous coverage where in situ observations are sparse or unavailable. In this study, we present a remote sensing only forecasting framework that combines Transformers and BiLSTM to predict CyanoHAB intensities up to 14 days in advance. The system utilizes Cyanobacterial Index data from the Cyanobacterial Assessment Network and temperature data from Moderate Resolution Imaging Spectroradiometer satellites to capture long range dependencies and sequential dynamics in satellite time series. The dataset is very sparse, missing more than 30% of the Cyanobacterial Index data and 90% of the temperature data. A two stage preprocessing pipeline addressed data gaps by applying forward fill and weighted temporal imputation at the pixel level, followed by smoothing to reduce the discontinuities of CyanoHAB events. The raw dataset is transformed into meaningful features through equal frequency binning for the Cyanobacterial Index values and extracted temperature statistics. Transformer BiLSTM model demonstrates strong forecasting performance across multiple horizons, achieving F1 scores of 89.5%, 86.4%, and 85.5% at one, two, and three-day forecasts, respectively, and maintaining an F1 score of 78.9% with an AUC of 82.6% at the 14-day horizon. These results confirm the model's ability to capture complex spatiotemporal dynamics from sparse satellite data and to provide reliable early warning for CyanoHABs management.

</details>


### [91] [NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification](https://arxiv.org/abs/2512.06921)
*Ziyang Song,Zelin Zang,Xiaofan Ye,Boqiang Xu,Long Bai,Jinlin Wu,Hongliang Ren,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: 首个针对神经外科解剖理解的多模态基准测试NeuroABench，揭示当前MLLMs在解剖结构识别任务上仅达40.87%准确率，显著落后于人类表现。


<details>
  <summary>Details</summary>
Motivation: 现有手术视频理解研究主要关注手术流程和工作流，忽视了临床实践中至关重要的解剖理解能力。外科医生依赖精确的解剖知识来解读、回顾和学习手术视频，因此需要专门的解剖理解评估基准。

Method: 提出NeuroABench基准，包含9小时标注的神经外科视频，涵盖89种不同手术，采用新型多模态标注流程和多轮评审机制。评估68个临床解剖结构的识别能力，并对10多个SOTA MLLMs进行测试。

Result: 最佳MLLM在解剖识别任务中仅达到40.87%准确率。神经外科实习生测试显示：最佳学生准确率56%，最低28%，平均46.5%。MLLM表现与最低分学生相当，但显著低于人类平均水平。

Conclusion: MLLMs在解剖理解方面取得进展但仍存在显著差距。NeuroABench为评估和改进手术视频理解中的解剖认知能力提供了标准化框架，揭示了实现人类水平性能仍需重大突破。

Abstract: Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.

</details>


### [92] [Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics](https://arxiv.org/abs/2512.06612)
*Kazuya Nishimura,Haruka Hirose,Ryoma Bise,Kaito Shiku,Yasuhiro Kojima*

Main category: cs.CV

TL;DR: 提出STRank损失函数，通过相对表达模式而非绝对表达值来减少RNA测序噪声和批次效应的影响


<details>
  <summary>Details</summary>
Motivation: 病理图像估计基因表达可降低RNA测序成本，但现有方法使用逐点损失函数估计绝对表达值，受测序技术复杂性和细胞内在变异性的影响，观测到的基因表达包含随机噪声和批次效应，准确估计绝对表达值具有挑战性

Method: 提出学习相对表达模式而非绝对表达水平的新目标，假设基因的相对表达水平在独立实验中呈现一致模式，即使绝对表达值受批次效应和随机噪声影响。基于此假设，提出STRank损失函数，对噪声和批次效应具有鲁棒性

Result: 在合成数据集和真实数据集上的实验证明了所提方法的有效性

Conclusion: 通过关注相对表达模式而非绝对表达值，STRank损失函数能够更有效地从病理图像中估计基因表达，减少噪声和批次效应的影响

Abstract: Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/naivete5656/STRank.

</details>


### [93] [Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues](https://arxiv.org/abs/2512.07034)
*Tuan-Anh Vu,Hai Nguyen-Truong,Ziqiang Zheng,Binh-Son Hua,Qing Guo,Ivor Tsang,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: TransCues：一种用于透明物体分割的Transformer架构，通过边界特征增强和反射特征增强模块，在多个基准数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 玻璃等透明物体在日常中普遍存在，但由于其透明性和反射特性，现有分割方法难以将其与不透明材料区分开来。人类感知依赖边界和反射物体特征来识别玻璃物体，但现有文献尚未充分捕捉这两种特性。

Method: 提出TransCues框架，采用金字塔式Transformer编码器-解码器架构，包含边界特征增强模块和反射特征增强模块，以相互促进的方式整合这两种强大的视觉线索。

Result: 在两个模块的有效协同下，在多个基准数据集上取得显著性能提升：Trans10K-v2 (+4.2% mIoU)、MSD (+5.6% mIoU)、RGBD-Mirror (+10.1% mIoU)、TROSD (+13.1% mIoU)、Stanford2D3D (+8.3% mIoU)，大幅超越现有最优方法。

Conclusion: 通过边界特征增强和反射特征增强模块的协同作用，TransCues框架能有效处理透明物体分割问题，在多个数据集上展现出卓越性能，验证了该方法对玻璃物体的有效性。

Abstract: Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.

</details>


### [94] [Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach](https://arxiv.org/abs/2512.06613)
*Yueying Ke*

Main category: cs.CV

TL;DR: 提出分层卷积网络用于硅藻分类，通过嵌入分类学层次结构提升准确性和错误定位能力，相比传统平面分类方法在物种分类准确率相当的情况下，显著提高了上层分类级别准确率并减少分类错误距离。


<details>
  <summary>Details</summary>
Motivation: 传统硅藻分类依赖专家，现有深度学习多采用平面分类方法，仅预测单一分类级别。需要探索将分类学层次结构嵌入神经网络架构是否能同时提高准确性和错误定位能力。

Method: 提出分层卷积网络，包含五个级联头部，联合预测纲、目、科、属、种。每个头部接收共享骨干特征和来自更高层级的概率分布，训练和推理时使用二元掩码限制预测到有效后代。

Result: 分层模型在物种级别准确率与平面基准相当（69.4%），但在所有上层分类级别均表现更优。当物种预测失败时，92.5%的错误物种在属级别被正确分类（平面基准为67.2%），平均分类距离减少38.2%。

Conclusion: 分层约束掩码自上而下限制预测空间，细粒度级别的梯度自下而上通过共享骨干传播，双向机制提高了分类准确性和鲁棒性，为多级分类学分类提供了更稳健、可解释且生物学对齐的预测方法。

Abstract: Accurate taxonomic identification of diatoms is essential for aquatic ecosystem monitoring, yet conventional methods depend heavily on expert taxonomists. Recent deep learning approaches improve automation, but most treat diatom recognition as flat classification predicting only one taxonomic rank. We investigate whether embedding taxonomic hierarchy into neural network architectures can improve both accuracy and error locality.
  We introduce a hierarchical convolutional network with five cascaded heads that jointly predict class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Using a filtered dataset of 1,456 diatom images covering 82 species, we compare hierarchical and flat models under identical settings.
  The hierarchical model matches flat baselines at species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, errors remain taxonomically local: 92.5 % of misclassified species are correctly predicted at genus level, versus 67.2% for flat baselines. The hierarchical model reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).
  Progressive training reveals bidirectional mechanisms: hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through the shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification.

</details>


### [95] [DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation](https://arxiv.org/abs/2512.07051)
*Adnan Munir,Shujaat Khan*

Main category: cs.CV

TL;DR: DAUNet是一个轻量级UNet变体，结合可变形卷积和参数自由注意力机制，在医疗图像分割任务中实现更好的空间适应性和上下文感知特征融合，同时保持模型效率。


<details>
  <summary>Details</summary>
Motivation: 医疗图像分割在自动化诊断和治疗规划系统中至关重要。现有方法在处理几何变化、上下文缺失和低对比度区域时存在局限性，同时需要保持模型轻量级以适应实时和资源受限的临床环境。

Method: 提出DAUNet，集成Deformable V2 Convolutions和Parameter-Free Attention (SimAM)。瓶颈部分使用动态可变形核处理几何变化，解码器和跳跃连接路径使用SimAM注意力模块进行显著性感知细化，不增加模型复杂度。

Result: 在两个挑战性数据集（FH-PS-AoP超声图像和FUMPE CT肺栓塞检测）上，DAUNet在Dice分数、HD95和ASD指标上优于最先进模型，同时保持优越的参数效率。消融研究验证了可变形卷积和SimAM注意力的各自贡献。

Conclusion: DAUNet对缺失上下文和低对比度区域的鲁棒性，使其适合部署在实时和资源受限的临床环境中，为医疗图像分割提供了高效且有效的解决方案。

Abstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.

</details>


### [96] [$\mathrm{D}^{\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction](https://arxiv.org/abs/2512.07062)
*Changliang Xia,Chengyou Jia,Minnan Luo,Zhuohang Dang,Xin Shen,Bowen Ping*

Main category: cs.CV

TL;DR: 提出D³-Predictor，一种无噪声确定性框架，通过重新表述预训练扩散模型消除随机性，将扩散模型转化为时间步依赖的视觉专家集成，用于密集预测任务。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的密集预测方法存在核心限制：扩散采样的随机噪声与密集预测所需的确定性映射不匹配，这种随机噪声会破坏细粒度空间线索并导向时间步特定的噪声目标，从而破坏有意义的几何结构映射。

Method: 提出D³-Predictor框架，将预训练扩散模型重新表述为无随机噪声的确定性模型。该方法将扩散网络视为时间步依赖的视觉专家集成，通过自监督方式将异构先验聚合为单一、干净、完整的几何先验，并利用任务特定监督无缝适应密集预测任务。

Result: 在多种密集预测任务上的广泛实验表明，D³-Predictor在多样化场景中达到竞争性或最先进的性能，同时所需训练数据不到先前方法的一半，并能以单步推理高效执行。

Conclusion: 通过消除扩散模型中的随机噪声，D³-Predictor成功将扩散先验转化为确定性密集预测框架，在保持高性能的同时显著提高了数据效率和推理速度。

Abstract: Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^{\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^{\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^{\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.

</details>


### [97] [TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning](https://arxiv.org/abs/2512.07135)
*Zebin Xing,Pengxuan Yang,Linbo Wang,Yichen Zhang,Yiming Hu,Yupeng Zheng,Junli Wang,Yinfeng Gao,Guang Li,Kun Ma,Long Chen,Zhongpu Xia,Qichao Zhang,Hangjun Ye,Dongbin Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一种改进的自动驾驶规划方法，通过MoE为不同场景提供不同的轨迹先验，并使用强化学习微调轨迹评分机制，在navsim ICCV基准测试中获得第三名。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶系统虽然使用轨迹先验分布来提升规划性能，但存在两个关键问题：1) 不同驾驶场景需要不同的轨迹先验；2) 轨迹评估机制缺乏策略驱动的细化，受限于单阶段监督训练的局限性。

Method: 采用混合专家(MoE)方法为不同场景提供定制的轨迹先验；使用强化学习(RL)微调轨迹评分机制；集成不同感知骨干网络以增强感知特征。

Result: 集成模型在navsim ICCV基准测试中获得51.08分，排名第三。

Conclusion: 通过场景特定的轨迹先验和策略驱动的轨迹评分细化，可以显著提升自动驾驶规划性能，证明了多阶段训练和场景自适应方法的价值。

Abstract: Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.

</details>


### [98] [Personalized Image Descriptions from Attention Sequences](https://arxiv.org/abs/2512.06662)
*Ruoyu Xue,Hieu Le,Jingyi Xu,Sounak Mondal,Abe Leite,Gregory Zelinsky,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: DEPER模型通过学习用户的视觉关注模式和语言风格，结合辅助注意力预测任务，实现个性化图像描述生成，比现有方法平均提升24%


<details>
  <summary>Details</summary>
Motivation: 现有个性化图像描述模型只关注语言风格，忽略了个人观看模式（关注区域、对象、细节的顺序）的差异。不同人对同一图像有不同的观看方式和描述风格，导致描述存在显著差异

Method: 提出DEPER方法，学习同时捕捉语言风格和观看行为的主题嵌入，通过辅助注意力预测任务指导学习。使用轻量级适配器将这些嵌入与冻结的视觉语言模型对齐，实现少样本个性化而无需重新训练

Result: 在四个数据集上（涵盖不同观看任务和简短/详细描述），DEPER平均提升24%，表明建模个性化注意力能产生更符合人类认知和更高质量的描述

Conclusion: 理解人们如何观看有助于预测他们会说什么；建模人类感知多样性可以提升多模态系统的性能和人类对齐性

Abstract: People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.

</details>


### [99] [A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning](https://arxiv.org/abs/2512.07136)
*Siyang Jiang,Mu Yuan,Xiang Ji,Bufang Yang,Zeyu Liu,Lilin Xu,Yang Li,Yuting He,Liran Dong,Wenrui Lu,Zhenyu Yan,Xiaofan Jiang,Wei Gao,Hongkai Chen,Guoliang Xing*

Main category: cs.CV

TL;DR: CUHK-X是一个大规模多模态数据集，用于人类动作识别、理解和推理，包含58,445个样本、40种动作，提供数据-标签和数据-描述两种标注，旨在解决现有模型对非RGB模态处理能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（尤其是视觉语言模型）在处理深度、IMU、毫米波等非RGB模态时表现不佳，缺乏大规模数据-描述资源。现有的人类动作识别数据集主要提供粗糙的数据-标签标注，无法捕捉细粒度动作动态，不足以支持人类动作理解和推理任务。

Method: 提出CUHK-X数据集，包含58,445个样本，覆盖30名参与者在两个室内环境执行的40种动作。采用基于提示的场景创建方法，利用LLM生成逻辑连贯的活动序列，然后进行人工验证，以提高描述的一致性。

Result: 实验结果显示，在CUHK-X的三个基准测试中，人类动作识别平均准确率为76.52%，人类动作理解平均准确率为40.76%，人类动作推理平均准确率为70.25%。

Conclusion: CUHK-X数据集和基准套件旨在推动社区应用和发展数据密集型学习方法，实现鲁棒的多模态人类活动分析，填补了非RGB模态数据-描述资源的空白。

Abstract: Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.

</details>


### [100] [CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks](https://arxiv.org/abs/2512.06663)
*Yu Qi,Yumeng Zhang,Chenting Gong,Xiao Tan,Weiming Zhang,Wei Zhang,Jingdong Wang*

Main category: cs.CV

TL;DR: CoT4Det通过将感知任务重新表述为分类、计数和定位三个可解释步骤，显著提升大视觉语言模型在物体检测等感知任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在通用视觉问答任务上表现出色，但在感知任务（如物体检测、语义分割）上表现远不如任务专用模型。例如Qwen2.5-VL-7B-Instruct在COCO2017上只有19% mAP，特别是在密集场景和小物体检测上表现不佳。

Method: 提出Chain-of-Thought for Detection (CoT4Det)策略，将感知任务重新表述为三个可解释步骤：1) 分类：识别图像中的物体类别；2) 计数：统计每个类别的物体数量；3) 定位：确定每个物体的具体位置。这种方法更符合大视觉语言模型的推理能力。

Result: CoT4Det显著提升了感知性能：在标准Qwen2.5-VL-7B-Instruct上，将COCO2017 val的mAP从19.0%提升到33.0%。在多个感知基准测试中取得竞争性结果：RefCOCO系列提升+2%，Flickr30k entities提升19%。

Conclusion: CoT4Det是一种简单而有效的策略，通过将感知任务重新表述为更符合大视觉语言模型推理能力的步骤，显著提升了感知性能，同时不损害通用视觉语言能力。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR). However, their performance on perception-centric tasks -- such as object detection, semantic segmentation, and depth estimation -- remains significantly inferior to that of task-specific expert models. For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall. In this work, we introduce Chain-of-Thought for Detection (CoT4Det), a simple but efficient strategy that reformulates perception tasks into three interpretable steps: classification, counting, and grounding -- each more naturally aligned with the reasoning capabilities of LVLMs. Extensive experiments demonstrate that our method significantly improves perception performance without compromising general vision language capabilities. With a standard Qwen2.5-VL-7B-Instruct, CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val and achieves competitive results across a variety of perception benchmarks, outperforming baselines by +2% on RefCOCO series and 19% on Flickr30k entities.

</details>


### [101] [Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach](https://arxiv.org/abs/2512.07170)
*Jiayang Li,Chengjie Jiang,Junjun Jiang,Pengwei Liang,Jiayi Ma,Liqiang Nie*

Main category: cs.CV

TL;DR: DiTFuse：基于扩散Transformer的指令驱动图像融合框架，通过自然语言指令实现端到端、语义感知的多模态图像融合，支持多种融合任务和用户控制。


<details>
  <summary>Details</summary>
Motivation: 现有图像融合方法在鲁棒性、适应性和可控性方面存在局限：1）多数方法针对特定任务，缺乏灵活性；2）难以融入用户意图；3）缺乏真实融合图像作为ground truth；4）数据集规模小，难以训练端到端模型同时理解高层语义和细粒度多模态对齐。

Method: 提出DiTFuse框架：1）基于扩散Transformer架构，在共享潜在空间中联合编码两幅图像和自然语言指令；2）采用多退化掩码图像建模训练策略，无需ground truth图像，同时学习跨模态对齐、模态不变恢复和任务感知特征选择；3）构建多粒度指令数据集，赋予模型交互式融合能力。

Result: 在红外-可见光、多焦点、多曝光融合基准测试中，DiTFuse在定量和定性评估上均表现优异，生成纹理更清晰、语义保留更好的融合结果。模型支持多级用户控制，并能零样本泛化到其他多图像融合场景（包括指令条件分割）。

Conclusion: DiTFuse统一了多种图像融合任务和文本控制细化功能，通过指令驱动框架实现了端到端、语义感知的图像融合，克服了传统方法的局限性，为多模态图像融合提供了更灵活、可控的解决方案。

Abstract: Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.

</details>


### [102] [1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning](https://arxiv.org/abs/2512.06673)
*Shida Gao,Feng Xue,Xiangfeng Wang,Anlong Ming,Teng Long,Yihua Shao,Haozhe Wang,Zhaowen Lin,Wei Wang,Nicu Sebe*

Main category: cs.CV

TL;DR: DEViL是一个检测器赋能的视频大语言模型，通过参考语义令牌连接视频LLM和开放词汇检测器，解决了现有方法中自回归空间解码导致的误差累积和定位漂移问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在时空定位和推理任务中，将边界框作为文本令牌进行自回归生成，导致输出序列过长、空间误差随时间累积以及定位结果在视频中逐渐漂移的问题。

Method: 1. 提出DEViL模型，将视频LLM与开放词汇检测器通过参考语义令牌连接；2. RST令牌将用户查询提炼为丰富的语义表示，既作为控制信号又替代OVD的文本嵌入；3. 在OVD中引入管状挖掘的时间正则化，确保目标对象查询的时间一致性。

Result: 实验表明DEViL在各种细粒度视频理解任务中表现优异，特别是在STVG和GroundedVQA任务上取得了强劲性能。

Conclusion: DEViL通过结合视频LLM和开放词汇检测器，并引入参考语义令牌和时间正则化，有效解决了时空定位中的误差累积问题，实现了更准确和一致的时空定位与推理。

Abstract: Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.

</details>


### [103] [START: Spatial and Textual Learning for Chart Understanding](https://arxiv.org/abs/2512.07186)
*Zhuoming Liu,Xiaofeng Gao,Feiyang Niu,Qiaozi Gao,Liu Liu,Robinson Piramuthu*

Main category: cs.CV

TL;DR: START提出了一种结合空间和文本学习的图表理解方法，通过图表元素定位和图表到代码生成来增强MLLM对图表视觉布局和数据细节的理解能力。


<details>
  <summary>Details</summary>
Motivation: 图表理解对于MLLM在现实场景中的应用至关重要。与自然图像不同，图表结合了结构化视觉布局（空间属性）和底层数据表示（文本属性），需要同时理解两者才能进行精确的细粒度图表推理。

Method: 1. 提出图表元素定位和图表到代码生成两种方法，增强MLLM对图表视觉布局和数据细节的理解；2. 开发START数据集生成流水线，利用MLLM将真实图表图像转换为可执行图表代码，恢复底层数据表示并保持真实图表视觉分布；3. 使用LLM演化代码以确定图表元素位置，捕捉图表视觉结构；4. 提出图表空间理解基准CS-Bench，填补图表理解评估空白。

Result: START在不同模型规模和基准测试中都取得了相对于基础模型的持续提升，并且明显超越了先前的最先进方法。

Conclusion: 通过结合空间和文本学习，START在图表理解任务上取得了显著进展，为MLLM在图表分析应用中的部署提供了有效解决方案。

Abstract: Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.

</details>


### [104] [RunawayEvil: Jailbreaking the Image-to-Video Generative Models](https://arxiv.org/abs/2512.06674)
*Songping Wang,Rufan Qian,Yueming Lyu,Qinglong Liu,Linzhuang Zou,Jie Qin,Songhua Liu,Caifeng Shan*

Main category: cs.CV

TL;DR: RunawayEvil：首个针对图像转视频模型的多模态越狱框架，采用"策略-战术-行动"范式，具备动态进化能力，在Open-Sora 2.0和CogVideoX等商业模型上实现了最先进的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前图像转视频生成系统虽然提供了显著的创意控制能力，但其安全性特别是对越狱攻击的脆弱性尚未得到充分研究。多模态系统的安全漏洞是一个关键但被忽视的问题。

Method: 基于"策略-战术-行动"范式构建的三层框架：1) 策略感知命令单元：通过强化学习驱动的策略定制和基于LLM的策略探索实现自我进化；2) 多模态战术规划单元：根据选定策略生成协调的文本越狱指令和图像篡改指南；3) 战术行动单元：执行和评估多模态协调攻击。

Result: 在商业I2V模型（如Open-Sora 2.0和CogVideoX）上实现了最先进的攻击成功率，在COCO2017数据集上比现有方法高出58.5%到79%。

Conclusion: 该工作为I2V模型的漏洞分析提供了关键工具，为构建更鲁棒的视频生成系统奠定了基础，揭示了多模态生成系统面临的新型安全威胁。

Abstract: Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a "Strategy-Tactic-Action" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.

</details>


### [105] [VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation](https://arxiv.org/abs/2512.07215)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: 比较CLIP和DINOv2在抓取场景6D位姿估计中的表现：CLIP语义理解强，DINOv2几何特征优


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型(VFMs)和视觉语言模型(VLMs)为计算机视觉提供了丰富的语义和几何表示，但它们在具体任务如手-物体抓取的6D位姿估计中的表现差异尚未系统比较

Method: 在基准数据集上进行全面的视觉比较，评估基于CLIP和DINOv2的方法在6D物体位姿估计任务中的表现

Result: CLIP基于语言接地在语义理解方面表现优异，DINOv2提供更优的密集几何特征；CLIP方法实现更好的语义一致性，DINOv2方法在几何精度方面具有竞争力

Conclusion: 两种模型具有互补优势，为机器人操作和抓取应用中选择合适的视觉模型提供了指导

Abstract: Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.

</details>


### [106] [EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy](https://arxiv.org/abs/2512.06684)
*Yumeng He,Zanwei Zhou,Yekun Zheng,Chen Liang,Yunbo Wang,Xiaokang Yang*

Main category: cs.CV

TL;DR: EMGauss：基于高斯溅射的3D重建框架，将2D切片序列重构为3D体积，避免各向同性假设限制，适用于体积电子显微镜等成像领域


<details>
  <summary>Details</summary>
Motivation: 体积电子显微镜(vEM)存在各向异性分辨率限制，现有深度学习方法基于各向同性假设，但在形态各向异性结构上失效，需要更通用的切片到3D重建方法

Method: 将切片到3D重建重构为基于高斯溅射的3D动态场景渲染问题，将轴向切片进展建模为2D高斯点云的时间演化，并引入师生引导机制增强数据稀疏区域保真度

Result: 相比扩散和GAN方法，EMGauss显著提升插值质量，支持连续切片合成，无需大规模预训练，在vEM中表现优异

Conclusion: EMGauss提供了一种通用的切片到3D重建框架，突破各向同性假设限制，适用于vEM及其他成像领域，具有广泛适用性

Abstract: Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.

</details>


### [107] [Towards Robust Protective Perturbation against DeepFake Face Swapping](https://arxiv.org/abs/2512.07228)
*Hengyang Yao,Lin Li,Ke Sun,Jianing Qiu,Huiping Chen*

Main category: cs.CV

TL;DR: 论文提出EOLT框架，通过学习变换分布而非固定采样，显著提升DeepFake人脸交换防御的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有DeepFake防御方法通过在图像中嵌入不可见扰动来保护隐私安全，但这些扰动对压缩、调整大小等基本变换非常脆弱。传统期望变换(EOT)采用均匀采样，但研究发现防御鲁棒性对训练变换选择高度敏感，导致标准方法效果不佳

Method: 提出期望学习变换分布(EOLT)框架，将变换分布作为可学习组件而非固定设计。使用策略网络通过强化学习自动优先处理关键变换，自适应生成实例特定扰动，明确建模防御瓶颈同时保持广泛可迁移性

Result: 实验表明该方法相比最先进方法有显著改进：平均鲁棒性提高26%，在具有挑战性的变换类别上提升高达30%

Conclusion: EOLT框架通过将变换分布作为可学习组件，能够有效提升DeepFake防御的鲁棒性，为对抗性防御提供了更智能的变换采样策略

Abstract: DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.

</details>


### [108] [Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation](https://arxiv.org/abs/2512.06689)
*Jisoo Park,Seonghak Lee,Guisik Kim,Taewoo Kim,Junseok Kwon*

Main category: cs.CV

TL;DR: UniVoiceLite：轻量级无监督视听框架，统一语音增强和语音分离任务


<details>
  <summary>Details</summary>
Motivation: 现实世界音频通常同时包含背景噪声和重叠说话人，需要统一解决方案。现有方法多为多阶段架构，模型复杂、参数多且依赖监督训练，限制了可扩展性和泛化能力。

Method: 利用唇部运动和面部身份线索引导语音提取，采用Wasserstein距离正则化稳定潜在空间，无需配对噪声-干净数据。

Result: UniVoiceLite在噪声和多说话人场景下均表现优异，兼具高效性和鲁棒泛化能力。

Conclusion: 提出轻量级无监督视听框架，成功统一语音增强和语音分离任务，在效率和泛化方面具有优势。

Abstract: Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.

</details>


### [109] [Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models](https://arxiv.org/abs/2512.07234)
*Biao Chen,Lin Zuo,Mengmeng Jing,Kunbin He,Yuchen Wang*

Main category: cs.CV

TL;DR: 提出Dropout Prompt Learning方法，通过评估文本和视觉分支中token的重要性，为每个token设置灵活的dropout概率，并结合残差熵正则化来提升视觉语言模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Dropout是一种广泛使用的正则化技术，通过随机丢弃神经元来提高模型的泛化能力。本文旨在将dropout应用于视觉语言模型，提升其在挑战性场景下的鲁棒性。

Method: 1) 在文本和视觉分支的token上应用dropout，考虑模态内上下文和模态间对齐来评估token重要性，为每个token设置灵活的dropout概率；2) 提出残差熵正则化，在保持语义对齐的同时鼓励dropout引入的多样化表示。

Result: 在15个基准测试上验证了方法的有效性，特别是在低样本学习、长尾分类和分布外泛化等挑战性场景中表现优异。在base-to-novel泛化任务上，性能超过KgCoOp 5.10%，超过PromptSRC 2.13%。

Conclusion: Dropout Prompt Learning通过灵活的token级dropout和残差熵正则化，显著提升了视觉语言模型在多种挑战性场景下的鲁棒性和泛化能力。

Abstract: Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.

</details>


### [110] [DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement](https://arxiv.org/abs/2512.07253)
*Handing Xu,Zhenguo Nie,Tairan Peng,Huimin Pan,Xin-Jun Liu*

Main category: cs.CV

TL;DR: 提出一种基于退化感知的实时内窥镜视频增强框架，通过跨帧传播退化表示实现高质量增强


<details>
  <summary>Details</summary>
Motivation: 内窥镜手术依赖术中视频，但视频常因光照不均、组织散射、遮挡和运动模糊而质量下降，影响手术安全。现有深度学习方法计算量大，难以实时应用。

Method: 1) 使用对比学习从图像中提取退化表示；2) 引入融合机制，用退化表示调制图像特征；3) 单帧增强模型通过循环一致性约束训练，提高鲁棒性和泛化能力。

Result: 实验表明该框架在性能与效率之间取得优越平衡，优于多种先进方法，验证了退化感知建模对实时内窥镜视频增强的有效性。

Conclusion: 隐式学习和传播退化表示为临床应用提供了实用途径，实现了实时高质量的内窥镜视频增强。

Abstract: Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.

</details>


### [111] [Graph Convolutional Long Short-Term Memory Attention Network for Post-Stroke Compensatory Movement Detection Based on Skeleton Data](https://arxiv.org/abs/2512.06736)
*Jiaxing Fan,Jiaojiao Liu,Wenkong Wang,Yang Zhang,Xin Ma,Jichen Zhang*

Main category: cs.CV

TL;DR: 提出基于骨架数据的图卷积长短期记忆注意力网络(GCN-LSTM-ATT)用于检测中风后补偿性运动，相比传统机器学习方法显著提高检测准确率


<details>
  <summary>Details</summary>
Motivation: 大多数中风患者存在上肢运动功能障碍，康复训练中普遍存在补偿性运动，这对患者长期恢复不利，因此检测补偿性运动具有重要意义

Method: 使用Kinect深度相机采集16名中风患者执行特定康复动作的骨架数据，构建GCN-LSTM-ATT模型，并与SVM、KNN、RF等传统机器学习算法对比

Result: GCN-LSTM-ATT模型的检测准确率达到0.8580，显著高于传统机器学习算法；消融实验表明模型的每个组件都对性能提升有显著贡献

Conclusion: 该研究为中风后补偿性运动检测提供了更精确有效的工具，有望促进中风患者康复训练策略的优化

Abstract: Most stroke patients experience upper limb motor dysfunction. Compensatory movements are prevalent during rehabilitation training, which is detrimental to patients' long-term recovery. Therefore, detecting compensatory movements is of great significance. In this study, a Graph Convolutional Long Short-Term Memory Attention Network (GCN-LSTM-ATT) based on skeleton data is proposed for the detection of compensatory movements after stroke. Sixteen stroke patients were selected in the research. The skeleton data of the patients performing specific rehabilitation movements were collected using the Kinect depth camera. After data processing, detection models were constructed respectively using the GCN-LSTM-ATT model, the Support Vector Machine(SVM), the K-Nearest Neighbor algorithm(KNN), and the Random Forest(RF). The results show that the detection accuracy of the GCN-LSTM-ATT model reaches 0.8580, which is significantly higher than that of traditional machine learning algorithms. Ablation experiments indicate that each component of the model contributes significantly to the performance improvement. These findings provide a more precise and powerful tool for the detection of compensatory movements after stroke, and are expected to facilitate the optimization of rehabilitation training strategies for stroke patients.

</details>


### [112] [Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation](https://arxiv.org/abs/2512.07275)
*Siyu Wang,Hua Wang,Huiyu Li,Fan Zhang*

Main category: cs.CV

TL;DR: 提出一种基于多尺度残差结构的编码器-解码器网络，通过MRCF模块、CMAM模块和EAB桥接机制，显著提升了皮肤病变分割的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以有效处理皮肤病变的不规则形状和低对比度问题，传统U-Net的跳跃连接存在信息损失问题，需要更有效的特征提取和融合机制。

Method: 1. 基于多尺度残差结构的编码器-解码器网络架构；2. 多分辨率多通道融合(MRCF)模块捕获跨尺度特征；3. 交叉混合注意力模块(CMAM)重新定义注意力范围并动态计算权重；4. 外部注意力桥(EAB)补偿上采样过程中的信息损失。

Result: 在多个皮肤病变分割数据集上的实验表明，该模型显著优于现有的基于Transformer和卷积神经网络的方法，展现出卓越的分割准确性和鲁棒性。

Conclusion: 提出的创新网络架构通过多尺度特征提取、跨尺度融合、动态注意力机制和有效的桥接设计，成功解决了皮肤病变分割中的形状不规则和对比度低等挑战，为医疗图像分析提供了有效的解决方案。

Abstract: In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.

</details>


### [113] [FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation](https://arxiv.org/abs/2512.06738)
*M Yashwanth,Sampath Koti,Arunabh Singh,Shyam Marjit,Anirban Chakraborty*

Main category: cs.CV

TL;DR: FedSCAl是一个联邦学习框架，用于解决联邦无源域自适应问题，通过服务器-客户端对齐机制减少客户端漂移，提高伪标签准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦无源域自适应问题中，客户端持有未标记数据且存在显著的域间差异，传统方法在联邦学习中面临客户端漂移问题，导致伪标签不可靠。

Method: 提出FedSCAl框架，采用服务器-客户端对齐机制，通过对齐客户端和服务器模型的预测来正则化客户端更新，缓解客户端漂移。

Result: 在基准视觉数据集上的实验表明，FedSCAl在联邦无源域自适应设置中，分类任务上持续优于最先进的联邦学习方法。

Conclusion: FedSCAl通过服务器-客户端对齐机制有效解决了联邦无源域自适应中的客户端漂移问题，提高了伪标签准确性，在异构数据场景下表现优异。

Abstract: We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients' domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients' and server model's predictions. We observe an improvement in the clients' pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.

</details>


### [114] [Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts](https://arxiv.org/abs/2512.07302)
*Mingning Guo,Mengwei Wu,Shaoxian Li,Haifeng Li,Chao Tao*

Main category: cs.CV

TL;DR: AerialVP：首个用于无人机图像感知的任务提示增强代理框架，通过提取多维辅助信息增强任务提示，解决传统VLM方法在复杂无人机图像中的局限性


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的图像感知方法在处理无人机图像时面临目标混淆、尺度变化和复杂背景等挑战，因为VLM对图像内容的理解依赖于视觉和文本token的语义对齐。当任务提示简单而图像内容复杂时，难以实现有效对齐，限制了模型关注任务相关信息的能力。

Method: 提出AerialVP框架，主动从无人机图像中提取多维辅助信息来增强任务提示。增强过程包括三个阶段：(1)分析任务提示以识别任务类型和增强需求；(2)从工具库中选择合适工具；(3)基于分析和选定工具生成增强的任务提示。

Result: 实验结果表明，AerialVP显著增强了任务提示的指导性，在开源和专有VLM中都带来了稳定且显著的性能提升。同时引入了AerialSense基准测试，用于评估无人机图像感知任务。

Conclusion: AerialVP通过任务提示增强有效解决了传统VLM方法在无人机图像感知中的局限性，为复杂视觉场景下的任务理解提供了新框架，显著提升了模型性能。

Abstract: Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.

</details>


### [115] [ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation](https://arxiv.org/abs/2512.07328)
*Ziyang Mai,Yu-Wing Tai*

Main category: cs.CV

TL;DR: ContextAnyone是一个上下文感知的扩散框架，能够从文本和单张参考图像生成角色一致的视频，解决了现有方法在保持发型、服装、体型等上下文一致性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成方法在保持角色身份一致性方面存在局限，特别是无法有效保留发型、服装、体型等关键上下文线索，这些对于视觉连贯性至关重要。

Method: 提出上下文感知扩散框架，联合重建参考图像和生成新视频帧；通过Emphasize-Attention模块选择性增强参考感知特征；采用双引导损失结合扩散和参考重建目标；提出Gap-RoPE位置嵌入分离参考和视频token以稳定时序建模。

Result: 实验表明ContextAnyone在身份一致性和视觉质量方面优于现有参考到视频方法，能够生成跨多样动作和场景的一致且保留上下文的角色视频。

Conclusion: ContextAnyone通过上下文感知的扩散框架有效解决了角色一致性视频生成的挑战，在保持角色身份完整性和视觉质量方面取得了显著进展。

Abstract: Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.

</details>


### [116] [UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement](https://arxiv.org/abs/2512.06750)
*Weiqi Li,Xuanyu Zhang,Bin Chen,Jingfen Xie,Yan Wang,Kexin Zhang,Junlin Li,Li Zhang,Jian Zhang,Shijie Zhao*

Main category: cs.CV

TL;DR: UARE是首个统一图像质量评估、修复和增强的视觉语言模型，通过多任务协同训练，利用IQA指导提升图像修复性能。


<details>
  <summary>Details</summary>
Motivation: 虽然图像质量评估(IQA)和图像修复在概念上紧密相关，但现有工作大多将它们分开处理。统一的多模态理解-生成模型的最新进展表明，更强的理解能力可以提升生成性能，这促使研究者探索一个统一IQA和修复的模型，研究IQA如何指导修复。

Method: 基于预训练的统一理解和生成模型，提出两阶段训练框架：1）渐进式从单一类型失真扩展到高阶混合退化，使模型能处理多种退化；2）使用交错文本-图像数据进行统一微调，将IQA信号与修复目标对齐，通过多任务协同训练让IQA提升修复性能。

Result: 在IQA、修复和增强任务上的大量实验证明了UARE的有效性。

Conclusion: UARE是首个统一图像质量评估、修复和增强的视觉语言模型，通过多任务协同训练成功利用IQA指导提升图像修复性能，代码和模型将开源。

Abstract: Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at https://github.com/lwq20020127/UARE.

</details>


### [117] [DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection](https://arxiv.org/abs/2512.07351)
*Sayeem Been Zaman,Wasimul Karim,Arefin Ittesafun Abian,Reem E. Mohamed,Md Rafiqul Islam,Asif Karim,Sami Azam*

Main category: cs.CV

TL;DR: DeepAgent：基于多智能体协作的深度伪造检测框架，融合视觉和音频模态，通过随机森林元分类器提升检测性能


<details>
  <summary>Details</summary>
Motivation: 合成媒体特别是深度伪造的日益普及给数字内容验证带来挑战。现有方法通常将音频和视觉信息集成在单一模型中，容易受到模态不匹配、噪声和操纵的影响。需要更鲁棒的检测框架来应对这些挑战。

Method: 提出DeepAgent多智能体协作框架：Agent-1使用简化的AlexNet CNN检测深度伪造操作痕迹；Agent-2通过声学特征、Whisper音频转录和EasyOCR帧读取序列检测视听不一致性。两个智能体的决策通过随机森林元分类器融合，利用不同决策边界提升性能。

Result: 在三个基准数据集上评估：Agent-1在Celeb-DF和FakeAVCeleb组合数据集上达到94.35%测试准确率；Agent-2在FakeAVCeleb上达到93.69%；元分类器在FakeAVCeleb上达到81.56%。跨数据集验证中，在DeepFakeTIMIT上元分类器达到97.49%准确率，显示强泛化能力。

Conclusion: 基于层次融合的多智能体方法通过缓解单个模态的弱点增强了鲁棒性，证明该方法能有效应对深度伪造中的多样化操纵类型。多智能体协作框架在深度伪造检测中具有显著优势。

Abstract: The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.

</details>


### [118] [Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2512.07360)
*Qiming Huang,Hao Ai,Jianbo Jiao*

Main category: cs.CV

TL;DR: 提出结构感知特征校正方法，通过区域邻接图利用图像的低级特征来增强CLIP特征的局部判别能力，改善开放词汇语义分割中的噪声和不一致问题。


<details>
  <summary>Details</summary>
Motivation: CLIP在图像-文本对预训练中主要关注全局语义对齐，导致在细粒度视觉区域与文本关联时表现不佳，产生噪声和不一致的预测，特别是在局部区域。这源于对比训练范式带来的分散偏差，仅使用CLIP特征难以缓解。

Method: 提出结构感知特征校正方法：1）基于低级特征（如颜色和纹理）构建区域邻接图（RAG）来捕捉局部结构关系；2）利用该图来精炼CLIP特征，增强局部判别能力。

Result: 在多个开放词汇分割基准测试中，该方法有效抑制了分割噪声，提高了区域级一致性，并取得了强劲的性能表现。

Conclusion: 通过结合从图像直接提取的实例特定先验（区域邻接图），能够有效校正CLIP特征的局部判别不足，显著提升开放词汇语义分割的质量和一致性。

Abstract: Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.

</details>


### [119] [JOCA: Task-Driven Joint Optimisation of Camera Hardware and Adaptive Camera Control Algorithms](https://arxiv.org/abs/2512.06763)
*Chengyang Yan,Mitch Bryson,Donald G. Dansereau*

Main category: cs.CV

TL;DR: 提出联合优化相机硬件参数和自适应控制算法的方法，通过混合优化框架提升下游视觉任务性能


<details>
  <summary>Details</summary>
Motivation: 现有相机系统设计大多只优化固定的制造参数，而许多参数（如曝光设置）需要在运行时自适应控制。需要一种能联合优化硬件参数和自适应控制算法的方法来提升视觉任务性能。

Method: 提出统一优化框架，结合梯度优化和无导数优化方法，支持连续和离散参数、不可微图像形成过程以及基于神经网络的自适应控制算法。针对运动模糊等不可微效应，提出DF-Grad混合优化策略，使用无导数优化器的信号训练自适应控制网络。

Result: 实验表明，该方法在低光照和快速运动等挑战性条件下，优于分别优化静态和动态参数的基线方法，显著提升了感知性能。

Conclusion: 联合优化硬件参数和自适应控制算法能有效提升感知性能，为任务驱动的相机系统设计提供了统一方法。

Abstract: The quality of captured images strongly influences the performance of downstream perception tasks. Recent works on co-designing camera systems with perception tasks have shown improved task performance. However, most prior approaches focus on optimising fixed camera parameters set at manufacturing, while many parameters, such as exposure settings, require adaptive control at runtime. This paper introduces a method that jointly optimises camera hardware and adaptive camera control algorithms with downstream vision tasks. We present a unified optimisation framework that integrates gradient-based and derivative-free methods, enabling support for both continuous and discrete parameters, non-differentiable image formation processes, and neural network-based adaptive control algorithms. To address non-differentiable effects such as motion blur, we propose DF-Grad, a hybrid optimisation strategy that trains adaptive control networks using signals from a derivative-free optimiser alongside unsupervised task-driven learning. Experiments show that our method outperforms baselines that optimise static and dynamic parameters separately, particularly under challenging conditions such as low light and fast motion. These results demonstrate that jointly optimising hardware parameters and adaptive control algorithms improves perception performance and provides a unified approach to task-driven camera system design.

</details>


### [120] [Data-driven Exploration of Mobility Interaction Patterns](https://arxiv.org/abs/2512.07415)
*Gabriele Galatolo,Mirco Nanni*

Main category: cs.CV

TL;DR: 提出基于数据挖掘的方法，从移动数据中发现个体间相互作用的证据和复杂模式，以改进人群模拟模型


<details>
  <summary>Details</summary>
Motivation: 现有解决方案通常基于预设的行为模型，但需要直接从数据中理解个体间的相互影响，特别是在人群模拟和应急管理等应用中

Method: 采用数据挖掘视角，从移动数据中搜索可能反映个体间相互作用的移动事件，并在此基础上寻找复杂、持久的模式和随时间演变的事件配置

Result: 在两个真实案例研究（汽车和行人）上实例化该方法，进行了全面的实验评估，包括性能、参数敏感性和样本结果解释

Conclusion: 通过研究这些模式可以为个体间移动相互作用的机制提供新见解，有助于改进现有的模拟模型

Abstract: Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.

</details>


### [121] [When normalization hallucinates: unseen risks in AI-powered whole slide image processing](https://arxiv.org/abs/2512.07426)
*Karel Moens,Matthew B. Blaschko,Tinne Tuytelaars,Bart Diricx,Jonas De Vylder,Mustafa Yousif*

Main category: cs.CV

TL;DR: WSI归一化方法存在幻觉伪影风险，传统评估方法难以检测，提出新指标评估真实临床数据上的表现


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的WSI归一化方法倾向于输出平均化结果，可能掩盖诊断特征，更严重的是会产生视觉难以检测的幻觉伪影，对下游分析构成严重威胁，而现有评估方法往往忽视这一问题

Method: 提出一种新颖的图像比较度量方法，专门用于自动检测归一化输出中的幻觉伪影，并使用该度量系统评估在真实临床数据上重新训练的多个知名归一化方法

Result: 在真实临床数据上重新训练和评估时，发现多个知名归一化方法存在显著的幻觉伪影和不一致性，这些问题是传统评估指标无法捕捉的

Conclusion: 研究强调了在临床部署中需要更鲁棒、可解释的归一化技术和更严格的验证协议，以解决幻觉伪影带来的风险

Abstract: Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.

</details>


### [122] [Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models](https://arxiv.org/abs/2512.07564)
*Kassoum Sanogo,Renzo Ardiccioni*

Main category: cs.CV

TL;DR: 提出无需训练的视觉语言模型自校正框架，通过不确定性引导的视觉重注意机制迭代修正幻觉内容，在保持模型冻结的情况下显著降低幻觉率。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型经常生成看似合理但错误的幻觉内容，现有方法通常需要微调或额外训练，缺乏无需训练的自校正机制来利用模型自身能力修正错误。

Method: 提出训练免费的自校正框架，结合多维不确定性量化（令牌熵、注意力分散度、语义一致性、声明置信度）和注意力引导的未探索区域裁剪，通过不确定性引导的视觉重注意迭代修正响应。

Result: 在POPE和MMHAL BENCH基准测试中，使用Qwen2.5-VL-7B架构，相比基线将幻觉率降低9.8个百分点，在对抗性分割上物体存在准确性提高4.7点，定性分析确认不确定性引导的重注意能成功基于视觉证据进行修正。

Conclusion: 提出的无需训练自校正框架能有效减少视觉语言模型的幻觉，通过不确定性引导的视觉重注意机制利用模型自身能力修正错误，为可信多模态系统研究提供新方向。

Abstract: Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.

</details>


### [123] [Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos](https://arxiv.org/abs/2512.06783)
*Tobias Leuthold,Michele Xiloyannis,Yves Zimmermann*

Main category: cs.CV

TL;DR: 提出一种实时后处理算法，融合BlazePose的3D和2D估计，通过加权优化结合骨骼长度和生物力学模型约束，提升姿态估计的解剖学准确性。


<details>
  <summary>Details</summary>
Motivation: 物理治疗等自动教练应用需要准确稳健的单目视频姿态估计。现有模型如BlazePose缺乏解剖学约束，有改进空间。

Method: 实时后处理算法融合BlazePose的3D和2D估计，采用加权优化惩罚骨骼长度偏差和生物力学模型偏差。使用卡尔曼滤波器根据个体解剖学精化骨骼长度估计。

Result: 在Physio2.2M数据集上，相比BlazePose 3D估计，3D MPJPE降低10.2%，身体段间角度误差减少16.6%。

Conclusion: 该方法提供稳健、解剖学一致的姿态估计，计算效率高，适用于自动物理治疗、医疗保健和运动教练，可在消费级设备上运行。

Abstract: Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.

</details>


### [124] [Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation](https://arxiv.org/abs/2512.07568)
*Xuecheng Li,Weikuan Jia,Alisher Kurbonaliev,Qurbonaliev Alisher,Khudzhamkulov Rustam,Ismoilov Shuhratjon,Eshmatov Javhariddin,Yuanjie Zheng*

Main category: cs.CV

TL;DR: DSRSD-Net通过双流残差分解和语义解相关约束，解决多模态学习中模态主导、冗余耦合和虚假相关等问题，提升预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多模态表示常面临模态主导、冗余信息耦合和虚假跨模态相关性问题，导致泛化能力差、可解释性有限。高方差模态容易掩盖弱但语义重要的信号，而简单的融合策略会以不受控的方式纠缠模态共享和模态特定因素。

Method: 提出双流残差语义解相关网络(DSRSD-Net)：1) 双流表示学习模块通过残差投影分离模态内(私有)和模态间(共享)潜在因子；2) 残差语义对齐头通过对比和回归目标将不同模态的共享因子映射到共同空间；3) 解相关和正交性损失正则化共享空间的协方差结构，同时强制共享流和私有流之间的正交性。

Result: 在两个大规模教育基准测试中，DSRSD-Net在下一步预测和最终结果预测方面持续优于强单模态、早期融合、晚期融合和协同注意力基线。

Conclusion: DSRSD-Net通过残差分解和显式语义解相关约束，有效解耦模态特定和模态共享信息，解决了多模态学习中的关键挑战，提升了预测性能和模型可解释性。

Abstract: Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while naïve fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.

</details>


### [125] [Generalized Geometry Encoding Volume for Real-time Stereo Matching](https://arxiv.org/abs/2512.06793)
*Jiaxin Liu,Gangwei Xu,Xianqi Wang,Chengliang Zhang,Xin Yang*

Main category: cs.CV

TL;DR: GGEV是一个实时立体匹配网络，通过提取深度感知特征和动态成本聚合，在保持实时性的同时实现了强大的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有实时立体匹配方法主要关注域内性能而忽视泛化能力，而具有泛化能力的立体基础模型通常推理延迟很高。需要解决实时性与泛化能力之间的权衡问题。

Method: 提出广义几何编码体积(GGEV)：1) 提取编码域不变结构先验的深度感知特征；2) 引入深度感知动态成本聚合(DDCA)模块，自适应地将这些先验融入每个视差假设，增强未见场景中的脆弱匹配关系。

Result: 在零样本泛化能力上超越了所有现有实时方法，在KITTI 2012、KITTI 2015和ETH3D基准测试中达到了最先进的性能。

Conclusion: GGEV通过轻量级的深度感知特征提取和动态成本聚合，成功解决了实时立体匹配中泛化能力与推理速度的权衡问题，实现了既实时又具有强大泛化能力的立体匹配。

Abstract: Real-time stereo matching methods primarily focus on enhancing in-domain performance but often overlook the critical importance of generalization in real-world applications. In contrast, recent stereo foundation models leverage monocular foundation models (MFMs) to improve generalization, but typically suffer from substantial inference latency. To address this trade-off, we propose Generalized Geometry Encoding Volume (GGEV), a novel real-time stereo matching network that achieves strong generalization. We first extract depth-aware features that encode domain-invariant structural priors as guidance for cost aggregation. Subsequently, we introduce a Depth-aware Dynamic Cost Aggregation (DDCA) module that adaptively incorporates these priors into each disparity hypothesis, effectively enhancing fragile matching relationships in unseen scenes. Both steps are lightweight and complementary, leading to the construction of a generalized geometry encoding volume with strong generalization capability. Experimental results demonstrate that our GGEV surpasses all existing real-time methods in zero-shot generalization capability, and achieves state-of-the-art performance on the KITTI 2012, KITTI 2015, and ETH3D benchmarks.

</details>


### [126] [An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research](https://arxiv.org/abs/2512.07652)
*Hamad Almazrouei,Mariam Al Nasseri,Maha Alzaabi*

Main category: cs.CV

TL;DR: 提出AI驱动的自主水下航行器系统，整合YOLOv12 Nano实时检测、ResNet50特征提取、PCA降维、K-Means++聚类和GPT-4o Mini生成报告，用于自动化水下物体检测分析


<details>
  <summary>Details</summary>
Motivation: 传统海洋勘探面临极端条件、能见度低、成本高等挑战，导致大量海域未探索，需要自动化解决方案降低风险、提高效率

Method: 集成YOLOv12 Nano实时物体检测、ResNet50特征提取、PCA降维（保留98%方差）、K-Means++聚类分组，以及GPT-4o Mini生成结构化报告

Result: 在55,000+图像数据集上达到mAP@0.5为0.512、精度0.535、召回率0.438；PCA有效降维，K-Means成功聚类，LLM生成有洞察力的检测报告

Conclusion: 该集成系统显著降低潜水风险，提高任务效率，加速水下数据分析，为挑战性海洋环境中的科学研究提供有效途径

Abstract: Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.

</details>


### [127] [VDOT: Efficient Unified Video Creation via Optimal Transport Distillation](https://arxiv.org/abs/2512.06802)
*Yutong Wang,Haiyu Zhang,Tianfan Xue,Yu Qiao,Yaohui Wang,Chang Xu,Xinyuan Chen*

Main category: cs.CV

TL;DR: VDOT是一个高效统一的视频生成模型，采用分布匹配蒸馏范式，结合最优传输技术和判别器，在4步推理中就能达到或超越其他模型100步的效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型要么只能处理少数特定条件，要么因推理复杂导致生成时间过长，难以实际应用。需要开发高效统一的视频生成解决方案。

Method: 1. 采用分布匹配蒸馏范式训练；2. 使用最优传输技术优化真实与生成分数分布的差异，避免KL散度蒸馏中的梯度崩溃问题；3. 集成判别器感知真实视频数据；4. 开发自动化视频数据标注和过滤管道；5. 创建统一测试基准UVCBench。

Result: 实验表明，4步推理的VDOT模型在性能上达到或超越了其他需要100步去噪的基线模型，实现了高效高质量的视频生成。

Conclusion: VDOT通过最优传输技术和判别器集成，解决了现有视频生成模型的效率和泛化问题，为实际应用提供了可行的解决方案。

Abstract: The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.

</details>


### [128] [DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations](https://arxiv.org/abs/2512.07674)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: DIST-CLIP：基于CLIP指导的解耦风格迁移框架，用于MRI数据标准化，可灵活使用目标图像或DICOM元数据进行指导，解决医学影像数据异质性问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像分析中面临临床泛化受限的问题，主要障碍是数据异质性。在MRI中，扫描仪硬件差异、采集协议多样和序列参数变化导致显著的域偏移，掩盖了潜在的生物信号。现有数据标准化方法不足：基于图像的方法需要目标图像，而基于文本的方法依赖过于简化的标签，无法捕捉复杂的采集细节，且通常局限于有限变异性的数据集。

Method: 提出DIST-CLIP（基于CLIP指导的解耦风格迁移）框架，将解剖内容与图像对比度明确解耦。使用预训练的CLIP编码器提取对比度表示，然后通过新颖的自适应风格迁移模块将这些对比度嵌入整合到解剖内容中。框架可灵活使用目标图像或DICOM元数据进行指导。

Result: 在多样化的真实世界临床数据集上训练和评估DIST-CLIP，与最先进方法相比，在风格转换保真度和解剖结构保留方面均显示出显著改进，为风格迁移和MRI数据标准化提供了灵活的解决方案。

Conclusion: DIST-CLIP为MRI数据标准化提供了统一的框架，能够有效处理真实世界临床环境中的异质性，通过解耦解剖内容和图像对比度，并使用CLIP指导实现灵活的风格迁移，显著提升了医学影像分析的临床泛化能力。

Abstract: Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.

</details>


### [129] [MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2512.06810)
*Yueqian Wang,Songxiang Liu,Disong Wang,Nuo Xu,Guanglu Wan,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 提出MMDuet2，一种基于多轮RL训练的视频多模态大语言模型，能够主动决定在视频播放过程中何时回应，无需精确回复时间标注。


<details>
  <summary>Details</summary>
Motivation: 现有视频MLLM大多采用回合制交互，只能在用户回合后回复。对于实时应用，模型主动决定何时在视频播放过程中回应是一个有前景但具有挑战性的方向。

Method: 提出文本到文本的主动交互方法，模型基于对话历史和当前帧视觉上下文自主决定回应或保持沉默。采用多轮RL训练方法，无需精确回复时间标注，鼓励及时准确的回应。

Result: 在52k视频数据集上通过SFT和RL训练MMDuet2，在ProactiveVideoQA基准测试中优于现有主动视频MLLM基线，在回应时机和质量上达到最先进性能。

Conclusion: MMDuet2通过多轮RL训练实现了有效的主动视频交互，解决了手动调整回应决策阈值和标注精确回复时间的困难，为实时视频应用提供了更好的交互体验。

Abstract: Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.

</details>


### [130] [Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment](https://arxiv.org/abs/2512.07702)
*Sangha Park,Eunji Kim,Yeongtak Oh,Jooyoung Choi,Sungroh Yoon*

Main category: cs.CV

TL;DR: 提出NPC（Negative Prompting for Image Correction）方法，通过自动识别和应用负向提示来抑制不相关内容，从而提升文本到图像生成的对齐精度。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像生成取得了显著进展，但对于具有丰富组合结构或想象元素的提示，实现精确的文本-图像对齐仍然具有挑战性。现有方法在处理复杂提示时往往生成与文本描述不符的内容。

Method: NPC是一个自动化流程，通过分析交叉注意力模式来理解为什么有针对性的负向提示（与对齐错误直接相关）和无针对性的负向提示（与提示无关但出现在生成图像中的标记）都能增强对齐。采用验证器-描述器-提议器框架生成候选负向提示，并使用显著文本空间分数进行排名，无需额外图像合成即可有效选择负向提示。

Result: 在GenEval++和Imagine-Bench基准测试中，NPC表现优于强基线方法：在GenEval++上达到0.571 vs 0.371，在Imagine-Bench上获得最佳整体性能。

Conclusion: 通过指导模型不生成什么内容，NPC为扩散模型中的文本-图像对齐提供了一种原则性、完全自动化的方法，显著提升了生成图像与文本描述的匹配度。

Abstract: Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.

</details>


### [131] [Improving action classification with brain-inspired deep networks](https://arxiv.org/abs/2512.07729)
*Aidas Aglinskas,Stefano Anzellotti*

Main category: cs.CV

TL;DR: 研究比较了深度神经网络与人类在动作识别中对身体和背景信息的利用差异，并提出了受大脑领域特异性启发的双流网络架构


<details>
  <summary>Details</summary>
Motivation: 探索深度神经网络在动作识别中如何利用身体和背景信息，以及是否受大脑领域特异性结构启发能开发出更接近人类表现的网络架构

Method: 1) 比较DNN在HAA500数据集上对完整、无身体、无背景三种刺激版本的表现；2) 测试28名人类参与者在相同刺激下的表现；3) 设计受大脑领域特异性启发的双流网络架构（分别处理身体和背景信息）并进行测试

Result: 1) DNN在无背景刺激上表现接近随机水平，但对无身体刺激表现良好；2) 人类在所有三种刺激版本上都能准确识别动作，且在仅有身体的刺激上表现优于仅有背景的刺激；3) 双流架构提升了动作识别性能，且其在不同刺激版本上的准确率模式更接近人类表现

Conclusion: 受大脑领域特异性启发的双流网络架构能够更好地利用身体和背景信息进行动作识别，表现出更接近人类的性能模式，为开发更类人的计算机视觉系统提供了新思路

Abstract: Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.

</details>


### [132] [MeshSplatting: Differentiable Rendering with Opaque Meshes](https://arxiv.org/abs/2512.06818)
*Jan Held,Sanghyun Son,Renaud Vandeghen,Daniel Rebain,Matheus Gadelha,Yi Zhou,Anthony Cioppa,Ming C. Lin,Marc Van Droogenbroeck,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: MeshSplatting：一种基于网格的重建方法，通过可微分渲染联合优化几何和外观，实现实时渲染的高质量网格生成，在Mip-NeRF360上比当前最佳方法PSNR提升0.69dB，训练速度快2倍且内存使用少2倍。


<details>
  <summary>Details</summary>
Motivation: 现有的基于基元的splatting方法（如3D高斯splatting）虽然实现了实时渲染的新视角合成，但其基于点的表示与AR/VR和游戏引擎中基于网格的流水线不兼容。需要一种能够生成高质量网格表示的方法，以桥接神经渲染和交互式3D图形。

Method: MeshSplatting通过可微分渲染联合优化几何和外观，使用受限Delaunay三角剖分强制网格连通性，并优化表面一致性，从而创建端到端平滑、视觉高质量的网格，能够在实时3D引擎中高效渲染。

Result: 在Mip-NeRF360数据集上，MeshSplatting比当前最先进的基于网格的新视角合成方法MiLo的PSNR提升了+0.69dB，同时训练速度快2倍，内存使用少2倍，实现了高质量的实时网格渲染。

Conclusion: MeshSplatting成功桥接了神经渲染和交互式3D图形，为AR/VR和游戏引擎提供了高质量、实时渲染的网格表示，实现了无缝的实时场景交互。

Abstract: Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.

</details>


### [133] [SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination](https://arxiv.org/abs/2512.07730)
*Sangha Park,Seungryong Yoo,Jisoo Mok,Sungroh Yoon*

Main category: cs.CV

TL;DR: SAVE框架通过稀疏自编码器特征引导来减少多模态大语言模型中的物体幻觉问题


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型仍然容易受到语言先验和视觉信息丢失导致的物体幻觉问题影响

Method: 提出SAVE框架，使用稀疏自编码器特征引导：通过二值物体存在问答探针识别视觉理解特征，然后沿这些特征引导模型

Result: 在标准基准测试中优于最先进的免训练方法，CHAIR_S提升10%，在POPE和MMHal-Bench上也有持续改进

Conclusion: SAVE通过抑制不确定物体token生成和增加对图像token的关注，有效减少幻觉，具有鲁棒性和泛化性

Abstract: Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\%p improvement in CHAIR\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.

</details>


### [134] [SparseCoop: Cooperative Perception with Kinematic-Grounded Queries](https://arxiv.org/abs/2512.06838)
*Jiahao Wang,Zhongwei Jiang,Wenchao Sun,Jiaru Zhong,Haibao Yu,Yuner Zhang,Chenyang Lu,Chuang Zhang,Lei He,Shaobing Xu,Jianqiang Wang*

Main category: cs.CV

TL;DR: SparseCoop是一个完全稀疏的协同感知框架，用于3D检测和跟踪，完全抛弃了中间BEV表示，通过实例查询、粗到细聚合和协同实例去噪实现高效协同感知。


<details>
  <summary>Details</summary>
Motivation: 当前协同感知方法存在通信成本高、灵活性差、对齐不精确等问题。基于BEV特征的方法通信成本呈二次方增长，缺乏对异步或不同视角的精确对齐灵活性。稀疏查询方法则存在几何表示不足、融合策略次优和训练不稳定等问题。

Method: 提出SparseCoop框架，包含三个创新：1) 基于运动学的实例查询，使用包含3D几何和速度的显式状态向量进行精确时空对齐；2) 粗到细聚合模块实现鲁棒融合；3) 协同实例去噪任务加速和稳定训练。

Result: 在V2X-Seq和Griffin数据集上达到最先进性能，具有优越的计算效率、低传输成本和强大的通信延迟鲁棒性。

Conclusion: SparseCoop通过完全稀疏的协同感知框架，解决了当前方法的通信成本、对齐精度和训练稳定性问题，为自动驾驶协同感知提供了高效、鲁棒的解决方案。

Abstract: Cooperative perception is critical for autonomous driving, overcoming the inherent limitations of a single vehicle, such as occlusions and constrained fields-of-view. However, current approaches sharing dense Bird's-Eye-View (BEV) features are constrained by quadratically-scaling communication costs and the lack of flexibility and interpretability for precise alignment across asynchronous or disparate viewpoints. While emerging sparse query-based methods offer an alternative, they often suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability. In this paper, we propose SparseCoop, a fully sparse cooperative perception framework for 3D detection and tracking that completely discards intermediate BEV representations. Our framework features a trio of innovations: a kinematic-grounded instance query that uses an explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; a coarse-to-fine aggregation module for robust fusion; and a cooperative instance denoising task to accelerate and stabilize training. Experiments on V2X-Seq and Griffin datasets show SparseCoop achieves state-of-the-art performance. Notably, it delivers this with superior computational efficiency, low transmission cost, and strong robustness to communication latency. Code is available at https://github.com/wang-jh18-SVM/SparseCoop.

</details>


### [135] [WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling](https://arxiv.org/abs/2512.07821)
*Shaoheng Fang,Hanwen Jiang,Yunpeng Bai,Niloy J. Mitra,Qixing Huang*

Main category: cs.CV

TL;DR: WorldReel是一个4D视频生成器，通过联合生成RGB帧和4D场景表示（点云图、相机轨迹、密集光流映射），实现时空一致的视频生成，在动态场景和移动相机下保持几何一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成器虽然能达到逼真的视觉效果，但在3D一致性方面存在根本性不足。需要开发能够原生保持时空一致性的4D视频生成方法，以支持更稳定的世界建模和交互应用。

Method: 提出WorldReel框架，联合生成RGB帧和4D场景表示（点云图、相机轨迹、密集光流映射）。采用合成数据和真实数据结合的训练策略：合成数据提供精确的4D监督（几何、运动、相机），真实视频提供视觉多样性和真实感。

Result: WorldReel在动态场景和移动相机下的视频生成中达到了新的最先进水平，显著改善了几何一致性、运动连贯性指标，减少了视角-时间伪影。能够泛化到野外拍摄的视频，同时保持强大的几何保真度。

Conclusion: WorldReel通过显式的4D表示和混合数据训练策略，将视频生成推向4D一致的世界建模，为智能体通过单一稳定的时空表示进行渲染、交互和场景推理奠定了基础。

Abstract: Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.

</details>


### [136] [CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles](https://arxiv.org/abs/2512.06840)
*Satoshi Hashimoto,Tatsuya Konishi,Tomoya Kaichi,Kazunori Matsumoto,Mori Kurokawa*

Main category: cs.CV

TL;DR: 该论文提出了首个结合持续学习与弱监督视频异常检测的方法CADE，通过双生成器和多判别器集成解决领域漂移和遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督视频异常检测方法主要针对静态数据集，忽视了数据领域可能变化的问题。当数据领域发生漂移时，仅用新数据训练会导致对先前数据的性能下降（遗忘），因此需要持续学习视角。

Method: 提出CADE方法：1）使用双生成器解决数据不平衡和标签不确定性问题；2）提出多判别器集成，捕获因遗忘而错过的异常模式，缓解"不完整性"问题。

Result: 在ShanghaiTech和Charlotte Anomaly等多场景VAD数据集上的大量实验表明，CADE显著优于现有的VAD方法。

Conclusion: CADE是首个结合持续学习和弱监督视频异常检测的方法，有效解决了领域漂移和遗忘问题，在多场景异常检测中表现出色。

Abstract: Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the "incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.

</details>


### [137] [One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation](https://arxiv.org/abs/2512.07829)
*Yuan Gao,Chen Chen,Tianrong Chen,Jiatao Gu*

Main category: cs.CV

TL;DR: FAE是一个简单有效的框架，通过耦合两个深度解码器，将预训练视觉表征适配到适合生成的低维潜在空间，仅需单个注意力层即可实现高质量图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将理解导向的预训练视觉表征适配到生成友好的低维潜在空间，因为高维表征编码器需要捕捉掩码区域的多种假设，而生成模型需要低维潜在空间来忠实保留注入噪声，这种不匹配导致先前工作需要复杂的目标和架构。

Method: 提出FAE框架，耦合两个独立的深度解码器：一个训练用于重建原始特征空间，另一个以重建特征为输入进行图像生成。该方法通用性强，可与多种自监督编码器（如DINO、SigLIP）结合，并适配到扩散模型和归一化流两种生成模型家族。

Result: 在ImageNet 256x256上，带CFG的扩散模型达到接近SOTA的FID 1.29（800轮）和1.70（80轮）；不带CFG时达到SOTA的FID 1.48（800轮）和2.08（80轮），展示了高质量和快速学习能力。

Conclusion: FAE提供了一种简单有效的方法，将预训练视觉表征适配到生成友好的低维潜在空间，仅需最小架构修改即可实现高质量图像生成，在类条件生成和文本到图像任务中表现优异。

Abstract: Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.

</details>


### [138] [Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2512.06845)
*Satoshi Hashimoto,Hitoshi Nishimura,Yanan Wang,Mori Kurokawa*

Main category: cs.CV

TL;DR: PA-VAD：一种无需真实异常视频的生成驱动视频异常检测方法，通过合成伪异常视频进行训练，在标准弱监督设置下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 实际部署视频异常检测面临真实异常视频稀缺且收集成本高的问题，需要开发无需真实异常视频的训练方法。

Method: 1) 使用CLIP选择类别相关初始图像，通过视觉语言模型优化文本提示，利用视频扩散模型合成伪异常视频；2) 设计域对齐正则化模块，结合域对齐和内存使用感知更新，缓解合成异常中的过度时空幅度问题。

Result: 在ShanghaiTech上达到98.2%，在UCF-Crime上达到82.5%，分别超过最强真实异常方法0.6%和超过UVAD SOTA方法1.9%。

Conclusion: 无需收集真实异常即可实现高精度异常检测，为可扩展部署提供了实用路径。

Abstract: Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.

</details>


### [139] [Relational Visual Similarity](https://arxiv.org/abs/2512.07833)
*Thao Nguyen,Sicheng Mo,Krishna Kumar Singh,Yilin Wang,Jing Shi,Nicholas Kolkin,Eli Shechtman,Yong Jae Lee,Yuheng Li*

Main category: cs.CV

TL;DR: 该论文提出了关系相似性的概念，开发了首个能够测量图像间关系相似性的视觉语言模型，填补了现有视觉相似性度量只关注感知属性而忽略关系结构的空白。


<details>
  <summary>Details</summary>
Motivation: 人类不仅能感知属性相似性，还能识别关系相似性（如地球与桃子的结构对应关系），但现有视觉相似性度量（LPIPS、CLIP、DINO等）只关注感知属性相似性，无法捕捉人类感知的丰富关系相似性。

Method: 1. 将关系图像相似性形式化为可测量问题；2. 构建包含11.4万张图像-匿名化描述的数据集，描述场景的关系逻辑而非表面内容；3. 使用该数据集微调视觉语言模型来测量图像间的关系相似性。

Result: 开发了首个能够连接图像底层关系结构而非表面外观的模型，实证研究表明关系相似性具有许多实际应用，但现有图像相似性模型无法捕捉这种关系相似性。

Conclusion: 关系相似性是视觉计算中一个关键但被忽视的维度，该研究填补了这一空白，为未来基于关系结构的图像理解和检索开辟了新方向。

Abstract: Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.

</details>


### [140] [Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT](https://arxiv.org/abs/2512.06849)
*Matan Atad,Alexander W. Marka,Lisa Steinhelfer,Anna Curto-Vilalta,Yannik Leonhardt,Sarah C. Foreman,Anna-Sophia Walburga Dietrich,Robert Graf,Alexandra S. Gersing,Bjoern Menze,Daniel Rueckert,Jan S. Kirschke,Hendrik Möller*

Main category: cs.CV

TL;DR: 提出一种弱监督方法，仅使用椎体级别的健康/恶性标签（无需病灶掩码），通过扩散自编码器和像素差异图生成候选病灶区域，再通过Hide-and-Seek Attribution机制筛选出真正的恶性区域，实现椎体转移瘤的准确分割。


<details>
  <summary>Details</summary>
Motivation: CT中椎体转移瘤的准确分割在临床上很重要，但难以规模化，因为体素级标注稀缺，且溶骨性和成骨性病变常与良性退行性变化相似。

Method: 结合扩散自编码器（DAE）生成椎体的分类器引导健康编辑，通过像素差异图提出候选病灶区域。引入Hide-and-Seek Attribution机制：逐个揭示候选区域同时隐藏其他区域，将编辑后的图像通过DAE投影回数据流形，使用潜在空间分类器量化该组件的孤立恶性贡献。高评分区域形成最终的溶骨性或成骨性分割。

Result: 在保留的放射科医生标注上，尽管没有掩码监督，仍实现了强大的成骨性/溶骨性性能（F1: 0.91/0.85; Dice: 0.87/0.78），超过基线方法（F1: 0.79/0.67; Dice: 0.74/0.55）。

Conclusion: 椎体级别标签可以转化为可靠的病灶掩码，表明生成式编辑结合选择性遮挡支持CT中准确的弱监督分割。

Abstract: Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.

</details>


### [141] [Omni-Referring Image Segmentation](https://arxiv.org/abs/2512.06862)
*Qiancheng Zheng,Yunhang Shen,Gen Luo,Baiyang Song,Xing Sun,Xiaoshuai Sun,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出Omni-Referring Image Segmentation (OmniRIS)任务，支持文本指令和带掩码/框/涂鸦的参考图像作为全提示输入，实现高度通用化的图像分割。构建了OmniRef数据集和OmniSegNet基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有单模态条件分割任务（如RIS和visual RIS）存在局限性，需要一种能同时利用文本和视觉模态优势的通用分割方法。文本模态擅长细粒度属性指代，视觉模态擅长罕见物体定位，两者结合能实现更强大的分割能力。

Method: 提出OmniRIS任务框架，支持文本指令和多种视觉提示（掩码、边界框、涂鸦）作为全模态输入。构建了包含186,939个全提示的OmniRef数据集，并设计了OmniSegNet基线模型，重点解决全提示编码等关键技术挑战。

Result: 实验验证了OmniSegNet能够有效遵循全模态指令进行分割，并展示了OmniRIS在高度通用化图像分割方面的优越性。支持多种分割设置（一对一、多对多），增强了实际应用价值。

Conclusion: OmniRIS通过融合文本和视觉模态的优势，实现了高度通用化的图像分割。提出的OmniRef数据集和OmniSegNet基线为这一新研究方向奠定了基础，展示了全模态提示在图像分割中的巨大潜力。

Abstract: In this paper, we propose a novel task termed Omni-Referring Image Segmentation (OmniRIS) towards highly generalized image segmentation. Compared with existing unimodally conditioned segmentation tasks, such as RIS and visual RIS, OmniRIS supports the input of text instructions and reference images with masks, boxes or scribbles as omni-prompts. This property makes it can well exploit the intrinsic merits of both text and visual modalities, i.e., granular attribute referring and uncommon object grounding, respectively. Besides, OmniRIS can also handle various segmentation settings, such as one v.s. many and many v.s. many, further facilitating its practical use. To promote the research of OmniRIS, we also rigorously design and construct a large dataset termed OmniRef, which consists of 186,939 omni-prompts for 30,956 images, and establish a comprehensive evaluation system. Moreover, a strong and general baseline termed OmniSegNet is also proposed to tackle the key challenges of OmniRIS, such as omni-prompt encoding. The extensive experiments not only validate the capability of OmniSegNet in following omni-modal instructions, but also show the superiority of OmniRIS for highly generalized image segmentation.

</details>


### [142] [Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training](https://arxiv.org/abs/2512.06864)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: AutoQ-VIS：一种通过质量引导自训练实现无监督视频实例分割的新框架，在YouTubeVIS-2019上达到52.6 AP50，超越之前最佳方法4.4%


<details>
  <summary>Details</summary>
Motivation: 视频实例分割面临像素级掩码和时间一致性标注的双重挑战。现有无监督方法如VideoCutLER虽然通过合成数据消除了光流依赖，但仍受限于合成到真实域的差距。

Method: 提出AutoQ-VIS框架，通过质量引导的自训练建立伪标签生成和自动质量评估的闭环系统，实现从合成视频到真实视频的渐进式适应。

Result: 在YouTubeVIS-2019验证集上达到52.6 AP50，超越之前最佳方法VideoCutLER 4.4%，且无需任何人工标注。

Conclusion: 证明了质量感知自训练在无监督视频实例分割中的可行性，为克服合成到真实域的差距提供了有效解决方案。

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 $\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.

</details>


### [143] [Spatial Retrieval Augmented Autonomous Driving](https://arxiv.org/abs/2512.06865)
*Xiaosong Jia,Chenhe Zhang,Yule Jiang,Songbur Wong,Zhiyuan Zhang,Chen Chen,Shaofeng Zhang,Xuanhe Zhou,Xue Yang,Junchi Yan,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出空间检索范式，通过引入离线检索的地理图像作为额外输入，增强自动驾驶系统的感知能力，特别是在视野受限、遮挡或极端天气条件下。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统依赖车载传感器进行环境感知，但受限于实时感知范围，在视野受限、遮挡、黑暗或雨天等极端条件下容易失效。人类驾驶员能够在能见度差时回忆道路结构，因此希望赋予模型这种"回忆"能力。

Method: 提出空间检索范式，从离线缓存（如Google Maps或存储的自动驾驶数据集）中检索地理图像作为额外输入。扩展nuScenes数据集，通过Google Maps API检索地理图像并与自车轨迹对齐。在五个核心自动驾驶任务上建立基线。

Result: 扩展的模态能够增强某些任务的性能。将开源数据集构建代码、数据和基准测试，供进一步研究这种新的自动驾驶范式。

Conclusion: 空间检索范式为自动驾驶系统提供了"回忆"能力，通过离线地理图像增强感知，特别是在视野受限条件下，是一种即插即用的扩展方案。

Abstract: Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.
  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.

</details>


### [144] [Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective](https://arxiv.org/abs/2512.06870)
*Wangkai Li,Rui Sun,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

TL;DR: ECOCSeg利用纠错输出码改进语义分割中的伪标签学习，通过细粒度编码和位级去噪机制提升伪标签质量，在UDA和SSL任务中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 伪标签学习在语义分割中广泛应用，但传统方法使用one-hot编码会放大错误伪标签的影响，导致训练不稳定和泛化能力差。

Method: 提出ECOCSeg框架：1）引入基于ECOC的分类器，将类别分解为属性并处理部分错误位；2）开发位级标签去噪机制，生成更高质量的伪标签；3）可轻松集成到现有方法中。

Result: 在多个UDA和半监督学习基准测试中，ECOCSeg与不同分割架构结合都取得了显著改进。

Conclusion: ECOCSeg通过纠错输出码提供了一种新颖的视角，能够生成更鲁棒的伪标签监督，有效解决了伪标签学习中的错误传播问题。

Abstract: Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at https://github.com/Woof6/ECOCSeg.

</details>


### [145] [SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification](https://arxiv.org/abs/2512.06877)
*Mohammed Q. Alkhatib,Ali Jamali,Swalpa Kumar Roy*

Main category: cs.CV

TL;DR: 提出基于卷积混合器范式的轻量级遥感场景分类模型，在AID和EuroSAT数据集上实现精度与效率的良好平衡


<details>
  <summary>Details</summary>
Motivation: 遥感场景分类对地球观测至关重要，但现有CNN和ViT模型在空间分辨率、视角、方向等变化下泛化能力有限，需要更高效的解决方案

Method: 采用卷积混合器架构，通过多尺度深度卷积进行空间混合，点卷积进行通道混合，高效提取局部和上下文信息，保持低参数量和计算量

Result: 在AID数据集上达到74.7%总体精度、74.57%平均精度和73.79 Kappa值；在EuroSAT上达到93.90%总体精度、93.93%平均精度和93.22 Kappa值

Conclusion: 提出的轻量级模型在精度和效率之间取得了良好平衡，优于广泛使用的CNN和transformer模型，代码将公开提供

Abstract: Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: https://github.com/mqalkhatib/SceneMixer

</details>


### [146] [Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion](https://arxiv.org/abs/2512.06882)
*Yu Zhu,Naoya Chiba,Koichi Hashimoto*

Main category: cs.CV

TL;DR: 提出分层图像引导的3D分割框架，从实例级到部件级逐步细化分割，解决工业场景中遮挡和尺度差异问题


<details>
  <summary>Details</summary>
Motivation: 工业环境中密集布局和多尺度物体导致几何边界模糊，现有3D点云方法需要昂贵标注，图像引导方法存在跨视图语义不一致问题

Method: 分层框架：1) 实例分割：渲染俯视图，用YOLO-World提示SAM生成掩码并反向投影到3D点云；2) 部件分割：对每个实例渲染多视图图像，应用相同2D分割和反向投影，通过贝叶斯更新融合确保跨视图语义一致性

Result: 在真实工厂数据上有效处理遮挡和结构复杂性，获得高每类mIoU分数；在公共数据集上验证了泛化能力，展现了鲁棒性、标注效率和适应性

Conclusion: 提出的分层图像引导3D分割框架能够有效解决工业场景中的遮挡和尺度差异问题，具有鲁棒性、标注效率和适应多样化3D环境的能力

Abstract: Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.

</details>


### [147] [Balanced Learning for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2512.06886)
*Wangkai Li,Rui Sun,Bohao Liao,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

TL;DR: BLDA提出了一种平衡学习方法，通过分析预测logits分布来识别过预测和欠预测类别，使用共享锚分布对齐logits分布，并在损失函数中加入logits校正项，以解决UDA语义分割中的类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 在无监督域自适应语义分割中，由于固有的类别不平衡以及域间数据和标签空间的分布偏移，自训练方法难以平衡地学习各个类别。现有方法通常需要先验知识来应对分布偏移，而本文旨在直接评估和缓解类别偏差，无需依赖先验分布知识。

Method: 1. 通过分析预测logits分布识别过预测和欠预测类别；2. 使用共享锚分布对logits分布进行后处理对齐；3. 在线估计logits分布并在损失函数中引入logits校正项；4. 利用累积密度作为域共享的结构知识连接源域和目标域。

Result: 在两个标准UDA语义分割基准上的大量实验表明，BLDA能够持续提升性能，特别是对于欠预测类别。当集成到多种现有方法中时，BLDA均表现出稳定的改进效果。

Conclusion: BLDA提供了一种无需先验分布知识的平衡学习方法，能够有效解决UDA语义分割中的类别不平衡问题，显著提升欠预测类别的性能，为自训练方法中的类别偏差问题提供了新的解决方案。

Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains. To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift. First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits. Subsequently, we introduce a post-hoc approach to align the logits distributions across different classes using shared anchor distributions. To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function. Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains. Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods. Code is available at https://github.com/Woof6/BLDA.

</details>


### [148] [Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation](https://arxiv.org/abs/2512.06888)
*Liyang Song,Hardik Bishnoi,Sai Kumar Reddy Manne,Sarah Ostadabbas,Briana J. Taylor,Michael Wan*

Main category: cs.CV

TL;DR: 提出首个可复现的婴儿呼吸监测计算机视觉系统，包括新数据集AIR-400和基于光流增强的时空神经网络方法


<details>
  <summary>Details</summary>
Motivation: 婴儿呼吸异常与神经发育障碍和婴儿猝死综合征相关，但现有呼吸监测方法主要针对成人，缺乏婴儿专用的视频数据集和可复现算法

Method: 开发婴儿特异性感兴趣区域检测和基于光流输入增强的时空神经网络处理管道，创建包含400个视频的AIR-400数据集

Result: 建立了首个可复现的视觉婴儿呼吸估计基准，提供了公开可用的数据集、代码库和训练模型

Conclusion: 该工作填补了婴儿无接触呼吸监测领域的空白，为早期检测和治疗呼吸异常提供了重要工具，促进了该领域的研究发展

Abstract: The development of contactless respiration monitoring for infants could enable advances in the early detection and treatment of breathing irregularities, which are associated with neurodevelopmental impairments and conditions like sudden infant death syndrome (SIDS). But while respiration estimation for adults is supported by a robust ecosystem of computer vision algorithms and video datasets, only one small public video dataset with annotated respiration data for infant subjects exists, and there are no reproducible algorithms which are effective for infants. We introduce the annotated infant respiration dataset of 400 videos (AIR-400), contributing 275 new, carefully annotated videos from 10 recruited subjects to the public corpus. We develop the first reproducible pipelines for infant respiration estimation, based on infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs. We establish, through comprehensive experiments, the first reproducible benchmarks for the state-of-the-art in vision-based infant respiration estimation. We make our dataset, code repository, and trained models available for public use.

</details>


### [149] [Scaling Zero-Shot Reference-to-Video Generation](https://arxiv.org/abs/2512.06905)
*Zijian Zhou,Shikun Liu,Haozhe Liu,Haonan Qiu,Zhaochong An,Weiming Ren,Zhiheng Liu,Xiaoke Huang,Kam Woh Ng,Tian Xie,Xiao Han,Yuren Cong,Hang Li,Chuyan Zhu,Aditya Patel,Tao Xiang,Sen He*

Main category: cs.CV

TL;DR: Saber是一个无需R2V训练数据的零参考视频生成框架，通过掩码训练和注意力设计实现身份一致性和参考感知，在OpenS2V-Eval基准上超越需要R2V数据的方法。


<details>
  <summary>Details</summary>
Motivation: 当前参考到视频生成方法依赖于昂贵的图像-视频-文本三元组数据，这种数据构建成本高且难以扩展，需要绕过这个瓶颈。

Method: 提出Saber框架：1) 仅使用视频-文本对训练；2) 采用掩码训练策略；3) 设计基于注意力的模型架构学习身份一致和参考感知表示；4) 集成掩码增强技术减少复制粘贴伪影。

Result: 在OpenS2V-Eval基准上表现优于需要R2V数据的方法，展示了出色的泛化能力，能够处理不同数量的参考图像。

Conclusion: Saber通过零样本框架成功绕过了R2V数据依赖的瓶颈，实现了可扩展的参考到视频生成，为视频生成领域提供了更实用的解决方案。

Abstract: Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.

</details>


### [150] [Can We Go Beyond Visual Features? Neural Tissue Relation Modeling for Relational Graph Analysis in Non-Melanoma Skin Histology](https://arxiv.org/abs/2512.06949)
*Shravan Venkatraman,Muthu Subash Kavitha,Joe Dhanith P R,V Manikandarajan,Jia Wu*

Main category: cs.CV

TL;DR: NTRM是一种用于皮肤癌组织病理图像分割的新框架，通过图神经网络建模组织间空间和功能关系，在边界密集区域实现结构一致的预测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于CNN的组织病理图像分割方法主要依赖视觉纹理，将组织视为独立区域，缺乏对组织间空间关系和生物背景的建模，特别是在重叠或形态相似的组织区域中表现不佳。

Method: 提出神经组织关系建模（NTRM）框架，将CNN与组织级图神经网络结合：1）在预测区域上构建图；2）通过消息传递传播上下文信息；3）通过空间投影细化分割结果，显式编码组织间依赖关系。

Result: 在Histopathology Non-Melanoma Skin Cancer Segmentation Dataset基准测试中，NTRM优于最先进方法，Dice相似系数比最佳模型高出4.9%到31.25%。

Conclusion: 关系建模为组织病理分割提供了更注重上下文和可解释性的方法，相比缺乏组织级结构感知的局部感受野架构，能够实现更结构一致的预测。

Abstract: Histopathology image segmentation is essential for delineating tissue structures in skin cancer diagnostics, but modeling spatial context and inter-tissue relationships remains a challenge, especially in regions with overlapping or morphologically similar tissues. Current convolutional neural network (CNN)-based approaches operate primarily on visual texture, often treating tissues as independent regions and failing to encode biological context. To this end, we introduce Neural Tissue Relation Modeling (NTRM), a novel segmentation framework that augments CNNs with a tissue-level graph neural network to model spatial and functional relationships across tissue types. NTRM constructs a graph over predicted regions, propagates contextual information via message passing, and refines segmentation through spatial projection. Unlike prior methods, NTRM explicitly encodes inter-tissue dependencies, enabling structurally coherent predictions in boundary-dense zones. On the benchmark Histopathology Non-Melanoma Skin Cancer Segmentation Dataset, NTRM outperforms state-of-the-art methods, achieving a robust Dice similarity coefficient that is 4.9\% to 31.25\% higher than the best-performing models among the evaluated approaches. Our experiments indicate that relational modeling offers a principled path toward more context-aware and interpretable histological segmentation, compared to local receptive-field architectures that lack tissue-level structural awareness. Our code is available at https://github.com/shravan-18/NTRM.

</details>


### [151] [Selective Masking based Self-Supervised Learning for Image Semantic Segmentation](https://arxiv.org/abs/2512.06981)
*Yuemin Wang,Ian Stavness*

Main category: cs.CV

TL;DR: 提出了一种用于语义分割的自监督学习方法，通过选择性掩码图像重建作为预训练任务，相比随机掩码方法在多个数据集上取得更好的分割精度。


<details>
  <summary>Details</summary>
Motivation: 传统随机掩码图像建模方法在预训练中可能不够高效，需要更智能的掩码策略来提升自监督学习效果，特别是在计算资源受限的场景下。

Method: 提出选择性掩码方法，通过迭代步骤选择重建损失最高的图像块进行掩码，利用已训练模型的知识来指导掩码选择过程。

Result: 在Pascal VOC、Cityscapes等通用数据集上比随机掩码方法提升2.9%精度，在杂草分割数据集上提升2.5%，对性能最差的类别改进尤为显著。

Conclusion: 选择性掩码图像重建方法为端到端语义分割工作流提供了有效实用的解决方案，特别适合需要有限模型容量以满足推理速度和计算资源要求的场景。

Abstract: This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.

</details>


### [152] [Evaluating and Preserving High-level Fidelity in Super-Resolution](https://arxiv.org/abs/2512.07037)
*Josep M. Rocafort,Shaolin Su,Javier Vazquez-Corral,Alexandra Gomez-Villa*

Main category: cs.CV

TL;DR: 本文提出衡量超分辨率模型的高层语义保真度作为补充评价标准，构建首个带保真度标注的数据集，分析现有指标相关性，并展示通过保真度反馈优化模型可同时提升语义保真度和感知质量。


<details>
  <summary>Details</summary>
Motivation: 当前超分辨率模型虽然能生成视觉质量高的图像，但过强的生成能力有时会产生幻觉，改变图像内容。这种高层语义变化容易被人类识别，但现有低层图像质量指标未能很好衡量。需要建立高层保真度测量作为补充标准，揭示生成式超分辨率模型的可靠性。

Method: 1) 构建首个带保真度标注的数据集，包含不同超分辨率模型的输出；2) 评估SOTA超分辨率模型在保持高层保真度方面的表现；3) 分析现有图像质量指标与保真度测量的相关性；4) 展示基础模型能更好地处理高层语义任务；5) 基于保真度反馈微调超分辨率模型。

Result: 1) 建立了高层保真度测量标准；2) 发现现有图像质量指标与保真度测量相关性有限；3) 基础模型在高层语义任务上表现更好；4) 通过保真度反馈微调模型可同时提升语义保真度和感知质量。

Conclusion: 高层语义保真度是评估超分辨率模型的重要补充标准，对模型评估和优化具有潜在价值。提出的保真度测量方法能揭示生成式超分辨率模型的可靠性，通过基于保真度的优化可同时改善语义准确性和视觉质量。

Abstract: Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.

</details>


### [153] [RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting](https://arxiv.org/abs/2512.07052)
*Hoang-Nhat Tran,Francesco Di Sario,Gabriele Spadaro,Giuseppe Valenzise,Enzo Tartaglione*

Main category: cs.CV

TL;DR: 提出一种灵活的3D高斯泼溅压缩方案，支持在预定边界内的任意速率插值，无需重新训练即可实现动态码率控制


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅（3DGS）虽然能实现实时逼真渲染，但存在内存需求大和训练成本高的问题。现有压缩方法只能以固定速率工作，无法适应不同带宽和设备限制

Method: 提出灵活的3DGS压缩方案，支持在预定边界内任意速率插值，计算轻量且无需为不同速率重新训练

Result: 该方法在广泛的运行点上保持渲染质量，实现高效高质量压缩，同时提供动态码率控制

Conclusion: 提出的灵活压缩方案适合沉浸式应用的实际部署，代码将在论文接受后开源

Abstract: Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code will be provided open-source upon acceptance of the work.

</details>


### [154] [Persistent Homology-Guided Frequency Filtering for Image Compression](https://arxiv.org/abs/2512.07065)
*Anil Chintapalli,Peter Tenholder,Henry Chen,Arjun Rao*

Main category: cs.CV

TL;DR: 使用离散傅里叶变换结合持续同调分析，从噪声图像中提取特定频率对应拓扑特征，实现压缩并提升分类性能


<details>
  <summary>Details</summary>
Motivation: 噪声图像数据集中的特征提取面临模型可靠性挑战，需要能区分有意义数据并压缩图像的方法

Method: 结合离散傅里叶变换和持续同调分析，提取与图像拓扑特征对应的特定频率，实现频率过滤和图像压缩

Result: 压缩效果与JPEG相当（使用六种指标评估），在增强卷积神经网络时能提升二分类任务性能

Conclusion: 持续同调引导的频率过滤能提高噪声条件下图像压缩的可靠性，为传统特征提取和压缩方法提供改进

Abstract: Feature extraction in noisy image datasets presents many challenges in model reliability. In this paper, we use the discrete Fourier transform in conjunction with persistent homology analysis to extract specific frequencies that correspond with certain topological features of an image. This method allows the image to be compressed and reformed while ensuring that meaningful data can be differentiated. Our experimental results show a level of compression comparable to that of using JPEG using six different metrics. The end goal of persistent homology-guided frequency filtration is its potential to improve performance in binary classification tasks (when augmenting a Convolutional Neural Network) compared to traditional feature extraction and compression methods. These findings highlight a useful end result: enhancing the reliability of image compression under noisy conditions.

</details>


### [155] [Context-measure: Contextualizing Metric for Camouflage](https://arxiv.org/abs/2512.07076)
*Chen-Yang Wang,Gepeng Ji,Song Shao,Ming-Ming Cheng,Deng-Ping Fan*

Main category: cs.CV

TL;DR: 提出Context-measure，一种考虑空间上下文依赖性的新评估指标，用于伪装物体分割任务，比现有指标更符合人类感知


<details>
  <summary>Details</summary>
Motivation: 当前伪装场景的评估指标忽略了上下文依赖性这一关键因素，这些指标原本是为评估一般或显著物体设计的，假设空间上下文不相关，导致评估不准确

Method: 基于概率像素感知相关框架构建上下文化评估范式Context-measure，通过纳入空间依赖性和像素级伪装量化，使评估更符合人类感知

Result: 在三个具有挑战性的伪装物体分割数据集上的广泛实验表明，Context-measure比现有的上下文无关指标提供更可靠的评估

Conclusion: Context-measure可以为涉及伪装模式的计算机视觉应用（如农业、工业和医疗场景）提供基础评估基准

Abstract: Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.

</details>


### [156] [DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection](https://arxiv.org/abs/2512.07078)
*Bo Gao,Jingcheng Tong,Xingsheng Chen,Han Yu,Zichen Li*

Main category: cs.CV

TL;DR: DFIR-DETR：通过动态特征聚合和频域处理解决小目标检测问题，在无人机遥感和工业缺陷检测中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 无人机遥感图像中的小目标检测和工业表面缺陷识别面临共同挑战：特征稀疏且弱、背景杂乱、目标尺度变化大。现有基于Transformer的检测器存在三个关键问题：特征随下采样严重退化、空间卷积无法有效捕获长程依赖、标准上采样方法导致特征图不必要膨胀。

Method: 提出DFIR-DETR，包含三个核心模块：1) DCFA模块使用动态K稀疏注意力将复杂度从O(N²)降至O(NK)，并采用空间门控线性单元增强非线性建模；2) DFPN模块应用幅度归一化上采样防止特征膨胀，使用双路径混洗卷积保留跨尺度空间细节；3) FIRC3模块在频域操作，实现全局感受野而不牺牲效率。

Result: 在NEU-DET和VisDrone数据集上分别达到92.9%和51.6%的mAP50分数，均为当前最佳性能。模型轻量，仅11.7M参数和41.2 GFLOPs，在资源受限环境下表现出良好的跨场景小目标检测能力。

Conclusion: DFIR-DETR通过动态特征聚合和频域处理有效解决了小目标检测的关键挑战，在两个不同领域都取得了SOTA性能，证明了其在资源受限环境下跨场景小目标检测的有效性和泛化能力。

Abstract: Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.

</details>


### [157] [COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision](https://arxiv.org/abs/2512.07107)
*Jaeyoon Lee,Hojoon Jung,Sungtae Hwang,Jihyong Oh,Jongwon Choi*

Main category: cs.CV

TL;DR: COREA是一个统一框架，联合学习可重照明的3D高斯和SDF，用于精确几何重建和忠实重照明，通过3D到3D对齐策略解决现有方法几何粗糙和BRDF-光照分解不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯方法虽然扩展到网格重建和PBR渲染，但其几何仍从2D渲染学习，导致表面粗糙且BRDF-光照分解不可靠。需要直接在3D空间中学习几何信号的方法。

Method: 提出粗到细的双向3D到3D对齐策略：深度提供粗对齐，深度梯度和法线细化精细结构；密度控制机制稳定高斯增长；联合学习可重照明3D高斯和SDF。

Result: 在标准基准测试中，COREA在新视角合成、网格重建和PBR渲染方面均取得优越性能，在统一框架内实现精确几何重建和忠实重照明。

Conclusion: COREA通过3D到3D对齐策略成功解决了现有方法的局限性，实现了精确几何重建和稳定BRDF-光照分解的统一框架，为3D重建和重照明提供了有效解决方案。

Abstract: We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.

</details>


### [158] [MSN: Multi-directional Similarity Network for Hand-crafted and Deep-synthesized Copy-Move Forgery Detection](https://arxiv.org/abs/2512.07110)
*Liangwei Jiang,Jinluo Xie,Yecheng Huang,Hua Zhang,Hongyu Yang,Di Huang*

Main category: cs.CV

TL;DR: 提出MSN双流模型用于检测复制-移动图像伪造，通过多方向CNN编码和2D相似度矩阵解码器，在表示和定位两方面改进现有方法，并在多个数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 复制-移动图像伪造检测面临复杂变换和精细操作的挑战，现有深度检测模型在表示能力和定位精度方面存在局限，需要更有效的检测方法。

Method: 提出多方向相似度网络（MSN），包含：1）多方向CNN网络进行分层编码，通过尺度旋转增强提高特征相似度度量；2）2D相似度矩阵解码器，充分利用整幅图像的空间信息进行精确定位。

Result: 在CASIA CMFD、CoMoFoD等经典数据集和新构建的深度合成伪造数据集上进行了广泛实验，取得了最先进的检测结果，证明了方法的有效性。

Conclusion: MSN模型通过改进表示学习和定位机制，显著提升了复制-移动伪造检测性能，同时构建的新数据集为检测深度合成伪造提供了基准。

Abstract: Copy-move image forgery aims to duplicate certain objects or to hide specific contents with copy-move operations, which can be achieved by a sequence of manual manipulations as well as up-to-date deep generative network-based swapping. Its detection is becoming increasingly challenging for the complex transformations and fine-tuned operations on the tampered regions. In this paper, we propose a novel two-stream model, namely Multi-directional Similarity Network (MSN), to accurate and efficient copy-move forgery detection. It addresses the two major limitations of existing deep detection models in \textbf{representation} and \textbf{localization}, respectively. In representation, an image is hierarchically encoded by a multi-directional CNN network, and due to the diverse augmentation in scales and rotations, the feature achieved better measures the similarity between sampled patches in two streams. In localization, we design a 2-D similarity matrix based decoder, and compared with the current 1-D similarity vector based one, it makes full use of spatial information in the entire image, leading to the improvement in detecting tampered regions. Beyond the method, a new forgery database generated by various deep neural networks is presented, as a new benchmark for detecting the growing deep-synthesized copy-move. Extensive experiments are conducted on two classic image forensics benchmarks, \emph{i.e.} CASIA CMFD and CoMoFoD, and the newly presented one. The state-of-the-art results are reported, which demonstrate the effectiveness of the proposed approach.

</details>


### [159] [Training-free Clothing Region of Interest Self-correction for Virtual Try-On](https://arxiv.org/abs/2512.07126)
*Shengjie Lu,Zhibin Wan,Jiejie Liu,Quan Zhang,Mingjie Sun*

Main category: cs.CV

TL;DR: 提出CSC-VTON方法，通过能量函数约束注意力图，提升虚拟试衣的细节一致性，并设计了新的评估指标VTID


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣方法在生成的服装图案、纹理和边界方面与目标服装存在差异，且现有评估指标只关注图像真实性而忽略与目标元素的对齐

Method: 使用能量函数约束生成过程中的注意力图，使注意力更集中于服装区域；设计了新的评估指标VTID来全面评估生成质量

Result: 在VITON-HD和DressCode数据集上，在LPIPS、FID、KID和VTID指标上分别超越SOTA方法1.4%、2.3%、12.3%和5.8%；在下游CC-Reid任务中，在LTCC、PRCC、VC-Clothes数据集上Rank-1指标分别提升2.5%、1.1%和1.6%

Conclusion: 提出的CSC-VTON方法通过注意力约束机制显著提升了虚拟试衣的细节一致性，新设计的VTID指标为领域提供了更全面的评估标准

Abstract: VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.

</details>


### [160] [MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP](https://arxiv.org/abs/2512.07128)
*Chau Truong,Hieu Ta Quang,Dung D. Le*

Main category: cs.CV

TL;DR: MulCLIP：一种新颖的端到端多级对齐框架，通过全局对比对齐、局部特征校准和子标题聚合补丁对齐，有效解决CLIP模型在处理长文本描述时的局限性，提升细粒度理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（如CLIP）虽然在图像和文本对齐方面表现出色，但由于训练数据主要是简短描述，在处理长文本、详细描述时存在困难。虽然已有方法利用区域提议信息来映射视觉区域与长文本句子，但这会带来显著的部署成本。

Method: MulCLIP采用多级对齐框架：1）保持图像与摘要及长标题的全局对比对齐，并扩展位置嵌入以支持更长文本序列；2）提出基于局部校准特征的标记重建对齐，增强单词与图像补丁之间的语义连接；3）提出子标题聚合补丁对齐，自动提取和聚合每个子标题的上下文丰富补丁。

Result: 在多个基准测试上的实验结果表明，该方法能持续提升下游任务性能。消融研究证实其多尺度对齐是驱动比区域提议辅助方法更好细粒度能力的关键因素。

Conclusion: MulCLIP通过创新的多级对齐策略，有效解决了CLIP在处理长文本描述时的局限性，在保持部署效率的同时显著提升了细粒度理解能力，特别适合多样化的现实世界应用。

Abstract: Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.

</details>


### [161] [Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models](https://arxiv.org/abs/2512.07141)
*Fenghua Weng,Chaochao Lu,Xia Hu,Wenqi Shao,Wenjie Wang*

Main category: cs.CV

TL;DR: TRR是一个三阶段训练框架，通过策略引导的自我反思增强大型视觉语言模型的安全对齐，将安全响应率从42.8%提升到87.7%


<details>
  <summary>Details</summary>
Motivation: 现有单次推理范式容易受到上下文或视觉越狱攻击，无法有效识别自身输出中的有害内容，需要利用反思机制实现真正的自我修正

Method: 提出Think-Reflect-Revise三阶段框架：1) 构建包含5000个例子的ReSafe数据集；2) 使用该数据集微调模型初始化反思行为；3) 通过强化学习加强策略引导的反思

Result: TRR显著提升了LVLMs的安全性能，在Qwen2.5-VL-7B上将安全响应率从42.8%提高到87.7%，同时在MMMU和MMStar等通用基准上保持稳定性能

Conclusion: 通过反思机制利用单次推理中浪费的有害信号，TRR框架能有效增强LVLMs的安全对齐，实现真正的自我修正，防止不安全生成

Abstract: As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.

</details>


### [162] [CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics](https://arxiv.org/abs/2512.07155)
*Dahyeon Kye,Jeahun Sung,MinKyu Jeon,Jihyong Oh*

Main category: cs.CV

TL;DR: CHIMERA是一个零样本扩散模型框架，通过自适应缓存注入和语义锚点提示实现平滑图像变形，解决了现有方法过渡突兀和语义不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像变形任务中往往产生突兀的过渡或过饱和的外观，缺乏自适应的结构和语义对齐机制，导致变形效果不自然。

Method: 1. 将变形定义为缓存反转引导的去噪过程；2. 自适应缓存注入(ACI)：在DDIM反转时缓存输入图像的特征，在去噪时自适应重新注入；3. 语义锚点提示(SAP)：使用视觉语言模型生成共享锚点提示作为语义桥梁；4. 提出全局-局部一致性评分(GLCS)作为变形专用评估指标。

Result: 大量实验和用户研究表明，CHIMERA相比现有方法实现了更平滑、语义更一致的变形过渡，在图像变形任务上达到了新的最先进水平。

Conclusion: CHIMERA通过创新的自适应缓存注入和语义锚点提示机制，有效解决了扩散模型在图像变形中的挑战，为高质量图像变形提供了新的解决方案。

Abstract: Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.

</details>


### [163] [MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation](https://arxiv.org/abs/2512.07165)
*Muyu Xu,Fangneng Zhan,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: MuSASplat是一个用于稀疏视图3D高斯溅射的轻量级框架，通过多尺度适配器和特征融合聚合器大幅减少训练参数和计算成本，同时保持高质量的新视角合成效果。


<details>
  <summary>Details</summary>
Motivation: 现有的姿态无关前馈方法虽然能处理稀疏视图的3D高斯溅射，但通常需要完整微调大型ViT骨干网络，导致巨大的GPU计算成本。需要一种既能保持渲染质量又能显著降低计算负担的解决方案。

Method: 提出MuSASplat框架，包含两个核心组件：1) 轻量级多尺度适配器，仅微调ViT架构的一小部分参数；2) 特征融合聚合器，有效整合多视图特征，避免传统内存库的高内存和计算开销。

Result: 在多个数据集上的实验表明，MuSASplat在保持最先进渲染质量的同时，显著减少了参数数量和训练资源需求，相比现有方法具有明显优势。

Conclusion: MuSASplat通过创新的轻量级适配和高效特征融合设计，为稀疏视图3D高斯溅射提供了一种计算效率高且渲染质量优秀的解决方案，大幅降低了姿态无关前馈方法的训练成本。

Abstract: Sparse-view 3D Gaussian splatting seeks to render high-quality novel views of 3D scenes from a limited set of input images. While recent pose-free feed-forward methods leveraging pre-trained 3D priors have achieved impressive results, most of them rely on full fine-tuning of large Vision Transformer (ViT) backbones and incur substantial GPU costs. In this work, we introduce MuSASplat, a novel framework that dramatically reduces the computational burden of training pose-free feed-forward 3D Gaussian splats models with little compromise of rendering quality. Central to our approach is a lightweight Multi-Scale Adapter that enables efficient fine-tuning of ViT-based architectures with only a small fraction of training parameters. This design avoids the prohibitive GPU overhead associated with previous full-model adaptation techniques while maintaining high fidelity in novel view synthesis, even with very sparse input views. In addition, we introduce a Feature Fusion Aggregator that integrates features across input views effectively and efficiently. Unlike widely adopted memory banks, the Feature Fusion Aggregator ensures consistent geometric integration across input views and meanwhile mitigates the memory usage, training complexity, and computational costs significantly. Extensive experiments across diverse datasets show that MuSASplat achieves state-of-the-art rendering quality but has significantly reduced parameters and training resource requirements as compared with existing methods.

</details>


### [164] [When Privacy Meets Recovery: The Overlooked Half of Surrogate-Driven Privacy Preservation for MLLM Editing](https://arxiv.org/abs/2512.07166)
*Siyuan Xu,Yibing Liu,Peilin Chen,Yung-Hui Li,Shiqi Wang,Sam Kwong*

Main category: cs.CV

TL;DR: 该论文提出了一种在MLLMs中恢复受保护隐私数据的方法，通过构建SPPE数据集和条件生成框架，在保护隐私的同时保持MLLM编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs隐私保护方法虽然能有效隐藏隐私信息，但缺乏对隐私数据真实性和恢复质量的评估，无法在保护隐私的同时保证MLLM的可用性。

Method: 1) 构建SPPE数据集，包含多种隐私类别和用户指令，提供受保护代理及其MLLM编辑版本；2) 将隐私恢复建模为基于多模态信号的引导生成任务；3) 提出统一方法可靠重建隐私内容同时保持MLLM编辑保真度。

Result: 在SPPE和InstructPix2Pix数据集上的实验表明，该方法能很好地泛化到不同视觉内容和编辑任务，在隐私保护和MLLM可用性之间达到良好平衡。

Conclusion: 该工作填补了MLLMs隐私恢复评估的研究空白，提出的方法能有效恢复受保护隐私数据，为隐私保护与MLLM可用性的权衡提供了实用解决方案。

Abstract: Privacy leakage in Multimodal Large Language Models (MLLMs) has long been an intractable problem. Existing studies, though effectively obscure private information in MLLMs, often overlook the evaluation of the authenticity and recovery quality of user privacy. To this end, this work uniquely focuses on the critical challenge of how to restore surrogate-driven protected data in diverse MLLM scenarios. We first bridge this research gap by contributing the SPPE (Surrogate Privacy Protected Editable) dataset, which includes a wide range of privacy categories and user instructions to simulate real MLLM applications. This dataset offers protected surrogates alongside their various MLLM-edited versions, thus enabling the direct assessment of privacy recovery quality. By formulating privacy recovery as a guided generation task conditioned on complementary multimodal signals, we further introduce a unified approach that reliably reconstructs private content while preserving the fidelity of MLLM-generated edits. The experiments on both SPPE and InstructPix2Pix further show that our approach generalizes well across diverse visual content and editing tasks, achieving a strong balance between privacy protection and MLLM usability.

</details>


### [165] [TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration](https://arxiv.org/abs/2512.07171)
*Shravan Venkatraman,Rakesh Raj Madavan,Pavan Kumar S,Muthu Subash Kavitha*

Main category: cs.CV

TL;DR: TIDE是一个两阶段逆退化估计框架，通过专门先验分解来建模水下图像退化特征并进行针对性修复，将退化分解为四个关键因素并设计专门修复专家，在标准基准和浑浊水域条件下都表现出色。


<details>
  <summary>Details</summary>
Motivation: 水下图像修复对于海洋应用至关重要，但现有方法通常对整个图像应用统一的修复策略，难以处理空间变化且同时发生的多种退化问题。需要一种能够明确建模退化特征并进行针对性修复的方法。

Method: 提出TIDE框架：1）将水下退化分解为四个关键因素：颜色失真、雾霾、细节损失和噪声；2）为每个因素设计专门的修复专家；3）生成专门的修复假设并根据局部退化模式自适应融合；4）通过渐进细化阶段校正残留伪影。

Result: 在标准基准和挑战性浑浊水域条件下的广泛实验表明，TIDE在基于参考的保真度指标上具有竞争力，同时在非参考感知质量指标上优于最先进方法，在颜色校正和对比度增强方面有显著改进。

Conclusion: TIDE通过明确建模退化特征和专门先验分解，有效解决了水下图像修复中复杂且空间变化的退化问题，能够平衡竞争性退化因素并在高度退化区域产生自然结果。

Abstract: Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\underline{t}$wo stage $\underline{i}$nverse $\underline{d}$egradation $\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.

</details>


### [166] [Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification](https://arxiv.org/abs/2512.07190)
*Pengfei Gu,Huimin Li,Haoteng Tang,Dongkuan,Xu,Erik Enriquez,DongChul Kim,Bin Fu,Danny Z. Chen*

Main category: cs.CV

TL;DR: 提出了一种新的拓扑引导分类框架，通过多尺度多过滤持续同调特征增强医学图像分类，在三个公开数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经网络要么强调像素强度特征而忽略基本解剖结构，要么只能通过单参数持续性捕获简单的拓扑特征，需要更全面的拓扑视角来识别复杂解剖结构。

Method: 1) 计算多分辨率/尺度的立方持续性图；2) 开发"葡萄园"算法将多个持续性图整合为单一稳定图；3) 设计基于交叉注意力的神经网络直接处理整合后的持续性图；4) 将拓扑嵌入与CNN或Transformer特征图融合。

Result: 在三个公开数据集上的评估显示，该方法相比强基线和最先进方法取得了持续且显著的改进，证明了全面拓扑视角对鲁棒和可解释医学图像分类的价值。

Conclusion: 通过整合多尺度和多过滤拓扑到端到端架构中，该方法增强了模型识别复杂解剖结构的能力，为医学图像分类提供了更全面的拓扑视角。

Abstract: Modern deep neural networks have shown remarkable performance in medical image classification. However, such networks either emphasize pixel-intensity features instead of fundamental anatomical structures (e.g., those encoded by topological invariants), or they capture only simple topological features via single-parameter persistence. In this paper, we propose a new topology-guided classification framework that extracts multi-scale and multi-filtration persistent topological features and integrates them into vision classification backbones. For an input image, we first compute cubical persistence diagrams (PDs) across multiple image resolutions/scales. We then develop a ``vineyard'' algorithm that consolidates these PDs into a single, stable diagram capturing signatures at varying granularities, from global anatomy to subtle local irregularities that may indicate early-stage disease. To further exploit richer topological representations produced by multiple filtrations, we design a cross-attention-based neural network that directly processes the consolidated final PDs. The resulting topological embeddings are fused with feature maps from CNNs or Transformers. By integrating multi-scale and multi-filtration topologies into an end-to-end architecture, our approach enhances the model's capacity to recognize complex anatomical structures. Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating the value of our comprehensive topological perspective for robust and interpretable medical image classification.

</details>


### [167] [RefLSM: Linearized Structural-Prior Reflectance Model for Medical Image Segmentation and Bias-Field Correction](https://arxiv.org/abs/2512.07191)
*Wenqi Zhao,Jiacheng Sang,Fenghua Cheng,Yonglu Shu,Dong Li,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 提出RefLSM模型，将Retinex反射率分解融入水平集分割，通过反射率分割解决医学图像不均匀光照问题，引入线性结构先验和松弛二值水平集提升精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临强度不均匀、噪声、边界模糊和结构不规则等挑战。传统水平集方法依赖近似偏置场估计，在严重非均匀成像条件下效果不佳。

Method: 提出反射率水平集模型(RefLSM)，将观察图像分解为反射率和偏置场分量，直接分割光照不变的反射率。引入线性结构先验指导平滑反射率梯度，采用松弛二值水平集避免重新初始化扩散，使用ADMM优化方案求解变分问题。

Result: 在多个医学影像数据集上的实验表明，RefLSM相比最先进的水平集方法，在分割精度、鲁棒性和计算效率方面均表现出优越性能。

Conclusion: RefLSM通过整合Retinex反射率分解，有效解决了医学图像分割中的光照不均匀问题，结合结构先验和优化策略，实现了更准确、鲁棒且高效的分割。

Abstract: Medical image segmentation remains challenging due to intensity inhomogeneity, noise, blurred boundaries, and irregular structures. Traditional level set methods, while effective in certain cases, often depend on approximate bias field estimations and therefore struggle under severe non-uniform imaging conditions. To address these limitations, we propose a novel variational Reflectance-based Level Set Model (RefLSM), which explicitly integrates Retinex-inspired reflectance decomposition into the segmentation framework. By decomposing the observed image into reflectance and bias field components, RefLSM directly segments the reflectance, which is invariant to illumination and preserves fine structural details. Building on this foundation, we introduce two key innovations for enhanced precision and robustness. First, a linear structural prior steers the smoothed reflectance gradients toward a data-driven reference, providing reliable geometric guidance in noisy or low-contrast scenes. Second, a relaxed binary level-set is embedded in RefLSM and enforced via convex relaxation and sign projection, yielding stable evolution and avoiding reinitialization-induced diffusion. The resulting variational problem is solved efficiently using an ADMM-based optimization scheme. Extensive experiments on multiple medical imaging datasets demonstrate that RefLSM achieves superior segmentation accuracy, robustness, and computational efficiency compared to state-of-the-art level set methods.

</details>


### [168] [HVQ-CGIC: Enabling Hyperprior Entropy Modeling for VQ-Based Controllable Generative Image Compression](https://arxiv.org/abs/2512.07192)
*Niu Yi,Xu Tianyi,Ma Mingming,Wang Xinkun*

Main category: cs.CV

TL;DR: HVQ-CGIC提出了一种基于VQ超先验的可控生成图像压缩框架，通过引入超先验到VQ索引的熵模型中，显著提升了率失真性能，相比现有方法在相同感知质量下可节省61.3%的比特率。


<details>
  <summary>Details</summary>
Motivation: 现有基于向量量化(VQ)的生成图像压缩方法通常使用静态全局概率分布来估计VQ索引的熵，无法适应每张图像的具体内容，导致比特率潜力未充分挖掘且难以实现灵活的码率控制。

Method: 提出了HVQ-CGIC框架，为VQ索引熵模型引入超先验建立了严格的数学基础，通过新颖的损失设计首次在基于VQ的生成图像压缩中实现了率失真平衡与控制，配合轻量级超先验估计网络。

Result: 在Kodak数据集上，与Control-GIC、CDC和HiFiC等方法相比，在保持相同LPIPS感知质量的情况下，平均节省61.3%的比特率，在率失真性能上显著优于当前最先进的生成压缩方法。

Conclusion: HVQ-CGIC有潜力成为基于VQGAN的图像压缩的基础组件，类似于超先验框架在神经图像压缩中的核心作用，为生成图像压缩提供了有效的率失真控制方案。

Abstract: Generative learned image compression methods using Vector Quantization (VQ) have recently shown impressive potential in balancing distortion and perceptual quality. However, these methods typically estimate the entropy of VQ indices using a static, global probability distribution, which fails to adapt to the specific content of each image. This non-adaptive approach leads to untapped bitrate potential and challenges in achieving flexible rate control. To address this challenge, we introduce a Controllable Generative Image Compression framework based on a VQ Hyperprior, termed HVQ-CGIC. HVQ-CGIC rigorously derives the mathematical foundation for introducing a hyperprior to the VQ indices entropy model. Based on this foundation, through novel loss design, to our knowledge, this framework is the first to introduce RD balance and control into vector quantization-based Generative Image Compression. Cooperating with a lightweight hyper-prior estimation network, HVQ-CGIC achieves a significant advantage in rate-distortion (RD) performance compared to current state-of-the-art (SOTA) generative compression methods. On the Kodak dataset, we achieve the same LPIPS as Control-GIC, CDC and HiFiC with an average of 61.3% fewer bits. We posit that HVQ-CGIC has the potential to become a foundational component for VQGAN-based image compression, analogous to the integral role of the HyperPrior framework in neural image compression.

</details>


### [169] [SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting](https://arxiv.org/abs/2512.07197)
*Seokhyun Youn,Soohyun Lee,Geonho Kim,Weeyoung Kwon,Sung-Ho Bae,Jihyong Oh*

Main category: cs.CV

TL;DR: 这篇综述首次系统梳理了高效3D/4D高斯溅射技术，将其分为参数压缩和结构压缩两大类，总结了方法趋势、数据集、评估指标，并讨论了当前局限和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射（3DGS）虽然能实现实时高保真3D重建和新视角合成，但其存储和渲染数百万高斯函数需要巨大的内存和计算资源，在4D动态场景中问题更加严重。因此需要研究高效压缩方法以降低冗余同时保持重建质量。

Method: 将现有高效3D/4D高斯溅射技术系统分类为两大方向：1）参数压缩：直接压缩高斯参数；2）结构压缩：重新组织高斯结构。对每类方法的核心思想和方法趋势进行全面总结，并涵盖广泛使用的数据集、评估指标和代表性基准比较。

Result: 提供了首个统一的3D/4D高效高斯溅射技术综述，建立了系统分类框架，总结了当前技术发展趋势，为研究者提供了全面的技术概览和比较基准。

Conclusion: 高效高斯溅射技术正在快速发展，但仍面临挑战。未来研究方向包括实现可扩展、紧凑、实时的静态和动态3D场景表示，需要进一步优化压缩方法并提升实际应用性能。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.

</details>


### [170] [Generating Storytelling Images with Rich Chains-of-Reasoning](https://arxiv.org/abs/2512.07198)
*Xiujie Song,Qi Jia,Shota Watanabe,Xiaoyi Pang,Ruijie Chen,Mengyue Wu,Kenny Q. Zhu*

Main category: cs.CV

TL;DR: 提出StorytellingPainter兩階段流程，結合LLM創意推理與T2I視覺合成，生成富含邏輯鏈結的敘事圖像，並建立專用評估框架。


<details>
  <summary>Details</summary>
Motivation: 敘事圖像能透過視覺線索傳達豐富的邏輯連接與故事，但因其語義複雜性而難以創作，導致此類圖像稀缺。需要探索生成式AI如何創造這類圖像。

Method: 提出StorytellingPainter兩階段流程：1) LLM生成故事敘述與視覺元素描述；2) T2I模型合成圖像。同時開發包含語義複雜度、多樣性、故事-圖像對齊的三個評估器。為縮小開源與專有LLM差距，訓練Mini-Storytellers模型。

Result: 實驗結果證明方法的可行性與有效性，成功生成富含邏輯鏈結的敘事圖像，並提供可用的評估框架。

Conclusion: StorytellingPainter結合LLM與T2I模型能有效生成敘事圖像，專用評估框架確保生成質量，Mini-Storytellers模型縮小開源與專有LLM性能差距，為敘事圖像生成提供可行方案。

Abstract: An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.

</details>


### [171] [Understanding Diffusion Models via Code Execution](https://arxiv.org/abs/2512.07201)
*Cheng Yu*

Main category: cs.CV

TL;DR: 一篇约300行代码的简洁实现，从代码执行角度解释扩散模型，包含前向扩散、反向采样、噪声预测网络和训练循环等核心组件


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模中表现出色，但其理论基础复杂，且论文中的数学公式与开源实现之间存在差距。现有教程主要关注公式推导，对代码如何实际运行指导有限

Method: 提供一个约300行代码的简洁实现，从代码执行角度解释扩散模型，保留前向扩散、反向采样、噪声预测网络和训练循环等核心组件，去除不必要的工程细节

Result: 提供了一个清晰、以实现为先的理解方式，展示了扩散模型在实践中如何工作，以及代码与理论如何对应。代码和预训练模型已开源

Conclusion: 该技术报告为研究人员提供了从代码执行角度理解扩散模型的清晰途径，弥合了理论与实现之间的差距

Abstract: Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.

</details>


### [172] [MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning](https://arxiv.org/abs/2512.07203)
*Xuhui Zheng,Kang An,Ziliang Wang,Yuhang Wang,Faqiang Qian,Yichao Wu*

Main category: cs.CV

TL;DR: MMRPT是一个基于强化学习的掩码多模态预训练框架，通过奖励视觉基础而非文本模仿来增强多模态大模型的视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态预训练受限于图像-描述对的描述性偏差，导致模型更依赖表面语言线索而非真正的视觉理解。需要一种能强化视觉推理的预训练方法。

Method: 1) 通过视觉token的注意力估计句子级别的视觉依赖性，掩码高度依赖视觉的文本片段；2) 引入强化学习，使用语义-视觉奖励引导模型通过视觉基础推理重建被掩码的文本片段；3) 将强化学习直接整合到大型视觉语言模型的预训练中。

Result: 实验显示：1) 在多个基准测试中获得一致的零样本性能提升；2) 在有监督微调下显著提高了鲁棒性；3) 证明强化驱动的掩码推理为多模态模型提供了更可靠和可泛化的预训练目标。

Conclusion: MMRPT通过强化学习奖励视觉基础而非文本模仿，有效增强了多模态模型的视觉推理能力，提供了更可靠和可泛化的预训练方法。

Abstract: Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.

</details>


### [173] [AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT](https://arxiv.org/abs/2512.07206)
*Boyang Pan,Zeyu Zhang,Hongyu Meng,Bin Cui,Yingying Zhang,Wenli Hou,Junhao Li,Langdi Zhong,Xiaoxiao Chen,Xiaoyu Xu,Changjin Zuo,Chao Cheng,Nan-Jie Gong*

Main category: cs.CV

TL;DR: AutoLugano是一个全自动深度学习系统，能够从FDG-PET/CT扫描中自动进行淋巴瘤病灶分割、解剖定位和Lugano分期，在外部验证中达到88.31%的总体准确率。


<details>
  <summary>Details</summary>
Motivation: 开发一个端到端的自动化系统，从基线FDG-PET/CT扫描中自动完成淋巴瘤分类，包括病灶分割、解剖定位和Lugano分期，以辅助临床决策和治疗分层。

Method: 系统包含三个顺序模块：1) 基于3D nnU-Net的解剖感知病灶分割；2) 使用TotalSegmentator工具包进行基于图谱的解剖定位，将病灶映射到21个预定义淋巴结区域；3) 自动Lugano分期，将受累区域的空间分布转化为Lugano分期和治疗分组。

Result: 在外部验证集上，区域受累检测的总体准确率为88.31%，敏感性74.47%，特异性94.21%，F1分数80.80%。在关键的治疗分层任务中，准确率达到85.07%，特异性90.48%，敏感性82.61%。

Conclusion: AutoLugano是首个全自动端到端系统，能够将单个FDG-PET/CT扫描转化为完整的Lugano分期，在初始分期、治疗分层和临床决策支持方面具有强大潜力。

Abstract: Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.

</details>


### [174] [Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds](https://arxiv.org/abs/2512.07211)
*Frederik Hagelskjær,Dimitrios Arapis,Steffen Madsen,Thorbjørn Mosekjær Iversen*

Main category: cs.CV

TL;DR: 提出首个仅使用3D无色数据的深度学习物体姿态不确定性估计方法，解决工业场景中颜色信息缺失的问题


<details>
  <summary>Details</summary>
Motivation: 传统单姿态估计无法捕捉视觉模糊性带来的不确定性，现有姿态分布方法过度依赖颜色信息，而工业场景中颜色信息往往不可用

Method: 基于神经网络的3D无色数据姿态不确定性估计方法，专注于反射和旋转对称性，框架可扩展至完整SE(3)姿态分布估计

Result: 在具有不同几何模糊性的真实世界分拣场景中验证了方法的有效性，源代码已公开

Conclusion: 该方法首次实现了不依赖RGB输入的深度学习姿态分布估计，为工业机器人感知提供了可靠的姿态不确定性量化方案

Abstract: Object pose estimation is crucial to robotic perception and typically provides a single-pose estimate. However, a single estimate cannot capture pose uncertainty deriving from visual ambiguity, which can lead to unreliable behavior. Existing pose distribution methods rely heavily on color information, often unavailable in industrial settings.
  We propose a novel neural network-based method for estimating object pose uncertainty using only 3D colorless data. To the best of our knowledge, this is the first approach that leverages deep learning for pose distribution estimation without relying on RGB input. We validate our method in a real-world bin picking scenario with objects of varying geometric ambiguity. Our current implementation focuses on symmetries in reflection and revolution, but the framework is extendable to full SE(3) pose distribution estimation. Source code available at opde3d.github.io

</details>


### [175] [ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery](https://arxiv.org/abs/2512.07229)
*Fang Zhou,Zhiqiang Chen,Martin Pavlovski,Yizhong Zhang*

Main category: cs.CV

TL;DR: ReLKD是一个端到端的广义类别发现框架，通过利用隐式类别间关系来增强新类别的分类，包含三个关键模块：目标粒度模块、粗粒度模块和蒸馏模块。


<details>
  <summary>Details</summary>
Motivation: 广义类别发现(GCD)面临对包含已知和未知类别的未标记数据进行分类的挑战，现有方法通常独立处理每个类别，忽略了类别间的内在关系，而直接获取这些关系在现实场景中具有显著挑战。

Method: 提出ReLKD框架，包含三个模块：1)目标粒度模块学习判别性表示；2)粗粒度模块捕获层次化类别关系；3)蒸馏模块将知识从粗粒度模块转移到目标粒度模块以优化表示学习。

Result: 在四个数据集上的广泛实验证明了ReLKD的有效性，特别是在标记数据有限的场景下表现优异。

Conclusion: ReLKD通过利用隐式类别间关系有效增强了广义类别发现中新类别的分类性能，特别是在数据标注有限的情况下。

Abstract: Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.

</details>


### [176] [STRinGS: Selective Text Refinement in Gaussian Splatting](https://arxiv.org/abs/2512.07230)
*Abhinav Raundhal,Gaurav Behera,P J Narayanan,Ravi Kiran Sarvadevabhatla,Makarand Tapaswi*

Main category: cs.CV

TL;DR: STRinGS：针对3D高斯泼溅（3DGS）的文本感知选择性优化框架，通过分别处理文本和非文本区域，显著提升3D重建中文本的可读性


<details>
  <summary>Details</summary>
Motivation: 现实场景中的文本（如标志、标签、说明）包含重要上下文信息，但现有3D表示方法（如3DGS）难以保留细粒度文本细节，文本重建的小误差会导致显著的语义损失

Method: 提出STRinGS框架，将文本和非文本区域分开处理：先优化文本区域，再与非文本区域合并进行全场景优化；引入OCR字符错误率（CER）作为文本可读性评估指标

Result: STRinGS在仅7K次迭代下，相比3DGS在文本区域获得63.6%的相对改进；创建了STRinGS-360数据集，包含多样化的文本场景用于评估3D重建中的文本可读性

Conclusion: STRinGS方法和数据集共同推动了文本丰富环境中3D场景理解的边界，为更鲁棒的文本感知重建方法铺平了道路

Abstract: Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.

</details>


### [177] [Unified Camera Positional Encoding for Controlled Video Generation](https://arxiv.org/abs/2512.07237)
*Cheng Zhang,Boying Li,Meng Wei,Yan-Pei Cao,Camilo Cruz Gambardella,Dinh Phung,Jianfei Cai*

Main category: cs.CV

TL;DR: 提出UCPE（统一相机位置编码），通过相对射线编码和绝对方向编码，在预训练视频扩散Transformer中实现精确的相机控制，仅增加不到1%的可训练参数。


<details>
  <summary>Details</summary>
Motivation: 现有相机编码方法通常基于简化的针孔假设，限制了在真实世界相机多样内参和镜头畸变下的泛化能力。需要一种几何一致的表示来统一完整的相机信息。

Method: 提出相对射线编码（Relative Ray Encoding）统一相机位姿、内参和镜头畸变；识别俯仰和横滚作为绝对方向编码的有效组件；通过轻量级空间注意力适配器集成到预训练视频扩散Transformer中。

Result: 在相机可控文本到视频生成任务中达到最先进的相机控制能力和视觉保真度；构建了包含广泛相机运动和镜头类型的大规模视频数据集；验证了UCPE作为Transformer通用相机表示的潜力。

Conclusion: UCPE提供了一种几何一致的相机表示，能够精确控制相机参数，为未来多视图、视频和3D任务中的Transformer提供了通用的相机编码解决方案。

Abstract: Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.

</details>


### [178] [Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture](https://arxiv.org/abs/2512.07241)
*Md. Srabon Chowdhury,Syeda Fahmida Tanzim,Sheekar Banerjee,Ishtiak Al Mamoon,AKM Muzahidul Islam*

Main category: cs.CV

TL;DR: 提出结合SqueezeNet v1和EfficientNet-B0的混合深度学习模型，加入手工放射组学特征，用于脑肿瘤MRI自动分类，在公开数据集上达到98.93%准确率。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤诊断需要及时准确，但MRI肿瘤勾画过程困难、耗时且易受观察者间差异影响。现有深度学习模型在计算效率和诊断准确性之间存在权衡。

Method: 结合轻量级SqueezeNet v1和高性能EfficientNet-B0的混合深度学习模型，增强手工放射组学特征（HOG、LBP、Gabor滤波器和小波变换），使用公开的Nickparvar脑肿瘤MRI数据集（7,023张T1加权轴位MRI切片）。

Result: 模型测试准确率达到98.93%，使用测试时间增强后提升至99.08%。模型参数少于210万，计算量低于1.2 GFLOPs，在计算效率和诊断准确性之间取得良好平衡。

Conclusion: 提出的混合网络在脑肿瘤MRI自动分类中表现出接近临床可靠性的性能，具有在临床决策支持系统中应用的潜力。

Abstract: Brain tumors are one of the most common and dangerous neurological diseases which require a timely and correct diagnosis to provide the right treatment procedures. Even with the promotion of magnetic resonance imaging (MRI), the process of tumor delineation is difficult and time-consuming, which is prone to inter-observer error. In order to overcome these limitations, this work proposes a hybrid deep learning model based on SqueezeNet v1 which is a lightweight model, and EfficientNet-B0, which is a high-performing model, and is enhanced with handcrafted radiomic descriptors, including Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gabor filters and Wavelet transforms. The framework was trained and tested only on publicly available Nickparvar Brain Tumor MRI dataset, which consisted of 7,023 contrast-enhanced T1-weighted axial MRI slices which were categorized into four groups: glioma, meningioma, pituitary tumor, and no tumor. The testing accuracy of the model was 98.93% that reached a level of 99.08% with Test Time Augmentation (TTA) showing great generalization and power. The proposed hybrid network offers a compromise between computation efficiency and diagnostic accuracy compared to current deep learning structures and only has to be trained using fewer than 2.1 million parameters and less than 1.2 GFLOPs. The handcrafted feature addition allowed greater sensitivity in texture and the EfficientNet-B0 backbone represented intricate hierarchical features. The resulting model has almost clinical reliability in automated MRI-based classification of tumors highlighting its possibility of use in clinical decision-support systems.

</details>


### [179] [Zero-Shot Textual Explanations via Translating Decision-Critical Features](https://arxiv.org/abs/2512.07245)
*Toshinori Yamauchi,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.CV

TL;DR: TEXTER通过隔离决策关键特征来生成更忠实、可解释的图像分类器文本解释，相比现有方法能更好地反映模型推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有零样本解释方法通常将全局图像特征与语言对齐，只能描述可见内容而非驱动预测的关键因素。大型视觉语言模型虽然能生成标题，但并非专门为分类器特定推理设计。

Method: TEXTER首先识别对预测有贡献的神经元，强调这些神经元编码的决策关键特征，然后将这些强调的特征映射到CLIP特征空间以检索反映模型推理的文本解释。稀疏自编码器进一步提高了Transformer架构的可解释性。

Result: 大量实验表明，TEXTER生成的解释比现有方法更忠实、更可解释。

Conclusion: TEXTER通过隔离决策关键特征的方法，能够生成更准确地反映图像分类器推理过程的文本解释，提高了透明度和可解释性。

Abstract: Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.

</details>


### [180] [AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing](https://arxiv.org/abs/2512.07247)
*Ziming Hong,Tianyu Huang,Runnan Chen,Shanshan Ye,Mingming Gong,Bo Han,Tongliang Liu*

Main category: cs.CV

TL;DR: AdLift：首个3D高斯泼溅（3DGS）编辑保护方法，通过将严格有界的2D对抗扰动提升为3D高斯表示的安全防护，防止任意视角和维度的指令驱动编辑。


<details>
  <summary>Details</summary>
Motivation: 随着基于扩散的指令驱动2D图像编辑扩展到3DGS，虽然促进了3D内容创作，但也使3DGS资产面临未经授权编辑和恶意篡改的风险。现有2D图像的对抗扰动保护方法难以直接应用于3DGS，面临视角泛化保护和可见性与保护能力平衡两大挑战。

Method: 提出AdLift方法：1）将严格有界的2D对抗扰动提升为3D高斯表示的安全防护；2）使用定制的Lifted PGD进行渐进优化，在训练视角间交替进行梯度截断和图像到高斯拟合操作，确保对抗扰动有效且不可见。

Result: 实验结果表明，AdLift能有效防止最先进的指令驱动2D图像和3DGS编辑，在不同视角下提供一致的对抗性保护性能，并能泛化到新视角。

Conclusion: AdLift是首个针对3DGS的编辑保护方法，成功解决了视角泛化保护和可见性-保护能力平衡的挑战，为3DGS资产提供了有效的安全防护。

Abstract: Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.

</details>


### [181] [See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement](https://arxiv.org/abs/2512.07251)
*Junqi Liu,Zejun Wu,Pedro R. A. S. Bassi,Xinze Zhou,Wenxuan Li,Ibrahim E. Hamamci,Sezgin Er,Tianyu Lin,Yi Luo,Szymon Płotka,Bjoern Menze,Daguang Xu,Kai Ding,Kang Wang,Yang Yang,Yucheng Tang,Alan L. Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: SMILE是一个解剖感知的扩散模型，用于医学图像增强，它学习器官形状和对比剂摄取模式，只增强临床相关区域而不改变其他区域。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像增强模型经常过度编辑，导致器官变形、产生虚假发现并遗漏小肿瘤，因为它们不理解解剖结构或对比剂动力学。

Method: 提出SMILE模型，采用三个关键技术：(1)结构感知监督，遵循真实器官边界和对比模式；(2)无需配准的学习，直接处理未对齐的多期相CT扫描；(3)统一推理，在所有对比期相中提供快速一致的增强。

Result: 在六个外部数据集上，SMILE在图像质量方面优于现有方法（SSIM提高14.2%，PSNR提高20.6%，FID改善50%），并产生解剖准确和诊断意义的图像。还能提高非对比CT的癌症检测，F1分数提升达10%。

Conclusion: SMILE通过解剖感知的增强方法，在保持解剖准确性的同时提高医学图像质量，支持临床决策并改善癌症检测性能。

Abstract: Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.

</details>


### [182] [A graph generation pipeline for critical infrastructures based on heuristics, images and depth data](https://arxiv.org/abs/2512.07269)
*Mike Diessner,Yannick Tarant*

Main category: cs.CV

TL;DR: 提出基於攝影測量的圖生成流程，使用立體相機和深度學習檢測關鍵基礎設施中的物體及其關係，替代昂貴的雷射掃描


<details>
  <summary>Details</summary>
Motivation: 傳統使用雷射掃描器獲取3D點雲來建立關鍵基礎設施虛擬模型成本高昂且需要專業知識，需要更經濟實惠的替代方案

Method: 使用立體相機獲取RGB影像和深度數據，透過深度學習進行物體檢測和實例分割，結合用戶定義的啟發式規則推斷物體關係，建立圖結構表示

Result: 在兩個水力系統上的實驗顯示，該方法能生成接近真實情況的圖結構，具有靈活性和透明度

Conclusion: 提出的攝影測量圖生成流程提供了一種成本效益高、可定制且透明的關鍵基礎設施建模方法，適合高風險決策應用

Abstract: Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.

</details>


### [183] [RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2512.07273)
*Zhi Rao,Yucheng Zhou,Benjia Zhou,Yiqing Huang,Sergio Escalera,Jun Wan*

Main category: cs.CV

TL;DR: 提出RVLF三阶段框架解决无注释手语翻译的两大挑战：通过融合骨架运动与视觉特征改进手语表示，结合GRPO强化学习优化句子级语义对齐，在多个数据集上显著提升BLEU分数。


<details>
  <summary>Details</summary>
Motivation: 当前无注释手语翻译面临两大挑战：1）现有手语表示方法无法捕捉细微的视觉线索；2）基于LLM的方法存在句子级语义错位问题，限制了翻译质量。

Method: 提出三阶段强化视觉语言框架RVLF：1）构建专门的手语大视觉语言模型，融合骨架运动线索与DINOv2提取的视觉特征，通过指令调优获得SLT-SFT基线；2）引入GRPO优化策略，结合BLEU和ROUGE奖励函数微调模型，获得SLT-GRPO模型。

Result: 在CSL-Daily、PHOENIX-2014T、How2Sign和OpenASL数据集上，BLEU-4分数分别提升+5.1、+1.11、+1.4和+1.61，无需外部大规模手语数据集预训练。

Conclusion: RVLF框架有效解决了手语表示不足和句子级语义错位问题，首次将GRPO引入手语翻译，实验验证了该方法在提升翻译质量和语义一致性方面的有效性。

Abstract: Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.

</details>


### [184] [Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery](https://arxiv.org/abs/2512.07276)
*Mai Tsujimoto,Junjue Wang,Weihao Xuan,Naoto Yokoya*

Main category: cs.CV

TL;DR: Geo3DVQA是一个用于评估视觉语言模型在仅使用RGB遥感影像进行3D地理空间推理能力的基准测试，包含11万个问题-答案对，涵盖16个任务类别和三个复杂度级别，现有VLMs表现不佳，领域特定微调可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D地理空间分析方法依赖昂贵的专业传感器（如LiDAR和多光谱），限制了全球可访问性。现有的传感器驱动和规则驱动方法难以整合多个3D线索、处理多样化查询并提供可解释的推理。需要开发仅使用RGB影像就能进行3D地理空间推理的方法。

Method: 提出了Geo3DVQA基准测试，包含11万个精心策划的问题-答案对，涵盖16个任务类别，分为三个复杂度级别：单特征推理、多特征推理和应用级空间分析。评估了10个最先进的视觉语言模型，并进行了领域特定微调实验。

Result: 评估结果显示当前VLMs在RGB到3D推理方面表现困难：GPT-4o准确率28.6%，Gemini-2.5-Flash准确率33.0%。而领域特定微调的Qwen2.5-VL-7B模型达到了49.6%的准确率，提升了24.8个百分点。

Conclusion: Geo3DVQA揭示了当前视觉语言模型在3D地理空间推理方面的局限性，同时证明了领域适应的有效性。该基准为可扩展、可访问和全面的3D地理空间分析设立了新的挑战前沿。

Abstract: Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.

</details>


### [185] [Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset](https://arxiv.org/abs/2512.07305)
*Tobias Abraham Haider*

Main category: cs.CV

TL;DR: 重新评估预训练CNN模型在野生动物物种识别中的表现，使用新数据集验证原研究的可重复性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 验证Carl等人研究的可重复性和泛化性，评估预训练模型在不同数据集上的表现，探索野生动物物种自动识别的实际应用潜力

Method: 重新实现原实验，使用公开可用资源，采用900张图像覆盖90个物种的新数据集，进行最小预处理，评估Google Inception-ResNet-v2模型的分类性能

Result: 获得62%的整体分类准确率，与原研究的71%接近；各类别性能差异显著，宏观F1分数为0.28，表明模型在标签与ImageNet类别不完全匹配时泛化能力有限

Conclusion: 预训练CNN可作为野生动物物种识别的基础方法，但需要物种特定的适应或迁移学习才能获得一致的高质量预测结果

Abstract: This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, closely aligning with the 71% reported in the original work despite differences in datasets. As in the original study, per-class performance varied substantially, as indicated by a macro F1 score of 0.28,highlighting limitations in generalization when labels do not align directly with ImageNet classes. Our results confirm that pretrained convolutional neural networks can provide a practical baseline for wildlife species identification but also reinforce the need for species-specific adaptation or transfer learning to achieve consistent, high-quality predictions.

</details>


### [186] [The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers](https://arxiv.org/abs/2512.07331)
*Kanishk Awadhiya*

Main category: cs.CV

TL;DR: ViTs在训练中会自发形成"U形"熵分布，这种"归纳瓶颈"是数据依赖的适应性机制，而非架构缺陷，其深度与任务所需的语义抽象程度相关。


<details>
  <summary>Details</summary>
Motivation: 尽管ViTs理论上可以在所有层保持高维表示，但实际观察发现它们经常自发形成"U形"熵分布（中间层压缩信息，最后层再扩展）。本研究旨在探究这种"归纳瓶颈"的本质，是架构缺陷还是数据驱动的适应性机制。

Method: 通过分析DINO训练的ViTs在不同数据集（UC Merced、Tiny ImageNet、CIFAR-100）上的层间有效编码维度（EED），研究瓶颈深度与任务语义抽象需求之间的相关性。

Result: 研究发现：1）瓶颈深度与任务所需的语义抽象程度强相关；2）纹理丰富的数据集在整个网络中保持高秩表示；3）以物体为中心的数据集驱动网络在中间层抑制高频信息，有效"学习"瓶颈来隔离语义特征。

Conclusion: ViTs中的"归纳瓶颈"不是架构缺陷，而是数据依赖的适应性机制，网络根据任务复杂性自动调整表示压缩程度，以更好地提取语义特征。

Abstract: Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a "U-shaped" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this "Inductive Bottleneck" is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively "learning" a bottleneck to isolate semantic features.

</details>


### [187] [Generalized Referring Expression Segmentation on Aerial Photos](https://arxiv.org/abs/2512.07338)
*Luís Marnoto,Alexandre Bernardino,Bruno Martins*

Main category: cs.CV

TL;DR: Aerial-D是一个新的大规模航空图像指代表达分割数据集，包含37,288张图像和1,522,523个指代表达，覆盖259,709个标注目标，包含21个类别。采用自动流水线结合规则生成和LLM增强构建，支持现代和历史航空图像的分割任务。


<details>
  <summary>Details</summary>
Motivation: 航空图像指代表达分割面临独特挑战：空间分辨率差异大、色彩使用不一致、目标像素少、物体密度高、部分遮挡等。现有数据集不足以应对这些挑战，需要专门针对航空图像的大规模数据集。

Method: 1. 构建Aerial-D数据集：通过全自动流水线结合系统化规则生成和大型语言模型增强，丰富语言多样性和视觉细节关注；2. 使用滤波器模拟历史成像条件；3. 采用RSRefSeg架构，在Aerial-D和先前航空数据集上联合训练模型。

Result: 联合训练在当代基准测试中取得竞争性性能，同时在单色、棕褐色和颗粒状退化（历史航空摄影中常见）条件下保持强准确性。数据集、训练模型和完整软件流水线已公开。

Conclusion: Aerial-D为航空图像指代表达分割提供了大规模、多样化的数据集，支持现代和历史图像的实例和语义分割，推动了该领域的发展。

Abstract: Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .

</details>


### [188] [Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting](https://arxiv.org/abs/2512.07345)
*Shilong Jin,Haoran Duan,Litao Hua,Wentao Huang,Yuan Zhou*

Main category: cs.CV

TL;DR: TD-Attn是一个解决T2I扩散模型中先验视角偏见的框架，通过3D感知注意力引导和分层注意力调制来增强多视角一致性，可作为通用插件用于各种3D任务。


<details>
  <summary>Details</summary>
Motivation: 现有的从文本到图像扩散模型蒸馏的3D任务存在先验视角偏见问题，导致不同视角间的外观冲突。这种偏见使得主体词在跨注意力计算中优先激活先验视角特征，而忽略目标视角条件。

Method: 提出TD-Attn框架，包含两个核心组件：1) 3D感知注意力引导模块，构建视角一致的3D注意力高斯分布来增强空间一致性；2) 分层注意力调制模块，使用语义引导树和语义响应分析器来定位和调制对视角条件敏感的跨注意力层。

Result: 实验表明TD-Attn能显著提升多视角一致性，可作为通用插件增强各种3D任务的表现。

Conclusion: TD-Attn通过数学分析揭示了T2I模型中先验视角偏见的根源，并提出有效的解决方案，为3D生成和编辑任务提供了更好的多视角一致性。

Abstract: Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.

</details>


### [189] [MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition](https://arxiv.org/abs/2512.07348)
*Xinyu Wei,Kangrui Cen,Hongyang Wei,Zhen Guo,Bairui Li,Zeqing Wang,Jinrui Zhang,Lei Zhang*

Main category: cs.CV

TL;DR: 论文提出了MICo-150K数据集、MICo-Bench基准和Qwen-MICo基线模型，用于解决多图像组合生成任务的数据缺乏和评估问题。


<details>
  <summary>Details</summary>
Motivation: 多图像组合生成任务缺乏高质量训练数据，现有方法难以合成一致且连贯的多参考图像，需要系统性的数据集和评估基准。

Method: 1) 将MICo分类为7个代表性任务；2) 收集高质量源图像并构建多样化的MICo提示；3) 使用强大模型合成平衡的合成图像，通过人工筛选和精炼得到MICo-150K数据集；4) 构建分解-重组子集；5) 创建MICo-Bench评估基准和Weighted-Ref-VIEScore指标；6) 在MICo-150K上微调多个模型。

Result: MICo-150K有效提升了模型的多图像组合能力，基线模型Qwen-MICo在3图像组合任务上达到Qwen-Image-2509水平，且支持任意多图像输入。数据集、基准和模型为多图像组合研究提供了宝贵资源。

Conclusion: 该工作系统性地解决了多图像组合生成的数据和评估问题，提出的资源将推动该领域进一步发展。

Abstract: In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.

</details>


### [190] [Enhancing Small Object Detection with YOLO: A Novel Framework for Improved Accuracy and Efficiency](https://arxiv.org/abs/2512.07379)
*Mahila Moghadami,Mohammad Ali Keyvanrad,Melika Sabaghian*

Main category: cs.CV

TL;DR: 本文提出了一种改进的SW-YOLO方法，用于大规模航拍图像中的小目标检测，通过优化滑动窗口裁剪策略和网络架构改进，在VisDrone2019数据集上取得了显著精度提升。


<details>
  <summary>Details</summary>
Motivation: 随着航拍图像在各种关键和工业应用中的重要性日益增长，需要鲁棒的小目标检测框架。当前方法通常涉及图像裁剪和检测器架构修改，但仍有改进空间。

Method: 基于SW-YOLO方法，优化滑动窗口的裁剪尺寸和重叠度，并在架构上改进：在颈部添加高级特征提取模块增强特征图，在骨干网络集成CBAM以保留空间和通道信息，引入新头部提升小目标检测精度。

Result: 在VisDrone2019数据集上，将mAP .5:.5精度从YOLOv5L的35.5提升到61.2，显著优于SAHI和CZDet（58.36）等现有方法。

Conclusion: 提出的方法通过结合优化裁剪策略和网络架构改进，显著提升了航拍图像中小目标检测的精度，为大规模航拍图像处理提供了有效解决方案。

Abstract: This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial applications, the need for robust frameworks for small object detection becomes imperative. To address this need, we adopted the base SW-YOLO approach to enhance speed and accuracy in small object detection by refining cropping dimensions and overlap in sliding window usage and subsequently enhanced it through architectural modifications. we propose a novel model by modifying the base model architecture, including advanced feature extraction modules in the neck for feature map enhancement, integrating CBAM in the backbone to preserve spatial and channel information, and introducing a new head to boost small object detection accuracy. Finally, we compared our method with SAHI, one of the most powerful frameworks for processing large-scale images, and CZDet, which is also based on image cropping, achieving significant improvements in accuracy. The proposed model achieves significant accuracy gains on the VisDrone2019 dataset, outperforming baseline YOLOv5L detection by a substantial margin. Specifically, the final proposed model elevates the mAP .5.5 accuracy on the VisDrone2019 dataset from the base accuracy of 35.5 achieved by the YOLOv5L detector to 61.2. Notably, the accuracy of CZDet, which is another classic method applied to this dataset, is 58.36. This research demonstrates a significant improvement, achieving an increase in accuracy from 35.5 to 61.2.

</details>


### [191] [Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects](https://arxiv.org/abs/2512.07381)
*Shuohan Tao,Boyao Zhou,Hanzhang Tu,Yuwang Wang,Yebin Liu*

Main category: cs.CV

TL;DR: 提出Tessellation GS方法，将2D高斯约束在网格面上，通过层次神经特征推断属性，结合自适应面细分策略和重建基础模型先验，实现单相机动态场景重建。


<details>
  <summary>Details</summary>
Motivation: 传统3D高斯泼溅在视角外推方面表现不佳，容易过拟合且泛化能力差，特别是在稀疏视图和动态场景重建中。现有方法难以从单个静态相机重建一般动态物体。

Method: 1) 将2D高斯约束在网格面上，通过层次神经特征推断属性；2) 使用细节感知损失函数驱动的自适应面细分策略指导高斯细分；3) 利用重建基础模型的先验初始化高斯变形。

Result: 在表观和网格重建任务上，LPIPS降低29.1%，Chamfer距离减少49.2%，优于先前SOTA方法，能够从单个静态相机重建一般动态物体。

Conclusion: Tessellation GS通过结构化方法有效解决了3D高斯泼溅在动态场景重建中的局限性，实现了从单相机的高质量动态场景重建，为优化方法开辟了新可能性。

Abstract: 3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.

</details>


### [192] [LogicCBMs: Logic-Enhanced Concept-Based Learning](https://arxiv.org/abs/2512.07383)
*Deepika SN Vemuri,Gautham Bellamkonda,Aditya Pola,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: 论文提出LogicCBM，在概念瓶颈模型基础上引入可微逻辑模块，用逻辑运算替代线性组合来提升模型表达能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型(CBMs)主要通过概念的线性组合进行预测，这种线性组合方式存在固有局限性，无法捕捉概念间的复杂关系。

Method: 提出LogicCBM，在CBM基础上引入精心设计的逻辑模块，通过可微逻辑运算连接学习到的概念，支持多种逻辑操作生成最终预测，同时保持端到端可学习性。

Result: 在知名基准测试和合成数据集上的实验表明，LogicCBM具有更好的准确性，能进行有效干预，并保持高度可解释性。

Conclusion: 通过逻辑运算增强概念瓶颈模型能提升模型表达能力，更好地捕捉概念间关系，同时保持可解释性优势。

Abstract: Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.

</details>


### [193] [How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline](https://arxiv.org/abs/2512.07385)
*Chunhui Zhang,Li Liu,Zhipeng Zhang,Yong Wang,Hao Wen,Xi Zhou,Shiming Ge,Yanfeng Wang*

Main category: cs.CV

TL;DR: 提出新的无人机反无人机视觉跟踪任务UAV-Anti-UAV，构建百万级数据集，并提出基于Mamba的基线方法MambaSTS


<details>
  <summary>Details</summary>
Motivation: 当前反无人机研究主要关注固定地面摄像头拍摄的RGB、红外或RGB-IR视频，缺乏从移动无人机平台跟踪目标无人机的研究。无人机反无人机任务面临双重动态干扰的挑战。

Method: 提出MambaSTS方法，使用Mamba和Transformer分别学习全局语义和空间特征，利用状态空间模型在长序列建模的优势，通过时间令牌传播机制建立视频级长期上下文。

Result: 构建了包含1,810个视频的百万级数据集，每个视频都有人工标注的边界框、语言提示和15个跟踪属性。对50种现代深度跟踪算法的实验评估表明，该领域仍有很大改进空间。

Conclusion: 提出了新的无人机反无人机跟踪任务和数据集，展示了该任务的挑战性，并提供了有效的基线方法，为未来研究奠定了基础。

Abstract: Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.

</details>


### [194] [GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring](https://arxiv.org/abs/2512.07391)
*Đorđe Nedeljković*

Main category: cs.CV

TL;DR: GlimmerNet：一种超轻量级卷积网络，通过分组扩张深度卷积实现多尺度特征提取，无需额外参数，在无人机应急监测任务中达到新的精度-效率平衡前沿。


<details>
  <summary>Details</summary>
Motivation: 虽然Vision Transformers通过自注意力机制增强了全局上下文理解，但引入了显著的计算开销。本研究旨在保留强大全局感知能力的同时，避免依赖计算昂贵的组件，特别针对资源受限的无人机平台实时应急监测任务。

Method: 提出GlimmerNet，基于"将感受野多样性与特征重组分离"的原则。核心创新包括：1）分组扩张深度卷积块（GDBlocks），将通道分组并为每组分配不同扩张率，实现无额外参数的多尺度特征提取；2）聚合器模块，使用分组点卷积高效重组跨组表征，显著降低参数开销。

Result: 仅用31K参数，比最新基线减少29%的FLOPs，在无人机聚焦的AIDERv2数据集上实现了0.966的加权F1分数，创下新的最先进水平。

Conclusion: GlimmerNet为资源受限无人机平台的实时应急监测建立了新的精度-效率平衡前沿，证明了无需计算昂贵组件也能实现强大全局感知的可能性。

Abstract: Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at https://github.com/djordjened92/gdd-cnn.

</details>


### [195] [Reconstructing Objects along Hand Interaction Timelines in Egocentric Video](https://arxiv.org/abs/2512.07394)
*Zhifan Zhu,Siddhant Bansal,Shashank Tripathi,Dima Damen*

Main category: cs.CV

TL;DR: 提出ROHIT任务，通过定义手交互时间线(HIT)来重建物体在手持交互过程中的3D姿态，使用约束优化传播(COP)框架提升重建精度


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在手持交互视频中准确重建物体3D姿态，特别是在手稳定抓握物体期间。需要一种无需3D真值就能评估重建质量的方法

Method: 定义手交互时间线(HIT)，包含物体从静止、被抓握、稳定使用到释放的过程。提出约束优化传播(COP)框架，利用HIT中的姿态约束传播物体姿态

Result: 在两个第一人称数据集(HOT3D和EPIC-Kitchens)上评估，COP将稳定抓握重建提升6.2-11.3%，HIT重建提升达24.5%

Conclusion: ROHIT任务和COP框架能有效重建手持交互过程中的物体3D姿态，无需3D真值即可评估，为日常交互视频中的物体重建提供了新方法

Abstract: We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.

</details>


### [196] [InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs](https://arxiv.org/abs/2512.07410)
*Bin Li,Ruichi Zhang,Han Liang,Jingyan Zhang,Juze Zhang,Xin Chen,Lan Xu,Jingyi Yu,Jingya Wang*

Main category: cs.CV

TL;DR: InterAgent：首个基于物理的文本驱动多智能体人形控制端到端框架，通过自回归扩散变换器和交互图外感受表示实现多智能体协调


<details>
  <summary>Details</summary>
Motivation: 现有方法主要局限于单智能体场景，忽视了多智能体交互中必要的物理合理相互作用，需要开发能够模拟人类社交行为复杂协调的多智能体控制框架

Method: 提出InterAgent框架，核心包括：1）配备多流块的自回归扩散变换器，解耦本体感受、外感受和动作以减少跨模态干扰；2）新颖的交互图外感受表示，显式捕捉细粒度关节间空间依赖；3）稀疏边基注意力机制，动态修剪冗余连接并强调关键智能体间空间关系

Result: InterAgent在多个强基线上表现优异，达到最先进性能，能够仅从文本提示生成连贯、物理合理且语义忠实的多智能体行为

Conclusion: InterAgent是首个基于物理的文本驱动多智能体人形控制框架，通过创新的架构设计成功实现了多智能体协调，为未来研究提供了重要基础

Abstract: Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.

</details>


### [197] [Unified Video Editing with Temporal Reasoner](https://arxiv.org/abs/2512.07469)
*Xiangpeng Yang,Ji Xie,Yiyuan Yang,Yan Huang,Min Xu,Qiang Wu*

Main category: cs.CV

TL;DR: VideoCoF提出了一种基于帧链推理的视频编辑方法，通过预测编辑区域潜在表示作为显式推理步骤，实现无需掩码的精确指令到区域对齐和细粒度编辑。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法面临关键权衡：专家模型精度高但依赖任务特定的掩码先验，难以统一；统一的时间上下文学习模型无需掩码但缺乏显式空间线索，导致指令到区域映射不精确和定位能力弱。

Method: 提出VideoCoF框架，采用"看、推理、再编辑"的帧链方法，强制视频扩散模型先预测推理标记（编辑区域潜在表示），再生成目标视频标记。引入RoPE对齐策略利用推理标记确保运动对齐并支持超出训练时长的长度外推。

Result: 仅用5万视频对的最小数据成本，在VideoCoF-Bench上达到最先进性能，验证了方法的效率和有效性。

Conclusion: VideoCoF通过显式推理步骤解决了视频编辑中精度与统一性的冲突，实现了无需用户提供掩码的精确指令到区域对齐和细粒度编辑，同时支持运动对齐和长度外推。

Abstract: Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.

</details>


### [198] [Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance](https://arxiv.org/abs/2512.07480)
*Naifu Xue,Zhaoyang Jia,Jiahao Li,Bin Li,Zihan Zheng,Yuan Zhang,Yan Lu*

Main category: cs.CV

TL;DR: S2VC：基于单步扩散的视频编解码器，通过条件编码框架和高效单步扩散生成器，在低码率下实现逼真重建，同时降低采样成本。


<details>
  <summary>Details</summary>
Motivation: 传统和神经视频编解码器在低码率下的感知质量仍有挑战：现有方法要么受限于生成能力产生伪影，要么依赖预训练扩散模型但采样复杂度高。需要一种既能提高感知质量又能降低计算成本的方法。

Method: 1. 提出S2VC单步扩散视频编解码器，结合条件编码框架和高效单步扩散生成器；2. 引入上下文语义指导，从缓冲特征中提取帧自适应语义，替代文本描述；3. 加入时间一致性指导，在扩散U-Net中强制帧间时间一致性。

Result: S2VC在感知质量上达到最先进水平，相比之前的感知方法平均节省52.73%的码率，证明了单步扩散在高效高质量视频压缩中的潜力。

Conclusion: S2VC通过单步扩散生成器和创新的语义与时间指导机制，成功解决了低码率视频压缩中感知质量与计算效率的平衡问题，为高效高质量视频压缩提供了新方向。

Abstract: While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.

</details>


### [199] [Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior](https://arxiv.org/abs/2512.07498)
*Chih-Chung Hsu,Shao-Ning Chen,Chia-Ming Lee,Yi-Fang Wang,Yi-Shiuan Chou*

Main category: cs.CV

TL;DR: 提出LR-GCN方法，通过构建无时序约束的图结构来检测DeepFake视频，对噪声、遮挡和对抗攻击具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有DeepFake检测器通常假设面部序列时序一致且干净，但现实场景中压缩伪影、遮挡和对抗攻击会导致面部检测不稳定，产生无效或误检的面部，需要更鲁棒的检测方法。

Method: 提出Laplacian正则化图卷积网络(LR-GCN)：1)构建无时序约束的图嵌入(OF-TGE)，基于语义相似度组织帧级CNN特征；2)采用双层级稀疏机制抑制无效面部影响；3)引入图拉普拉斯谱先验作为高通算子，突出结构异常和伪造伪影；4)通过低通GCN聚合实现任务驱动的谱带通机制。

Result: 在FF++、Celeb-DFv2和DFDC数据集上达到SOTA性能，在严重全局和局部干扰（包括缺失面部、遮挡和对抗性扰动）下显著提升鲁棒性。

Conclusion: LR-GCN方法通过自适应图结构和谱域处理，能够有效检测噪声或无序面部序列中的DeepFake，仅需在干净数据上训练即可实现鲁棒检测。

Abstract: Ensuring the authenticity of video content remains challenging as DeepFake generation becomes increasingly realistic and robust against detection. Most existing detectors implicitly assume temporally consistent and clean facial sequences, an assumption that rarely holds in real-world scenarios where compression artifacts, occlusions, and adversarial attacks destabilize face detection and often lead to invalid or misdetected faces. To address these challenges, we propose a Laplacian-Regularized Graph Convolutional Network (LR-GCN) that robustly detects DeepFakes from noisy or unordered face sequences, while being trained only on clean facial data. Our method constructs an Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into an adaptive sparse graph based on semantic affinities. Unlike traditional methods constrained by strict temporal continuity, OF-TGE captures intrinsic feature consistency across frames, making it resilient to shuffled, missing, or heavily corrupted inputs. We further impose a dual-level sparsity mechanism on both graph structure and node features to suppress the influence of invalid faces. Crucially, we introduce an explicit Graph Laplacian Spectral Prior that acts as a high-pass operator in the graph spectral domain, highlighting structural anomalies and forgery artifacts, which are then consolidated by a low-pass GCN aggregation. This sequential design effectively realizes a task-driven spectral band-pass mechanism that suppresses background information and random noise while preserving manipulation cues. Extensive experiments on FF++, Celeb-DFv2, and DFDC demonstrate that LR-GCN achieves state-of-the-art performance and significantly improved robustness under severe global and local disruptions, including missing faces, occlusions, and adversarially perturbed face detections.

</details>


### [200] [MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer](https://arxiv.org/abs/2512.07500)
*Penghui Liu,Jiangshan Wang,Yutong Shen,Shanhui Mo,Chenyang Qi,Yue Ma*

Main category: cs.CV

TL;DR: MultiMotion：基于DiT的多对象视频运动迁移框架，通过Mask-aware Attention Motion Flow实现多对象运动解耦与控制，并引入RectPC高效采样器，在首个DiT多对象运动迁移基准上取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 当前Diffusion Transformer架构在多对象视频运动迁移中存在运动纠缠和缺乏对象级控制的问题，需要一种能够精确控制多个对象运动并保持时间一致性的解决方案。

Method: 1. 提出Mask-aware Attention Motion Flow (AMF)，利用SAM2掩码在DiT流程中显式解耦和控制多个对象的运动特征；2. 引入RectPC，一种高阶预测-校正求解器，用于高效准确的多实体生成采样；3. 构建首个DiT多对象运动迁移基准数据集。

Result: MultiMotion实现了精确、语义对齐且时间一致的多对象运动迁移，保持了DiT的高质量和可扩展性，在构建的基准数据集上表现出色。

Conclusion: MultiMotion通过创新的AMF机制和RectPC采样器，成功解决了DiT在多对象运动迁移中的核心挑战，为多对象视频生成提供了统一的解决方案框架。

Abstract: Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.

</details>


### [201] [SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation](https://arxiv.org/abs/2512.07503)
*Yao Teng,Zhihuan Jiang,Han Shi,Xian Liu,Xuefei Ning,Guohao Dai,Yu Wang,Zhenguo Li,Xihui Liu*

Main category: cs.CV

TL;DR: SJD++是一种无需训练的概率并行解码算法，通过多令牌预测和推测采样机制，将自回归文本到图像生成的推理延迟减少2-3倍，步骤压缩2-7倍，同时保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 大型自回归模型虽然能生成高质量高分辨率图像，但推理速度慢，需要数百到数千次顺序前向传递进行下一个令牌预测，这限制了实际应用。

Method: 提出Speculative Jacobi Decoding++ (SJD++)算法，结合Jacobi解码的迭代多令牌预测机制和推测采样的概率起草-验证机制，并重用高置信度起草令牌以进一步加速。

Result: 在多个代表性自回归文本到图像生成模型上的实验表明，SJD++实现了2-3倍的推理延迟减少和2-7倍的步骤压缩，同时保持视觉质量无显著下降。

Conclusion: SJD++是一种有效的训练免费加速方法，显著提升了自回归文本到图像生成的效率，为实际应用提供了可行的解决方案。

Abstract: Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\times$ to $3\times$ inference latency reduction and $2\times$ to $7\times$ step compression, while preserving visual quality with no observable degradation.

</details>


### [202] [ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points](https://arxiv.org/abs/2512.07504)
*Ryota Okumura,Kaede Shiohara,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: ControlVP：一个用户引导的框架，用于纠正生成图像中的消失点不一致问题，通过建筑轮廓的结构引导和几何约束来增强几何一致性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型（如Stable Diffusion）在视觉质量上表现出色，但经常存在几何不一致问题，特别是消失点不一致，导致平行线在2D空间中无法正确收敛，破坏了场景的结构真实感，尤其是在建筑场景中。

Method: 扩展预训练扩散模型，通过建筑轮廓提供结构引导，并引入几何约束来显式鼓励图像边缘与透视线索的对齐。

Result: 该方法增强了全局几何一致性，同时保持了与基线相当的视觉保真度，特别适用于需要准确空间结构的应用，如图像到3D重建。

Conclusion: ControlVP通过用户引导的消失点校正，有效解决了生成图像中的几何不一致问题，提升了建筑场景的结构真实感，为图像到3D重建等应用提供了有价值的工具。

Abstract: Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .

</details>


### [203] [MeshRipple: Structured Autoregressive Generation of Artist-Meshes](https://arxiv.org/abs/2512.07514)
*Junkai Lin,Hang Long,Huipeng Guo,Jielei Zhang,JiaYi Yang,Tianle Guo,Yang Yang,Jianwen Li,Wenxiao Zhang,Matthias Nießner,Wei Yang*

Main category: cs.CV

TL;DR: MeshRipple：一种新的网格生成方法，通过前沿感知的BFS标记化和扩展预测策略解决自回归网格生成中的长程几何依赖问题，生成具有高表面保真度和拓扑完整性的网格。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归网格生成器由于内存限制使用截断序列和滑动窗口推理，这破坏了长程几何依赖关系，导致生成网格出现孔洞和碎片化组件。

Method: 1. 前沿感知的BFS标记化：将生成顺序与表面拓扑对齐；2. 扩展预测策略：保持连贯、连接的表面增长；3. 稀疏注意力全局内存：提供有效无界的感受野来解决长程拓扑依赖。

Result: MeshRipple能够生成具有高表面保真度和拓扑完整性的网格，在性能上超越了近期强大的基线方法。

Conclusion: MeshRipple通过集成设计解决了自回归网格生成中的关键限制，实现了连贯的网格生成，避免了孔洞和碎片化问题。

Abstract: Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.

</details>


### [204] [From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images](https://arxiv.org/abs/2512.07527)
*Fei Yu,Yu Liu,Luyang Tang,Mingchao Sun,Zengye Ge,Rui Bu,Yuchao Jin,Haisen Zhao,He Sun,Yangyan Li,Mu Xu,Wenzheng Chen,Baoquan Chen*

Main category: cs.CV

TL;DR: 提出一种从稀疏卫星图像重建城市规模3D模型的方法，通过2.5D高度图建模和纹理恢复网络，解决极端视角外推问题，实现从轨道图像合成逼真地面视图。


<details>
  <summary>Details</summary>
Motivation: 从卫星图像进行城市规模3D重建面临极端视角外推的挑战，需要从稀疏的轨道图像（视角差异近90度）合成地面新视图。现有方法如NeRF和3DGS在处理严重透视缩短的建筑立面和缺陷纹理时会失败。

Method: 1) 将城市几何建模为2.5D高度图，采用Z单调符号距离场(SDF)匹配自上而下的城市建筑布局；2) 通过可微分渲染技术从卫星图像为网格着色；3) 训练生成式纹理恢复网络，从退化的输入中恢复高频、合理的纹理细节。

Result: 在4平方公里真实世界区域上，仅用少量卫星图像就能重建出高质量3D模型，在合成逼真地面视图方面达到最先进性能。生成的模型不仅视觉效果好，还能作为高保真、可直接应用的资产用于城市规划等下游任务。

Conclusion: 该方法通过专门针对城市结构和卫星输入的设计选择，成功解决了从稀疏卫星图像进行城市规模3D重建的极端视角外推问题，实现了可扩展且鲁棒的大规模城市重建。

Abstract: City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.
  To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs.
  Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\,\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.

</details>


### [205] [All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs](https://arxiv.org/abs/2512.07580)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Longzhen Yang,Yihang Liu,Chengmei Yang,Ying Wen,Xianfeng Tang,Hui Liu,Yuyin Zhou,Lianghua He*

Main category: cs.CV

TL;DR: 研究发现视觉大语言模型中深层视觉token信息逐渐消失，提出"信息地平线"概念，发现简单随机剪枝在深层效果更好，结合现有方法实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 视觉大语言模型依赖大量视觉token导致计算成本高，现有训练无关剪枝方法在深层表现不佳，需要探究深层视觉token信息退化原因。

Method: 提出量化token信息含量的方法，通过移除token后模型输出概率变化来衡量；分析发现"信息地平线"现象；提出在深层使用随机剪枝，并与现有方法结合。

Result: DivPrune结合随机剪枝在Qwen-2.5-VL-7B上剪枝50%视觉token，保持96.9%性能，达到SOTA；信息地平线位置随任务视觉强度和模型能力变化。

Conclusion: 视觉token信息随网络深度逐渐消失，深层随机剪枝是有效策略；信息地平线概念为视觉token剪枝提供了理论指导，简单方法结合现有技术可显著提升效率。

Abstract: Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by "vanishing token information", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as "information horizon", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.

</details>


### [206] [LongCat-Image Technical Report](https://arxiv.org/abs/2512.07584)
*Meituan LongCat Team,Hanghang Ma,Haoxian Tan,Jiale Huang,Junqiang Wu,Jun-Yan He,Lishuai Gao,Songlin Xiao,Xiaoming Wei,Xiaoqi Ma,Xunliang Cai,Yayong Guan,Jie Hu*

Main category: cs.CV

TL;DR: LongCat-Image是一个开源的英中双语图像生成基础模型，在文本渲染、真实感、部署效率和开发者可访问性方面取得突破，采用紧凑的6B参数设计，并建立了全面的开源生态系统。


<details>
  <summary>Details</summary>
Motivation: 解决当前主流模型在多语言文本渲染、真实感、部署效率和开发者可访问性方面的核心挑战，特别是在中文文本渲染方面建立新的行业标准。

Method: 通过预训练、中期训练和SFT阶段的严格数据策展策略，结合RL阶段精心设计的奖励模型协调使用，采用紧凑的6B参数扩散模型架构，显著小于常见的20B+ MoE架构。

Result: 在文本渲染能力和真实感方面达到新的SOTA水平，特别是在中文字符渲染方面超越开源和商业解决方案；在图像编辑任务上也取得SOTA结果；部署效率高，VRAM使用少，推理速度快。

Conclusion: LongCat-Image通过全面的开源生态系统（包括多个模型版本和完整训练工具链）为开发者和研究者提供强大支持，推动视觉内容创作的前沿发展。

Abstract: We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.

</details>


### [207] [Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation](https://arxiv.org/abs/2512.07590)
*Kaili Qi,Zhongyi Huang,Wenli Yang*

Main category: cs.CV

TL;DR: 提出鲁棒版VM_TUNet，结合变分方法和深度学习，通过物理先验、边缘检测和平均曲率项改进Cahn-Hilliard方程，平衡性能与计算效率。


<details>
  <summary>Details</summary>
Motivation: 针对噪声图像分割中边界模糊或断裂的挑战，需要结合变分PDE的边界平滑优势和深度神经网络强大表示能力的方法。

Method: 提出鲁棒版VM_TUNet，集成物理先验、边缘检测和平均曲率项到改进的Cahn-Hilliard方程中。架构包含F模块（频域预处理）和T模块（精确局部计算），并有稳定性估计支持。

Result: 在三个基准数据集上的实验表明，该方法在性能和计算效率之间取得平衡，相比纯CNN模型获得竞争性定量结果和改善的视觉质量，同时以合理计算成本达到接近transformer方法的性能。

Conclusion: 提出的鲁棒VM_TUNet框架成功结合了变分PDE的边界平滑优势和深度神经网络的表示能力，为噪声图像分割提供了一种有效且计算高效的解决方案。

Abstract: To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.

</details>


### [208] [More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery](https://arxiv.org/abs/2512.07596)
*Wenzhen Dong,Jieming Yu,Yiming Huang,Hongqiu Wang,Lei Zhu,Albert C. S. Chung,Hongliang Ren,Long Bai*

Main category: cs.CV

TL;DR: SAM 3在机器人辅助手术中表现出色，在空间提示下的图像和视频分割优于前代，语言提示在手术领域表现欠佳，3D重建能力有潜力但复杂动态场景仍有局限


<details>
  <summary>Details</summary>
Motivation: 评估SAM 3在机器人辅助手术中的性能，特别是其零样本分割能力（包括点、边界框和语言提示）以及3D感知能力，探索其在动态手术场景中的适用性

Method: 在MICCAI EndoVis 2017和2018基准上进行综合测试，评估SAM 3在图像和视频分割中的表现；在SCARED、StereoMIS和EndoNeRF上进行零样本评估，测试单目深度估计和3D器械重建能力

Result: SAM 3在空间提示（点和边界框）下的图像和视频分割性能明显优于SAM和SAM 2；语言提示在手术领域表现欠佳，需要领域特定训练；3D重建能力展示出潜力，但复杂动态手术场景仍有局限性

Conclusion: SAM 3在机器人辅助手术中展现出显著进步，特别是在空间提示下的分割性能，但语言提示需要进一步优化，3D重建能力有前景但需改进以适应复杂动态手术环境

Abstract: The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.

</details>


### [209] [Online Segment Any 3D Thing as Instance Tracking](https://arxiv.org/abs/2512.07599)
*Hanshi Wang,Zijian Cai,Jin Gao,Yiwei Zhang,Weiming Hu,Ke Wang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: AutoSeg3D将在线3D分割重构为实例跟踪问题，通过对象查询进行时空信息传播，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的3D分割方法主要关注空间信息传播，忽视了感知的动态性和时间维度。在具身智能中，视角变化导致物体部分可见，需要时间理解来获得完整物体认知。

Method: 1) 将在线3D分割重构为实例跟踪问题；2) 使用对象查询进行时间信息传播：长期实例关联保持特征和身份一致性，短期实例更新丰富即时观测；3) 引入空间一致性学习缓解VFMs的碎片化问题；4) 稀疏查询避免密集点云交互的计算负担。

Result: 在ScanNet200上超越ESAM 2.8 AP，在ScanNet、SceneNN和3RScan数据集上均取得一致性能提升，达到新的SOTA。

Conclusion: 通过将3D分割重构为实例跟踪问题，并利用对象查询进行时空信息传播，AutoSeg3D显著提升了具身智能代理的环境感知能力，特别是在处理部分可见物体和碎片化分割方面表现出色。

Abstract: Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.

</details>


### [210] [Decomposition Sampling for Efficient Region Annotations in Active Learning](https://arxiv.org/abs/2512.07606)
*Jingna Qiu,Frauke Wilm,Mathias Öttl,Jonas Utz,Maja Schlereth,Moritz Schillinger,Marc Aubreville,Katharina Breininger*

Main category: cs.CV

TL;DR: DECOMP是一种新的主动学习采样策略，通过将图像分解为类别特定组件并采样每个类别的区域，提高密集预测任务中的注释多样性，特别关注困难类别。


<details>
  <summary>Details</summary>
Motivation: 密集预测任务（特别是医学影像）中注释成本高、时间密集，现有区域级注释方法存在计算成本高、区域选择不相关、过度依赖不确定性采样等问题。

Method: 使用伪标签将图像分解为类别特定组件，从每个类别采样区域，结合类别预测置信度指导采样过程，确保困难类别获得更多注释。

Result: 在ROI分类、2D分割和3D分割任务中，DECOMP始终优于基线方法，能更好地采样少数类别区域并提升这些困难类别的性能。

Conclusion: DECOMP解决了现有区域级主动学习方法的局限性，通过类别分解和置信度指导的采样策略，有效提高了密集预测任务的注释效率和模型性能。

Abstract: Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.

</details>


### [211] [MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation](https://arxiv.org/abs/2512.07628)
*Zhiqi Li,Wenhuan Li,Tengfei Wang,Zhenwei Wang,Junta Wu,Haoyuan Wang,Yunhan Yang,Zehuan Huang,Yang Li,Peidong Liu,Chunchao Guo*

Main category: cs.CV

TL;DR: MoCA提出了一种可扩展的组合式3D生成模型，通过重要性组件路由和未选组件压缩来降低计算复杂度，实现高效细粒度的3D资产创建。


<details>
  <summary>Details</summary>
Motivation: 现有的部分感知3D生成方法在增加组件数量时，由于二次全局注意力成本导致可扩展性差，需要更高效的可组合3D生成方法。

Method: 设计了两个关键技术：(1) 基于重要性的组件路由，选择top-k相关组件进行稀疏全局注意力；(2) 不重要组件压缩，在降低计算复杂度的同时保留未选组件的上下文先验。

Result: 在组合式物体和场景生成任务上，MoCA均优于基线方法，实现了高效、细粒度的可组合3D资产创建。

Conclusion: MoCA通过创新的注意力机制设计，解决了现有组合式3D生成方法的可扩展性问题，为大规模组件组合的3D生成提供了有效解决方案。

Abstract: Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA

</details>


### [212] [Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method](https://arxiv.org/abs/2512.07651)
*Yuanye Liu,Hanxiao Zhang,Nannan Shi,Yuxin Shi,Arif Mahmood,Murtaza Taj,Xiahai Zhuang*

Main category: cs.CV

TL;DR: LiQA数据集包含440名患者的多期相、多中心MRI扫描，用于肝脏分割和纤维化分期算法基准测试，提出半监督学习和多视图共识方法提升临床鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 肝纤维化是全球重大健康负担，需要准确分期以进行有效临床管理。现有算法在复杂真实世界条件下（如域偏移、模态缺失、空间不对齐）表现不佳，需要建立标准化基准数据集和方法。

Method: 1. 建立LiQA数据集：440名患者的多期相、多中心MRI扫描；2. 提出基准方法：半监督学习框架结合外部数据进行鲁棒分割；3. 使用多视图共识方法结合CAM正则化进行分期。

Result: 评估表明，利用多源数据和解剖约束显著增强了模型在临床环境中的鲁棒性。该方法在CARE 2024挑战中表现最佳。

Conclusion: LiQA数据集为肝脏分割和纤维化分期提供了标准化基准，提出的方法通过整合多源数据和解剖先验知识，有效解决了临床实践中的复杂挑战。

Abstract: Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.

</details>


### [213] [Optimization-Guided Diffusion for Interactive Scene Generation](https://arxiv.org/abs/2512.07661)
*Shiaho Li,Naisheng Ye,Tianyu Li,Kashyap Chitta,Tuo An,Peng Su,Boyang Wang,Haiou Liu,Chen Lv,Hongyang Li*

Main category: cs.CV

TL;DR: OMEGA：一种基于优化的无训练框架，通过约束优化引导扩散采样，生成物理合理且行为一致的多智能体驾驶场景，特别针对安全关键事件生成


<details>
  <summary>Details</summary>
Motivation: 自动驾驶评估需要真实多样的多智能体驾驶场景，但现有数据集中安全关键事件稀少且代表性不足。现有数据驱动场景生成模型缺乏可控性或产生违反物理/社会约束的样本，限制了其实用性。

Method: 提出OMEGA框架：1）在扩散模型的反向采样步骤中通过约束优化重新锚定，引导生成物理合理且行为一致的轨迹；2）将自我车辆与攻击者交互建模为分布空间中的博弈论优化，近似纳什均衡以生成真实的安全关键对抗场景。

Result: 在nuPlan和Waymo数据集上，OMEGA将物理和行为有效场景比例从32.35%提升至72.27%（自由探索），从11%提升至80%（可控生成）。能生成5倍多的近碰撞帧（碰撞时间<3秒）同时保持场景真实性。

Conclusion: OMEGA通过优化引导的扩散采样显著提升了场景生成的现实性、一致性和可控性，为自动驾驶评估提供了高质量的安全关键场景生成能力。

Abstract: Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.

</details>


### [214] [EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset](https://arxiv.org/abs/2512.07668)
*Ronan John,Aditya Kesari,Vincenzo DiMatteo,Kristin Dana*

Main category: cs.CV

TL;DR: 提出了EgoCampus数据集和EgoCampusNet方法，用于预测户外校园环境中行人导航时的视觉注意力。


<details>
  <summary>Details</summary>
Motivation: 解决在真实世界导航中预测人类视觉注意力的挑战，现有数据集大多关注室内任务或缺乏眼动数据，需要户外环境下的注意力研究资源。

Method: 使用Meta的Project Aria眼镜收集数据，包含眼动追踪、RGB摄像头、惯性传感器和GPS，创建了EgoCampus数据集（25条路径，6公里，80多名行人），并开发了EgoCampusNet方法来预测导航行人的眼动注视。

Result: 建立了包含丰富眼动注释视频的多样化数据集，为研究真实世界注意力和未来导航眼动预测模型提供了新资源。

Conclusion: 该工作填补了户外环境行人视觉注意力研究的空白，提供了数据集和方法框架，有助于推进导航场景下的眼动预测研究。

Abstract: We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .

</details>


### [215] [sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only](https://arxiv.org/abs/2512.07698)
*Arslan Artykov,Corentin Sautier,Vincent Lepetit*

Main category: cs.CV

TL;DR: 首个从自由移动单目视频中联合预测部件分割和关节参数的数据驱动方法，仅用合成数据训练即可泛化到真实物体


<details>
  <summary>Details</summary>
Motivation: 理解铰接物体是机器人和数字孪生创建的基础挑战。现有方法主要依赖多视角系统、物体扫描或静态相机，缺乏从自由移动单目视频中恢复部件分割和关节参数的实用解决方案

Method: 提出首个数据驱动方法，从自由移动相机拍摄的单目视频中联合预测部件分割和关节参数。仅使用合成数据进行训练，能够直接处理随意录制的视频

Result: 方法在合成数据训练后展现出对真实世界物体的强大泛化能力，为铰接物体理解提供了可扩展且实用的解决方案，适用于动态环境中的实时应用

Conclusion: 该方法首次实现了从自由移动单目视频中联合恢复铰接物体的部件分割和关节参数，为机器人学和数字孪生应用提供了高效、可扩展的解决方案

Abstract: Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/

</details>


### [216] [PVeRA: Probabilistic Vector-Based Random Matrix Adaptation](https://arxiv.org/abs/2512.07703)
*Leo Fillioux,Enzo Ferrante,Paul-Henry Cournède,Maria Vakalopoulou,Stergios Christodoulidis*

Main category: cs.CV

TL;DR: PVeRA是一种概率版本的VeRA适配器，通过概率化修改低秩矩阵来处理输入中的固有模糊性，并在训练和测试时支持不同采样配置，在参数高效适应方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型需要大量数据和计算资源进行训练或微调，成本高昂。适应方法通过向冻结骨干网络添加少量可训练模块来解决这一限制，但现有方法如VeRA适配器仍有改进空间。

Method: 提出PVeRA（概率VeRA适配器），对VeRA的低秩矩阵进行概率化修改。该方法使用冻结的随机低秩矩阵对，通过概率方式处理输入模糊性，支持训练和测试时的不同采样配置。

Result: 在VTAB-1k基准测试中，PVeRA在七个适配器比较中表现优异，超越了VeRA和其他适配器。

Conclusion: PVeRA通过概率化方法改进了参数高效适应技术，能更好地处理输入模糊性，在资源受限环境下为大型基础模型的适应提供了有效解决方案。

Abstract: Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.

</details>


### [217] [UnCageNet: Tracking and Pose Estimation of Caged Animal](https://arxiv.org/abs/2512.07712)
*Sayak Dutta,Harish Katti,Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: 提出三阶段预处理流水线解决笼子结构遮挡对动物追踪和姿态估计的影响，包括笼子分割、修复和评估，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有动物追踪和姿态估计系统（如STEP和ViTPose）在处理带有笼子结构和系统性遮挡的图像视频时性能大幅下降，需要解决这一限制

Method: 三阶段预处理流水线：1) 使用Gabor增强的ResNet-UNet架构进行笼子分割，含72个方向可调滤波器；2) 使用CRFill进行内容感知的笼子修复；3) 在修复后的帧上进行姿态估计和追踪评估

Result: 实验验证表明，通过该流水线去除笼子遮挡后，姿态估计和追踪性能达到与无遮挡环境相当的水平，关键点检测精度和轨迹一致性显著改善

Conclusion: 提出的预处理流水线有效解决了笼子遮挡问题，使动物追踪和姿态估计系统在复杂遮挡环境下仍能保持高性能

Abstract: Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.

</details>


### [218] [ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation](https://arxiv.org/abs/2512.07720)
*Fan Yang,Heyuan Li,Peihao Li,Weihao Yuan,Lingteng Qiu,Chaoyue Song,Cheng Chen,Yisheng He,Shifeng Zhang,Xiaoguang Han,Steven Hoi,Guosheng Lin*

Main category: cs.CV

TL;DR: 提出一种结合3D重建模型和视频扩散模型的方法，从单张输入图像生成高质量的上半身3D虚拟形象，解决现有方法在纹理模糊、运动僵硬和结构不稳定等问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D虚拟形象生成方法存在矛盾：基于大重建模型的方法能产生稳定身体结构但纹理模糊、运动僵硬；生成式视频模型能合成逼真动态结果但存在身体结构错误和身份漂移问题。需要结合两者优势来生成高质量数字虚拟形象。

Method: 提出一个新颖框架，使用3D重建模型提供稳健的结构和外观先验，然后引导实时自回归视频扩散模型进行渲染。这种结合使模型能够实时合成高频、逼真的细节和流畅动态，同时减少纹理模糊和运动僵硬。

Result: 实验表明，该方法显著减少了伪影，在视觉质量上相比领先方法有实质性提升。能够生成具有逼真外观和动态、时间一致运动的高保真数字虚拟形象。

Conclusion: 通过将3D重建的几何稳定性与视频模型的生成能力相结合，该方法为游戏和虚拟现实等实时应用提供了稳健高效的解决方案，实现了高质量虚拟形象生成。

Abstract: Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa

</details>


### [219] [SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery](https://arxiv.org/abs/2512.07733)
*Meng Cao,Xingyu Li,Xue Liu,Ian Reid,Xiaodan Liang*

Main category: cs.CV

TL;DR: SpatialDreamer是一个强化学习框架，通过主动探索、世界模型视觉想象和证据推理的闭环过程，提升多模态大语言模型在复杂空间推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在需要心理模拟的复杂空间推理任务上表现有限，主要依赖被动观察空间数据，缺乏主动的心理意象过程。

Method: 提出SpatialDreamer强化学习框架，包含主动探索、世界模型视觉想象和证据推理的闭环过程。针对长序列推理任务缺乏细粒度奖励监督的问题，提出几何策略优化（GeoPO），采用树结构采样和基于几何一致性约束的步骤级奖励估计。

Result: 在多个具有挑战性的基准测试中，SpatialDreamer取得了极具竞争力的结果。

Conclusion: SpatialDreamer代表了多模态大语言模型在类人主动空间心理模拟方面的重要进展。

Abstract: Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.

</details>


### [220] [HLTCOE Evaluation Team at TREC 2025: VQA Track](https://arxiv.org/abs/2512.07738)
*Dengjia Zhang,Charles Weng,Katherine Guerrerio,Yi Lu,Kenton Murray,Alexander Martin,Reno Kriz,Benjamin Van Durme*

Main category: cs.CV

TL;DR: HLTCOE团队在TREC VQA的答案生成任务中，提出了一个列表式学习框架，通过生成候选答案并重排序来提高语义精度和排名一致性。


<details>
  <summary>Details</summary>
Motivation: 改进视频问答中的答案生成质量，特别是在语义精度和排名一致性方面。传统方法在生成多个候选答案时缺乏有效的排序机制，需要更好的方法来评估和排列候选答案。

Method: 采用两阶段方法：1) 基础多模态模型生成多个候选答案；2) 使用新颖的Masked Pointer Cross-Entropy Loss with Rank Weights训练的模型对候选答案进行重排序。该损失函数结合了基于指针的候选选择、排名相关权重和词汇限制下的掩码交叉熵。

Result: 实验显示在准确性和排名稳定性方面取得了一致的提升，特别是在需要时间推理和语义消歧的问题上表现更好。

Conclusion: 通过将生成式建模与判别式排序相结合，该方法能够产生连贯、细粒度的答案列表，实现了稳定的列表式优化。

Abstract: The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.

</details>


### [221] [DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving](https://arxiv.org/abs/2512.07745)
*Jialv Zou,Shaoyu Chen,Bencheng Liao,Zhiyu Zheng,Yuehao Song,Lefei Zhang,Qian Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: DiffusionDriveV2 使用强化学习改进端到端自动驾驶的扩散模型，解决模式坍塌问题，在保持多样性的同时提升轨迹质量。


<details>
  <summary>Details</summary>
Motivation: 现有的生成扩散模型在端到端自动驾驶中存在模式坍塌问题，倾向于生成保守和同质化的行为。虽然 DiffusionDrive 使用预定义锚点来划分动作空间并生成多样化轨迹，但其依赖模仿学习缺乏足够约束，导致多样性和一致高质量之间的困境。

Method: 提出 DiffusionDriveV2，利用强化学习约束低质量模式并探索更优轨迹：1) 使用尺度自适应乘性噪声促进广泛探索；2) 采用锚点内 GRPO 管理单个锚点内样本的优势估计，以及锚点间截断 GRPO 在不同锚点间纳入全局视角，防止不同意图间的不当优势比较。

Result: 在 NAVSIM v1 数据集上达到 91.2 PDMS，在 NAVSIM v2 数据集上达到 85.5 EPDMS（使用对齐的 ResNet-34 骨干网络），创造了新记录。实验验证了该方法解决了截断扩散模型在多样性和一致高质量之间的困境，实现了最佳权衡。

Conclusion: DiffusionDriveV2 通过强化学习显著提升了端到端自动驾驶扩散模型的整体输出质量，同时保持了高斯混合模型固有的多模态特性，解决了多样性与高质量之间的困境。

Abstract: Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at https://github.com/hustvl/DiffusionDriveV2

</details>


### [222] [Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation](https://arxiv.org/abs/2512.07747)
*Shihao Zhao,Yitong Chen,Zeyinzi Jiang,Bojia Zi,Shaozhe Hao,Yu Liu,Chaojie Mao,Kwan-Yee K. Wong*

Main category: cs.CV

TL;DR: Unison是一个低成本的统一多模态理解与生成模型，采用两阶段方案，仅需50万训练样本和50GPU小时，能自动解析用户意图和任务参数，覆盖多种理解与生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态方法存在两个问题：自回归方法需要大量计算资源，两阶段方法任务覆盖有限且生成质量差。两者都无法自动解析输入元信息，需要手动配置参数。

Method: 采用两阶段方案，连接预训练的理解和生成模型进行对齐微调。模型能自动解析用户意图、确定任务类型、提取所需元信息，实现任务全自动化。

Result: 仅用50万训练样本和50GPU小时，模型能准确自动识别任务和提取参数，在多种理解和生成任务上表现优异，包括文本、图像、视频理解和文本到视觉内容生成等。

Conclusion: Unison证明了在极低训练成本下实现统一多模态理解与生成的可行性，通过自动任务解析实现了全自动化处理，为普通研究者提供了可行的解决方案。

Abstract: Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.

</details>


### [223] [UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction](https://arxiv.org/abs/2512.07756)
*Mayank Anand,Ujair Alam,Surya Prakash,Priya Shukla,Gora Chand Nandi,Domenec Puig*

Main category: cs.CV

TL;DR: UltrasODM是一个双流框架，通过校准的逐帧不确定性、显著性诊断和可操作提示来辅助超声医师采集，减少重建误差，提高临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 临床超声采集高度依赖操作者，快速探头运动和亮度波动常导致重建误差，降低信任度和临床效用。需要一种能辅助超声医师、提高重建可靠性的系统。

Method: 提出UltrasODM双流框架：1) 基于运动相似性的对比排序模块分组帧；2) 光流流与Dual-Mamba时序模块融合用于6-DoF姿态估计；3) 人机交互层结合贝叶斯不确定性、临床校准阈值和显著性图。当不确定性超过阈值时，系统发出非侵入性警报，建议纠正措施。

Result: 在临床自由手超声数据集上评估，相比UltrasOM，UltrasODM减少漂移15.2%、距离误差12.1%、Hausdorff距离10.1%，同时生成逐帧不确定性和显著性输出。

Conclusion: UltrasODM通过强调透明度和临床医师反馈，提高了重建可靠性，支持更安全、更可信的临床工作流程。代码已公开。

Abstract: Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.

</details>


### [224] [Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2512.07760)
*Menglin Wang,Xiaojin Gong,Jiachen Li,Genlin Ji*

Main category: cs.CV

TL;DR: 提出一种无监督可见光-红外行人重识别方法，通过模态感知Jaccard距离缓解模态差异，结合"分割-对比"策略学习模态不变特征，在基准数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 无监督可见光-红外行人重识别面临模态差异大的挑战，现有方法通常使用最优传输关联跨模态聚类，容易传播局部聚类错误且忽视全局实例级关系。需要解决跨模态学习的两个关键问题：偏差缓解的全局关联和模态不变表示学习。

Method: 1) 提出模态感知Jaccard距离来缓解由模态差异引起的距离偏差，通过全局聚类估计更可靠的跨模态关联；2) 设计"分割-对比"策略获取模态特定的全局原型，在全局关联指导下显式对齐这些原型，实现模态不变且身份可区分的表示学习。

Result: 在基准VI-ReID数据集上取得了最先进的性能，显著优于现有方法，验证了方法的有效性。

Conclusion: 通过挖掘和关注可见光-红外模态偏差，从偏差缓解的全局关联和模态不变表示学习两方面解决跨模态学习问题，提出的方法虽然概念简单但效果显著，为无监督跨模态行人重识别提供了有效解决方案。

Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.

</details>


### [225] [GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring](https://arxiv.org/abs/2512.07776)
*Maximilian Schall,Felix Leonard Knöfel,Noah Elias König,Jan Jonas Kubeler,Maximilian von Klinski,Joan Wilhelm Linnemann,Xiaoshi Liu,Iven Jelle Schlegelmilch,Ole Woyciniuk,Alexandra Schild,Dante Wasmuht,Magdalena Bermejo Espinet,German Illera Basas,Gerard de Melo*

Main category: cs.CV

TL;DR: 开发了GorillaWatch系统，包含三个新数据集和端到端流程，用于自动识别濒危西部低地大猩猩，通过多帧自监督预训练和注意力验证提升性能，并公开代码和数据促进物种保护


<details>
  <summary>Details</summary>
Motivation: 目前监测濒危西部低地大猩猩面临巨大挑战，需要从大量相机陷阱视频中手动重新识别个体，自动化过程缺乏适合训练深度学习模型的大规模野外视频数据集

Method: 1) 创建三个新数据集：Gorilla-SPAC-Wild（最大野外灵长类重识别数据集）、Gorilla-Berlin-Zoo（跨域重识别评估）、Gorilla-SPAC-MoT（多目标跟踪评估）；2) 开发GorillaWatch端到端流程，集成检测、跟踪和重识别；3) 引入多帧自监督预训练策略，利用轨迹一致性学习领域特征；4) 使用可微分的AttnLRP验证模型依赖生物特征而非背景相关性；5) 将时空约束集成到标准聚类中解决无监督种群计数问题

Result: 大规模图像骨干网络的特征聚合优于专用视频架构；多帧自监督预训练有效学习领域特征；注意力验证确认模型依赖判别性生物特征；时空约束聚类缓解过分割问题；所有代码和数据集已公开

Conclusion: GorillaWatch系统为濒危物种监测提供了可扩展、非侵入性的解决方案，通过创新的数据集、多帧自监督学习和科学验证方法，显著提升了野外大猩猩自动识别的准确性和可靠性

Abstract: Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, "in-the-wild" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species

</details>


### [226] [Distribution Matching Variational AutoEncoder](https://arxiv.org/abs/2512.07778)
*Sen Ye,Jianning Pei,Mengde Xu,Shuyang Gu,Chunyu Wang,Liwei Wang,Han Hu*

Main category: cs.CV

TL;DR: DMVAE通过显式匹配潜在分布与任意参考分布，而非传统VAE的高斯先验，系统探索了何种潜在分布更适合建模，发现SSL特征分布能在重建保真度和建模效率间取得最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成模型（如VAE和基础模型对齐编码器）隐含地约束潜在空间而未显式塑造其分布，不清楚何种分布最适合建模。需要明确研究潜在分布结构对建模效果的影响。

Method: 提出分布匹配VAE（DMVAE），通过分布匹配约束显式对齐编码器的潜在分布与任意参考分布，可推广到传统VAE的高斯先验之外，支持自监督特征、扩散噪声或其他先验分布。

Result: SSL衍生分布在重建保真度和建模效率间提供最佳平衡，在ImageNet上仅用64个训练周期达到gFID=3.2。表明选择合适的潜在分布结构（通过分布级对齐实现）是弥合易建模潜在空间与高保真图像合成之间差距的关键。

Conclusion: 显式对齐潜在分布与适当参考分布（而非依赖固定先验）是提升生成模型性能的关键。DMVAE框架为系统探索最优潜在分布提供了工具，SSL特征分布展现出优越性能。

Abstract: Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \textbf{Distribution-Matching VAE} (\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.

</details>


### [227] [OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory](https://arxiv.org/abs/2512.07802)
*Zhaochong An,Menglin Jia,Haonan Qiu,Zijian Zhou,Xiaoke Huang,Zhiheng Liu,Weiming Ren,Kumara Kahatapitiya,Ding Liu,Sen He,Chenyang Zhang,Tao Xiang,Fanny Yang,Serge Belongie,Tian Xie*

Main category: cs.CV

TL;DR: OneStory提出了一种新的多镜头视频生成方法，通过全局跨镜头上下文建模实现一致且可扩展的叙事生成，将MSV重新定义为下一个镜头生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有多镜头视频生成方法难以有效建模长距离跨镜头上下文，因为它们依赖有限的时间窗口或单关键帧条件，导致在复杂叙事下性能下降。

Method: 将MSV重新定义为下一个镜头生成任务，利用预训练图像到视频模型进行视觉条件化。引入两个关键模块：帧选择模块构建基于先前镜头信息帧的语义相关全局记忆，自适应条件器执行重要性引导的补丁化以生成紧凑上下文进行直接条件化。

Result: 在精心策划的60K数据集上微调后，OneStory在文本和图像条件设置下，在多样复杂场景中实现了最先进的叙事连贯性，支持可控和沉浸式长视频叙事。

Conclusion: OneStory通过全局且紧凑的跨镜头上下文建模，实现了连贯且可扩展的多镜头视频生成，显著提升了叙事连贯性，为长视频叙事生成提供了有效解决方案。

Abstract: Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.

</details>


### [228] [Multi-view Pyramid Transformer: Look Coarser to See Broader](https://arxiv.org/abs/2512.07806)
*Gyeongjin Kang,Seungkwon Yang,Seungtae Nam,Younggeun Lee,Jungwoo Kim,Eunbyung Park*

Main category: cs.CV

TL;DR: MVP是一种可扩展的多视角Transformer架构，能够从数十到数百张图像中单次前向传播重建大型3D场景，结合局部到全局的层次结构实现高效高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理大规模多视角图像重建大型3D场景时面临计算效率和表示能力的挑战，需要一种既能处理大量输入视图又能保持细节重建质量的解决方案。

Method: MVP采用双层次结构：1)局部到全局的视图间层次，从局部视图逐步扩展到完整场景；2)细到粗的视图内层次，从详细空间表示逐步聚合为紧凑信息密集的token。结合3D高斯泼溅作为底层3D表示。

Result: 在多样化数据集上验证，当与3D高斯泼溅结合时，MVP实现了最先进的可泛化重建质量，同时保持高效率和可扩展性，适用于广泛的视角配置。

Conclusion: MVP通过创新的双层次Transformer架构成功解决了大规模多视角3D场景重建的效率和表示挑战，为大型复杂场景的快速高质量重建提供了有效解决方案。

Abstract: We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details," MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.

</details>


### [229] [Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes](https://arxiv.org/abs/2512.07807)
*Shai Krakovsky,Gal Fiebelman,Sagie Benaim,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 提出一种在3D高斯表示中嵌入语言场的新方法，通过极低维语义瓶颈特征和多分辨率哈希编码器，解决了现有特征蒸馏方法在大规模互联网数据上的语义特征错位和效率问题。


<details>
  <summary>Details</summary>
Motivation: 将语言场嵌入3D表示可以实现更丰富的空间环境语义理解，连接几何与描述性意义，从而支持自然语言查询/编辑场景、场景检索、导航和多模态推理等任务。然而，现有特征蒸馏方法在处理大规模互联网数据时面临语义特征错位和内存/运行时效率低下的挑战。

Method: 1) 在底层3D高斯表示中引入极低维语义瓶颈特征，通过渲染并传递给多分辨率、基于特征的哈希编码器处理，显著提高运行时和GPU内存效率；2) 引入衰减下采样器模块，并提出几种正则化方法来解决地面真实2D特征的语义错位问题。

Result: 在野外HolyScenes数据集上评估，该方法在性能和效率方面都超越了现有方法。

Conclusion: 提出的方法有效解决了大规模场景中语言场嵌入的语义特征错位和效率问题，为更丰富的空间语义理解和直观的人机交互提供了可行方案。

Abstract: Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.

</details>


### [230] [OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing](https://arxiv.org/abs/2512.07826)
*Haoyang He,Jie Wang,Jiangning Zhang,Zhucun Xue,Xingyuan Bu,Qiangpeng Yang,Shilei Wen,Lei Xie*

Main category: cs.CV

TL;DR: OpenVE-3M是一个开源的大规模高质量视频编辑指令数据集，包含空间对齐和非空间对齐两类编辑任务，并建立了OpenVE-Bench基准测试，训练出的OpenVE-Edit模型在基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 目前基于指令的图像编辑数据集质量和多样性不断提升，但大规模高质量的指令视频编辑数据集仍然稀缺，需要填补这一空白。

Method: 构建OpenVE-3M数据集，包含空间对齐编辑（全局风格、背景变化、局部变化、局部移除、局部添加、字幕编辑）和非空间对齐编辑（摄像机多镜头编辑和创意编辑），通过精心设计的数据流水线和严格质量过滤生成。同时构建OpenVE-Bench基准测试，包含431个视频编辑对，涵盖多样化编辑任务。

Result: OpenVE-3M在规模、编辑类型多样性、指令长度和整体质量上超越现有开源数据集。基于该数据集训练的5B参数OpenVE-Edit模型在OpenVE-Bench基准测试中达到新的SOTA，超越了包括14B基线在内的所有先前开源模型。

Conclusion: OpenVE-3M填补了大规模高质量指令视频编辑数据集的空白，OpenVE-Bench为领域提供了统一基准，OpenVE-Edit模型展示了数据集的有效性，为视频编辑研究提供了重要资源。

Abstract: The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.

</details>


### [231] [UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation](https://arxiv.org/abs/2512.07831)
*Jiehui Huang,Yuechen Zhang,Xu He,Yuan Gao,Zhi Cen,Bin Xia,Yan Zhou,Xin Tao,Pengfei Wan,Jiaya Jia*

Main category: cs.CV

TL;DR: UnityVideo是一个统一的多模态视频生成框架，通过联合学习多种模态（分割掩码、人体骨架、DensePose、光流、深度图）和训练范式，实现世界感知的视频生成。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型受限于单模态条件，缺乏对世界的整体理解，这源于跨模态交互不足和模态多样性有限，无法全面表示世界知识。

Method: 提出两个核心组件：1) 动态噪声化统一异构训练范式；2) 模态切换器与上下文学习器，通过模块化参数和上下文学习实现统一处理。构建了130万样本的大规模统一数据集。

Result: UnityVideo加速了收敛过程，显著增强了零样本泛化能力到未见数据，实现了更优的视频质量、一致性，并更好地符合物理世界约束。

Conclusion: UnityVideo通过多模态联合学习框架，克服了单模态条件的限制，实现了更全面、更符合物理世界约束的视频生成，为世界感知的视频生成提供了有效解决方案。

Abstract: Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo

</details>


### [232] [Voxify3D: Pixel Art Meets Volumetric Rendering](https://arxiv.org/abs/2512.07834)
*Yi-Chuan Huang,Jiewen Chan,Hao-Jen Chien,Yu-Lun Liu*

Main category: cs.CV

TL;DR: Voxify3D：一个两阶段可微分框架，通过正交像素艺术监督、基于补丁的CLIP对齐和调色板约束的Gumbel-Softmax量化，实现从3D网格到体素艺术的高质量自动生成。


<details>
  <summary>Details</summary>
Motivation: 体素艺术在游戏和数字媒体中广泛应用，但从3D网格自动生成面临几何抽象、语义保持和离散颜色一致性之间的冲突。现有方法要么过度简化几何，要么无法达到像素级精确、调色板约束的体素艺术美学。

Method: 提出Voxify3D框架，包含三个核心组件：1）正交像素艺术监督消除透视畸变，实现体素-像素精确对齐；2）基于补丁的CLIP对齐在不同离散化级别保持语义；3）调色板约束的Gumbel-Softmax量化，在离散颜色空间进行可微分优化，支持可控调色板策略。

Result: 实验显示优越性能（37.12 CLIP-IQA，77.90%用户偏好），在多样化角色和可控抽象（2-8种颜色，20x-50x分辨率）上表现优异。

Conclusion: Voxify3D通过协同整合三个组件，解决了极端离散化下的语义保持、通过体积渲染实现像素艺术美学，以及端到端离散优化等基本挑战，实现了高质量的体素艺术生成。

Abstract: Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [233] [Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals](https://arxiv.org/abs/2512.05998)
*Michael Todasco*

Main category: cs.AI

TL;DR: LLM评估任务中引入虚构预测市场机制，通过投注游戏让模型表达置信度，虽然准确率提升有限，但能产生可解读的置信信号


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估其他模型时通常缺乏置信度表示，需要探索如何让LLM表达内部信念和风险意识

Method: 设计100个数学逻辑问题，让6个基线模型回答，3个预测模型在两种条件下预测基线模型回答正确性：控制组（简单预测）和激励组（预测+1-100,000 LLMCoin投注）

Result: 激励组准确率略高（81.5% vs 79.1%），学习速度显著更快（第1轮到第4轮提升12.0% vs 2.9%），投注金额与置信度相关：大额投注（40,000+）正确率约99%，小额投注（<1,000）正确率约74%

Conclusion: 投注机制为LLM评估创造了可解读的置信信号，使内部信念可见可用，为LLM间预测市场和元评估系统奠定基础

Abstract: Large language models are increasingly used to evaluate other models, yet these judgments typically lack any representation of confidence. This pilot study tests whether framing an evaluation task as a betting game (a fictional prediction market with its own LLM currency) improves forecasting accuracy and surfaces calibrated confidence signals. We generated 100 math and logic questions with verifiable answers. Six Baseline models (three current-generation, three prior-generation) answered all items. Three Predictor models then forecasted, for each question-baseline pair, if the baseline would answer correctly. Each predictor completed matched runs in two conditions: Control (simple correct/incorrect predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin under even odds, starting from a 1,000,000 LLMCoin bankroll). Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089, d = 0.86) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence. "Whale" bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is not that fictional money makes models smarter; accuracy gains were modest and did not reach statistical significance (p = .089) in this pilot. Rather, the betting mechanic created a legible confidence signal absent from binary yes/no outputs. This suggests that simple financial framing may help transform LLMs into risk-aware forecasters, making their internal beliefs visible and usable. The protocol offers a foundation for future work for meta-evaluation systems and what may become LLM-to-LLM prediction markets.

</details>


### [234] [Deep learning for autism detection using clinical notes: A comparison of transfer learning for a transparent and black-box approach](https://arxiv.org/abs/2512.06161)
*Gondy Leroy,Prakash Bisht,Sai Madhuri Kandula,Nell Maltman,Sydney Rice*

Main category: cs.AI

TL;DR: 提出一种基于BioBERT的可解释机器学习方法，通过分析临床文本来诊断自闭症谱系障碍，在混合数据集训练下达到97%敏感性和98%特异性，优于黑盒模型。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍诊断需求增加，现有机器学习模型多为黑盒且通常基于单一数据集训练，限制了其可解释性和泛化能力。

Method: 使用BioBERT语言模型分析非结构化临床文本，训练模型标记行为描述并映射到诊断标准，然后分配最终标签（ASD或非ASD）。评估了两种训练策略：顺序训练和混合训练，并与黑盒方法比较。

Result: 透明模型表现稳健，混合数据训练策略效果最佳（敏感性97%，特异性98%）。顺序训练导致性能略有下降。黑盒模型在顺序或混合训练下表现较差（敏感性90%，特异性96%）。

Conclusion: 透明方法优于黑盒方法，混合数据集训练可获得更好性能，为神经发育诊断中更可信、可泛化和临床可操作的AI工具铺平道路。

Abstract: Autism spectrum disorder (ASD) is a complex neurodevelopmental condition whose rising prevalence places increasing demands on a lengthy diagnostic process. Machine learning (ML) has shown promise in automating ASD diagnosis, but most existing models operate as black boxes and are typically trained on a single dataset, limiting their generalizability. In this study, we introduce a transparent and interpretable ML approach that leverages BioBERT, a state-of-the-art language model, to analyze unstructured clinical text. The model is trained to label descriptions of behaviors and map them to diagnostic criteria, which are then used to assign a final label (ASD or not). We evaluate transfer learning, the ability to transfer knowledge to new data, using two distinct real-world datasets. We trained on datasets sequentially and mixed together and compared the performance of the best models and their ability to transfer to new data. We also created a black-box approach and repeated this transfer process for comparison. Our transparent model demonstrated robust performance, with the mixed-data training strategy yielding the best results (97 % sensitivity, 98 % specificity). Sequential training across datasets led to a slight drop in performance, highlighting the importance of training data order. The black-box model performed worse (90 % sensitivity, 96 % specificity) when trained sequentially or with mixed data. Overall, our transparent approach outperformed the black-box approach. Mixing datasets during training resulted in slightly better performance and should be the preferred approach when practically possible. This work paves the way for more trustworthy, generalizable, and clinically actionable AI tools in neurodevelopmental diagnostics.

</details>


### [235] [ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment](https://arxiv.org/abs/2512.06196)
*Charlie Masters,Marta Grześkiewicz,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: ARCANE框架将AI对齐问题转化为多智能体协作，通过动态生成可验证的自然语言评分标准来代表利益相关者偏好，实现可解释、无需重新训练即可调整的对齐。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体越来越多地部署到长期任务中，保持它们与利益相关者偏好的一致性变得至关重要。需要可解释的奖励模型，让利益相关者能够理解和审核模型目标，并且能够在交互时引导智能体，无需重新训练即可纳入偏好变化。

Method: 提出ARCANE框架，将对齐问题构建为多智能体协作问题，动态地将利益相关者偏好表示为自然语言评分标准（加权可验证标准）。受效用理论启发，将评分标准学习构建为重构问题，应用正则化的组序列策略优化(GSPO)程序，平衡可解释性、忠实性和计算效率。

Result: 使用从GDPVal基准派生的219个标注评分标准语料库，在需要多步推理和工具使用的挑战性任务上评估ARCANE。学习的评分标准产生紧凑、易读的评估，并支持可配置的权衡（如正确性与简洁性）而无需重新训练。

Conclusion: 基于评分标准的奖励模型为复杂、长期AI系统提供了一条有前景的路径，实现了可解释、测试时自适应的对齐方法。

Abstract: As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.

</details>


### [236] [On measuring grounding and generalizing grounding problems](https://arxiv.org/abs/2512.06205)
*Daniel Quigley,Eric Maynard*

Main category: cs.AI

TL;DR: 该论文将符号接地问题重新定义为包含真实性、保持性、忠实性、鲁棒性和组合性等多个维度的审计框架，并应用于四种接地模式和三个案例研究，为哲学、计算机科学、语言学和数学领域提供了系统研究意义和接地的共同语言。


<details>
  <summary>Details</summary>
Motivation: 解决符号接地问题——即符号如何与现实世界实体建立联系，而不仅仅是形式系统中的形状操作。传统二元判断方法不足以全面评估符号接地，需要更系统化的框架来评估不同接地模式的有效性。

Method: 提出一个多维度审计框架，包含五个核心要求：真实性（机制在智能体内部并通过学习/进化获得）、保持性（原子意义保持完整）、忠实性（相关性和因果性）、鲁棒性（在声明扰动下的优雅降级）和组合性（系统地从部分构建整体）。通过评估元组（上下文、意义类型、威胁模型、参考分布）来索引这些要求。

Result: 将框架应用于四种接地模式（符号、指称、向量、关系）和三个案例：模型论语义学实现精确组合但缺乏因果保证；大语言模型在语言任务上显示相关拟合和局部鲁棒性，但在无接地交互的世界任务中缺乏成功选择；人类语言通过进化和发展获得满足强真实性要求。

Conclusion: 通过将哲学上的表征问题操作化，为科学哲学家、计算机科学家、语言学家和数学家提供了研究接地和意义的共同语言和技术框架，使系统化调查成为可能，超越了传统的二元判断方法。

Abstract: The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.

</details>


### [237] [AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems](https://arxiv.org/abs/2512.06240)
*Chuanhao Nie,Yunbo Liu,Chao Wang*

Main category: cs.AI

TL;DR: 本文综述了AI在反洗钱(AML)中的应用，提出基于图检索增强生成(RAG Graph)的KYC系统，实验显示该架构能提高检测准确性、降低误报率，并增强KYC流程的效率和透明度。


<details>
  <summary>Details</summary>
Motivation: 洗钱和金融欺诈每年造成数万亿美元损失，威胁全球金融稳定。传统AML工作流程存在检测准确性低、误报率高、人工调查负担重等问题，需要AI技术来现代化AML系统，支持更可持续的合规实践。

Method: 1. 综述AI在AML中的应用现状；2. 提出未来研究方向：联邦学习、公平可解释AI、强化学习、人机协同可视化系统；3. 设计AI驱动的KYC应用，集成图检索增强生成(RAG Graph)与生成模型，提升KYC流程效率。

Result: 实验结果显示，RAG-Graph架构在不同评估设置下表现出高忠实度和强答案相关性，显著提高了KYC客户尽职调查(CDD)和增强尽职调查(EDD)工作流程的效率和透明度，有助于实现资源优化的合规实践。

Conclusion: AI技术能够现代化AML工作流程，提高检测准确性、降低误报率、减少人工负担。提出的RAG-Graph KYC系统展示了AI在增强KYC流程透明度和效率方面的潜力，为下一代透明、可问责、鲁棒的AML架构指明了方向。

Abstract: Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.

</details>


### [238] [How Sharp and Bias-Robust is a Model? Dual Evaluation Perspectives on Knowledge Graph Completion](https://arxiv.org/abs/2512.06296)
*Sooho Moon,Yunyong Ko*

Main category: cs.AI

TL;DR: 提出PROBE评估框架，通过秩转换器和秩聚合器解决现有知识图谱补全评估中忽视预测锐度和流行度偏差的问题


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱补全(KGC)评估指标存在两个关键问题：1) 忽视预测锐度（对单个预测的严格程度评估）；2) 忽视流行度偏差鲁棒性（对低流行度实体的预测能力）

Method: 提出PROBE评估框架，包含秩转换器(RT)和秩聚合器(RA)。RT根据所需预测锐度级别估计每个预测的分数，RA以流行度感知方式聚合所有分数

Result: 在真实世界知识图谱上的实验表明，现有指标倾向于高估或低估KGC模型的准确性，而PROBE能提供对KGC模型的全面理解和可靠评估结果

Conclusion: PROBE框架能够更全面地评估知识图谱补全模型，解决现有评估指标在预测锐度和流行度偏差方面的不足

Abstract: Knowledge graph completion (KGC) aims to predict missing facts from the observed KG. While a number of KGC models have been studied, the evaluation of KGC still remain underexplored. In this paper, we observe that existing metrics overlook two key perspectives for KGC evaluation: (A1) predictive sharpness -- the degree of strictness in evaluating an individual prediction, and (A2) popularity-bias robustness -- the ability to predict low-popularity entities. Toward reflecting both perspectives, we propose a novel evaluation framework (PROBE), which consists of a rank transformer (RT) estimating the score of each prediction based on a required level of predictive sharpness and a rank aggregator (RA) aggregating all the scores in a popularity-aware manner. Experiments on real-world KGs reveal that existing metrics tend to over- or under-estimate the accuracy of KGC models, whereas PROBE yields a comprehensive understanding of KGC models and reliable evaluation results.

</details>


### [239] [DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization](https://arxiv.org/abs/2512.06337)
*Xuan Xie,Xuan Wang,Wenjie Wang*

Main category: cs.AI

TL;DR: DaGRPO通过序列级梯度修正和离策略数据增强，解决了GRPO训练不稳定和样本效率低的问题，在数学推理和OOD泛化任务上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: GRPO虽然在激发大语言模型的长程推理能力方面表现出色，但存在训练不稳定和样本效率低的问题。研究发现根本原因在于策略内样本缺乏区分度：对于常规查询，高度同质的样本导致破坏性梯度冲突；对于困难查询，有效正样本稀缺导致优化无效。

Method: 提出DaGRPO方法，包含两个核心机制：1) 序列级梯度修正：利用细粒度评分动态掩码低区分度的样本对，从源头消除梯度冲突；2) 离策略数据增强：引入高质量锚点样本，为困难任务恢复训练信号。

Result: 在9个数学推理和OOD泛化基准测试中，DaGRPO显著超越现有SFT、GRPO和混合基线，实现新的SOTA性能（如在数学基准上平均准确率提升+4.7%）。深入分析证实DaGRPO有效缓解梯度爆炸并加速长链推理能力的涌现。

Conclusion: DaGRPO通过解决GRPO的区分度问题，显著提升了训练稳定性和样本效率，为大语言模型的长程推理能力优化提供了有效解决方案。

Abstract: The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.

</details>


### [240] [Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression](https://arxiv.org/abs/2512.06393)
*Qiming Bao,Xiaoxuan Fu*

Main category: cs.AI

TL;DR: LLMs在逻辑推理任务中表现出稳定的语义保持变换不变性，但对缺失规则和矛盾证据极其脆弱，准确率分别降至25%和0%。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多自然语言任务中表现出色，但其在逻辑上下文中的结构扰动泛化能力尚未得到充分理解。研究者希望建立一个受控评估框架，系统测试LLMs在逻辑推理中的可靠性。

Method: 引入一个受控评估框架，包含四种针对性压力测试：1)规则删除（冗余或必要规则）；2)矛盾证据注入；3)逻辑保持重写（使用六种等价定律）；4)多定律等价堆叠（2-5个同时变换）。在BERT、Qwen2和LLaMA-like三个模型家族上进行实验。

Result: 所有模型在基础任务上达到完美准确率，对冗余规则删除和所有等价重写（单定律或多定律）完全泛化，但在必要规则删除下准确率降至25%，在明确矛盾存在时完全崩溃（0%准确率）。

Conclusion: LLMs对语义保持的逻辑变换具有稳定的不变性，但对缺失或冲突证据仍然极其脆弱。该框架为诊断推理失败模式提供了清晰工具，突显了当前LLMs在逻辑泛化能力上的持续差距。

Abstract: Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.
  Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.

</details>


### [241] [GENIUS: An Agentic AI Framework for Autonomous Design and Execution of Simulation Protocols](https://arxiv.org/abs/2512.06404)
*Mohammad Soleymanibrojeni,Roland Aydin,Diego Guedes-Sobrinho,Alexandre C. Dias,Maurício J. Piotrowski,Wolfgang Wenzel,Celso Ricardo Caldeira Rêgo*

Main category: cs.AI

TL;DR: GENIUS是一个AI代理工作流，融合量子ESPRESSO知识图谱和分层大语言模型，通过有限状态错误恢复机监督，将自由文本提示转换为验证过的输入文件，实现电子结构DFT模拟的自动化。


<details>
  <summary>Details</summary>
Motivation: 尽管预测性原子模拟推动了材料发现，但常规设置和调试仍需要计算机专家，这种知识差距限制了集成计算材料工程（ICME）的发展。现有先进代码对非专家来说仍然繁琐。

Method: GENIUS工作流结合智能量子ESPRESSO知识图谱和分层大语言模型，由有限状态错误恢复机监督，将自由文本提示转换为验证过的输入文件。

Result: 在295个多样化基准测试中，约80%成功运行到完成，其中76%被自主修复，成功率呈指数衰减至7%基线。相比纯LLM基线，GENIUS将推理成本减半并几乎消除幻觉。

Conclusion: 该框架通过智能自动化协议生成、验证和修复，实现了电子结构DFT模拟的民主化，为大规模筛选和加速ICME设计循环开辟了道路。

Abstract: Predictive atomistic simulations have propelled materials discovery, yet routine setup and debugging still demand computer specialists. This know-how gap limits Integrated Computational Materials Engineering (ICME), where state-of-the-art codes exist but remain cumbersome for non-experts. We address this bottleneck with GENIUS, an AI-agentic workflow that fuses a smart Quantum ESPRESSO knowledge graph with a tiered hierarchy of large language models supervised by a finite-state error-recovery machine. Here we show that GENIUS translates free-form human-generated prompts into validated input files that run to completion on $\approx$80% of 295 diverse benchmarks, where 76% are autonomously repaired, with success decaying exponentially to a 7% baseline. Compared with LLM-only baselines, GENIUS halves inference costs and virtually eliminates hallucinations. The framework democratizes electronic-structure DFT simulations by intelligently automating protocol generation, validation, and repair, opening large-scale screening and accelerating ICME design loops across academia and industry worldwide.

</details>


### [242] [UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems](https://arxiv.org/abs/2512.06406)
*Xianzong Wu,Xiaohong Li,Lili Quan,Qiang Hu*

Main category: cs.AI

TL;DR: UncertaintyZoo是一个统一的不确定性量化工具包，集成了29种方法，用于评估大语言模型在代码漏洞检测等任务中的预测置信度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在安全关键场景中可能做出错误预测，现有不确定性量化方法缺乏统一工具，阻碍了实际应用和研究发展。

Method: 开发UncertaintyZoo工具包，集成29种不确定性量化方法，涵盖五大类别，提供标准化接口，并在CodeBERT和ChatGLM3模型上进行代码漏洞检测评估。

Result: UncertaintyZoo能有效揭示模型预测的不确定性，为不确定性量化方法提供了实用的评估工具。

Conclusion: UncertaintyZoo填补了不确定性量化工具集的空白，促进了该领域的研究和应用发展。

Abstract: Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.

</details>


### [243] [Smart Spatial Planning in Egypt: An Algorithm-Driven Approach to Public Service Evaluation in Qena City](https://arxiv.org/abs/2512.06431)
*Mohamed Shamroukh,Mohamed Alkhuzamy Aziz*

Main category: cs.AI

TL;DR: 本研究为埃及基纳市开发了基于Voronoi图的定制化规划模型，通过Python算法分析公共服务覆盖情况，发现平均覆盖率为81.3%，揭示了市中心与郊区服务分布不均的问题。


<details>
  <summary>Details</summary>
Motivation: 埃及国家公共服务规划标准往往无法适应地方独特特征，需要开发能够反映地方特性的定制化规划模型来解决这一差距。

Method: 采用混合方法（描述性、分析性和实验性），利用Python编程开发基于Voronoi图的智能空间分析算法，生成城市特定的规划标准并评估公共服务设施当前覆盖情况。

Result: 模型应用显示：平均服务覆盖率为81.3%；救护车站效率最高（99.8%），公园和开放空间覆盖最低（10%）；市中心服务密度高（>45个/km²），郊区显著降低（<5个/km²）；Hajer基纳区未服务区域最多，第一区服务覆盖水平最高。

Conclusion: 成功开发了本地化规划标准模型和自动化评估算法，为埃及城市提供了可复制的数据驱动城市规划框架，能够更精准地识别和解决公共服务分布不均问题。

Abstract: National planning standards for public services in Egypt often fail to align with unique local characteristics. Addressing this gap, this study develops a tailored planning model for Qena City. Using a hybrid methodology (descriptive, analytical, and experimental), the research utilizes Python programming to generate an intelligent spatial analysis algorithm based on Voronoi Diagrams. This approach creates city-specific planning criteria and evaluates the current coverage of public facilities. The primary contribution of this study is the successful derivation of a localized planning standards model and the deployment of an automated algorithm to assess service efficiency. Application of this model reveals a general service coverage average of 81.3%. Ambulance stations demonstrated the highest efficiency (99.8%) due to recent upgrades, while parks and open spaces recorded the lowest coverage (10%) caused by limited land availability. Spatial analysis indicates a high service density in midtown (>45 services/km^2), which diminishes significantly towards the outskirts (<5 services/km^2). Consequently, the Hajer Qena district contains the highest volume of unserved areas, while the First District (Qesm 1) exhibits the highest level of service coverage. This model offers a replicable framework for data-driven urban planning in Egyptian cities.

</details>


### [244] [The Effect of Belief Boxes and Open-mindedness on Persuasion](https://arxiv.org/abs/2512.06573)
*Onur Bilgin,Abdullah As Sami,Sriram Sai Vujjini,John Licato*

Main category: cs.AI

TL;DR: 研究探索LLM智能体通过"信念盒"技术维护命题信念对其行为和说服力的影响，发现信念陈述和开放心态指令会影响智能体的信念改变倾向和说服效果。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统在推理和决策应用中日益普及，需要让基于LLM的智能体具备类似命题信念的能力。当前简单方法是在提示空间中包含信念陈述（信念盒），但需要研究这种信念盒如何影响智能体的行为和信念倾向，以及在多智能体场景中如何影响说服力。

Method: 通过一系列实验探索信念盒技术的影响：1）在智能体提示中包含信念陈述及其强度；2）测试智能体对相反观点的抵抗力和说服力；3）研究开放心态指令对行为的影响；4）在辩论中测试少数派情境（同伴压力场景）下的信念改变可能性。

Result: 1）开放心态指令确实影响智能体对信念改变的接受程度；2）信念陈述及其强度会影响智能体对相反观点的抵抗力和说服力；3）当智能体在辩论中被相反观点包围时（同伴压力场景），信念改变的可能性增加；4）证明了信念盒技术在推理和决策任务中的可行性和有效性。

Conclusion: 信念盒技术是有效的，能够影响LLM智能体的信念相关行为。开放心态指令、信念陈述强度和同伴压力都是影响信念改变的重要因素。这为多智能体系统中的信念建模和交互提供了实证基础。

Abstract: As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.

</details>


### [245] [FlatFormer: A Flat Transformer Knowledge Tracing Model Based on Cognitive Bias Injection](https://arxiv.org/abs/2512.06629)
*Xiao-li Xia,Hou-biao Li*

Main category: cs.AI

TL;DR: FlatFormer：一种扁平化Transformer架构，通过信息注入而非结构堆叠来解决知识追踪中的"性能-复杂度陷阱"，在保持高性能的同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 知识追踪模型面临"性能-复杂度陷阱"：捕捉复杂认知动态（如学习会话和记忆衰减）通常需要深度层次架构，但这会导致实时部署时计算成本过高。需要一种既能保持高性能又计算高效的解决方案。

Method: 提出FlatFormer，采用"信息注入而非结构堆叠"的设计范式。使用标准扁平Transformer，通过两种轻量级注入机制增强：1）混合输入编码策略（可学习会话标识符+固定正弦步长嵌入）；2）预计算幂律偏置直接集成到注意力对数中，显式建模遗忘曲线。

Result: 在四个大规模数据集（如EdNet、Junyi）上的实验表明，FlatFormer达到最先进性能。在EdNet数据集上，相比最强层次基线（HiTSKT），绝对AUC提升8.3%，参数使用量不到15%，推理速度约快3倍。

Conclusion: 高认知保真度不一定需要架构复杂性。FlatFormer通过信息注入而非结构堆叠，在保持高性能的同时显著降低了计算成本，解决了知识追踪中的性能-复杂度权衡问题。

Abstract: Knowledge Tracing (KT) models face a critical ``Performance-Complexity Trap'': capturing complex cognitive dynamics like learning sessions and memory decay typically requires deep hierarchical architectures, which incur prohibitive computational costs for real-time deployment. To resolve this, we propose FlatFormer, a streamlined architecture based on the novel design paradigm of ``Information Injection over Structural Stacking.'' Unlike parameter-heavy hierarchical models, FlatFormer leverages a standard flat Transformer augmented with two lightweight injection mechanisms: (i) a hybrid input encoding strategy combining learnable session identifiers with fixed sinusoidal step embeddings; and (ii) a pre-computed power-law bias integrated directly into attention logits to explicitly model the forgetting curve. Extensive experiments on four large-scale datasets (e.g., EdNet, Junyi) show that FlatFormer achieves state-of-the-art performance. For example, on the EdNet dataset, compared to the strongest hierarchical baseline (HiTSKT), its absolute AUC increased by 8.3%, while using less than 15% of parameters, and inference speed was about three times faster. These results validate that high cognitive fidelity does not necessitate architectural complexity.

</details>


### [246] [LightSearcher: Efficient DeepSearch via Experiential Memory](https://arxiv.org/abs/2512.06653)
*Hengzhi Lan,Yue Yu,Li Qian,Li Peng,Jie Wu,Wei Liu,Jian Luan,Ting Bai*

Main category: cs.AI

TL;DR: LightSearcher是一个高效的强化学习框架，通过文本经验记忆和自适应奖励机制，在保持准确性的同时显著减少DeepSearch系统中的工具调用次数和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的RL驱动的DeepSearch系统存在准确性与效率之间的权衡问题：频繁调用外部搜索工具可以提高事实准确性，但会导致不必要的计算开销和效率降低。

Method: 1) 引入文本经验记忆，通过学习对比推理轨迹来生成成功推理模式的可解释摘要；2) 采用自适应奖励塑造机制，仅在正确答案场景中惩罚冗余工具调用。

Result: 在四个多跳QA基准测试中，LightSearcher保持了与SOTA基线ReSearch相当的准确性，同时将搜索工具调用减少39.6%，推理时间减少48.6%，token消耗减少21.2%。

Conclusion: LightSearcher通过创新的记忆和奖励机制设计，有效解决了DeepSearch范式中的准确性与效率权衡问题，实现了更高效的深度推理。

Abstract: DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.

</details>


### [247] [Academic journals' AI policies fail to curb the surge in AI-assisted academic writing](https://arxiv.org/abs/2512.06705)
*Yongyuan He,Yi Bu*

Main category: cs.AI

TL;DR: 尽管70%的期刊制定了AI使用政策，但研究人员使用AI写作工具的情况在各学科中急剧增加，且政策期刊与非政策期刊之间无显著差异。全文分析显示透明度差距巨大：2023年以来发表的7.5万篇论文中，只有76篇（0.1%）明确披露了AI使用。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在学术写作中的快速整合，期刊和出版商制定了广泛的政策回应，但这些政策的有效性尚不清楚。本研究旨在评估AI使用指南在现实世界中的实际影响。

Method: 分析了5,114种期刊和超过520万篇论文，通过大规模数据分析评估AI政策的效果。特别对164,000篇科学出版物进行了全文分析，以检测AI使用的披露情况。

Result: 1. 尽管70%的期刊采用了AI政策（主要是要求披露），但研究人员使用AI写作工具的情况在各学科中急剧增加；2. 有政策期刊与无政策期刊之间在AI使用增长方面无显著差异；3. 非英语国家、物理科学和高开放获取期刊显示出最高的增长率；4. 透明度差距显著：2023年以来发表的75,000篇论文中，只有76篇（0.1%）明确披露了AI使用。

Conclusion: 当前政策在很大程度上未能促进透明度或限制AI采用。需要重新评估伦理框架，以促进科学中负责任的AI整合。

Abstract: The rapid integration of generative AI into academic writing has prompted widespread policy responses from journals and publishers. However, the effectiveness of these policies remains unclear. Here, we analyze 5,114 journals and over 5.2 million papers to evaluate the real-world impact of AI usage guidelines. We show that despite 70% of journals adopting AI policies (primarily requiring disclosure), researchers' use of AI writing tools has increased dramatically across disciplines, with no significant difference between journals with or without policies. Non-English-speaking countries, physical sciences, and high-OA journals exhibit the highest growth rates. Crucially, full-text analysis on 164k scientific publications reveals a striking transparency gap: Of the 75k papers published since 2023, only 76 (0.1%) explicitly disclosed AI use. Our findings suggest that current policies have largely failed to promote transparency or restrain AI adoption. We urge a re-evaluation of ethical frameworks to foster responsible AI integration in science.

</details>


### [248] [Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass Correlation](https://arxiv.org/abs/2512.06710)
*Zairah Mustahsan,Abel Lim,Megna Anand,Saahil Jain,Bryan McCann*

Main category: cs.AI

TL;DR: 论文提出使用组内相关系数(ICC)来评估语言模型代理系统的稳定性，区分任务难度和代理不一致性，提高评估可靠性


<details>
  <summary>Details</summary>
Motivation: 当前评估实践仅报告单次运行的准确率，掩盖了结果背后的方差，无法区分真实能力提升和随机采样运气。随着大语言模型成为更大代理系统的组件，评估可靠性变得至关重要

Method: 采用测量科学中的组内相关系数(ICC)来分解观察方差：任务间方差(任务难度)和任务内方差(代理不一致性)。在GAIA(代理能力)和FRAMES(检索和事实性)数据集上进行评估

Result: ICC随任务结构变化显著：推理和检索任务(FRAMES)ICC=0.4955-0.7118，代理任务(GAIA)ICC=0.304-0.774。ICC在结构化任务中n=8-16次试验收敛，复杂推理任务需要n>=32次试验

Conclusion: 建议将准确率与ICC和任务内方差一起作为标准报告实践，更新评估卡片以包含这些指标。通过使评估稳定性可见，将代理基准测试从不透明的排行榜竞争转变为可信赖的实验科学

Abstract: As large language models become components of larger agentic systems, evaluation reliability becomes critical: unreliable sub-agents introduce brittleness into downstream system behavior. Yet current evaluation practice, reporting a single accuracy number from a single run, obscures the variance underlying these results, making it impossible to distinguish genuine capability improvements from lucky sampling. We propose adopting Intraclass Correlation Coefficient (ICC), a metric from measurement science, to characterize this variance. ICC decomposes observed variance into between-query variance (task difficulty) and within-query variance (agent inconsistency), highlighting whether reported results reflect true capability or measurement noise. We evaluated on GAIA (Levels 1-3, measuring agentic capabilities across varying reasoning complexity) and FRAMES (measuring retrieval and factuality across multiple documents). We found that ICC varies dramatically with task structure, with reasoning and retrieval tasks (FRAMES) exhibit ICC=0.4955-0.7118 across models, and agentic tasks (GAIA) exhibiting ICC=0.304-0.774 across models. For sub-agent replacement decisions in agentic systems, accuracy improvements are only trustworthy if ICC also improves. We demonstrate that ICC converges by n=8-16 trials for structured tasks and n>=32 for complex reasoning, enabling practitioners to set evidence-based resampling budgets. We recommend reporting accuracy alongside ICC and within-query variance as standard practice, and propose updated Evaluation Cards capturing these metrics. By making evaluation stability visible, we aim to transform agentic benchmarking from opaque leaderboard competition to trustworthy experimental science. Our code is open-sourced at https://github.com/youdotcom-oss/stochastic-agent-evals.

</details>


### [249] [Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents](https://arxiv.org/abs/2512.06716)
*Zhibo Liang,Tianze Hu,Zaiye Chen,Mingjie Tang*

Main category: cs.AI

TL;DR: 论文提出认知控制架构（CCA），通过意图图和分层裁决器实现全生命周期认知监督，有效防御LLM代理的间接提示注入攻击，在安全、功能和效率间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理防御机制存在根本性缺陷：安全与功能之间存在权衡，防御架构碎片化，无法在整个任务执行流程中提供完整完整性保证，导致在多维安全、功能和效率之间做出不可接受的妥协。

Method: 提出认知控制架构（CCA），包含两个协同支柱：1）通过预生成的"意图图"实现主动控制流和数据流完整性执行；2）创新的"分层裁决器"，在检测到偏差时基于多维评分启动深度推理，专门应对复杂条件攻击。

Result: 在AgentDojo基准测试中，CCA不仅有效抵御了挑战其他先进防御方法的复杂攻击，而且在保持显著效率和鲁棒性的同时实现了无妥协的安全性，成功调和了多维权衡。

Conclusion: CCA通过全生命周期认知监督，基于攻击最终会表现为可检测的行动轨迹偏差这一核心洞察，构建了高效的双层防御系统，为LLM代理提供了完整的完整性保证，解决了现有防御机制的碎片化问题。

Abstract: Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated "Intent Graph"; and (ii) an innovative "Tiered Adjudicator" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.

</details>


### [250] [ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems](https://arxiv.org/abs/2512.06721)
*Bufang Yang,Lilin Xu,Liekang Zeng,Yunqi Guo,Siyang Jiang,Wenrui Lu,Kaiwei Liu,Hancheng Xiang,Xiaofan Jiang,Guoliang Xing,Zhenyu Yan*

Main category: cs.AI

TL;DR: ProAgent：首个端到端主动式LLM代理系统，通过多层级感知和上下文推理提供主动协助，在AR眼镜上实现并显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理主要采用被动响应模式，需要用户明确指令才能启动服务，这增加了用户的物理和认知负担。需要一种能够主动感知环境并提供协助的代理系统。

Method: 1. 主动导向的上下文提取：采用按需分层感知技术持续感知环境，提取包含感官和人物特征的分层上下文；2. 上下文感知的主动推理器：将上下文映射到用户需求和工具调用，提供主动协助

Result: 在真实世界测试平台、公共数据集和用户研究中评估，ProAgent比最先进基线方法：主动预测准确率提高33.4%，工具调用F1分数提高16.8%，用户满意度显著提升

Conclusion: ProAgent是首个端到端主动代理系统，通过结合大规模感官上下文和LLM推理，在AR眼镜上实现主动协助，标志着向主动助手迈出了重要一步

Abstract: Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.

</details>


### [251] [DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems](https://arxiv.org/abs/2512.06749)
*Ming Ma,Jue Zhang,Fangkai Yang,Yu Kang,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: DoVer是一个干预驱动的调试框架，通过主动干预验证失败假设，解决LLM多智能体系统调试难题，显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前基于日志的LLM多智能体系统调试方法存在两个关键局限：1) 仅基于日志的调试缺乏验证，产生未经验证的假设；2) 单步或单智能体归因往往不准确，因为多个不同的干预都可能独立修复失败任务。

Method: DoVer框架将假设生成与主动验证相结合，通过目标干预（如编辑消息、修改计划）来验证失败假设。采用结果导向的调试视角，关注系统是否解决失败或实现任务进展，而非传统的归因准确性。

Result: 在Magnetic-One智能体框架上，DoVer将18-28%的失败试验转为成功，实现高达16%的里程碑进展，验证或反驳30-60%的失败假设。在不同数据集(GSMPlus)和框架(AG2)上，恢复了49%的失败试验。

Conclusion: 干预是提高智能体系统可靠性的实用机制，为LLM多智能体系统提供了更稳健、可扩展的调试方法，开启了新的调试范式。

Abstract: Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.

</details>


### [252] [Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning](https://arxiv.org/abs/2512.06835)
*Tingyu Li,Zheng Sun,Jingxuan Wei,Siyuan Li,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: DoGe提出双解耦框架，通过分离思考者和解决者组件，结合两阶段强化学习，解决专业领域视觉语言模型训练中的数据稀缺和奖励破解问题，实现持续自我进化。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型通过强化学习实现自我进化，但在化学、地球科学等专业领域面临高质量多模态数据稀缺问题。现有方法如合成数据和自奖励机制存在分布有限和对齐困难，导致奖励破解问题——模型利用高奖励模式，使策略熵崩溃并破坏训练稳定性。

Method: DoGe采用双解耦框架：1）将学习过程解耦为思考者和解决者两个组件，引导模型首先从上下文学习而非直接解决问题；2）提出两阶段强化学习后训练方法，从自由探索上下文到实际解决任务；3）构建演化课程学习管道，包括扩展的本地领域知识语料库和迭代演化的种子问题池。

Result: 实验表明，DoGe方法在各种基准测试中持续优于基线方法，为解决视觉语言模型在专业领域的自我进化提供了可扩展的途径。

Conclusion: DoGe通过双解耦框架和演化课程学习，有效解决了专业领域视觉语言模型训练中的数据稀缺和奖励破解问题，为实现持续自我进化的大型视觉语言模型提供了可行的解决方案。

Abstract: Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.

</details>


### [253] [JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models](https://arxiv.org/abs/2512.06859)
*Ce Chi,Xing Wang,Zhendong Wang,Xiaofan Liu,Ce Li,Zhiyan Song,Chen Zhao,Kexin Yang,Boshen Shi,Jingjing Yang,Chao Deng,Junlan Feng*

Main category: cs.AI

TL;DR: JT-DA-8B是一个专门用于复杂表格推理任务的8B参数大语言模型，通过构建包含34个表格推理任务的多样化训练语料，采用SFT和RL优化，结合四阶段推理工作流程，在多种表格推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决表格推理场景中高质量监督数据缺乏的问题，开发能够处理多样化真实世界表格推理任务的专用模型。

Method: 1. 构建包含29个公开表格QA数据集和300万张表格的多样化训练语料；2. 提出自动管道生成现实多步分析任务；3. 基于开源JT-Coder-8B模型，采用LLM评分和工作流对齐过滤蒸馏高质量表格中心数据；4. 结合监督微调(SFT)和强化学习(RL)优化模型；5. 提出四阶段表格推理工作流程：表格预处理、表格感知、工具集成推理和提示工程。

Result: JT-DA-8B在各种表格推理任务中表现出强大的性能，证明了数据中心生成和工作流驱动优化的有效性。

Conclusion: 通过构建高质量训练数据和优化推理工作流程，JT-DA-8B成功解决了复杂表格推理任务，为表格分析领域提供了有效的解决方案。

Abstract: In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.

</details>


### [254] [Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?](https://arxiv.org/abs/2512.06867)
*John Licato,Stephen Steinle,Brayden Hollis*

Main category: cs.AI

TL;DR: 研究探讨人格提示对LLM在战略游戏PERIL中决策的影响，发现通过中介翻译过程将人格映射为启发式值能提升游戏表现，而直接推断的启发式效果不佳。


<details>
  <summary>Details</summary>
Motivation: 虽然人格提示能触发LLM生成不同风格的文本，但尚不清楚这些差异是否转化为可测量的行为差异，特别是在对抗性战略环境中。本研究旨在探究人格提示对战略决策的实际影响。

Method: 在PERIL（世界统治棋盘游戏）中测试人格提示的效果。引入结构化翻译过程作为中介，受探索性因子分析启发，将LLM生成的清单响应映射为启发式值。比较人格衍生的启发式策略与手动选择的策略。

Result: 某些与战略思维相关的人格能提升游戏表现，但仅当使用中介翻译过程时有效。该方法相比直接推断的启发式，提高了启发式的可靠性和表面效度，能更好地研究人格类型对决策的影响。

Conclusion: 人格提示确实影响LLM的决策制定，但需要通过结构化翻译过程将人格映射为启发式值才能有效。该方法将心理测量学原理应用于LLM，为研究人格类型对决策的影响提供了新工具。

Abstract: Although persona prompting in large language models appears to trigger different styles of generated text, it is unclear whether these translate into measurable behavioral differences, much less whether they affect decision-making in an adversarial strategic environment that we provide as open-source. We investigate the impact of persona prompting on strategic performance in PERIL, a world-domination board game. Specifically, we compare the effectiveness of persona-derived heuristic strategies to those chosen manually. Our findings reveal that certain personas associated with strategic thinking improve game performance, but only when a mediator is used to translate personas into heuristic values. We introduce this mediator as a structured translation process, inspired by exploratory factor analysis, that maps LLM-generated inventory responses into heuristics. Results indicate our method enhances heuristic reliability and face validity compared to directly inferred heuristics, allowing us to better study the effect of persona types on decision making. These insights advance our understanding of how persona prompting influences LLM-based decision-making and propose a heuristic generation method that applies psychometric principles to LLMs.

</details>


### [255] [On Memory: A comparison of memory mechanisms in world models](https://arxiv.org/abs/2512.06983)
*Eli J. Laird,Corey Clark*

Main category: cs.AI

TL;DR: 该论文研究了基于Transformer的世界模型的有效记忆跨度问题，分析了多种记忆增强机制，提出了一种记忆编码与记忆注入的分类法，并通过状态回忆任务评估了不同机制的性能。


<details>
  <summary>Details</summary>
Motivation: 世界模型能够通过预测未来状态来帮助智能体在想象环境中进行规划，但其长期规划能力受到基础架构有效记忆跨度的限制。这种限制导致长序列推演中出现感知漂移，阻碍模型在想象轨迹中完成闭环检测。

Method: 研究分析了基于Transformer的世界模型的多种记忆增强机制，提出了区分记忆编码和记忆注入机制的分类法，并通过残差流动态的视角解释这些机制如何扩展世界模型的记忆。使用状态回忆评估任务来测量每种机制的记忆回忆能力并分析其权衡。

Result: 研究发现记忆机制能够提高视觉Transformer的有效记忆跨度，并为在世界模型的想象中完成闭环检测提供了路径。

Conclusion: 记忆增强机制对于扩展基于Transformer的世界模型的记忆跨度至关重要，能够改善长序列推演中的感知漂移问题，使模型能够在想象轨迹中完成闭环检测，从而提升长期规划能力。

Abstract: World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.

</details>


### [256] [Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients](https://arxiv.org/abs/2512.06990)
*Krishna Arun,Moinak Bhattachrya,Paras Goel*

Main category: cs.AI

TL;DR: 开发了一个用于胶质母细胞瘤诊断和治疗规划的端到端AI系统，通过序列决策框架和强化学习优化，显著降低了计算成本并提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 目前医疗领域缺乏支持医生治疗胶质母细胞瘤（GBM）的AI系统，GBM是五年生存率仅5.1%的最致命癌症，需要端到端的解决方案来辅助诊断和治疗规划。

Method: 诊断阶段采用包含4个分类模型（CNN和SVM）的序列决策框架，逐步分类脑部图像；治疗规划阶段使用包含3个生成模型的强化学习系统：扩散模型预测切除结果、时空视觉Transformer预测放疗后进展、扩散模型生成化疗后MRI，并通过CNN生存率计算器和近端策略优化进行反馈循环优化。

Result: 序列决策框架降低计算成本22.28倍，Transformer减少肿瘤进展推理时间113小时，真实场景数据增强提高DICE分数2.9%，预计提高生存率0.9%，可能挽救约2250条生命。

Conclusion: 该AI系统为胶质母细胞瘤提供了首个端到端的诊断和治疗规划解决方案，通过创新的序列决策和强化学习方法显著提升了效率和准确性，有望改善患者生存率。

Abstract: Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient's brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain's progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.

</details>


### [257] [ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes](https://arxiv.org/abs/2512.07081)
*Rongjia Zhou,Chengzhuo Li,Carl Yang,Jiaying Lu*

Main category: cs.AI

TL;DR: ClinNoteAgents是一个基于大语言模型的多智能体框架，用于从临床文本笔记中提取心衰再入院风险因素并进行预测，减少对结构化数据和人工标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 心衰是美国老年人再住院的主要原因之一。临床笔记包含丰富的患者信息，但在心衰再入院风险分析中未得到充分利用。传统模型依赖专家规则和医学词典，难以处理临床笔记中的拼写错误、缩写和领域特定术语。

Method: 提出ClinNoteAgents框架，使用LLM-based多智能体将自由文本临床笔记转换为：(1) 临床和社会风险因素的结构化表示用于关联分析；(2) 临床医生风格的抽象表示用于30天再入院预测。

Result: 在3,544份笔记（来自2,065名患者，再入院率35.16%）上评估，在从自由文本提取风险因素、识别关键贡献因素和预测再入院风险方面表现出色。

Conclusion: 通过减少对结构化字段的依赖并最小化人工标注和模型训练，ClinNoteAgents为数据有限的医疗系统提供了可扩展且可解释的基于笔记的心衰再入院风险建模方法。

Abstract: Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.

</details>


### [258] [VIGIL: A Reflective Runtime for Self-Healing Agents](https://arxiv.org/abs/2512.07094)
*Christopher Cruz*

Main category: cs.AI

TL;DR: VIGIL是一个可验证的检查和防护迭代学习运行时系统，通过监督兄弟代理、分析行为日志、维护情感银行并生成诊断和修复计划，实现代理系统的自主维护和自我修复。


<details>
  <summary>Details</summary>
Motivation: 当前代理LLM框架虽然承诺通过任务分解、工具使用和迭代规划实现自主行为，但大多数部署系统仍然脆弱。它们缺乏运行时内省能力，无法诊断自身的故障模式，并且在没有人工干预的情况下无法随时间改进。许多代理系统退化为装饰性的LLM调用链，没有可靠性的结构机制。

Method: VIGIL是一个反射运行时系统，监督兄弟代理并执行自主维护而非任务执行。它摄入行为日志，将每个事件评估为结构化情感表示，维护具有衰减和上下文策略的持久情感银行，并生成RBT诊断（将近期行为分类为优势、机会和失败）。基于此分析，VIGIL生成既保护核心身份语义的防护提示更新，又生成由策略引擎基于日志证据和代码热点产生的只读代码提案。VIGIL作为状态门控管道运行，非法转换会产生明确错误而非让LLM即兴发挥。

Result: 在提醒延迟案例研究中，VIGIL识别出延迟升高，提出了提示和代码修复方案。当自身的诊断工具因模式冲突而失败时，它暴露了内部错误，生成了备用诊断，并发布了修复计划。这展示了在部署的代理运行时中实现元级别的自我修复能力。

Conclusion: VIGIL通过引入反射运行时监督、结构化情感分析、持久行为诊断和自主修复机制，解决了当前代理系统缺乏内省和自我改进能力的问题，实现了代理系统的元级别自我修复和持续改进。

Abstract: Agentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle. They lack runtime introspection, cannot diagnose their own failure modes, and do not improve over time without human intervention. In practice, many agent stacks degrade into decorated chains of LLM calls with no structural mechanisms for reliability. We present VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime that supervises a sibling agent and performs autonomous maintenance rather than task execution. VIGIL ingests behavioral logs, appraises each event into a structured emotional representation, maintains a persistent EmoBank with decay and contextual policies, and derives an RBT diagnosis that sorts recent behavior into strengths, opportunities, and failures. From this analysis, VIGIL generates both guarded prompt updates that preserve core identity semantics and read only code proposals produced by a strategy engine that operates on log evidence and code hotspots. VIGIL functions as a state gated pipeline. Illegal transitions produce explicit errors rather than allowing the LLM to improvise. In a reminder latency case study, VIGIL identified elevated lag, proposed prompt and code repairs, and when its own diagnostic tool failed due to a schema conflict, it surfaced the internal error, produced a fallback diagnosis, and emitted a repair plan. This demonstrates meta level self repair in a deployed agent runtime.

</details>


### [259] [A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy](https://arxiv.org/abs/2512.07109)
*Miguel Ingram,Arthur Joseph Merritt*

Main category: cs.AI

TL;DR: 提出首个9类别、400个任务的分类法，通过CNN验证视觉一致性，揭示Transformer架构存在组合性鸿沟和神经亲和力天花板效应。


<details>
  <summary>Details</summary>
Motivation: 响应Hodel等人对任务相关性形式化定义的呼吁，为ARC-AGI-2测试集提供系统化分析框架，诊断神经网络在抽象推理任务中的局限性。

Method: 1) 开发基于规则代码分析的9类别任务分类法；2) 使用原始网格像素训练CNN验证分类视觉一致性；3) 在302个任务上微调170万参数Transformer；4) 应用分类法分析ARC-AGI-2测试集和独立研究数据。

Result: 1) 分类法准确率达97.5%；2) CNN在S3上达95.24%准确率；3) 发现69.5%任务存在组合性鸿沟（局部模式>80%但全局合成<10%）；4) 低亲和力任务仅51.9%准确率，高亲和力达77.7%；5) 35.3%任务对Transformer亲和力低。

Conclusion: 神经网络在抽象推理任务中存在架构限制，需要亲和力对齐的混合架构。任务分类法能精确诊断性能天花板，为未来研究提供关键工具。

Abstract: Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,

</details>


### [260] [ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation](https://arxiv.org/abs/2512.07178)
*Latifa Dwiyanti,Sergio Ryan Wibisono,Hidetaka Nambo*

Main category: cs.AI

TL;DR: 提出一个结合SHAP和GPT的Python包，为机器学习解释生成上下文文本说明，提高非技术用户的理解性


<details>
  <summary>Details</summary>
Motivation: SHAP虽然能提供特征重要性可视化，但缺乏对非技术用户有意义的上下文解释，特别是在高风险领域部署机器学习模型时，可解释性AI变得日益重要

Method: 开发Python包，将SHAP与OpenAI的GPT大语言模型集成，通过用户定义的参数（如特征别名、描述和背景信息）生成上下文化的文本解释

Result: 在医疗相关案例研究中应用该包，通过李克特量表和后续访谈进行用户评估，结果显示生成的解释比纯视觉输出更容易理解和更符合上下文

Conclusion: 可视化与上下文文本结合可能支持更用户友好和可信的模型解释，虽然结果是初步的，但显示了这种方法的潜力

Abstract: Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.

</details>


### [261] [PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations](https://arxiv.org/abs/2512.07179)
*Wonbeen Lee,Channyoung Lee,Junho Sohn,Hansam Cho*

Main category: cs.AI

TL;DR: 提出PICKT模型，通过知识图谱处理多种输入数据格式，解决知识追踪中的冷启动问题，提升在真实ITS环境中的实用性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型存在输入数据格式受限、新学生/新问题冷启动问题、真实服务环境稳定性不足等局限，需要开发更实用的模型来支持个性化学习系统。

Method: 提出PICKT模型，利用知识图谱结构化概念间关系，结合题目和概念文本信息，有效处理多种输入数据格式，解决冷启动问题。

Result: 实验显示模型在真实操作环境中表现优异，在新学生注册和新问题添加两个核心冷启动挑战上显著优于现有模型，验证了稳定性和实用性。

Conclusion: PICKT模型为下一代智能辅导系统的实际应用提供了重要的理论和技术基础，增强了知识追踪模型在真实产品环境中的适用性。

Abstract: With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.

</details>


### [262] [Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation](https://arxiv.org/abs/2512.07212)
*Zhaoyang Liu,Mokai Pan,Zhongyi Wang,Kaizhen Zhu,Haotao Lu,Jingya Wang,Ye Shi*

Main category: cs.AI

TL;DR: BridgePolicy：一种基于扩散桥的生成式视觉运动策略，将观测直接嵌入扩散过程的随机动力学中，而非仅作为条件输入，从而提升机器人控制的精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的模仿学习方法通常将观测作为去噪网络的高层条件输入，而不是将其整合到扩散过程的随机动力学中。这导致采样必须从随机高斯噪声开始，削弱了感知与控制之间的耦合，往往产生次优性能。

Method: 提出BridgePolicy，通过扩散桥公式将观测嵌入随机微分方程中。设计了多模态融合模块和语义对齐器，统一视觉和状态输入，对齐观测和动作表示，使桥接适用于异构机器人数据。

Result: 在三个基准测试的52个仿真任务和五个真实世界任务上的广泛实验表明，BridgePolicy始终优于最先进的生成策略。

Conclusion: BridgePolicy通过将观测直接整合到扩散过程的随机动力学中，实现了从信息丰富的先验而非随机噪声开始采样，显著提高了机器人控制的精度和可靠性。

Abstract: Imitation learning with diffusion models has advanced robotic control by capturing multi-modal action distributions. However, existing approaches typically treat observations as high-level conditioning inputs to the denoising network, rather than integrating them into the stochastic dynamics of the diffusion process itself. As a result, sampling must begin from random Gaussian noise, weakening the coupling between perception and control and often yielding suboptimal performance. We introduce BridgePolicy, a generative visuomotor policy that explicitly embeds observations within the stochastic differential equation via a diffusion-bridge formulation. By constructing an observation-informed trajectory, BridgePolicy enables sampling to start from a rich, informative prior rather than random noise, substantially improving precision and reliability in control. A key challenge is that classical diffusion bridges connect distributions with matched dimensionality, whereas robotic observations are heterogeneous and multi-modal and do not naturally align with the action space. To address this, we design a multi-modal fusion module and a semantic aligner that unify visual and state inputs and align observation and action representations, making the bridge applicable to heterogeneous robot data. Extensive experiments across 52 simulation tasks on three benchmarks and five real-world tasks demonstrate that BridgePolicy consistently outperforms state-of-the-art generative policies.

</details>


### [263] [Cross-platform Product Matching Based on Entity Alignment of Knowledge Graph with RAEA model](https://arxiv.org/abs/2512.07232)
*Wenlong Liu,Jiahua Pan,Xingyu Zhang,Xinxin Gong,Yang Ye,Xujin Zhao,Xin Wang,Kent Wu,Hua Xiang,Houmin Yan,Qingpeng Zhang*

Main category: cs.AI

TL;DR: RAEA模型通过结合属性三元组和关系三元组的交互信息，提升产品匹配和实体对齐性能，在跨语言数据集DBP15K上相比12个基线模型平均提升6.59%的Hits@1。


<details>
  <summary>Details</summary>
Motivation: 现有实体对齐方法未能充分利用属性三元组和关系三元组，特别是它们之间的交互信息。产品匹配问题可以通过构建知识图谱转化为实体对齐任务，需要更有效的方法来整合这两种信息源。

Method: 采用两阶段流水线：粗略过滤和精细过滤。精细过滤阶段使用RAEA框架，包含属性感知实体编码器和关系感知图注意力网络，通过聚合属性和关系的对齐信号来学习实体表示。

Result: RAEA模型在跨语言数据集DBP15K上相比12个基线模型平均提升6.59%的Hits@1，在单语言数据集DWY100K上也取得有竞争力的结果。

Conclusion: RAEA模型通过有效整合属性三元组和关系三元组的交互信息，显著提升了实体对齐性能，为产品匹配等应用提供了有效解决方案。

Abstract: Product matching aims to identify identical or similar products sold on different platforms. By building knowledge graphs (KGs), the product matching problem can be converted to the Entity Alignment (EA) task, which aims to discover the equivalent entities from diverse KGs. The existing EA methods inadequately utilize both attribute triples and relation triples simultaneously, especially the interactions between them. This paper introduces a two-stage pipeline consisting of rough filter and fine filter to match products from eBay and Amazon. For fine filtering, a new framework for Entity Alignment, Relation-aware and Attribute-aware Graph Attention Networks for Entity Alignment (RAEA), is employed. RAEA focuses on the interactions between attribute triples and relation triples, where the entity representation aggregates the alignment signals from attributes and relations with Attribute-aware Entity Encoder and Relation-aware Graph Attention Networks. The experimental results indicate that the RAEA model achieves significant improvements over 12 baselines on EA task in the cross-lingual dataset DBP15K (6.59% on average Hits@1) and delivers competitive results in the monolingual dataset DWY100K. The source code for experiments on DBP15K and DWY100K is available at github (https://github.com/Mockingjay-liu/RAEA-model-for-Entity-Alignment).

</details>


### [264] [M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling](https://arxiv.org/abs/2512.07314)
*Yuxiao Luo,Songming Zhang,Sijie Ruan,Siran Chen,Kang Liu,Yang Xu,Yu Zheng,Ling Yin*

Main category: cs.AI

TL;DR: M-STAR是一个多尺度时空自回归框架，通过粗到细的时空预测过程生成长时轨迹，在保真度和生成速度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于AIGC的轨迹生成方法主要针对单日轨迹，在生成长时轨迹（如周轨迹）时效率低下，且缺乏显式的时空多尺度建模。

Method: 提出M-STAR框架：1）多尺度时空标记器编码层次化移动模式；2）基于Transformer的解码器进行下一尺度自回归预测；3）通过粗到细的时空预测过程生成长时轨迹。

Result: 在两个真实数据集上的实验表明，M-STAR在轨迹保真度上优于现有方法，并显著提高了生成速度。

Conclusion: M-STAR通过多尺度时空建模有效解决了长时轨迹生成的效率和精度问题，为移动性建模提供了新框架。

Abstract: Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.

</details>


### [265] [A Geometric Unification of Concept Learning with Concept Cones](https://arxiv.org/abs/2512.07355)
*Alexandre Rocchi--Henry,Thomas Fel,Gianni Franchi*

Main category: cs.AI

TL;DR: 该论文提出统一概念瓶颈模型(CBMs)和稀疏自编码器(SAEs)的几何框架，将两者视为学习激活空间中的概念锥，并建立评估SAEs学习概念与人类定义概念对齐程度的量化指标。


<details>
  <summary>Details</summary>
Motivation: CBMs和SAEs作为两种可解释性传统各自发展但缺乏交流：CBMs通过监督定义概念，SAEs通过无监督发现概念。论文旨在建立两者之间的理论桥梁，统一监督和无监督的概念发现方法。

Method: 提出几何框架：将CBMs和SAEs都视为学习激活空间中的线性方向，其非负组合形成概念锥。基于此提出包含性评估框架，用CBMs提供参考几何，评估SAEs学习的概念锥如何近似或包含CBM概念锥。

Result: 发现稀疏性和扩展因子的"最佳点"，能最大化与CBM概念的几何和语义对齐。建立了量化指标将SAEs的归纳偏置（如稀疏性、扩展比）与合理概念的出现联系起来。

Conclusion: 通过共享的几何框架统一了监督和无监督的概念发现，为衡量SAEs进展和评估发现概念与人类合理概念的对齐程度提供了原则性指标。

Abstract: Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.

</details>


### [266] [LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services](https://arxiv.org/abs/2512.07436)
*Hang He,Chuhuai Yue,Chengqi Dong,Mingxue Tian,Zhenfeng Liu,Jiajun Chai,Xiaohan Wang,Yufei Zhang,Qun Liao,Guojun Yin,Wei Lin,Chengcheng Wan,Haiying Sun,Ting Su*

Main category: cs.AI

TL;DR: LocalSearchBench：首个针对本地生活服务的智能体搜索基准，包含15万高质量条目和300个多跳QA任务，揭示当前大模型在该领域表现不佳（最佳模型正确率仅34.34%）


<details>
  <summary>Details</summary>
Motivation: 现有大推理模型研究多关注通用信息检索，很少探索具有独特挑战的垂直领域。本地生活服务领域的查询通常模糊且需要跨商家和产品的多跳推理，现有方法未能充分解决这些问题。

Method: 构建LocalSearchBench基准，包含来自不同城市和业务类型的15万高质量条目，基于真实用户查询构建300个多跳QA任务。同时开发LocalPlayground统一环境，集成多种工具供智能体交互。

Result: 实验显示即使最先进的大推理模型在LocalSearchBench上表现不佳：最佳模型DeepSeek-V3.1正确率仅34.34%，大多数模型在完整性（平均77.33%）和忠实性（平均61.99%）方面存在问题。

Conclusion: 本地生活服务领域需要专门的基准测试和领域特定的智能体训练，当前模型在该领域仍面临显著挑战。

Abstract: Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.

</details>


### [267] [How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations](https://arxiv.org/abs/2512.07497)
*JV Roig*

Main category: cs.AI

TL;DR: 研究通过KAMI基准测试分析LLM作为自主工具使用代理时的失败模式，发现模型规模并非代理可靠性的决定因素，识别出四种常见失败类型，强调需要注重交互式基础、恢复行为和环境感知适应的评估方法。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型作为具有工具使用能力的自主代理时的失败机制，而非仅仅关注聚合性能分数。旨在理解在多步工具执行中导致可靠性的具体行为模式和失败原因。

Method: 使用KAMI v0.1基准测试，分析三个代表性模型（Granite 4 Small、Llama 4 Maverick、DeepSeek V3.1）在文件系统、文本提取、CSV分析和SQL场景中的900个执行轨迹。进行细粒度的逐试验行为分析，而非仅关注聚合分数。

Result: 模型规模本身不能预测代理鲁棒性：400B的Llama 4 Maverick在某些不确定性驱动任务中仅比32B的Granite 4 Small略好；DeepSeek V3.1的优越可靠性主要来自后训练强化学习而非架构或规模。识别出四种重复出现的失败原型：缺乏基础前提的过早行动、替代缺失实体的过度帮助性、干扰诱导的上下文污染脆弱性、负载下的脆弱执行。

Conclusion: 可靠的企业部署不仅需要更强的模型，还需要有意识的训练和设计选择，以加强验证、约束发现和对真实数据源的遵守。代理评估方法应强调交互式基础、恢复行为和环境感知适应。

Abstract: We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.

</details>


### [268] [Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement](https://arxiv.org/abs/2512.07611)
*Yongsheng Lian*

Main category: cs.AI

TL;DR: 系统比较了三种强化学习算法（PPO、GRPO、DAPO）在提升大语言模型复杂推理能力上的效果，通过控制性迁移学习评估发现RL训练模型在所有任务上都优于基础模型，但改进程度因基准而异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索不同强化学习算法如何有效提升大语言模型的复杂推理能力，为RL-based LLM训练提供实用指导。

Method: 采用控制性迁移学习评估方法：首先在专门的Countdown Game上微调模型，然后在通用推理基准测试套件上进行评估。比较了PPO、GRPO和DAPO三种RL算法，并分析了GRPO和DAPO中组大小、KL惩罚系数等参数的影响。

Result: 在所有任务中，RL训练模型都优于对应的基础模型，但改进程度因基准而异。增加GRPO和DAPO中的组大小能带来更稳定的训练动态和更高准确率，KL惩罚系数的影响是非单调的。DAPO中的动态采样组件并未改善性能，禁用DS时DAPO获得最佳整体结果。

Conclusion: 强化学习能有效提升大语言模型的推理能力，但算法选择和参数设置对性能有重要影响。GRPO和DAPO中的组大小是稳定训练的关键参数，而DAPO的动态采样组件在实际应用中可能不必要。

Abstract: This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.

</details>


### [269] [The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds](https://arxiv.org/abs/2512.07631)
*Shahar Lutati*

Main category: cs.AI

TL;DR: 提出Agent Capability Problem (ACP)框架，通过信息获取视角预测智能体在资源约束下能否解决问题，用有效成本Ceff预测资源需求


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖经验启发式，缺乏理论框架预测智能体在资源约束下能否完成任务。需要系统化方法在搜索前预测资源需求，避免资源浪费

Method: 将问题解决建模为信息获取过程：智能体需要I_total比特识别解决方案，每步动作获得I_step比特信息，成本C_step。定义有效成本C_eff = (I_total/I_step)*C_step作为资源需求预测指标

Result: 理论证明C_eff是期望成本的下界，并提供紧致的概率上界。实验验证ACP预测与实际智能体性能高度一致，能有效约束搜索努力，相比贪婪和随机策略提高效率

Conclusion: ACP为智能体能力预测提供统一信息理论框架，连接主动学习、贝叶斯优化和强化学习原理，适用于LLM和智能体工作流，实现搜索前的资源需求预测

Abstract: When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\Itotal$ bits to identify a solution and gains $\Istep$ bits per action at cost $\Cstep$, yielding an effective cost $\Ceff = (\Itotal/\Istep), \Cstep$ that predicts resource requirements before search. We prove that $\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \

</details>


### [270] [Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE](https://arxiv.org/abs/2512.07710)
*Anxiang Zeng,Haibo Zhang,Hailing Zhang,Kaixiang Mo,Liang Yao,Ling Hu,Long Zhang,Shuman Liu,Shuyi Xie,Yanshi Li,Yizhang Chen,Yuepeng Sheng,Yuwei Huang,Zhaochen Xu,Zhiqiang Zhou,Ziqin Liew*

Main category: cs.AI

TL;DR: CompassMax-V3-Thinking是一个千亿规模的MoE推理模型，采用新的RL框架训练，核心原则是"每个提示都必须有意义"。通过多项创新技术解决了大规模RL训练中的效率问题，实现了稳定高效的大规模MoE模型训练。


<details>
  <summary>Details</summary>
Motivation: 将RL扩展到千亿规模时暴露出关键效率问题：零方差提示浪费rollout资源、长视野重要性采样不稳定、标准奖励模型导致的优势反转，以及rollout处理的系统性瓶颈。需要解决这些挑战以实现大规模MoE模型的稳定高效RL训练。

Method: 提出四项统一创新：(1)多阶段零方差消除，过滤非信息性提示并稳定基于组的策略优化；(2)ESPO熵自适应优化方法，平衡token级和序列级重要性采样；(3)路由器重放策略，对齐训练时MoE路由器决策与推理时行为，配合奖励模型调整防止优势反转；(4)高吞吐RL系统，采用FP8精度rollout、重叠奖励计算和长度感知调度。

Result: 这些贡献形成了一个连贯的pipeline，使千亿规模MoE模型的RL训练变得稳定高效。最终模型在内部和公开评估中都表现出强大的性能。

Conclusion: 通过解决大规模RL训练中的关键效率问题，成功开发了一个稳定高效的训练框架，实现了千亿规模MoE推理模型的高性能训练，为大规模语言模型的RL训练提供了系统性的解决方案。

Abstract: We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.

</details>


### [271] [RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2512.07761)
*Xiqiao Xiong,Ouxiang Li,Zhuo Liu,Moxin Li,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

TL;DR: 该论文提出了一种基于强化学习的多轮越狱攻击方法，通过训练攻击者LLM来从黑盒模型中引出有害内容，相比单轮优化方法显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易受到越狱攻击，威胁其在现实应用中的安全部署。现有方法通常依赖单轮优化，不足以学习长期攻击策略，因此需要更有效的多轮攻击方法。

Method: 将问题形式化为多轮强化学习任务，直接优化最终轮输出的有害性作为结果奖励。提出两种启发式过程奖励：1) 控制中间输出的有害性以防止触发黑盒模型的拒绝机制；2) 保持中间输出的语义相关性以避免偏离到无关内容。

Result: 在多个基准测试上的实验结果显示，该方法在多个模型上持续提高了攻击成功率，证明了方法的有效性。

Conclusion: 提出的基于强化学习的多轮越狱攻击方法能有效提升攻击成功率，为黑盒模型的安全部署提供了重要参考。

Abstract: Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.

</details>


### [272] [ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning](https://arxiv.org/abs/2512.07795)
*Nearchos Potamitis,Lars Klein,Akhil Arora*

Main category: cs.AI

TL;DR: ReasonBENCH是首个量化LLM推理不稳定性的基准测试，通过多轮评估协议提供统计可靠的性能指标，揭示当前推理方法普遍存在高不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理评估主要报告单次运行准确率，忽略了随机解码带来的内在不确定性，导致无法可靠评估方法的稳定性、可重复性和成本一致性，存在评估盲点。

Method: 提出ReasonBENCH基准测试，包含：(1)标准化推理框架、模型和任务的模块化评估库；(2)报告质量和成本统计可靠指标的多轮评估协议；(3)鼓励方差感知报告的公开排行榜。

Result: 跨领域任务分析显示，绝大多数推理策略和模型表现出高不稳定性。即使平均性能相似的策略，置信区间宽度可能相差四倍，且性能最佳的方法通常成本更高且更不稳定。

Conclusion: 推理不稳定性损害了跨运行的可重复性和报告性能的可靠性。可重复性是可靠LLM推理的关键维度，ReasonBENCH为未来推理方法和不确定性量化技术提供了基础。

Abstract: Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .

</details>


### [273] [Large Causal Models from Large Language Models](https://arxiv.org/abs/2512.07796)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: DEMOCRITUS是一个利用大语言模型构建大规模因果模型的新范式，通过提取、组织和可视化跨领域因果知识，将分散的因果陈述整合为统一的因果网络。


<details>
  <summary>Details</summary>
Motivation: 传统因果推理通常局限于特定领域和假设驱动的数值实验，而大语言模型蕴含了丰富的跨领域知识。本研究旨在利用LLMs的潜力，构建能够整合多个领域的大规模因果模型，克服传统方法的局限性。

Method: 使用高质量LLM提出主题、生成因果问题、从多领域提取因果陈述，然后通过新的范畴机器学习方法将这些分散、模糊、可能冲突的因果主张转化为关系因果三元组，并嵌入到大规模因果模型中。系统包含六个模块的流水线实现。

Result: DEMOCRITUS已在考古学、生物学、气候变化、经济学、医学和技术等多个领域成功应用，展示了跨领域大规模因果建模的可行性。同时分析了系统的计算成本瓶颈和扩展限制。

Conclusion: DEMOCRITUS为构建大规模因果模型提供了新范式，成功整合了LLMs的跨领域知识。当前系统存在局限性，但为未来扩展指明了方向，包括改进计算效率和增强模型一致性。

Abstract: We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.

</details>


### [274] [Auditing Games for Sandbagging](https://arxiv.org/abs/2512.07810)
*Jordan Taylor,Sid Black,Dillon Bowen,Thomas Read,Satvik Golechha,Alex Zelenka-Martin,Oliver Makins,Connor Kissane,Kola Ayonrinde,Jacob Merizian,Samuel Marks,Chris Cundy,Joseph Bloom*

Main category: cs.AI

TL;DR: 论文通过审计游戏测试了检测AI系统"藏拙"（sandbagging）能力的方法，发现现有技术难以可靠区分藏拙模型与良性模型，但基于训练的激发方法能有效引出藏拙模型的全部能力。


<details>
  <summary>Details</summary>
Motivation: 未来AI系统可能在评估中隐藏其真实能力（藏拙），误导开发者和审计者。需要测试现有检测技术的有效性，以应对这一潜在安全风险。

Method: 采用审计游戏方法：红队微调五个模型（部分有条件地表现不佳作为藏拙代理），蓝队使用黑盒、模型内部或基于训练的方法来识别藏拙模型。

Result: 蓝队无法可靠区分藏拙模型与良性模型。黑盒方法被有效模仿弱模型的行为击败。线性探针（模型内部方法）更有前景但易受红队植入行为影响。基于训练的激发能一致引出藏拙模型的全部性能，但良性模型性能有时也会提升，导致假阳性。

Conclusion: 短期建议开发者通过分布内训练消除潜在藏拙；长期需要进一步研究确保基于训练的激发有效性，并开发更稳健的藏拙检测方法。

Abstract: Future AI systems could conceal their capabilities ('sandbagging') during evaluations, potentially misleading developers and auditors. We stress-tested sandbagging detection techniques using an auditing game. First, a red team fine-tuned five models, some of which conditionally underperformed, as a proxy for sandbagging. Second, a blue team used black-box, model-internals, or training-based approaches to identify sandbagging models. We found that the blue team could not reliably discriminate sandbaggers from benign models. Black-box approaches were defeated by effective imitation of a weaker model. Linear probes, a model-internals approach, showed more promise but their naive application was vulnerable to behaviours instilled by the red team. We also explored capability elicitation as a strategy for detecting sandbagging. Although Prompt-based elicitation was not reliable, training-based elicitation consistently elicited full performance from the sandbagging models, using only a single correct demonstration of the evaluation task. However the performance of benign models was sometimes also raised, so relying on elicitation as a detection strategy was prone to false-positives. In the short-term, we recommend developers remove potential sandbagging using on-distribution training for elicitation. In the longer-term, further research is needed to ensure the efficacy of training-based elicitation, and develop robust methods for sandbagging detection. We open source our model organisms at https://github.com/AI-Safety-Institute/sandbagging_auditing_games and select transcripts and results at https://huggingface.co/datasets/sandbagging-games/evaluation_logs . A demo illustrating the game can be played at https://sandbagging-demo.far.ai/ .

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [275] [Proof of Concept for Mammography Classification with Enhanced Compactness and Separability Modules](https://arxiv.org/abs/2512.06575)
*Fariza Dahes*

Main category: eess.IV

TL;DR: 验证并扩展了用于医学图像分类的方法框架，将改进的ConvNeXt Tiny架构应用于乳腺X线摄影分类，评估了GAGM和SEVector模块的有效性，但发现特征平滑损失在乳腺摄影条件下无效。


<details>
  <summary>Details</summary>
Motivation: 验证先前在阿尔茨海默症MRI分类中表现良好的方法框架（包含GAGM、SEVector和FSL）是否可转移到乳腺X线摄影分类任务中，并扩展该框架以提供更全面的评估和临床工具。

Method: 使用Kaggle数据集（整合INbreast、MIAS和DDSM），比较基线CNN、ConvNeXt Tiny和InceptionV3骨干网络，并集成GAGM和SEVector模块。进行多指标评估（宏观F1、各类召回方差、ROC/AUC）、特征可解释性分析（Grad-CAM），并开发交互式临床探索仪表板。

Result: GAGM和SEVector模块有效增强了特征区分能力并减少了假阴性（特别是恶性病例），但特征平滑损失（FSL）在乳腺摄影分类条件下未产生可测量的改进。扩展框架提供了更全面的评估和临床探索工具。

Conclusion: 成功验证了GAGM和SEVector模块在乳腺摄影分类中的有效性，但FSL的效果依赖于特定架构和计算假设。研究扩展了原始框架，并指出未来需要探索改进类内紧凑性和类间分离性的方法，以更好地区分恶性和良性病例。

Abstract: This study presents a validation and extension of a recent methodological framework for medical image classification. While an improved ConvNeXt Tiny architecture, integrating Global Average and Max Pooling fusion (GAGM), lightweight channel attention (SEVector), and Feature Smoothing Loss (FSL), demonstrated promising results on Alzheimer MRI under CPU friendly conditions, our work investigates its transposability to mammography classification. Using a Kaggle dataset that consolidates INbreast, MIAS, and DDSM mammography collections, we compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enriched with GAGM and SEVector modules. Results confirm the effectiveness of GAGM and SEVector in enhancing feature discriminability and reducing false negatives, particularly for malignant cases. In our experiments, however, the Feature Smoothing Loss did not yield measurable improvements under mammography classification conditions, suggesting that its effectiveness may depend on specific architectural and computational assumptions. Beyond validation, our contribution extends the original framework through multi metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis (Grad CAM), and the development of an interactive dashboard for clinical exploration. As a perspective, we highlight the need to explore alternative approaches to improve intra class compactness and inter class separability, with the specific goal of enhancing the distinction between malignant and benign cases in mammography classification.

</details>
