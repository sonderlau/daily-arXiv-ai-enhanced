<div id=toc></div>

# Table of Contents

- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.CV](#cs.CV) [Total: 89]
- [cs.AI](#cs.AI) [Total: 17]


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [1] [A Constructed Closure of the Bering Strait can Prevent an AMOC Tipping](https://arxiv.org/abs/2508.19826)
*Jelle Soons,Henk A. Dijkstra*

Main category: physics.ao-ph

TL;DR: 通过人工封闭白令海峡来延长大西洋港流的安全碳预算，防止其失稳倒塌


<details>
  <summary>Details</summary>
Motivation: 大西洋港流(AMOC)是现代气候的关键倒塌元素，可能在气候变化下失稳倒塌。虽然白令海峡对AMOC稳定性的影响已有研究，但人工封闭是否能防止AMOC倒塌仍不明确。

Method: 使用中等复杂度的地球系统模型，模拟人工封闭白令海峡的情况，分析对AMOC稳定性和碳预算的影响

Result: 当AMOC强度下降调度小于工业面前水平的(6.1+/-0.5)%时，及旹封闭可以提供较大的安全碳预算(最多500PgC)；而当AMOC较弱时，封闭反而会减少碳预算

Conclusion: 人工封闭白令海峡可以成为一种可行的气候干预策略，但效果依赖于封闭时机的选择和AMOC的当前状态

Abstract: The Atlantic Meridional Overturning Circulation (AMOC) is a major tipping
element in the present-day climate, and could potentially collapse under
sufficient freshwater or CO2-forcing. While the effect of the Bering Strait on
AMOC stability has been well studied, it is unknown whether a constructed
closure of this Strait can prevent an AMOC collapse under climate change. Here,
we show in an Earth system Model of Intermediate Complexity that an artificial
closure of the Strait can extend the safe carbon budget of the AMOC, provided
that the AMOC is strong enough at the closure time. Specifically, for this
model, an equilibrium AMOC with a reduction below (6.1+/-0.5)% from
pre-industrial has an additional budget up to 500PgC given a sufficiently early
closure, while for a weaker AMOC a closure reduces this budget. This indicates
that constructing this closure can be a feasible climate intervention strategy
to prevent an AMOC collapse.

</details>


### [2] [Accounting for shelf width in selecting altimetry observations for coastal sea level variability improves its agreement with tide gauges](https://arxiv.org/abs/2508.20046)
*Vandana Sukumaran,Bramha Dutt Vishwakarma*

Main category: physics.ao-ph

TL;DR: 基于海洋测深信息的动态搜索半径算法，提升了卫星海平面观测在海岸带的准确性和可靠性


<details>
  <summary>Details</summary>
Motivation: 现有卫星海平面观测验证方法在海岸带应用范围有限，需要发展更有效的算法来提高海岸海平面变化的表征能力

Method: 开发了一种新的动态变化搜索半径算法，利用海洋测深信息选择能更好代表海岸海平面变化的卫星观测

Result: 在全球155个潮气边测站进行成功测试，显示出比现有方法更广泛的适用性，直接提升了低分辨率产品XTRACK的效果，使其可与高分辨率产品相比拔

Conclusion: 该算法显著提高了卫星海平面观测在海岸带的准确性，尤其在线性和非线性趋势分析中显示出更好的一致性，为海岸海平面监测提供了更有效的工具

Abstract: A novel dynamically varying search radius algorithm is developed that takes
advantage of bathymetry information to choose satellite observations that
represent coastal sea level variability better. The algorithm is successfully
tested at 155 tide gauge stations around the globe and demonstrates broader
applicability across different coastal regimes compared to existing validation
methods. This is supported by consistently higher median correlation and lower
median Normalized Root Mean Square Error (NRMSE). Furthermore, the new
algorithm improves the efficacy of the low-resolution product, X-TRACK SLA L2P
v2022 (XTRACK), and makes it comparable to the high-resolution (20Hz) coastal
products: Along-track sea level anomalies and trends v2.3. Using the algorithm
at over 267 stations, XTRACK data shows improved agreement with tide gauges for
both linear and non-linear trends. In some regions, such as tidally-dominated
estuaries and the Eastern Australian coast, lower correlation and higher RMSE
for residual signals are reported, which are discussed.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [3] [When Routers, Switches and Interconnects Compute: A processing-in-interconnect Paradigm for Scalable Neuromorphic AI](https://arxiv.org/abs/2508.19548)
*Madhuvanthi Srivatsav R,Chiranjib Bhattacharyya,Shantanu Chakrabartty,Chetan Singh Thakur*

Main category: cs.NE

TL;DR: 这篇论文提出了一种在交换互联结构中进行计算的新范式(π²)，利用包交换硬件的原生操作来实现神经网络运算，以解决大规模神经科学计算的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 路由、交换和互联结构虽然在神经科学计算中只起支撑作用，但对于大规模AI工作负荷而言，它们最终决定了能消耗和速度。该研究旨在解决这个性能瓶颈。

Method: 研究将AI工作负荷所需的操作映射到包交换硬件的原生操作上，如延迟、因果关系、超时、包丢弃和广播等。利用现有的缓冲和流量形成算法来实现神经元模型和程尾操作，并通过知识精炼框架训练网络。

Result: 分析模型显示，π²系统的能消耗缩放会随着互联带宽和能效的提升而改善，这与其他神经科学平台不同。预测利用互联技术趋势，π²架构可以更容易扩展到执行脑等级AI推理工作负荷，功耗在百瓦级别。

Conclusion: π²计算范式通过利用交换互联结构的原生操作能果案地解决大规模神经科学计算的性能瓶颈，具有良好的缩放性能和能效优势。

Abstract: Routing, switching, and the interconnect fabric are essential for large-scale
neuromorphic computing. While this fabric only plays a supporting role in the
process of computing, for large AI workloads it ultimately determines energy
consumption and speed. In this paper, we address this bottleneck by asking: (a)
What computing paradigms are inherent in existing routing, switching, and
interconnect systems, and how can they be used to implement a
processing-in-Interconnect (\pi^2) computing paradigm? and (b) leveraging
current and future interconnect trends, how will a \pi^2 system's performance
scale compared to other neuromorphic architectures? For (a), we show that
operations required for typical AI workloads can be mapped onto delays,
causality, time-outs, packet drop, and broadcast operations -- primitives
already implemented in packet-switching and packet-routing hardware. We show
that existing buffering and traffic-shaping embedded algorithms can be
leveraged to implement neuron models and synaptic operations. Additionally, a
knowledge-distillation framework can train and cross-map well-established
neural network topologies onto $\pi^2$ without degrading generalization
performance. For (b), analytical modeling shows that, unlike other neuromorphic
platforms, the energy scaling of $\pi^2$ improves with interconnect bandwidth
and energy efficiency. We predict that by leveraging trends in interconnect
technology, a \pi^2 architecture can be more easily scaled to execute
brain-scale AI inference workloads with power consumption levels in the range
of hundreds of watts.

</details>


### [4] [Walk the Robot: Exploring Soft Robotic Morphological Communication driven by Spiking Neural Networks](https://arxiv.org/abs/2508.19920)
*Matthew Meek,Guy Tallent,Thomas Breimer,James Gaskell,Abhay Kashyap,Atharv Tekurkar,Jonathan Fischman,Luodi Wang,Viet-Dung Nguyen,John Rieffel*

Main category: cs.NE

TL;DR: 研究探索了在软体机器人中利用非线性动态耦合进行形态通信，通过进化学习算法训练脉冲神经网络来实现控制器模块间的协调


<details>
  <summary>Details</summary>
Motivation: 传统方法通常抑制非线性动态耦合，而本研究旨在探索如何利用这种耦合作为机器人不同部分间的通信机制，特别是通过形态通信来促进独立控制器模块的协调

Method: 使用进化学习模型训练脉冲神经网络(SNN)作为机器人控制机制，在EvoGym环境中模拟软体机器人，研究形态通信的涌现现象

Result: 研究表明基于脉冲神经网络的进化学习方法能有效控制非刚性机器人，并能够实现形态通信的涌现

Conclusion: 利用非线性动态耦合进行形态通信是可行的，进化学习训练的脉冲神经网络能够有效实现软体机器人控制器模块间的协调通信

Abstract: Recently, researchers have explored control methods that embrace nonlinear
dynamic coupling instead of suppressing it. Such designs leverage dynamical
coupling for communication between different parts of the robot. Morphological
communication refers to when those dynamics can be used as an emergent data bus
to facilitate coordination among independent controller modules within the same
robot. Previous research with tensegrity-based robot designs has shown that
evolutionary learning models that evolve spiking neural networks (SNN) as robot
control mechanisms are effective for controlling non-rigid robots. Our own
research explores the emergence of morphological communication in an SNN-based
simulated soft robot in theEvoGym environment.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [5] [Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration](https://arxiv.org/abs/2508.19254)
*Jookyung Song,Mookyoung Kang,Nojun Kwak*

Main category: cs.CV

TL;DR: 一种实时生成式绘画系统，同时解释结构意图和语义意图，通过多阶段生成流程实现低延迟的协作视觉创作


<details>
  <summary>Details</summary>
Motivation: 元素传统文本提示生成系统主要关注高级语义描述，而忽视了基础级的直觉几何特征，需要一种能同时处理结构和语义意图的统一转换过程

Method: 系统同时分析线条轨迹、比例和空间布局等基础几何特征，以及通过视觉-语言模型提取的高级语义线索，通过多阶段生成流水线实现边框保持的结构控制与风格和内容感知的图像合成

Result: 实现了低延迟的两阶段转换，支持多用户在共享画布上的协作创作，构建了一个允许任何艺术专业水平参与者进行同步共同创作的平台

Conclusion: 该系统重新定义了人工智能与人类的交互为一种共创和相互增强的过程，为协作视觉创作提供了新的可能性

Abstract: This paper presents a real-time generative drawing system that interprets and
integrates both formal intent - the structural, compositional, and stylistic
attributes of a sketch - and contextual intent - the semantic and thematic
meaning inferred from its visual content - into a unified transformation
process. Unlike conventional text-prompt-based generative systems, which
primarily capture high-level contextual descriptions, our approach
simultaneously analyzes ground-level intuitive geometric features such as line
trajectories, proportions, and spatial arrangement, and high-level semantic
cues extracted via vision-language models. These dual intent signals are
jointly conditioned in a multi-stage generation pipeline that combines
contour-preserving structural control with style- and content-aware image
synthesis. Implemented with a touchscreen-based interface and distributed
inference architecture, the system achieves low-latency, two-stage
transformation while supporting multi-user collaboration on shared canvases.
The resulting platform enables participants, regardless of artistic expertise,
to engage in synchronous, co-authored visual creation, redefining human-AI
interaction as a process of co-creation and mutual enhancement.

</details>


### [6] [TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models](https://arxiv.org/abs/2508.19257)
*Chenghao Liu,Jiachen Zhang,Chengxuan Li,Zhimu Zhou,Shixin Wu,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: 通过时间分析与关注机制结合的双维检测方法，选择性融合历史与当前视觉表征，提升VLA模型在机器人操纵任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型每步独立处理视觉输入，忽略了操纵任务中连续帧间的时间相关性和一致性，导致对视视噪声敏感且抑制了性能

Method: 时间切片融合(TTF)方法：经济的灰度像素差异分析+关注机制语义相关性评估，通过硬融合策略和关键帧锚定实现选择性融合，避免错误累积

Result: 在LIBERO平均提升4.0个百分点(72.4% vs 68.4%)，SimplerEnv跨环境验证4.8%相对收益，实际机器人任务8.7%相对收益，模型无关性使用于OpenVLA和VLA-Cache

Conclusion: 时间融合能有效利用历史信息提升VLA性能，选择性Query矩阵重用反而提升性能，为直接KQV矩阵重用策略提供了新方向

Abstract: Vision-Language-Action (VLA) models process visual inputs independently at
each timestep, discarding valuable temporal information inherent in robotic
manipulation tasks. This frame-by-frame processing makes models vulnerable to
visual noise while ignoring the substantial coherence between consecutive
frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a
training-free approach that intelligently integrates historical and current
visual representations to enhance VLA inference quality. Our method employs
dual-dimension detection combining efficient grayscale pixel difference
analysis with attention-based semantic relevance assessment, enabling selective
temporal token fusion through hard fusion strategies and keyframe anchoring to
prevent error accumulation. Comprehensive experiments across LIBERO,
SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0
percentage points average on LIBERO (72.4\% vs 68.4\% baseline),
cross-environment validation on SimplerEnv (4.8\% relative improvement), and
8.7\% relative improvement on real robot tasks. Our approach proves
model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,
TTF reveals that selective Query matrix reuse in attention mechanisms enhances
rather than compromises performance, suggesting promising directions for direct
KQV matrix reuse strategies that achieve computational acceleration while
improving task success rates.

</details>


### [7] [Context-aware Sparse Spatiotemporal Learning for Event-based Vision](https://arxiv.org/abs/2508.19806)
*Shenqi Wang,Guangzhi Tang*

Main category: cs.CV

TL;DR: 提出了CSSL框架，通过上下文感知阈值动态调节神经元激活，无需显式稀疏约束即可实现高神经元稀疏度，在事件相机目标检测和光流估计任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习事件处理方法未能充分利用事件数据的稀疏特性，而脉冲神经网络在复杂视觉任务中性能不足，且实现高激活稀疏度需要复杂的手动调优。

Method: 提出Context-aware Sparse Spatiotemporal Learning (CSSL)框架，采用上下文感知阈值技术，根据输入分布动态调节神经元激活，自然降低激活密度。

Result: 在事件相机目标检测和光流估计任务中，CSSL达到或超越了最先进方法的性能，同时保持了极高的神经元稀疏度。

Conclusion: CSSL为神经形态处理实现高效事件视觉提供了关键解决方案，展示了在资源受限边缘应用中的巨大潜力。

Abstract: Event-based camera has emerged as a promising paradigm for robot perception,
offering advantages with high temporal resolution, high dynamic range, and
robustness to motion blur. However, existing deep learning-based event
processing methods often fail to fully leverage the sparse nature of event
data, complicating their integration into resource-constrained edge
applications. While neuromorphic computing provides an energy-efficient
alternative, spiking neural networks struggle to match of performance of
state-of-the-art models in complex event-based vision tasks, like object
detection and optical flow. Moreover, achieving high activation sparsity in
neural networks is still difficult and often demands careful manual tuning of
sparsity-inducing loss terms. Here, we propose Context-aware Sparse
Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware
thresholding to dynamically regulate neuron activations based on the input
distribution, naturally reducing activation density without explicit sparsity
constraints. Applied to event-based object detection and optical flow
estimation, CSSL achieves comparable or superior performance to
state-of-the-art methods while maintaining extremely high neuronal sparsity.
Our experimental results highlight CSSL's crucial role in enabling efficient
event-based vision for neuromorphic processing.

</details>


### [8] [Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation](https://arxiv.org/abs/2508.19289)
*Tai Inui,Steven Oh,Magdeline Kuan*

Main category: cs.CV

TL;DR: 提出了一种结合视觉设计指标和CLIP-ViT嵌入的无监督幻灯片质量评估方法，在专业讲座幻灯片上表现优于主流视觉语言模型


<details>
  <summary>Details</summary>
Motivation: 需要一种可扩展、客观的实时幻灯片质量评估方法，以提供自动化反馈，替代主观的人工评估

Method: 结合7个专家启发的视觉设计指标（留白、色彩丰富度、边缘密度等）和CLIP-ViT嵌入，使用隔离森林异常评分来评估幻灯片质量

Result: 在6个学术演讲的115张幻灯片上，与人类视觉质量评分的Pearson相关性达到0.83，比主流视觉语言模型强1.79-3.23倍

Conclusion: 将低级设计线索与多模态嵌入相结合能够很好地近似观众对幻灯片质量的感知，实现可扩展的实时客观反馈

Abstract: We present an unsupervised slide-quality assessment pipeline that combines
seven expert-inspired visual-design metrics (whitespace, colorfulness, edge
density, brightness contrast, text density, color harmony, layout balance) with
CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate
presentation slides. Trained on 12k professional lecture slides and evaluated
on six academic talks (115 slides), our method achieved Pearson correlations up
to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores
from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude
Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual
ratings, discriminant validity against speaker-delivery scores, and exploratory
alignment with overall impressions. Our results show that augmenting low-level
design cues with multimodal embeddings closely approximates audience
perceptions of slide quality, enabling scalable, objective feedback in real
time.

</details>


### [9] [Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation](https://arxiv.org/abs/2508.19290)
*Alexandros Gkillas,Ioulia Kapsali,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CV

TL;DR: 这篇论文提出了一种高效的模型基净化框架，专门用于2D距离视图LiDAR分割中的对抗防御，在保持计算效率的同时提供强大的对抗性。


<details>
  <summary>Details</summary>
Motivation: 现代LiDAR分割网络容易受到对抗攻击威胁，而现有防御方法多为基于原始3D点云的计算密集方案，对2D距离视图表示的轻量级防御缺乏研究。

Method: 提出在距离视图域的直接攻击模型，并基于数学正当化的优化问题开发了可解释的净化网络，实现低计算开销的强大对抗性。

Result: 在开放测试集上获得竞争力表现，一贵超越了生成模型和对抗训练基线方法。实际汽车部署证明了该框架在自动驾驶应用中的准确性。

Conclusion: 该研究为2D距离视图LiDAR分割领域提供了一种高效、轻量级的对抗防御方案，在保持计算效率的同时有效提升了系统的安全性和可靠性。

Abstract: LiDAR-based segmentation is essential for reliable perception in autonomous
vehicles, yet modern segmentation networks are highly susceptible to
adversarial attacks that can compromise safety. Most existing defenses are
designed for networks operating directly on raw 3D point clouds and rely on
large, computationally intensive generative models. However, many
state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D
range view representations. Despite their widespread adoption, dedicated
lightweight adversarial defenses for this domain remain largely unexplored. We
introduce an efficient model-based purification framework tailored for
adversarial defense in 2D range-view LiDAR segmentation. We propose a direct
attack formulation in the range-view domain and develop an explainable
purification network based on a mathematical justified optimization problem,
achieving strong adversarial resilience with minimal computational overhead.
Our method achieves competitive performance on open benchmarks, consistently
outperforming generative and adversarial training baselines. More importantly,
real-world deployment on a demo vehicle demonstrates the framework's ability to
deliver accurate operation in practical autonomous driving scenarios.

</details>


### [10] [Object Detection with Multimodal Large Vision-Language Models: An In-depth Review](https://arxiv.org/abs/2508.19294)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: 这篇综述论文系统分析了大型视觉语言模型(LVLMs)在目标检测领域的应用，重点探讨了其如何通过融合视觉和语言信息来提升目标检测的适应性、上下文推理能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习目标检测方法存在局限性，而视觉语言模型的融合为对象检测带来了革命性的变革，需要系统梳理该领域的最新进展和发展趋势。

Method: 采用三步研究综述方法：1)分析VLMs在目标检测中的工作原理；2)探讨最新LVLMs的架构创新、训练范式和输出灵活性；3)深入考察视觉与文本信息融合的方法。

Result: 研究表明LVLMs在多样化场景中表现出色，包括定位和分割任务，其性能有望很快达到或超越传统方法。同时指出了当前LVLMs的主要局限性并提出了解决方案。

Conclusion: LVLMs的最新进展已经并将继续对目标检测和机器人应用产生变革性影响，为该领域的未来发展提供了清晰路线图。

Abstract: The fusion of language and vision in large vision-language models (LVLMs) has
revolutionized deep learning-based object detection by enhancing adaptability,
contextual reasoning, and generalization beyond traditional architectures. This
in-depth review presents a structured exploration of the state-of-the-art in
LVLMs, systematically organized through a three-step research review process.
First, we discuss the functioning of vision language models (VLMs) for object
detection, describing how these models harness natural language processing
(NLP) and computer vision (CV) techniques to revolutionize object detection and
localization. We then explain the architectural innovations, training
paradigms, and output flexibility of recent LVLMs for object detection,
highlighting how they achieve advanced contextual understanding for object
detection. The review thoroughly examines the approaches used in integration of
visual and textual information, demonstrating the progress made in object
detection using VLMs that facilitate more sophisticated object detection and
localization strategies. This review presents comprehensive visualizations
demonstrating LVLMs' effectiveness in diverse scenarios including localization
and segmentation, and then compares their real-time performance, adaptability,
and complexity to traditional deep learning systems. Based on the review, its
is expected that LVLMs will soon meet or surpass the performance of
conventional methods in object detection. The review also identifies a few
major limitations of the current LVLM modes, proposes solutions to address
those challenges, and presents a clear roadmap for the future advancement in
this field. We conclude, based on this study, that the recent advancement in
LVLMs have made and will continue to make a transformative impact on object
detection and robotic applications in the future.

</details>


### [11] [Large VLM-based Stylized Sports Captioning](https://arxiv.org/abs/2508.19295)
*Sauptik Dhar,Nicholas Buoncristiani,Joe Anakata,Haoyu Zhang,Michelle Munson*

Main category: cs.CV

TL;DR: 本文提出了一个两级微调的视觉语言模型管道，专门用于从体育图像生成专业级的风格化描述，相比现有方法在F1分数上提升8-10%，BERT分数提升2-10%，并在超级碗比赛中成功应用。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型在通用体育活动描述方面表现良好，但缺乏领域特定的体育术语来生成自然的人类风格描述，无法满足专业体育新闻报道的需求。

Method: 采用两级微调的视觉语言模型管道，专门针对体育领域进行优化，能够生成风格化的专业体育描述。

Result: 相比替代方法，F1分数提升8-10%，BERT分数提升2-10%，具有较小的运行时内存占用和快速执行时间（6张图像/3-5秒）。在超级碗LIX比赛中成功处理了1000多张图像。

Conclusion: 该管道证明了其在实时专业体育新闻中的实际应用价值，能够生成高度准确和风格化的体育描述，填补了现有模型在体育领域专业描述方面的空白。

Abstract: The advent of large (visual) language models (LLM / LVLM) have led to a
deluge of automated human-like systems in several domains including social
media content generation, search and recommendation, healthcare prognosis, AI
assistants for cognitive tasks etc. Although these systems have been
successfully integrated in production; very little focus has been placed on
sports, particularly accurate identification and natural language description
of the game play. Most existing LLM/LVLMs can explain generic sports
activities, but lack sufficient domain-centric sports' jargon to create natural
(human-like) descriptions. This work highlights the limitations of existing
SoTA LLM/LVLMs for generating production-grade sports captions from images in a
desired stylized format, and proposes a two-level fine-tuned LVLM pipeline to
address that. The proposed pipeline yields an improvement > 8-10% in the F1,
and > 2-10% in BERT score compared to alternative approaches. In addition, it
has a small runtime memory footprint and fast execution time. During Super Bowl
LIX the pipeline proved its practical application for live professional sports
journalism; generating highly accurate and stylized captions at the rate of 6
images per 3-5 seconds for over 1000 images during the game play.

</details>


### [12] [DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models](https://arxiv.org/abs/2508.19298)
*Abu Sufian,Anirudha Ghosh,Debaditya Barman,Marco Leo,Cosimo Distante*

Main category: cs.CV

TL;DR: DemoBias研究评估了大型视觉语言模型在生物特征人脸识别任务中的 demographic biases，发现PaliGemma和LLaVA在Hispanic/Latino、Caucasian和South Asian群体中存在较大性能差异，而BLIP-2表现相对一致。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在各种下游任务中表现出色，但在生物特征人脸识别中存在 demographic biases 问题，需要评估这些模型在不同 demographic 群体间的公平性表现。

Method: 对LLaVA、BLIP-2和PaliGemma三个预训练LVLM模型进行微调，使用自建的demographic-balanced数据集，采用BERTScores和Fairness Discrepancy Rate等指标量化性能差异。

Result: 实验发现LVLMs存在demographic biases，PaliGemma和LLaVA在Hispanic/Latino、Caucasian和South Asian群体中表现出较高差异，BLIP-2相对一致。

Conclusion: 该研究揭示了LVLMs在生物特征人脸识别任务中的demographic biases问题，强调了模型公平性评估的重要性，为开发更公平的AI系统提供了实证依据。

Abstract: Large Vision Language Models (LVLMs) have demonstrated remarkable
capabilities across various downstream tasks, including biometric face
recognition (FR) with description. However, demographic biases remain a
critical concern in FR, as these foundation models often fail to perform
equitably across diverse demographic groups, considering ethnicity/race,
gender, and age. Therefore, through our work DemoBias, we conduct an empirical
evaluation to investigate the extent of demographic biases in LVLMs for
biometric FR with textual token generation tasks. We fine-tuned and evaluated
three widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own
generated demographic-balanced dataset. We utilize several evaluation metrics,
like group-specific BERTScores and the Fairness Discrepancy Rate, to quantify
and trace the performance disparities. The experimental results deliver
compelling insights into the fairness and reliability of LVLMs across diverse
demographic groups. Our empirical study uncovered demographic biases in LVLMs,
with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,
Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably
consistent. Repository: https://github.com/Sufianlab/DemoBias.

</details>


### [13] [Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities](https://arxiv.org/abs/2508.19305)
*Chen Chu,Cyrus Shahabi*

Main category: cs.CV

TL;DR: Geo2Vec是一种新颖的空间表示学习方法，直接在原始空间操作，通过自适应采样和符号距离场编码几何特征，避免了现有方法的分解计算成本高和几何对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有空间表示学习方法要么只针对单一地理实体类型，要么需要分解实体进行傅里叶变换，计算成本高且缺乏几何对齐，导致细粒度特征模糊。

Method: 基于符号距离场(SDF)思想，直接在原始空间自适应采样点并编码其符号距离，使用神经网络近似SDF生成紧凑的几何感知表示，并提出旋转不变位置编码。

Result: 实验结果表明Geo2Vec在形状和位置表示、拓扑和距离关系捕捉方面持续优于现有方法，并在实际GeoAI应用中实现更高效率。

Conclusion: Geo2Vec提供了一种统一、高效且几何感知的空间表示学习方法，能够为各种地理实体类型生成鲁棒的嵌入表示，提升下游GeoAI模型的性能。

Abstract: Spatial representation learning is essential for GeoAI applications such as
urban analytics, enabling the encoding of shapes, locations, and spatial
relationships (topological and distance-based) of geo-entities like points,
polylines, and polygons. Existing methods either target a single geo-entity
type or, like Poly2Vec, decompose entities into simpler components to enable
Fourier transformation, introducing high computational cost. Moreover, since
the transformed space lacks geometric alignment, these methods rely on uniform,
non-adaptive sampling, which blurs fine-grained features like edges and
boundaries. To address these limitations, we introduce Geo2Vec, a novel method
inspired by signed distance fields (SDF) that operates directly in the original
space. Geo2Vec adaptively samples points and encodes their signed distances
(positive outside, negative inside), capturing geometry without decomposition.
A neural network trained to approximate the SDF produces compact,
geometry-aware, and unified representations for all geo-entity types.
Additionally, we propose a rotation-invariant positional encoding to model
high-frequency spatial variations and construct a structured and robust
embedding space for downstream GeoAI models. Empirical results show that
Geo2Vec consistently outperforms existing methods in representing shape and
location, capturing topological and distance relationships, and achieving
greater efficiency in real-world GeoAI applications. Code and Data can be found
at: https://github.com/chuchen2017/GeoNeuralRepresentation.

</details>


### [14] [Advancements in Crop Analysis through Deep Learning and Explainable AI](https://arxiv.org/abs/2508.19307)
*Hamza Khan*

Main category: cs.CV

TL;DR: 本研究提出基于卷积神经网络的自动化方法，成功分类5种大米品种并诊断4种水稻叶部病害，结合可解释AI技术提高模型透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大米作为全球重要主食，质量监控对消费者满意度和国家声誉至关重要。传统人工检测劳动强度大、耗时长且易出错，需要自动化解决方案进行质量控制和产量提升。

Method: 使用包含75000张图像的公开数据集，采用CNN、VGG16、ResNet50和MobileNetV2等深度学习模型，结合SHAP和LIME等可解释AI技术分析特征重要性。

Result: 模型表现出高分类准确率，误分类极少，有效区分不同大米品种。同时开发出准确的水稻叶部病害诊断方法，包括褐斑病、稻瘟病、白叶枯病和东格鲁病。

Conclusion: 深度学习在农业应用中具有强大潜力，为构建稳健、可解释的自动化作物质量检测和病害诊断系统铺平道路，最终惠及农民、消费者和农业经济。

Abstract: Rice is a staple food of global importance in terms of trade, nutrition, and
economic growth. Among Asian nations such as China, India, Pakistan, Thailand,
Vietnam and Indonesia are leading producers of both long and short grain
varieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To
ensure consumer satisfaction and strengthen national reputations, monitoring
rice crops and grain quality is essential. Manual inspection, however, is
labour intensive, time consuming and error prone, highlighting the need for
automated solutions for quality control and yield improvement. This study
proposes an automated approach to classify five rice grain varieties using
Convolutional Neural Networks (CNN). A publicly available dataset of 75000
images was used for training and testing. Model evaluation employed accuracy,
recall, precision, F1-score, ROC curves, and confusion matrices. Results
demonstrated high classification accuracy with minimal misclassifications,
confirming the model effectiveness in distinguishing rice varieties. In
addition, an accurate diagnostic method for rice leaf diseases such as Brown
Spot, Blast, Bacterial Blight, and Tungro was developed. The framework combined
explainable artificial intelligence (XAI) with deep learning models including
CNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP
(SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic
Explanations) revealed how specific grain and leaf features influenced
predictions, enhancing model transparency and reliability. The findings
demonstrate the strong potential of deep learning in agricultural applications,
paving the way for robust, interpretable systems that can support automated
crop quality inspection and disease diagnosis, ultimately benefiting farmers,
consumers, and the agricultural economy.

</details>


### [15] [Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax](https://arxiv.org/abs/2508.19312)
*Ander Galván,Marivi Higuero,Jorge Sasiain,Eduardo Jacob*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于联邦学习和OpenMax算法的面部识别系统，通过交换平均激活向量和本地距离来区分已知和未知主体，以解决开集场景下的隐私和识别管理挑战。


<details>
  <summary>Details</summary>
Motivation: 人工智能面部识别虽然在特定场景中实现了高精度，但在开集场景中面临着隐私和识别管理的重大挑战，特别是当未知个体出现在操作环境中时。

Method: 将OpenMax算法集成到联邦学习框架中，利用平均激活向量和本地距离测量的交换来可靠地区分已知和未知主体。

Result: 实验结果验证了所提出方案的有效性，证明其在分布式环境中提高隐私意识和稳健性的潜力。

Conclusion: 该研究为开集场景下的面部识别提供了一种隐私意识的联邦学习解决方案，通过结合OpenMax算法有效地处理了已知和未知个体的识别问题。

Abstract: Facial recognition powered by Artificial Intelligence has achieved high
accuracy in specific scenarios and applications. Nevertheless, it faces
significant challenges regarding privacy and identity management, particularly
when unknown individuals appear in the operational context. This paper presents
the design, implementation, and evaluation of a facial recognition system
within a federated learning framework tailored to open-set scenarios. The
proposed approach integrates the OpenMax algorithm into federated learning,
leveraging the exchange of mean activation vectors and local distance measures
to reliably distinguish between known and unknown subjects. Experimental
results validate the effectiveness of the proposed solution, demonstrating its
potential for enhancing privacy-aware and robust facial recognition in
distributed environments.
  --
  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado
una alta precisi\'on en algunos escenarios y aplicaciones. Sin embargo,
presenta desaf\'ios relacionados con la privacidad y la identificaci\'on de
personas, especialmente considerando que pueden aparecer sujetos desconocidos
para el sistema que lo implementa. En este trabajo, se propone el dise\~no,
implementaci\'on y evaluaci\'on de un sistema de reconocimiento facial en un
escenario de aprendizaje federado, orientado a conjuntos abiertos.
Concretamente, se dise\~na una soluci\'on basada en el algoritmo OpenMax para
escenarios de aprendizaje federado. La propuesta emplea el intercambio de los
vectores de activaci\'on promedio y distancias locales para identificar de
manera eficaz tanto personas conocidas como desconocidas. Los experimentos
realizados demuestran la implementaci\'on efectiva de la soluci\'on propuesta.

</details>


### [16] [Automated classification of natural habitats using ground-level imagery](https://arxiv.org/abs/2508.19314)
*Mahdis Tourian,Sareh Rowlands,Remy Vandaele,Max Fancourt,Rebecca Mein,Hywel T. P. Williams*

Main category: cs.CV

TL;DR: 基于地面照片的生境分类方法，使用DeepLabV3-ResNet101深度学习模型，对英国18种生境进行分类，平均F1分数达0.61，并提供网页应用工具


<details>
  <summary>Details</summary>
Motivation: 传统卫星图像生境分类需野外验证，本研究通过地面照片提供更好的验证方式，支持公民科学大规模生境分类

Method: 使用DeepLabV3-ResNet101深度学习模型，对地面照片进行预处理（调数、归一化、增强），重采样平衡训练数据类，采用五折交叉验证

Result: 模型在18个生境类别上表现良好，平均F1分数0.61，视觉明显生境如空土、沙漠等F1分数超过0.90，混合生境分数较低

Conclusion: 地面照片生境分类方法具有很大潜力，照片获取方便，准确的计算方法在生态监测中有广泛应用前景，并提供了实用的网页应用工具

Abstract: Accurate classification of terrestrial habitats is critical for biodiversity
conservation, ecological monitoring, and land-use planning. Several habitat
classification schemes are in use, typically based on analysis of satellite
imagery with validation by field ecologists. Here we present a methodology for
classification of habitats based solely on ground-level imagery (photographs),
offering improved validation and the ability to classify habitats at scale (for
example using citizen-science imagery). In collaboration with Natural England,
a public sector organisation responsible for nature conservation in England,
this study develops a classification system that applies deep learning to
ground-level habitat photographs, categorising each image into one of 18
classes defined by the 'Living England' framework. Images were pre-processed
using resizing, normalisation, and augmentation; re-sampling was used to
balance classes in the training data and enhance model robustness. We developed
and fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label
to each photograph. Using five-fold cross-validation, the model demonstrated
strong overall performance across 18 habitat classes, with accuracy and
F1-scores varying between classes. Across all folds, the model achieved a mean
F1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and
Peat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or
ambiguous classes scoring lower. These findings demonstrate the potential of
this approach for ecological monitoring. Ground-level imagery is readily
obtained, and accurate computational methods for habitat classification based
on such data have many potential applications. To support use by practitioners,
we also provide a simple web application that classifies uploaded images using
our model.

</details>


### [17] [MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation](https://arxiv.org/abs/2508.19320)
*Ming Chen,Liyuan Cui,Wenyuan Zhang,Haoxian Zhang,Yan Zhou,Xiaohan Li,Xiaoqiang Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: 一种基于自回归大语言模型的交互式数字人视频生成框架，支持音频、姿态、文本等多模态输入，实现低延迟实时生成和精细控制


<details>
  <summary>Details</summary>
Motivation: 解决现有交互式数字人视频生成方法存在的高延迟、计算成本高、控制性有限等挑战

Method: 通过少量修改标准大语言模型，接受多模态条件编码，输出空间和语义一致的表征来指导去噪过程，构建20,000小时大规模对话数据集，使用深度压缩自编码器减轻自回归模型的长期推理负担

Result: 在双向对话、多语言人类合成、交互世界模型等应用中表现出低延迟、高效率和细粒度多模态控制能力

Conclusion: 该框架为实时交互式数字人视频生成提供了一种高效、可控的解决方案，在延迟、效率和控制性方面都显著优于现有方法

Abstract: Recently, interactive digital human video generation has attracted widespread
attention and achieved remarkable progress. However, building such a practical
system that can interact with diverse input signals in real time remains
challenging to existing methods, which often struggle with high latency, heavy
computational cost, and limited controllability. In this work, we introduce an
autoregressive video generation framework that enables interactive multimodal
control and low-latency extrapolation in a streaming manner. With minimal
modifications to a standard large language model (LLM), our framework accepts
multimodal condition encodings including audio, pose, and text, and outputs
spatially and semantically coherent representations to guide the denoising
process of a diffusion head. To support this, we construct a large-scale
dialogue dataset of approximately 20,000 hours from multiple sources, providing
rich conversational scenarios for training. We further introduce a deep
compression autoencoder with up to 64$\times$ reduction ratio, which
effectively alleviates the long-horizon inference burden of the autoregressive
model. Extensive experiments on duplex conversation, multilingual human
synthesis, and interactive world model highlight the advantages of our approach
in low latency, high efficiency, and fine-grained multimodal controllability.

</details>


### [18] [Deep Data Hiding for ICAO-Compliant Face Images: A Survey](https://arxiv.org/abs/2508.19324)
*Jefferson David Rodriguez Chivata,Davide Ghiani,Simone Maurizio La Cava,Marco Micheletto,Giulia Orrù,Federico Lama,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: 该论文调查数字水印和隐写术作为ICAO合规面部图像的防篡改解决方案，分析现有技术在身份验证系统中的适用性和局限性。


<details>
  <summary>Details</summary>
Motivation: ICAO合规面部图像广泛应用于身份验证，但标准化也带来了篡改风险（如换脸和深度伪造），传统实时检测方法无法提供捕获后的保护。

Method: 通过全面分析最先进的数字水印和隐写技术，评估这些方法在ICAO标准约束下的潜力和局限性。

Result: 研究发现数字水印和隐写术能够在不影响ICAO合规性的前提下提供持久的防篡改验证，但存在关键权衡问题。

Conclusion: 该研究为在实际身份系统中安全部署防篡改技术提供了指导，强调了数字水印和隐写术作为传统检测方法补充方案的重要性。

Abstract: ICAO-compliant facial images, initially designed for secure biometric
passports, are increasingly becoming central to identity verification in a wide
range of application contexts, including border control, digital travel
credentials, and financial services. While their standardization enables global
interoperability, it also facilitates practices such as morphing and deepfakes,
which can be exploited for harmful purposes like identity theft and illegal
sharing of identity documents. Traditional countermeasures like Presentation
Attack Detection (PAD) are limited to real-time capture and offer no
post-capture protection. This survey paper investigates digital watermarking
and steganography as complementary solutions that embed tamper-evident signals
directly into the image, enabling persistent verification without compromising
ICAO compliance. We provide the first comprehensive analysis of
state-of-the-art techniques to evaluate the potential and drawbacks of the
underlying approaches concerning the applications involving ICAO-compliant
images and their suitability under standard constraints. We highlight key
trade-offs, offering guidance for secure deployment in real-world identity
systems.

</details>


### [19] [PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI](https://arxiv.org/abs/2508.19325)
*Haoyang Su,Jin-Yi Xiang,Shaohao Rui,Yifan Gao,Xingyu Chen,Tingxuan Yin,Xiaosong Wang,Lian-Ming Wu*

Main category: cs.CV

TL;DR: PRISM是一个自监督框架，整合心脏MRI影像和电子健康记录进行生存分析，通过运动感知多视图蒸馏提取同步影像特征，并使用医学文本提示进行调制，在四个独立临床队列中超越传统和深度学习基线方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测主要不良心脏事件(MACE)是心血管预后的核心挑战，需要整合多模态数据来提升预测准确性。

Method: PRISM框架通过运动感知多视图蒸馏提取时间同步的影像特征，使用医学信息文本提示调制特征，整合非对比心脏电影MRI和结构化EHR数据进行生存分析。

Result: 在四个独立临床队列中，PRISM在内部和外部验证中 consistently超越经典生存预测模型和最先进的深度学习基线，发现了三个与MACE风险升高相关的影像特征，并识别出高血压、糖尿病和吸烟是主要风险因素。

Conclusion: PRISM提供的组合影像和EHR表征为不同人群的心脏风险提供了有价值的见解，证明了多模态整合在心血管预后中的有效性。

Abstract: Accurate prediction of major adverse cardiac events (MACE) remains a central
challenge in cardiovascular prognosis. We present PRISM (Prompt-guided
Representation Integration for Survival Modeling), a self-supervised framework
that integrates visual representations from non-contrast cardiac cine magnetic
resonance imaging with structured electronic health records (EHRs) for survival
analysis. PRISM extracts temporally synchronized imaging features through
motion-aware multi-view distillation and modulates them using medically
informed textual prompts to enable fine-grained risk prediction. Across four
independent clinical cohorts, PRISM consistently surpasses classical survival
prediction models and state-of-the-art (SOTA) deep learning baselines under
internal and external validation. Further clinical findings demonstrate that
the combined imaging and EHR representations derived from PRISM provide
valuable insights into cardiac risk across diverse cohorts. Three distinct
imaging signatures associated with elevated MACE risk are uncovered, including
lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior
elevated focus during diastole. Prompt-guided attribution further identifies
hypertension, diabetes, and smoking as dominant contributors among clinical and
physiological EHR factors.

</details>


### [20] [EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.19349)
*Mahdieh Behjat Khatooni,Mohsen Soryani*

Main category: cs.CV

TL;DR: 提出EffNetViTLoRA模型，结合CNN和Vision Transformer，使用LoRA技术微调，在完整ADNI MRI数据集上实现AD、MCI和CN三分类，准确率达92.52%


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病早期诊断至关重要，MCI诊断因类别间差异细微而具有挑战性。现有研究多使用有限数据子集，需要更鲁棒和公正的模型

Method: 集成CNN和ViT捕捉MRI图像的局部和全局特征，使用完整ADNI T1加权MRI数据集训练，采用LoRA技术有效适配预训练ViT模型到目标领域

Result: 在AD、MCI和CN三分类任务中达到92.52%的分类准确率和92.76%的F1分数

Conclusion: EffNetViTLoRA模型通过综合方法和LoRA技术，实现了高效的AD诊断，具有临床可靠性

Abstract: Alzheimer's disease (AD) is one of the most prevalent neurodegenerative
disorders worldwide. As it progresses, it leads to the deterioration of
cognitive functions. Since AD is irreversible, early diagnosis is crucial for
managing its progression. Mild Cognitive Impairment (MCI) represents an
intermediate stage between Cognitively Normal (CN) individuals and those with
AD, and is considered a transitional phase from normal cognition to Alzheimer's
disease. Diagnosing MCI is particularly challenging due to the subtle
differences between adjacent diagnostic categories. In this study, we propose
EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole
Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging
(MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a
Vision Transformer (ViT) to capture both local and global features from MRI
images. Unlike previous studies that rely on limited subsets of data, our
approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in
a more robust and unbiased model. This comprehensive methodology enhances the
model's clinical reliability. Furthermore, fine-tuning large pretrained models
often yields suboptimal results when source and target dataset domains differ.
To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt
the pretrained ViT model to our target domain. This method enables efficient
knowledge transfer and reduces the risk of overfitting. Our model achieves a
classification accuracy of 92.52% and an F1-score of 92.76% across three
diagnostic categories: AD, MCI, and CN for full ADNI dataset.

</details>


### [21] [Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage](https://arxiv.org/abs/2508.19477)
*Zachary L. Crang,Rich D. Johnston,Katie L. Mills,Johsan Billingham,Sam Robertson,Michael H. Cole,Jonathon Weakley,Adam Hewitt and,Grant M. Duthie*

Main category: cs.CV

TL;DR: 研究评估商业计算机视觉和AI球员追踪软件使用转播画面测量球员位置、速度和距离的准确性，以及相机画面和分辨率对精度的影响。


<details>
  <summary>Details</summary>
Motivation: 随着计算机视觉和AI技术在体育分析中的应用日益广泛，需要验证这些商业软件使用标准转播画面而非专用多相机系统的追踪准确性。

Method: 使用2022年卡塔尔世界杯比赛数据，比较三家商业追踪提供商与TRACAB Gen 5多相机系统的数据，计算位置和速度的均方根误差以及总距离的平均偏差。

Result: 位置RMSE范围为1.68-16.39米，速度RMSE为0.34-2.38 m/s，总距离平均偏差为-21.8%到+24.3%。战术画面能最大化球员检测并提高准确性。

Conclusion: 计算机视觉和AI球员追踪软件在检测到球员时具有合理精度，推荐使用战术画面进行追踪，720p和1080p分辨率在适当模型下都适用。

Abstract: This study aimed to: (1) understand whether commercially available
computer-vision and artificial intelligence (AI) player tracking software can
accurately measure player position, speed and distance using broadcast footage
and (2) determine the impact of camera feed and resolution on accuracy. Data
were obtained from one match at the 2022 Qatar Federation Internationale de
Football Association (FIFA) World Cup. Tactical, programme and camera 1 feeds
were used. Three commercial tracking providers that use computer-vision and AI
participated. Providers analysed instantaneous position (x, y coordinates) and
speed (m\,s^{-1}) of each player. Their data were compared with a
high-definition multi-camera tracking system (TRACAB Gen 5). Root mean square
error (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to
16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\,s^{-1}. Total match
distance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across
providers. Computer-vision and AI player tracking software offer the ability to
track players with fair precision when players are detected by the software.
Providers should use a tactical feed when tracking position and speed, which
will maximise player detection, improving accuracy. Both 720p and 1080p
resolutions are suitable, assuming appropriate computer-vision and AI models
are implemented.

</details>


### [22] [JVLGS: Joint Vision-Language Gas Leak Segmentation](https://arxiv.org/abs/2508.19485)
*Xinlong Zhao,Qixiang Pang,Shan Du*

Main category: cs.CV

TL;DR: 提出了JVLGS框架，通过融合视觉和文本模态来增强气体泄漏的表示和分割能力，在监督学习和少样本学习设置下均优于现有方法


<details>
  <summary>Details</summary>
Motivation: 气体泄漏对人类健康和环境造成严重威胁，但现有检测方法效果有限，特别是红外视频中气体云模糊和非刚性的特性限制了传统视觉方法的有效性

Method: 提出联合视觉-语言气体泄漏分割框架(JVLGS)，整合视觉和文本模态的互补优势，并包含后处理步骤以减少噪声和非目标物体引起的误报

Result: 在多样化场景下的广泛实验表明，JVLGS显著优于最先进的气体泄漏分割方法，在监督和少样本学习设置下均表现优异

Conclusion: JVLGS框架通过多模态融合有效解决了气体泄漏检测的挑战，为准确及时的气体泄漏识别提供了有效解决方案

Abstract: Gas leaks pose serious threats to human health and contribute significantly
to atmospheric pollution, drawing increasing public concern. However, the lack
of effective detection methods hampers timely and accurate identification of
gas leaks. While some vision-based techniques leverage infrared videos for leak
detection, the blurry and non-rigid nature of gas clouds often limits their
effectiveness. To address these challenges, we propose a novel framework called
Joint Vision-Language Gas leak Segmentation (JVLGS), which integrates the
complementary strengths of visual and textual modalities to enhance gas leak
representation and segmentation. Recognizing that gas leaks are sporadic and
many video frames may contain no leak at all, our method incorporates a
post-processing step to reduce false positives caused by noise and non-target
objects, an issue that affects many existing approaches. Extensive experiments
conducted across diverse scenarios show that JVLGS significantly outperforms
state-of-the-art gas leak segmentation methods. We evaluate our model under
both supervised and few-shot learning settings, and it consistently achieves
strong performance in both, whereas competing methods tend to perform well in
only one setting or poorly in both. Code available at:
https://github.com/GeekEagle/JVLGS

</details>


### [23] [UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models](https://arxiv.org/abs/2508.19498)
*Yimu Wang,Weiming Zhuang,Chen Chen,Jiabo Huang,Jingtao Li,Lingjuan Lyu*

Main category: cs.CV

TL;DR: UNIFORM框架通过投票机制整合异构预训练模型的知识，在logit和特征层面捕获共识，显著提升无监督目标识别性能并具有良好可扩展性


<details>
  <summary>Details</summary>
Motivation: 现有知识集成方法对训练数据分布和网络架构有强假设限制，无法有效利用异构预训练模型的集体知识

Method: 提出专用投票机制，在logit层面整合能预测目标类的教师模型，在特征层面利用任意标签空间学习的视觉表示

Result: 相比强基线显著提升无监督目标识别性能，能受益于100+教师模型，而现有方法在较小规模就饱和

Conclusion: UNIFORM框架成功解决了异构预训练模型知识整合的挑战，无需强假设约束，展现出卓越的可扩展性和性能优势

Abstract: In the era of deep learning, the increasing number of pre-trained models
available online presents a wealth of knowledge. These models, developed with
diverse architectures and trained on varied datasets for different tasks,
provide unique interpretations of the real world. Their collective consensus is
likely universal and generalizable to unseen data. However, effectively
harnessing this collective knowledge poses a fundamental challenge due to the
heterogeneity of pre-trained models. Existing knowledge integration solutions
typically rely on strong assumptions about training data distributions and
network architectures, limiting them to learning only from specific types of
models and resulting in data and/or inductive biases. In this work, we
introduce a novel framework, namely UNIFORM, for knowledge transfer from a
diverse set of off-the-shelf models into one student model without such
constraints. Specifically, we propose a dedicated voting mechanism to capture
the consensus of knowledge both at the logit level -- incorporating teacher
models that are capable of predicting target classes of interest -- and at the
feature level, utilizing visual representations learned on arbitrary label
spaces. Extensive experiments demonstrate that UNIFORM effectively enhances
unsupervised object recognition performance compared to strong knowledge
transfer baselines. Notably, it exhibits remarkable scalability by benefiting
from over one hundred teachers, while existing methods saturate at a much
smaller scale.

</details>


### [24] [Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery](https://arxiv.org/abs/2508.19499)
*Xiangxu Wang,Tianhong Zhao,Wei Tu,Bowen Zhang,Guanzhou Chen,Jinzhou Cao*

Main category: cs.CV

TL;DR: Sat2Flow是一个基于扩散模型的框架，仅使用卫星图像生成结构一致的OD流量矩阵，解决了现有方法对辅助特征依赖和空间拓扑敏感性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有OD流量生成方法存在两个关键局限：1) 依赖成本高、空间覆盖有限的辅助特征；2) 对空间拓扑敏感，区域索引重排序会破坏生成流量的结构一致性。

Method: 提出Sat2Flow框架，包含多核编码器捕捉区域交互，采用排列感知扩散过程确保不同区域排序下的潜在表示对齐，通过联合对比训练目标和等变扩散训练实现结构一致性。

Result: 在真实城市数据集上的实验表明，Sat2Flow在数值精度上优于物理基线和数据驱动基线，同时在索引置换下保持经验分布和空间结构。

Conclusion: Sat2Flow为数据稀缺城市环境提供了可扩展的OD流量生成解决方案，消除了区域特定辅助数据依赖，同时保持结构不变性以实现稳健的移动性建模。

Abstract: Origin-Destination (OD) flow matrices are essential for urban mobility
analysis, underpinning applications in traffic forecasting, infrastructure
planning, and policy design. However, existing methods suffer from two critical
limitations: (1) reliance on auxiliary features (e.g., Points of Interest,
socioeconomic statistics) that are costly to collect and have limited spatial
coverage; and (2) sensitivity to spatial topology, where minor index reordering
of urban regions (e.g., census tract relabeling) disrupts structural coherence
in generated flows. To address these challenges, we propose Sat2Flow, a latent
structure-aware diffusion-based framework that generates structurally coherent
OD flows using solely satellite imagery as input. Our approach introduces a
multi-kernel encoder to capture diverse regional interactions and employs a
permutation-aware diffusion process that aligns latent representations across
different regional orderings. Through a joint contrastive training objective
that bridges satellite-derived features with OD patterns, combined with
equivariant diffusion training that enforces structural consistency, Sat2Flow
ensures topological robustness under arbitrary regional reindexing.
Experimental results on real-world urban datasets demonstrate that Sat2Flow
outperforms both physics-based and data-driven baselines in numerical accuracy
while preserving empirical distributions and spatial structures under index
permutations. Sat2Flow offers a globally scalable solution for OD flow
generation in data-scarce urban environments, eliminating region-specific
auxiliary data dependencies while maintaining structural invariance for robust
mobility modeling.

</details>


### [25] [Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity](https://arxiv.org/abs/2508.19511)
*Alzayat Saleh,Shunsuke Hatano,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: 本研究提出诊断驱动的半监督框架，通过分析模型偏差并利用未标注数据提升杂草检测性能，解决了农业环境中深度学习的挑战性问题。


<details>
  <summary>Details</summary>
Motivation: 解决农业杂草自动管理中深度学习模型在真实环境中的性能下降问题，主要挑战包括复杂环境条件和数据标注成本高昂。

Method: 使用诊断分析发现模型存在"阴影偏差"问题，然后构建半监督学习管道，利用975张标注图像和10,000张未标注图像进行伪标签训练，提升模型鲁棒性。

Result: 监督基线模型达到F1分数0.90和mAP50超过0.82，半监督框架有效缓解阴影偏差并显著提升召回率，在低数据环境下验证了方法的有效性。

Conclusion: 提供了一个经过实地测试的框架，用于开发、诊断和改进精准农业中计算机视觉系统的鲁棒性，为复杂农业环境提供了实用解决方案。

Abstract: The automated management of invasive weeds is critical for sustainable
agriculture, yet the performance of deep learning models in real-world fields
is often compromised by two factors: challenging environmental conditions and
the high cost of data annotation. This study tackles both issues through a
diagnostic-driven, semi-supervised framework. Using a unique dataset of
approximately 975 labeled and 10,000 unlabeled images of Guinea Grass in
sugarcane, we first establish strong supervised baselines for classification
(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and
mAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by
interpretability tools, uncovered a pervasive "shadow bias," where models
learned to misidentify shadows as vegetation. This diagnostic insight motivated
our primary contribution: a semi-supervised pipeline that leverages unlabeled
data to enhance model robustness. By training models on a more diverse set of
visual information through pseudo-labeling, this framework not only helps
mitigate the shadow bias but also provides a tangible boost in recall, a
critical metric for minimizing weed escapes in automated spraying systems. To
validate our methodology, we demonstrate its effectiveness in a low-data regime
on a public crop-weed benchmark. Our work provides a clear and field-tested
framework for developing, diagnosing, and improving robust computer vision
systems for the complex realities of precision agriculture.

</details>


### [26] [MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment](https://arxiv.org/abs/2508.19527)
*Zhiting Gao,Dan Song,Diqiong Jiang,Chao Xue,An-An Liu*

Main category: cs.CV

TL;DR: 提出了TAPO和MotionFLUX统一框架，通过偏好优化对齐文本与运动语义，使用整流流匹配实现实时运动生成，在语义一致性和运动质量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动运动生成方法存在语义对齐不精确和推理速度慢的问题，需要同时提升语义对齐精度和生成效率。

Method: TAPO框架通过偏好优化对齐细微运动变化与文本修饰符，MotionFLUX基于确定性整流流匹配构建噪声分布与运动空间的最优传输路径，实现实时合成。

Result: 实验结果表明，该统一系统在语义一致性和运动质量方面优于最先进方法，同时显著加速了生成速度。

Conclusion: TAPO和MotionFLUX的结合提供了一个高效的解决方案，解决了文本到运动生成中的语义对齐和实时性挑战。

Abstract: Motion generation is essential for animating virtual characters and embodied
agents. While recent text-driven methods have made significant strides, they
often struggle with achieving precise alignment between linguistic descriptions
and motion semantics, as well as with the inefficiencies of slow, multi-step
inference. To address these issues, we introduce TMR++ Aligned Preference
Optimization (TAPO), an innovative framework that aligns subtle motion
variations with textual modifiers and incorporates iterative adjustments to
reinforce semantic grounding. To further enable real-time synthesis, we propose
MotionFLUX, a high-speed generation framework based on deterministic rectified
flow matching. Unlike traditional diffusion models, which require hundreds of
denoising steps, MotionFLUX constructs optimal transport paths between noise
distributions and motion spaces, facilitating real-time synthesis. The
linearized probability paths reduce the need for multi-step sampling typical of
sequential methods, significantly accelerating inference time without
sacrificing motion quality. Experimental results demonstrate that, together,
TAPO and MotionFLUX form a unified system that outperforms state-of-the-art
approaches in both semantic consistency and motion quality, while also
accelerating generation speed. The code and pretrained models will be released.

</details>


### [27] [CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning](https://arxiv.org/abs/2508.19542)
*Nannan Zhu,Yonghao Dong,Teng Wang,Xueqian Li,Shengjun Deng,Yijia Wang,Zheng Hong,Tiantian Geng,Guo Niu,Hanyan Huang,Xiongfei Yao,Shuaiwei Jiao*

Main category: cs.CV

TL;DR: CVBench是首个专门评估多视频关系推理能力的综合基准测试，包含1000个问答对，涵盖三个层次：跨视频对象关联、事件关联和复杂推理。测试发现当前MLLM在多视频推理方面存在显著性能差距，顶级模型如GPT-4o在因果推理任务上仅达60%准确率，远低于人类的91%。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大语言模型在单视频任务上表现良好，但其在多视频场景下的能力尚未得到充分探索，而这一能力对于现实应用（如多摄像头监控、跨视频程序学习）至关重要。

Method: 构建CVBench基准测试，包含来自五个不同领域视频集群的1000个问答对，分为三个层次：跨视频对象关联、事件关联和复杂推理。评估了10多个领先MLLM在零样本和思维链提示范式下的表现。

Result: 评估结果显示显著性能差距：顶级模型GPT-4o在因果推理任务上仅达到60%准确率，而人类表现达91%。分析揭示了当前MLLM架构的根本瓶颈，包括跨视频上下文保留不足和重叠实体消歧能力差。

Conclusion: CVBench为诊断和推进多视频推理建立了严格框架，为下一代MLLM提供了架构洞察。该基准测试揭示了当前模型在多视频关系推理方面的局限性，并指出了未来改进的方向。

Abstract: While multimodal large language models (MLLMs) exhibit strong performance on
single-video tasks (e.g., video question answering), their ability across
multiple videos remains critically underexplored. However, this capability is
essential for real-world applications, including multi-camera surveillance and
cross-video procedural learning. To bridge this gap, we present CVBench, the
first comprehensive benchmark designed to assess cross-video relational
reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning
three hierarchical tiers: cross-video object association (identifying shared
entities), cross-video event association (linking temporal or causal event
chains), and cross-video complex reasoning (integrating commonsense and domain
knowledge). Built from five domain-diverse video clusters (e.g., sports, life
records), the benchmark challenges models to synthesise information across
dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including
GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought
prompting paradigms. Key findings reveal stark performance gaps: even top
models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,
compared to the 91% accuracy of human performance. Crucially, our analysis
reveals fundamental bottlenecks inherent in current MLLM architectures, notably
deficient inter-video context retention and poor disambiguation of overlapping
entities. CVBench establishes a rigorous framework for diagnosing and advancing
multi-video reasoning, offering architectural insights for next-generation
MLLMs.The data and evaluation code are available at
https://github.com/Hokhim2/CVBench.

</details>


### [28] [WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization](https://arxiv.org/abs/2508.19544)
*Eduardo Davalos,Yike Zhang,Namrata Srivastava,Yashvitha Thatigotla,Jorge A. Salas,Sara McFadden,Sun-Joo Cho,Amanda Goodwin,Ashwin TS,Gautam Biswas*

Main category: cs.CV

TL;DR: WebEyeTrack是一个在浏览器中运行的轻量级视线追踪框架，通过集成头部姿态估计和少样本学习，仅需9个校准样本就能实现SOTA性能，误差2.32cm，推理速度2.4毫秒。


<details>
  <summary>Details</summary>
Motivation: 现有AI视线估计方法虽然性能优秀，但在实际应用中与商业眼动仪存在差距，且存在模型大小、推理时间和隐私问题。基于摄像头的眼动追踪方法因头部运动导致精度不足。

Method: 开发WebEyeTrack框架，在浏览器中直接集成轻量级SOTA视线估计模型，结合基于模型的头部姿态估计和设备端少样本学习（k<9个校准样本）。

Result: 在GazeCapture数据集上达到2.32cm的误差，在iPhone 14上实现2.4毫秒的实时推理速度，性能达到SOTA水平。

Conclusion: WebEyeTrack成功解决了现有方法的局限性，提供了一个高效、隐私友好且易于部署的浏览器内眼动追踪解决方案，代码已开源。

Abstract: With advancements in AI, new gaze estimation methods are exceeding
state-of-the-art (SOTA) benchmarks, but their real-world application reveals a
gap with commercial eye-tracking solutions. Factors like model size, inference
time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking
methods lack sufficient accuracy, in particular due to head movement. To tackle
these issues, we introduce We bEyeTrack, a framework that integrates
lightweight SOTA gaze estimation models directly in the browser. It
incorporates model-based head pose estimation and on-device few-shot learning
with as few as nine calibration samples (k < 9). WebEyeTrack adapts to new
users, achieving SOTA performance with an error margin of 2.32 cm on
GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.
Our open-source code is available at
https://github.com/RedForestAi/WebEyeTrack.

</details>


### [29] [MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery](https://arxiv.org/abs/2508.19555)
*Yu-Wei Zhang,Tongju Han,Lipeng Gao,Mingqiang Wei,Hui Liu,Changbao Li,Caiming Zhang*

Main category: cs.CV

TL;DR: MonoRelief V2是一个端到端模型，能够从单张图像中直接恢复2.5D浮雕，在复杂材质和光照变化下表现出色。相比仅使用合成数据训练的V1版本，V2通过结合伪真实数据和真实数据，显著提升了鲁棒性、准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决从单张图像恢复2.5D浮雕的挑战，特别是在复杂材质和光照变化条件下。传统方法主要依赖合成数据训练，缺乏对真实场景的适应能力，需要开发能够处理真实世界数据的更强大模型。

Method: 1) 使用文本到图像生成模型生成约15,000张伪真实图像，并通过深度和法线预测融合获得深度伪标签；2) 构建小规模真实世界数据集（800个样本），通过多视角重建和细节优化；3) 在伪真实和真实数据集上进行渐进式训练。

Result: 综合实验表明，MonoRelief V2在深度和法线预测方面都达到了最先进的性能，展现出在下游应用中的强大潜力。

Conclusion: MonoRelief V2通过结合伪真实数据和真实数据的渐进式训练策略，成功提升了从单张图像恢复2.5D浮雕的性能，为相关应用提供了有效的解决方案。

Abstract: This paper presents MonoRelief V2, an end-to-end model designed for directly
recovering 2.5D reliefs from single images under complex material and
illumination variations. In contrast to its predecessor, MonoRelief V1 [1],
which was solely trained on synthetic data, MonoRelief V2 incorporates real
data to achieve improved robustness, accuracy and efficiency. To overcome the
challenge of acquiring large-scale real-world dataset, we generate
approximately 15,000 pseudo real images using a text-to-image generative model,
and derive corresponding depth pseudo-labels through fusion of depth and normal
predictions. Furthermore, we construct a small-scale real-world dataset (800
samples) via multi-view reconstruction and detail refinement. MonoRelief V2 is
then progressively trained on the pseudo-real and real-world datasets.
Comprehensive experiments demonstrate its state-of-the-art performance both in
depth and normal predictions, highlighting its strong potential for a range of
downstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.

</details>


### [30] [FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection](https://arxiv.org/abs/2508.19565)
*Yuhang Zhao,Zixing Wang*

Main category: cs.CV

TL;DR: FlowDet是一个基于DETR架构的高速端到端目标检测器，通过解耦编码器优化策略、几何可变形单元和尺度感知注意力模块，在Intersection-Flow-5k数据集上实现了新的SOTA性能，同时显著降低了计算成本和提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决端到端目标检测器在复杂场景（如交叉路口交通监控）中计算成本高的问题，为实时应用提供NMS-free的高效检测方案。

Method: 提出了FlowDet检测器，采用解耦编码器优化策略，包含几何可变形单元(GDU)进行交通感知几何建模，以及尺度感知注意力(SAA)模块处理极端尺度变化。

Result: 在Intersection-Flow-5k数据集上，相比RT-DETR基线，AP(test)提升1.5%，AP50(test)提升1.6%，同时GFLOPs减少63.2%，推理速度提升16.2%。

Conclusion: FlowDet为构建高效准确的现实世界感知系统提供了新路径，特别是在高遮挡和高目标密度的复杂交通场景中表现出色。

Abstract: End-to-end object detectors offer a promising NMS-free paradigm for real-time
applications, yet their high computational cost remains a significant barrier,
particularly for complex scenarios like intersection traffic monitoring. To
address this challenge, we propose FlowDet, a high-speed detector featuring a
decoupled encoder optimization strategy applied to the DETR architecture.
Specifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for
traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to
maintain high representational power across extreme scale variations. To
rigorously evaluate the model's performance in environments with severe
occlusion and high object density, we collected the Intersection-Flow-5k
dataset, a new challenging scene for this task. Evaluated on
Intersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to
the strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by
1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference
speed by 16.2%. Our work demonstrates a new path towards building highly
efficient and accurate detectors for demanding, real-world perception systems.
The Intersection-Flow-5k dataset is available at
https://github.com/AstronZh/Intersection-Flow-5K.

</details>


### [31] [DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection](https://arxiv.org/abs/2508.19573)
*Luhu Li,Bowen Lin,Mukhtiar Khan,Shujun Fu*

Main category: cs.CV

TL;DR: 提出结合可训练编码器、原型引导重建和多样性感知对齐损失的统一框架，解决医学图像异常检测中的原型崩溃问题，显著提升表示质量和异常定位性能


<details>
  <summary>Details</summary>
Motivation: 医学图像异常检测面临标注有限和领域差异挑战，现有重建方法依赖冻结预训练编码器限制了领域适应性，原型学习方法存在原型崩溃问题影响多样性和泛化能力

Method: 使用可训练编码器（含动量分支）进行领域自适应特征学习，轻量级原型提取器挖掘信息丰富的正常原型，通过注意力机制指导解码器重建，引入多样性感知对齐损失防止原型崩溃

Result: 在多个医学影像基准测试中显著提升了表示质量和异常定位精度，超越了现有方法，可视化分析和原型分配验证了抗崩溃机制的有效性和可解释性增强

Conclusion: 提出的统一框架有效解决了原型崩溃问题，通过领域自适应特征学习和多样性约束实现了更精确的医学图像异常检测，具有更好的泛化能力和可解释性

Abstract: Anomaly detection in medical images is challenging due to limited annotations
and a domain gap compared to natural images. Existing reconstruction methods
often rely on frozen pre-trained encoders, which limits adaptation to
domain-specific features and reduces localization accuracy. Prototype-based
learning offers interpretability and clustering benefits but suffers from
prototype collapse, where few prototypes dominate training, harming diversity
and generalization. To address this, we propose a unified framework combining a
trainable encoder with prototype-guided reconstruction and a novel
Diversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum
branch, enables stable domain-adaptive feature learning. A lightweight
Prototype Extractor mines informative normal prototypes to guide the decoder
via attention for precise reconstruction. Our loss enforces balanced prototype
use through diversity constraints and per-prototype normalization, effectively
preventing collapse. Experiments on multiple medical imaging benchmarks show
significant improvements in representation quality and anomaly localization,
outperforming prior methods. Visualizations and prototype assignment analyses
further validate the effectiveness of our anti-collapse mechanism and enhanced
interpretability.

</details>


### [32] [Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation](https://arxiv.org/abs/2508.19574)
*Mingxi Fu,Fanglei Fu,Xitong Ling,Huaitian Yuan,Tian Guan,Yonghong He,Lianghui Zhu*

Main category: cs.CV

TL;DR: MPAMatch是一个新颖的多模态病理图像分割框架，通过图像和文本原型与像素标签的双重对比学习，在结构和语义层面提供监督，显著改善了语义边界建模。


<details>
  <summary>Details</summary>
Motivation: 解决病理图像分割中语义边界模糊和像素级标注成本高的问题，现有方法主要依赖图像模态内的扰动一致性，难以捕捉高层次语义先验。

Method: 提出MPAMatch框架，采用多模态原型引导监督范式进行像素级对比学习，包括图像原型-像素标签和文本原型-像素标签的双重对比学习方案，并使用病理预训练基础模型重构TransUNet架构。

Result: 在GLAS、EBHI-SEG-GLAND、EBHI-SEG-CANCER和KPI数据集上的广泛实验显示，MPAMatch优于最先进方法，验证了其在结构和语义建模方面的双重优势。

Conclusion: MPAMatch通过引入文本原型监督和双重对比学习方案，有效提升了病理图像分割的性能，特别是在语义边界建模方面取得了显著改进。

Abstract: Pathological image segmentation faces numerous challenges, particularly due
to ambiguous semantic boundaries and the high cost of pixel-level annotations.
Although recent semi-supervised methods based on consistency regularization
(e.g., UniMatch) have made notable progress, they mainly rely on
perturbation-based consistency within the image modality, making it difficult
to capture high-level semantic priors, especially in structurally complex
pathology images. To address these limitations, we propose MPAMatch - a novel
segmentation framework that performs pixel-level contrastive learning under a
multimodal prototype-guided supervision paradigm. The core innovation of
MPAMatch lies in the dual contrastive learning scheme between image prototypes
and pixel labels, and between text prototypes and pixel labels, providing
supervision at both structural and semantic levels. This coarse-to-fine
supervisory strategy not only enhances the discriminative capability on
unlabeled samples but also introduces the text prototype supervision into
segmentation for the first time, significantly improving semantic boundary
modeling. In addition, we reconstruct the classic segmentation architecture
(TransUNet) by replacing its ViT backbone with a pathology-pretrained
foundation model (Uni), enabling more effective extraction of
pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,
EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art
methods, validating its dual advantages in structural and semantic modeling.

</details>


### [33] [Interact-Custom: Customized Human Object Interaction Image Generation](https://arxiv.org/abs/2508.19575)
*Zhu Xu,Zhaowen Wang,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: 提出了定制化人机交互图像生成任务(CHOI)，通过两阶段模型Interact-Custom实现目标人物身份保持和交互语义控制，解决了现有方法缺乏细粒度交互控制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注目标实体的外观保持，而忽略了目标实体之间的细粒度交互控制。为了解决这个问题，研究者专注于人机交互场景，提出了CHOI任务。

Method: 首先处理大规模数据集，每个样本包含相同人机对的不同交互姿势；然后设计两阶段模型Interact-Custom，首先生成描述交互行为的前景掩码来显式建模空间配置，然后在掩码指导下生成保持身份特征的目标人机交互图像。

Result: 在专门为CHOI任务设计的指标上进行的大量实验证明了该方法的有效性。模型还提供了可选功能，允许用户指定背景图像和目标人机出现的位置。

Conclusion: 提出的Interact-Custom模型成功解决了同时保持身份特征和控制交互语义的挑战，为人机交互图像生成提供了有效的解决方案。

Abstract: Compositional Customized Image Generation aims to customize multiple target
concepts within generation content, which has gained attention for its wild
application.Existing approaches mainly concentrate on the target entity's
appearance preservation, while neglecting the fine-grained interaction control
among target entities.To enable the model of such interaction control
capability, we focus on human object interaction scenario and propose the task
of Customized Human Object Interaction Image Generation(CHOI), which
simultaneously requires identity preservation for target human object and the
interaction semantic control between them.Two primary challenges exist for
CHOI:(1)simultaneous identity preservation and interaction control demands
require the model to decompose the human object into self-contained identity
features and pose-oriented interaction features, while the current HOI image
datasets fail to provide ideal samples for such feature-decomposed
learning.(2)inappropriate spatial configuration between human and object may
lead to the lack of desired interaction semantics.To tackle it, we first
process a large-scale dataset, where each sample encompasses the same pair of
human object involving different interactive poses.Then we design a two-stage
model Interact-Custom, which firstly explicitly models the spatial
configuration by generating a foreground mask depicting the interaction
behavior, then under the guidance of this mask, we generate the target human
object interacting while preserving their identities features.Furthermore, if
the background image and the union location of where the target human object
should appear are provided by users, Interact-Custom also provides the optional
functionality to specify them, offering high content controllability. Extensive
experiments on our tailored metrics for CHOI task demonstrate the effectiveness
of our approach.

</details>


### [34] [High-Speed FHD Full-Color Video Computer-Generated Holography](https://arxiv.org/abs/2508.19579)
*Haomiao Zhang,Miao Cao,Xuan Yu,Hui Luo,Yanling Piao,Mengjie Qin,Zhangyuan Li,Ping Wang,Xin Yuan*

Main category: cs.CV

TL;DR: 提出SGDDM和HoloMamba两种方法解决计算机生成全息视频的高帧率显示与计算效率问题，SGDDM通过频谱引导优化相位分布实现高保真全彩显示，HoloMamba利用Mamba-Unet架构建模时空相关性，达到260+FPS的1080p全彩全息视频生成速度


<details>
  <summary>Details</summary>
Motivation: 解决计算机生成全息技术中学习模型产生的过度平滑相位导致色彩串扰问题，以及现有逐帧优化方法忽略时空相关性导致计算效率低下的局限性

Method: 1. SGDDM：频谱引导深度分割复用技术，通过频率调制优化相位分布；2. HoloMamba：轻量级非对称Mamba-Unet架构，显式建模视频序列的时空相关性

Result: SGDDM实现高帧率下的高保真全彩显示，HoloMamba生成1080p全彩全息视频速度超过260FPS，比现有最优方法快2.6倍以上

Conclusion: 提出的SGDDM和HoloMamba方案成功解决了计算机生成全息视频在帧率和色彩保真度之间的权衡问题，显著提升了计算效率和重建质量

Abstract: Computer-generated holography (CGH) is a promising technology for
next-generation displays. However, generating high-speed, high-quality
holographic video requires both high frame rate display and efficient
computation, but is constrained by two key limitations: ($i$) Learning-based
models often produce over-smoothed phases with narrow angular spectra, causing
severe color crosstalk in high frame rate full-color displays such as
depth-division multiplexing and thus resulting in a trade-off between frame
rate and color fidelity. ($ii$) Existing frame-by-frame optimization methods
typically optimize frames independently, neglecting spatial-temporal
correlations between consecutive frames and leading to computationally
inefficient solutions. To overcome these challenges, in this paper, we propose
a novel high-speed full-color video CGH generation scheme. First, we introduce
Spectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase
distributions via frequency modulation, enabling high-fidelity full-color
display at high frame rates. Second, we present HoloMamba, a lightweight
asymmetric Mamba-Unet architecture that explicitly models spatial-temporal
correlations across video sequences to enhance reconstruction quality and
computational efficiency. Extensive simulated and real-world experiments
demonstrate that SGDDM achieves high-fidelity full-color display without
compromise in frame rate, while HoloMamba generates FHD (1080p) full-color
holographic video at over 260 FPS, more than 2.6$\times$ faster than the prior
state-of-the-art Divide-Conquer-and-Merge Strategy.

</details>


### [35] [Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction](https://arxiv.org/abs/2508.19581)
*Dat Nguyen Cong,Hieu Tran Bao,Hoang Thanh-Tung*

Main category: cs.CV

TL;DR: 本文提出了Score-based Discriminator Correction (SBDC)方法，通过判别器训练和对抗损失来校正预训练条件扩散模型中的噪声，提高生成质量和可控性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在大规模数据集上表现出色，但这些数据集常包含人工标注错误。目前尚不清楚这些错误如何影响扩散模型的生成能力和可控性，因此需要研究有效的校正方法。

Method: 提出SBDC引导技术，基于判别器训练使用对抗损失，借鉴先前的噪声检测技术评估样本真实性。特别限制引导在生成过程的早期阶段使用以获得更好性能。

Result: 在不同噪声设置下的实验表明，该方法优于先前的最先进方法，计算效率高，仅略微增加推理时间，且无需重新训练扩散模型。

Conclusion: SBDC是一种有效的扩散模型校正方法，能够显著提升生成质量，同时保持计算效率，为解决数据集标注错误问题提供了实用解决方案。

Abstract: Diffusion models have gained prominence as state-of-the-art techniques for
synthesizing images and videos, particularly due to their ability to scale
effectively with large datasets. Recent studies have uncovered that these
extensive datasets often contain mistakes from manual labeling processes.
However, the extent to which such errors compromise the generative capabilities
and controllability of diffusion models is not well studied. This paper
introduces Score-based Discriminator Correction (SBDC), a guidance technique
for aligning noisy pre-trained conditional diffusion models. The guidance is
built on discriminator training using adversarial loss, drawing on prior noise
detection techniques to assess the authenticity of each sample. We further show
that limiting the usage of our guidance to the early phase of the generation
process leads to better performance. Our method is computationally efficient,
only marginally increases inference time, and does not require retraining
diffusion models. Experiments on different noise settings demonstrate the
superiority of our method over previous state-of-the-art methods.

</details>


### [36] [Generalizing Monocular 3D Object Detection](https://arxiv.org/abs/2508.19593)
*Abhinav Kumar*

Main category: cs.CV

TL;DR: 该论文针对单目3D目标检测在遮挡、数据集迁移、大目标检测和相机参数外推等泛化挑战，提出了GrooMeD-NMS、DEVIANT、SeaBird等创新方法，从数学角度分析和改进了模型在不同场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 单目3D目标检测在自动驾驶、增强现实等应用中需要准确的环境理解，但现有模型在面对遮挡、新数据集、大目标和不同相机参数时泛化能力不足，需要系统性解决方案。

Method: 1) 提出可微分NMS(GrooMeD-NMS)增强遮挡鲁棒性；2) 探索深度等变骨干网络(DEVIANT)提升数据集泛化；3) 引入基于分割的鸟瞰图方法(SeaBird)解决大目标检测的噪声敏感问题；4) 数学分析相机高度外推问题。

Result: 从数学和实验角度系统解决了单目3D检测的多个泛化挑战，提出了针对不同问题的有效解决方案，提升了模型在复杂场景下的性能。

Conclusion: 通过数学分析和创新方法设计，该工作显著提升了单目3D目标检测模型在遮挡、数据集迁移、大目标检测和相机参数变化等场景下的泛化能力，为实际应用提供了重要技术支撑。

Abstract: Monocular 3D object detection (Mono3D) is a fundamental computer vision task
that estimates an object's class, 3D position, dimensions, and orientation from
a single image. Its applications, including autonomous driving, augmented
reality, and robotics, critically rely on accurate 3D environmental
understanding. This thesis addresses the challenge of generalizing Mono3D
models to diverse scenarios, including occlusions, datasets, object sizes, and
camera parameters. To enhance occlusion robustness, we propose a mathematically
differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we
explore depth equivariant (DEVIANT) backbones. We address the issue of large
object detection, demonstrating that it's not solely a data imbalance or
receptive field problem but also a noise sensitivity issue. To mitigate this,
we introduce a segmentation-based approach in bird's-eye view with dice loss
(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D
models to unseen camera heights and improve Mono3D generalization in such
out-of-distribution settings.

</details>


### [37] [Quantization Robustness to Input Degradations for Object Detection](https://arxiv.org/abs/2508.19600)
*Toghrul Karimov,Hassan Imani,Allan Kazakov*

Main category: cs.CV

TL;DR: 这篇论文对YOLO目标检测模型在各种精度格式下的粗糖化稳健性进行了综合性实证研究，并提出了一种基于透度的定标策略，但该策略在大部分情况下并未实现一致性的稳健性提升。


<details>
  <summary>Details</summary>
Motivation: 质量化技术在实际部署中面临实际输入透度（如噪声、模糊、压缩效应）的挑战，需要研究粗糖化模型在这些情况下的稳健性表现。

Method: 采用多种精度格式（FP32、FP16、Dynamic UINT8、Static INT8）对YOLO模型进行后训练量化，并提出了一种透度感知定标策略，在TensorRT定标过程中混入清洁和综合透度图像。在COCO数据集上评估七种不同透度条件和混合透度场景。

Result: Static INT8 TensorRT引擎在清洁数据上实现了~1.5-3.3倍速度提升，准确度下降~3-7% mAP50-95。透度感知定标策略在大部分模型和透度条件下未能一致提升稳健性，仅在某些噪声条件下对大规模模型有显著效果。

Conclusion: 提高量化模型的稳健性面临重大挑战，模型容量可能影响定标策略的效果。这些发现为在非受控环境中部署量化检测器提供了重要见解。

Abstract: Post-training quantization (PTQ) is crucial for deploying efficient object
detection models, like YOLO, on resource-constrained devices. However, the
impact of reduced precision on model robustness to real-world input
degradations such as noise, blur, and compression artifacts is a significant
concern. This paper presents a comprehensive empirical study evaluating the
robustness of YOLO models (nano to extra-large scales) across multiple
precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8
(TensorRT). We introduce and evaluate a degradation-aware calibration strategy
for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix
of clean and synthetically degraded images. Models were benchmarked on the COCO
dataset under seven distinct degradation conditions (including various types
and levels of noise, blur, low contrast, and JPEG compression) and a
mixed-degradation scenario. Results indicate that while Static INT8 TensorRT
engines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop
(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did
not yield consistent, broad improvements in robustness over standard clean-data
calibration across most models and degradations. A notable exception was
observed for larger model scales under specific noise conditions, suggesting
model capacity may influence the efficacy of this calibration approach. These
findings highlight the challenges in enhancing PTQ robustness and provide
insights for deploying quantized detectors in uncontrolled environments. All
code and evaluation tables are available at https://github.com/AllanK24/QRID.

</details>


### [38] [IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2508.19604)
*Qizhe Fan,Chaoyu Liu,Zhonghua Qiao,Xiaoqin Shen*

Main category: cs.CV

TL;DR: 提出IELDM和IELFormer方法，通过逆演化层抑制扩散模型生成缺陷，提升域泛化语义分割性能


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的合成数据存在结构和语义缺陷，直接用于训练会导致分割模型性能下降和错误累积

Method: 1) IELDM：在生成过程中集成逆演化层(IELs)，基于拉普拉斯先验突出空间不连续性和语义不一致性；2) IELFormer：将IELs嵌入分割网络解码器，抑制伪影传播；3) 多尺度频率融合(MFF)模块进行频域分析，增强跨尺度语义一致性

Result: 在基准数据集上的大量实验表明，该方法相比现有方法实现了更优越的泛化性能

Conclusion: 逆演化层能有效抑制生成缺陷和伪影传播，结合多尺度频率融合，显著提升了域泛化语义分割的泛化能力

Abstract: Domain Generalized Semantic Segmentation (DGSS) focuses on training a model
using labeled data from a source domain, with the goal of achieving robust
generalization to unseen target domains during inference. A common approach to
improve generalization is to augment the source domain with synthetic data
generated by diffusion models (DMs). However, the generated images often
contain structural or semantic defects due to training imperfections. Training
segmentation models with such flawed data can lead to performance degradation
and error accumulation. To address this issue, we propose to integrate inverse
evolution layers (IELs) into the generative process. IELs are designed to
highlight spatial discontinuities and semantic inconsistencies using
Laplacian-based priors, enabling more effective filtering of undesirable
generative patterns. Based on this mechanism, we introduce IELDM, an enhanced
diffusion-based data augmentation framework that can produce higher-quality
images. Furthermore, we observe that the defect-suppression capability of IELs
can also benefit the segmentation network by suppressing artifact propagation.
Based on this insight, we embed IELs into the decoder of the DGSS model and
propose IELFormer to strengthen generalization capability in cross-domain
scenarios. To further strengthen the model's semantic consistency across
scales, IELFormer incorporates a multi-scale frequency fusion (MFF) module,
which performs frequency-domain analysis to achieve structured integration of
multi-resolution features, thereby improving cross-scale coherence. Extensive
experiments on benchmark datasets demonstrate that our approach achieves
superior generalization performance compared to existing methods.

</details>


### [39] [Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model](https://arxiv.org/abs/2508.19626)
*Jiajun Sun,Zhen Yu,Siyuan Yan,Jason J. Ong,Zongyuan Ge,Lei Zhang*

Main category: cs.CV

TL;DR: LF-VAR是一个可控的皮肤图像合成模型，利用量化病变测量分数和病变类型标签，通过语言提示生成具有特定病变特征的高质量皮肤图像。


<details>
  <summary>Details</summary>
Motivation: 真实临床实践中的皮肤图像数据有限，现有合成方法生成的图像质量低且缺乏对病变位置和类型的控制。

Method: 使用多尺度病变聚焦的VQVAE编码图像为离散潜在表示，然后训练视觉自回归变换器进行图像合成，整合病变测量和类型作为条件嵌入。

Result: 在七种病变类型上获得最佳FID分数（平均0.74），比之前SOTA方法提升6.3%。

Conclusion: 该方法能有效生成高保真度、临床相关的合成皮肤图像，解决了数据稀缺和可控合成的问题。

Abstract: Skin images from real-world clinical practice are often limited, resulting in
a shortage of training data for deep-learning models. While many studies have
explored skin image synthesis, existing methods often generate low-quality
images and lack control over the lesion's location and type. To address these
limitations, we present LF-VAR, a model leveraging quantified lesion
measurement scores and lesion type labels to guide the clinically relevant and
controllable synthesis of skin images. It enables controlled skin synthesis
with specific lesion characteristics based on language prompts. We train a
multiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to
encode images into discrete latent representations for structured tokenization.
Then, a Visual AutoRegressive (VAR) Transformer trained on tokenized
representations facilitates image synthesis. Lesion measurement from the lesion
region and types as conditional embeddings are integrated to enhance synthesis
fidelity. Our method achieves the best overall FID score (average 0.74) among
seven lesion types, improving upon the previous state-of-the-art (SOTA) by
6.3%. The study highlights our controllable skin synthesis model's
effectiveness in generating high-fidelity, clinically relevant synthetic skin
images. Our framework code is available at
https://github.com/echosun1996/LF-VAR.

</details>


### [40] [Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition](https://arxiv.org/abs/2508.19630)
*Xiaolei Wei,Yi Ouyang,Haibo Ye*

Main category: cs.CV

TL;DR: DQRoute是一个针对长尾视觉识别问题的模块化框架，通过难度感知优化和动态专家协作来提升性能，特别是在稀有和困难类别上表现显著


<details>
  <summary>Details</summary>
Motivation: 长尾视觉识别不仅面临类别不平衡问题，还有不同类别学习难度差异的挑战。简单的基于频率的类别重加权方法往往忽略了本质上难以学习的类别

Method: DQRoute结合难度感知优化和混合专家设计：1）基于预测不确定性和历史性能估计类别难度，指导自适应损失加权；2）采用混合专家架构，每个专家专注于类别分布的不同区域；3）推理时通过专家特定的OOD检测器生成置信度分数来加权专家预测，实现无需集中路由器的输入自适应路由；所有组件端到端联合训练

Result: 在标准长尾基准测试中，DQRoute显著提升了性能，特别是在稀有和困难类别上

Conclusion: 将难度建模与去中心化专家路由相结合具有明显优势，为解决长尾识别问题提供了有效方案

Abstract: Long-tailed visual recognition is challenging not only due to class imbalance
but also because of varying classification difficulty across categories. Simply
reweighting classes by frequency often overlooks those that are intrinsically
hard to learn. To address this, we propose \textbf{DQRoute}, a modular
framework that combines difficulty-aware optimization with dynamic expert
collaboration. DQRoute first estimates class-wise difficulty based on
prediction uncertainty and historical performance, and uses this signal to
guide training with adaptive loss weighting. On the architectural side, DQRoute
employs a mixture-of-experts design, where each expert specializes in a
different region of the class distribution. At inference time, expert
predictions are weighted by confidence scores derived from expert-specific OOD
detectors, enabling input-adaptive routing without the need for a centralized
router. All components are trained jointly in an end-to-end manner. Experiments
on standard long-tailed benchmarks demonstrate that DQRoute significantly
improves performance, particularly on rare and difficult classes, highlighting
the benefit of integrating difficulty modeling with decentralized expert
routing.

</details>


### [41] [Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception](https://arxiv.org/abs/2508.19638)
*Yang Li,Quan Yuan,Guiyang Luo,Xiaoyuan Fu,Rui Pan,Yujia Yang,Congzhang Shao,Yuewen Liu,Jinglin Li*

Main category: cs.CV

TL;DR: CoPLOT是一个新颖的协作感知框架，使用点级优化token来处理无序、海量的点云数据，通过语义感知重排序、频率增强状态空间模型和邻域到自车对齐模块，在降低通信和计算开销的同时实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有协作感知方法通常使用2D鸟瞰图表示，丢弃了关键的细粒度3D结构信息，影响物体识别和定位精度。点云数据具有无序性、海量性和位置敏感性，难以生成紧凑对齐的点级token序列。

Method: 提出点原生处理流程：1)语义感知token重排序模块利用场景级和token级语义信息生成自适应1D重排序；2)频率增强状态空间模型在空间和频谱域捕获长距离序列依赖；3)邻域到自车对齐模块结合全局agent级校正和局部token级精化来减轻定位噪声。

Result: 在仿真和真实数据集上的大量实验表明，CoPLOT超越了最先进模型，同时具有更低的通信和计算开销。

Conclusion: CoPLOT通过点级优化token有效解决了点云数据处理的挑战，在保持结构细节的同时实现了高效的协作感知，为3D协作感知提供了新的解决方案。

Abstract: Collaborative perception allows agents to enhance their perceptual
capabilities by exchanging intermediate features. Existing methods typically
organize these intermediate features as 2D bird's-eye-view (BEV)
representations, which discard critical fine-grained 3D structural cues
essential for accurate object recognition and localization. To this end, we
first introduce point-level tokens as intermediate representations for
collaborative perception. However, point-cloud data are inherently unordered,
massive, and position-sensitive, making it challenging to produce compact and
aligned point-level token sequences that preserve detailed structural
information. Therefore, we present CoPLOT, a novel Collaborative perception
framework that utilizes Point-Level Optimized Tokens. It incorporates a
point-native processing pipeline, including token reordering, sequence
modeling, and multi-agent spatial alignment. A semantic-aware token reordering
module generates adaptive 1D reorderings by leveraging scene-level and
token-level semantic information. A frequency-enhanced state space model
captures long-range sequence dependencies across both spatial and spectral
domains, improving the differentiation between foreground tokens and background
clutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop
process, combining global agent-level correction with local token-level
refinement to mitigate localization noise. Extensive experiments on both
simulated and real-world datasets show that CoPLOT outperforms state-of-the-art
models, with even lower communication and computation overhead. Code will be
available at https://github.com/CheeryLeeyy/CoPLOT.

</details>


### [42] [UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks](https://arxiv.org/abs/2508.19647)
*Bikash Kumar Badatya,Vipul Baghel,Ravi Hegde*

Main category: cs.CV

TL;DR: 这篇论文提出了一种轻量级的无监督骨架动作定位方法，利用空间-时间图神经网表征学习动作动态特征，无需手动标注即可在跳水视频中实现高精度的实时动作定位。


<details>
  <summary>Details</summary>
Motivation: 解决细粒度动作定位的挑战：现有监督和弱监督方法需要大量标注数据和高算力模型，计算成本高且适应性差，难以应用于实际场景。

Method: 使用Attention基础的空间-时间图卷积神经网网络(ASTGCN)进行骨架序列去噪预训练，通过块分区学习本质运动动态。推理时使用新的动作动态指标(ADM)，通过分析小维度嵌入的曲率变化来检测运动边界点。

Result: 在DSV跳水数据集上达到平均精度(mAP)82.66%，平均定位延迟29.09毫秒，性能可比监督方法，同时保持计算效率。无需重新训练即可演示出对未见野生跳水视频的良好演化能力。

Conclusion: 该方法提供了一种轻量级、无监督的解决方案，能够在嵌入式或动态环境中实现实时动作分析，具有强大的实际应用价值。

Abstract: Fine-grained action localization in untrimmed sports videos presents a
significant challenge due to rapid and subtle motion transitions over short
durations. Existing supervised and weakly supervised solutions often rely on
extensive annotated datasets and high-capacity models, making them
computationally intensive and less adaptable to real-world scenarios. In this
work, we introduce a lightweight and unsupervised skeleton-based action
localization pipeline that leverages spatio-temporal graph neural
representations. Our approach pre-trains an Attention-based Spatio-Temporal
Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with
blockwise partitions, enabling it to learn intrinsic motion dynamics without
any manual labeling. At inference, we define a novel Action Dynamics Metric
(ADM), computed directly from low-dimensional ASTGCN embeddings, which detects
motion boundaries by identifying inflection points in its curvature profile.
Our method achieves a mean Average Precision (mAP) of 82.66% and average
localization latency of 29.09 ms on the DSV Diving dataset, matching
state-of-the-art supervised performance while maintaining computational
efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving
footage without retraining, demonstrating its practical applicability for
lightweight, real-time action analysis systems in embedded or dynamic
environments.

</details>


### [43] [IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising](https://arxiv.org/abs/2508.19649)
*Dongjin Kim,Jaekyun Ko,Muhammad Kashif Ali,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 提出一种基于动态生成核的迭代图像去噪方法，通过特征提取、全局统计和局部相关性模块来预测像素级变化核，在单一高斯噪声训练下实现多种噪声类型和级别的优秀泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖特定噪声分布，泛化能力有限，且需要大量训练数据和计算资源，容易过拟合。需要开发更通用、高效的去噪方法。

Method: 使用特征提取模块获取噪声不变特征，全局统计和局部相关性模块捕获噪声特征和结构相关性，核预测模块生成像素级变化核，通过迭代动态滤波进行去噪。

Result: 紧凑模型（约0.04M参数）在单一高斯噪声训练下，对多种噪声类型和级别表现出优异性能，证明了迭代动态滤波的实用性。

Conclusion: 动态核生成方法有效防止过拟合，提高对未知噪声的鲁棒性，为实际图像去噪应用提供了有前景的解决方案。

Abstract: Image denoising is a fundamental challenge in computer vision, with
applications in photography and medical imaging. While deep learning-based
methods have shown remarkable success, their reliance on specific noise
distributions limits generalization to unseen noise types and levels. Existing
approaches attempt to address this with extensive training data and high
computational resources but they still suffer from overfitting. To address
these issues, we conduct image denoising by utilizing dynamically generated
kernels via efficient operations. This approach helps prevent overfitting and
improves resilience to unseen noise. Specifically, our method leverages a
Feature Extraction Module for robust noise-invariant features, Global
Statistics and Local Correlation Modules to capture comprehensive noise
characteristics and structural correlations. The Kernel Prediction Module then
employs these cues to produce pixel-wise varying kernels adapted to local
structures, which are then applied iteratively for denoising. This ensures both
efficiency and superior restoration quality. Despite being trained on
single-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse
noise types and levels, demonstrating the promise of iterative dynamic
filtering for practical image denoising.

</details>


### [44] [A bag of tricks for real-time Mitotic Figure detection](https://arxiv.org/abs/2508.19804)
*Christian Marzahl,Brian Napora*

Main category: cs.CV

TL;DR: 这篇论文提出了一系列训练技巧，通过RTMDet单阶段检测器实现了高效、健壮的有丝分裂图检测，在多域数据集上获得了0.78-0.84的F1分数，适合临床部署。


<details>
  <summary>Details</summary>
Motivation: 解决组织学图像中有丝分裂图检测面临的挑战：扫描仪变异性、染色协议差异、组织类型多样性以及干扰物存在。需要开发能够在多样化领域中稳健运行的实时检测方案。

Method: 基于RTMDet单阶段物体检测器，采用多域训练数据、均衡采样和精心设计的数据增帿来应对扫描仪变异性和肿瘤异质性。使用针对性的难以识别的负样本采集技术，重点关注坏死组织和细胞残弃物来降低假阶性。

Result: 在多个MF数据集上进行分组5折交叉验证，模型获得了0.78-0.84的F1分数。在MIDOG 2025挑战的预测试集上，RTMDet-S模型达到0.81的F1分数，表现超过更大模型并显示了对新领域的适应能力。

Conclusion: 该方案提供了准确性与速度之间的实用平衡，通过一系列训练技巧实现了在多样化领域中的健壮、实时的有丝分裂图检测，具有实际临床应用的潜力。

Abstract: Mitotic figure (MF) detection in histopathology images is challenging due to
large variations in slide scanners, staining protocols, tissue types, and the
presence of artifacts. This paper presents a collection of training techniques
- a bag of tricks - that enable robust, real-time MF detection across diverse
domains. We build on the efficient RTMDet single stage object detector to
achieve high inference speed suitable for clinical deployment. Our method
addresses scanner variability and tumor heterogeneity via extensive
multi-domain training data, balanced sampling, and careful augmentation.
Additionally, we employ targeted, hard negative mining on necrotic and debris
tissue to reduce false positives. In a grouped 5-fold cross-validation across
multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On
the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025
challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81,
outperforming larger models and demonstrating adaptability to new, unfamiliar
domains. The proposed solution offers a practical trade-off between accuracy
and speed, making it attractive for real-world clinical adoption.

</details>


### [45] [Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models](https://arxiv.org/abs/2508.19650)
*Hou Xia,Zheren Fu,Fangcan Ling,Jiajun Li,Yi Tu,Zhendong Mao,Yongdong Zhang*

Main category: cs.CV

TL;DR: Video-LevelGauge是一个专门评估视频大语言模型位置偏见的基准测试，包含438个手动策划的视频和1297个问题，测试27个先进模型后发现开源模型存在显著位置偏见，而商业模型表现更一致。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准主要评估整体性能，忽略了位置偏见这一关键但未被充分探索的方面，需要专门工具来系统评估视频大语言模型在不同位置的表现偏差。

Method: 采用标准化探针和定制化上下文设置，灵活控制上下文长度、探针位置和上下文类型，结合统计测量和形态模式识别的综合分析方法，构建包含438个视频和1297个高质量问题的基准。

Result: 评估27个先进模型发现，许多领先的开源模型存在显著位置偏见，通常表现出头部或邻近内容偏好，而Gemini2.5-Pro等商业模型在整个视频序列中表现一致出色。

Conclusion: 该基准有效揭示了视频大语言模型的位置偏见问题，为缓解偏见和指导模型改进提供了可行见解，上下文长度、变化和模型规模的分析结果具有重要指导意义。

Abstract: Large video language models (LVLMs) have made notable progress in video
understanding, spurring the development of corresponding evaluation benchmarks.
However, existing benchmarks generally assess overall performance across entire
video sequences, overlooking nuanced behaviors such as contextual positional
bias, a critical yet under-explored aspect of LVLM performance. We present
Video-LevelGauge, a dedicated benchmark designed to systematically assess
positional bias in LVLMs. We employ standardized probes and customized
contextual setups, allowing flexible control over context length, probe
position, and contextual types to simulate diverse real-world scenarios. In
addition, we introduce a comprehensive analysis method that combines
statistical measures with morphological pattern recognition to characterize
bias. Our benchmark comprises 438 manually curated videos spanning multiple
types, yielding 1,177 high-quality multiple-choice questions and 120 open-ended
questions, validated for their effectiveness in exposing positional bias. Based
on these, we evaluate 27 state-of-the-art LVLMs, including both commercial and
open-source models. Our findings reveal significant positional biases in many
leading open-source models, typically exhibiting head or neighbor-content
preferences. In contrast, commercial models such as Gemini2.5-Pro show
impressive, consistent performance across entire video sequences. Further
analyses on context length, context variation, and model scale provide
actionable insights for mitigating bias and guiding model enhancement.

</details>


### [46] [ERSR: An Ellipse-constrained pseudo-label refinement and symmetric regularization framework for semi-supervised fetal head segmentation in ultrasound images](https://arxiv.org/abs/2508.19815)
*Linkuan Zhou,Zhexin Chen,Yufei Shen,Junlin Xu,Ping Xuan,Yixin Zhu,Yuqi Fang,Cong Cong,Leyi Wei,Ran Su,Jia Zhou,Qiangguo Jin*

Main category: cs.CV

TL;DR: 基于双评分适应筛选、椭圆约束伪标签精细化和对称性多重一致性正则化的半监督方法ERSR，在胎儿头部超声图像分割中获得领先性能


<details>
  <summary>Details</summary>
Motivation: 解决超声图像质量差和标注数据缺乏导致的胎儿头部分割挑战，现有半监督方法在伪标签生成和一致性约束方面效果不佳

Method: 提出ERSR框架，包括：1)基于边界一致性和轮廓规则性的双评分适应筛选策略；2)通过最小二乘椭圆拟合精细化伪标签；3)基于对称性的多重一致性正则化

Result: 在HC18数据集上，使用10%和20%标记数据分别达到92.05%和95.36%的Dice分数；在PSFH数据集上分别达到91.68%和93.70%

Conclusion: ERSR框架通过提高伪标签质量和多级别一致性约束，有效提升了胎儿头部超声分割的稳健性和性能

Abstract: Automated segmentation of the fetal head in ultrasound images is critical for
prenatal monitoring. However, achieving robust segmentation remains challenging
due to the poor quality of ultrasound images and the lack of annotated data.
Semi-supervised methods alleviate the lack of annotated data but struggle with
the unique characteristics of fetal head ultrasound images, making it
challenging to generate reliable pseudo-labels and enforce effective
consistency regularization constraints. To address this issue, we propose a
novel semi-supervised framework, ERSR, for fetal head ultrasound segmentation.
Our framework consists of the dual-scoring adaptive filtering strategy, the
ellipse-constrained pseudo-label refinement, and the symmetry-based multiple
consistency regularization. The dual-scoring adaptive filtering strategy uses
boundary consistency and contour regularity criteria to evaluate and filter
teacher outputs. The ellipse-constrained pseudo-label refinement refines these
filtered outputs by fitting least-squares ellipses, which strengthens pixels
near the center of the fitted ellipse and suppresses noise simultaneously. The
symmetry-based multiple consistency regularization enforces multi-level
consistency across perturbed images, symmetric regions, and between original
predictions and pseudo-labels, enabling the model to capture robust and stable
shape representations. Our method achieves state-of-the-art performance on two
benchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36%
with 10% and 20% labeled data, respectively. On the PSFH dataset, the scores
are 91.68% and 93.70% under the same settings.

</details>


### [47] [Scalable Object Detection in the Car Interior With Vision Foundation Models](https://arxiv.org/abs/2508.19651)
*Bálint Mészáros,Ahmet Firintepe,Sebastian Schmidt,Stephan Günnemann*

Main category: cs.CV

TL;DR: 这篇论文提出了ODAL框架，通过分布式设计在车载系统与云端之间分配计算任务，解决车辆内部物体检测和定位的计算资源限制问题。细调后的ODAL-LLaVA模型在ODALbench指标上达刱89%，显著超过GPT-4o和基线模型。


<details>
  <summary>Details</summary>
Motivation: 车辆内部对外部引入物体的识别和定位对个人助手的响应质量至关重要，但车载系统的计算资源受限，屏障了直接在车辆中部署基础模型的可能性。

Method: 提出ODAL框架，利用视觉基础模型通过分布式架构，将计算任务分配在车载系统和云端之间，并介绍ODALbench评估指标。比较GPT-4o和LLaVA 1.5 7B模型，探索细调对轻量模型性能的提升效果。

Result: 细调后的ODAL-LLaVA模型在ODAL得分上达刱89%，较基线性能提升71%，较GPT-4o高出近20%。同时保持高检测准确性并显著减少幻觉，ODAL信噪比是GPT-4o的3倍。

Conclusion: ODAL框架通过分布式设计有效解决了车辆计算资源限制问题，细调后的轻量模型在检测和定位性能上显著超过大型基础模型，为车辆内部场景理解领域建立了新标准。

Abstract: AI tasks in the car interior like identifying and localizing externally
introduced objects is crucial for response quality of personal assistants.
However, computational resources of on-board systems remain highly constrained,
restricting the deployment of such solutions directly within the vehicle. To
address this limitation, we propose the novel Object Detection and Localization
(ODAL) framework for interior scene understanding. Our approach leverages
vision foundation models through a distributed architecture, splitting
computational tasks between on-board and cloud. This design overcomes the
resource constraints of running foundation models directly in the car. To
benchmark model performance, we introduce ODALbench, a new metric for
comprehensive assessment of detection and localization.Our analysis
demonstrates the framework's potential to establish new standards in this
domain. We compare the state-of-the-art GPT-4o vision foundation model with the
lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the
lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model
achieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its
baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the
fine-tuned model maintains high detection accuracy while significantly reducing
hallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.

</details>


### [48] [Gradient Rectification for Robust Calibration under Distribution Shift](https://arxiv.org/abs/2508.19830)
*Yilin Zhang,Cai Xu,You Wu,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的深度网络检查框架，通过频域视角分析和梯度校正机制，在无需目标域信息的情况下提高分布偏移情况下的模型检查性能。


<details>
  <summary>Details</summary>
Motivation: 深度网络在安全关键应用中存在过信任预测问题，特别是在分布偏移情况下问题更为严重。现有方法依赖目标域信息或模拟，在实际应用中有限。

Method: 从频域角度分析，识别分布偏移对高频视觉线索的影响，采用低频筛波策略促进模型依赖域不变特征。同时通过梯度基校正机制确保分布内检查性能。

Result: 在CIFAR-10/100-C和WILDS等合成和实际偏移数据集上验证，方法显著提高了分布偏移情况下的检查性能，同时保持了强劲的分布内性能。

Conclusion: 该方法提供了一种无需目标域信息的实用解决方案，有效改善了深度网络在分布偏移情况下的检查性问题，提高了模型在安全关键应用中的可靠性。

Abstract: Deep neural networks often produce overconfident predictions, undermining
their reliability in safety-critical applications. This miscalibration is
further exacerbated under distribution shift, where test data deviates from the
training distribution due to environmental or acquisition changes. While
existing approaches improve calibration through training-time regularization or
post-hoc adjustment, their reliance on access to or simulation of target
domains limits their practicality in real-world scenarios. In this paper, we
propose a novel calibration framework that operates without access to target
domain information. From a frequency-domain perspective, we identify that
distribution shifts often distort high-frequency visual cues exploited by deep
models, and introduce a low-frequency filtering strategy to encourage reliance
on domain-invariant features. However, such information loss may degrade
In-Distribution (ID) calibration performance. Therefore, we further propose a
gradient-based rectification mechanism that enforces ID calibration as a hard
constraint during optimization. Experiments on synthetic and real-world shifted
datasets, including CIFAR-10/100-C and WILDS, demonstrate that our method
significantly improves calibration under distribution shift while maintaining
strong in-distribution performance.

</details>


### [49] [Self-Rewarding Vision-Language Model via Reasoning Decomposition](https://arxiv.org/abs/2508.19652)
*Zongxia Li,Wenhao Yu,Chengsong Huang,Rui Liu,Zhenwen Liang,Fuxiao Liu,Jingxi Che,Dian Yu,Jordan Boyd-Graber,Haitao Mi,Dong Yu*

Main category: cs.CV

TL;DR: Vision-SR1是一种自奖励方法，通过强化学习改进视觉语言模型的视觉推理能力，无需外部视觉监督，有效减少视觉幻觉和语言捷径问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型存在视觉幻觉和语言捷径问题，现有方法依赖人工标注或外部模型监督，成本高且存在分布偏移风险。

Method: 将VLM推理分解为视觉感知和语言推理两阶段，通过自包含视觉感知生成和自奖励机制，结合最终输出监督进行强化学习训练。

Result: 实验表明Vision-SR1能有效提升视觉推理能力，减少视觉幻觉和语言捷径依赖，在多种视觉语言任务中表现优异。

Conclusion: 自奖励方法为VLM提供了一种无需外部监督的有效训练范式，能够同时强化视觉感知和语言推理能力。

Abstract: Vision-Language Models (VLMs) often suffer from visual hallucinations, saying
things that are not actually in the image, and language shortcuts, where they
skip the visual part and just rely on text priors. These issues arise because
most post-training methods for VLMs rely on simple verifiable answer matching
and supervise only final outputs, leaving intermediate visual reasoning without
explicit guidance. As a result, VLMs receive sparse visual signals and often
learn to prioritize language-based reasoning over visual perception. To
mitigate this, some existing methods add visual supervision using human
annotations or distilled labels from external large models. However, human
annotations are labor-intensive and costly, and because external signals cannot
adapt to the evolving policy, they cause distributional shifts that can lead to
reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method
that improves visual reasoning without relying on external visual supervisions
via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two
stages: visual perception and language reasoning. The model is first prompted
to produce self-contained visual perceptions that are sufficient to answer the
question without referring back the input image. To validate this
self-containment, the same VLM model is then re-prompted to perform language
reasoning using only the generated perception as input to compute reward. This
self-reward is combined with supervision on final outputs, providing a balanced
training signal that strengthens both visual perception and language reasoning.
Our experiments demonstrate that Vision-SR1 improves visual reasoning,
mitigates visual hallucinations, and reduces reliance on language shortcuts
across diverse vision-language tasks.

</details>


### [50] [Multispectral LiDAR data for extracting tree points in urban and suburban areas](https://arxiv.org/abs/2508.19881)
*Narges Takhtkeshha,Gabriele Mazzacca,Fabio Remondino,Juha Hyyppä,Gottfried Mandlburger*

Main category: cs.CV

TL;DR: 本研究评估了三种深度学习模型在MS-LiDAR数据上的树木点提取性能，发现SPT模型在时间和精度方面表现最佳，结合pNDVI特征可显著降低错误率。


<details>
  <summary>Details</summary>
Motivation: 城市树木动态监测对绿化政策和电力基础设施风险管理至关重要，但复杂城市环境和树木多样性给传统方法带来挑战。MS-LiDAR技术能同时捕获3D空间和光谱数据，为精细制图提供了新机遇。

Method: 使用多光谱激光雷达(MS-LiDAR)数据，评估三种最先进的深度学习模型：Superpoint Transformer (SPT)、Point Transformer V3 (PTv3)和Point Transformer V1 (PTv1)，并分析结合伪归一化植被指数(pNDVI)的效果。

Result: SPT模型表现最优，平均交并比(mIoU)达到85.28%，具有显著的时间效率和准确性。结合pNDVI与空间数据可获得最高检测精度，相比仅使用空间信息，错误率降低了10.61个百分点。

Conclusion: MS-LiDAR与深度学习技术的结合具有巨大潜力，能够显著改善树木提取精度，为未来树木清单管理提供有力支持。

Abstract: Monitoring urban tree dynamics is vital for supporting greening policies and
reducing risks to electrical infrastructure. Airborne laser scanning has
advanced large-scale tree management, but challenges remain due to complex
urban environments and tree variability. Multispectral (MS) light detection and
ranging (LiDAR) improves this by capturing both 3D spatial and spectral data,
enabling detailed mapping. This study explores tree point extraction using
MS-LiDAR and deep learning (DL) models. Three state-of-the-art models are
evaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point
Transformer V1 (PTv1). Results show the notable time efficiency and accuracy of
SPT, with a mean intersection over union (mIoU) of 85.28%. The highest
detection accuracy is achieved by incorporating pseudo normalized difference
vegetation index (pNDVI) with spatial data, reducing error rate by 10.61
percentage points (pp) compared to using spatial information alone. These
findings highlight the potential of MS-LiDAR and DL to improve tree extraction
and further tree inventories.

</details>


### [51] [Hardware-aware vs. Hardware-agnostic Energy Estimation for SNN in Space Applications](https://arxiv.org/abs/2508.19654)
*Matthias Höfflin,Jürgen Wassner*

Main category: cs.CV

TL;DR: 该研究对脉冲神经网络(SNN)在卫星位置估计任务中的能效进行了深入分析，发现硬件无关方法预测SNN比CNN节能50-60%，但硬件感知分析显示只有在神经形态硬件和高输入稀疏度下才能实现显著节能。


<details>
  <summary>Details</summary>
Motivation: SNN长期以来被认为具有天然能效优势，但近期研究表明这种优势在数字实现中可能被高估，特别是在多输出回归任务中需要重新评估SNN的实际能效表现。

Method: 使用Leaky Integrate-and-Fire神经元的膜电位进行SNN训练，在逼真的卫星数据集上进行3D位置估计，比较硬件无关和硬件感知两种能效评估方法。

Result: SNN在MSE指标上与参考CNN相当，但能效分析显示：硬件无关方法预测SNN节能50-60%，而硬件感知分析表明只有在神经形态硬件和高输入稀疏度下才能实现显著节能。暗像素比例对能耗有重要影响。

Conclusion: 研究强调了透明评估方法和明确披露底层假设的重要性，以确保神经网络能效比较的公平性，数据特性和硬件假设对SNN能效评估具有关键影响。

Abstract: Spiking Neural Networks (SNNs), inspired by biological intelligence, have
long been considered inherently energy-efficient, making them attractive for
resource-constrained domains such as space applications. However, recent
comparative studies with conventional Artificial Neural Networks (ANNs) have
begun to question this reputation, especially for digital implementations. This
work investigates SNNs for multi-output regression, specifically 3-D satellite
position estimation from monocular images, and compares hardware-aware and
hardware-agnostic energy estimation methods. The proposed SNN, trained using
the membrane potential of the Leaky Integrate-and-Fire (LIF) neuron in the
final layer, achieves comparable Mean Squared Error (MSE) to a reference
Convolutional Neural Network (CNN) on a photorealistic satellite dataset.
Energy analysis shows that while hardware-agnostic methods predict a consistent
50-60% energy advantage for SNNs over CNNs, hardware-aware analysis reveals
that significant energy savings are realized only on neuromorphic hardware and
with high input sparsity. The influence of dark pixel ratio on energy
consumption is quantified, emphasizing the impact of data characteristics and
hardware assumptions. These findings highlight the need for transparent
evaluation methods and explicit disclosure of underlying assumptions to ensure
fair comparisons of neural network energy efficiency.

</details>


### [52] [WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.19927)
*Fayaz Ali,Muhammad Zawish,Steven Davy,Radu Timofte*

Main category: cs.CV

TL;DR: WaveHiT-SR：基于小波变换的分层Transformer图像超分辨率方法，通过自适应分层窗口和多频段分解，在降低计算复杂度的同时提升长距离依赖建模能力


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的超分辨率方法由于窗口自注意力的二次计算复杂度，只能使用小固定窗口，限制了感受野范围，需要新的方法来平衡计算效率和性能

Method: 将小波变换嵌入分层Transformer框架，使用自适应分层窗口替代静态小窗口，通过小波变换将图像分解为多频段子带，分层处理逐步重建高分辨率图像

Result: 在SwinIR-Light、SwinIR-NG和SRFormer-Light等模型上实现最先进的超分辨率结果，参数量更少、FLOPs更低、速度更快，同时保持高性能

Conclusion: WaveHiT-SR通过小波变换和分层Transformer的有效结合，成功解决了计算复杂度与性能之间的平衡问题，为图像超分辨率提供了高效且有效的解决方案

Abstract: Transformers have demonstrated promising performance in computer vision
tasks, including image super-resolution (SR). The quadratic computational
complexity of window self-attention mechanisms in many transformer-based SR
methods forces the use of small, fixed windows, limiting the receptive field.
In this paper, we propose a new approach by embedding the wavelet transform
within a hierarchical transformer framework, called (WaveHiT-SR). First, using
adaptive hierarchical windows instead of static small windows allows to capture
features across different levels and greatly improve the ability to model
long-range dependencies. Secondly, the proposed model utilizes wavelet
transforms to decompose images into multiple frequency subbands, allowing the
network to focus on both global and local features while preserving structural
details. By progressively reconstructing high-resolution images through
hierarchical processing, the network reduces computational complexity without
sacrificing performance. The multi-level decomposition strategy enables the
network to capture fine-grained information in lowfrequency components while
enhancing high-frequency textures. Through extensive experimentation, we
confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined
versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR
results, achieving higher efficiency with fewer parameters, lower FLOPs, and
faster speeds.

</details>


### [53] [A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement](https://arxiv.org/abs/2508.19664)
*Weicheng Liao,Zan Chen,Jianyang Xie,Yalin Zheng,Yuhui Ma,Yitian Zhao*

Main category: cs.CV

TL;DR: 提出了一种新颖的频率感知自监督学习方法，用于超广角视网膜图像增强，通过频率解耦去模糊和Retinex引导的照明补偿模块，有效提升图像质量和疾病诊断性能。


<details>
  <summary>Details</summary>
Motivation: 超广角视网膜成像虽然提供了全面的视网膜视图，但经常受到模糊和光照不均等质量退化因素的影响，这些因素会掩盖精细细节和病理信息。现有方法无法满足UWF图像的特殊需求，特别是需要保留病理细节的要求。

Method: 采用频率感知自监督学习方法，包含频率解耦图像去模糊模块（采用非对称通道集成操作结合全局和局部视图）和Retinex引导的照明补偿模块（包含颜色保护单元提供多尺度空间和频率信息）。

Result: 实验结果表明，该方法不仅提高了可视化质量，还通过恢复和校正精细局部细节和不均匀强度来改善疾病诊断性能。

Conclusion: 这是首个针对超广角视网膜图像增强的尝试，为改善视网膜疾病管理提供了一个强大且具有临床价值的工具。

Abstract: Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics
by providing a comprehensive view of the retina. However, it often suffers from
quality-degrading factors such as blurring and uneven illumination, which
obscure fine details and mask pathological information. While numerous retinal
image enhancement methods have been proposed for other fundus imageries, they
often fail to address the unique requirements in UWF, particularly the need to
preserve pathological details. In this paper, we propose a novel
frequency-aware self-supervised learning method for UWF image enhancement. It
incorporates frequency-decoupled image deblurring and Retinex-guided
illumination compensation modules. An asymmetric channel integration operation
is introduced in the former module, so as to combine global and local views by
leveraging high- and low-frequency information, ensuring the preservation of
fine and broader structural details. In addition, a color preservation unit is
proposed in the latter Retinex-based module, to provide multi-scale spatial and
frequency information, enabling accurate illumination estimation and
correction. Experimental results demonstrate that the proposed work not only
enhances visualization quality but also improves disease diagnosis performance
by restoring and correcting fine local details and uneven intensity. To the
best of our knowledge, this work is the first attempt for UWF image
enhancement, offering a robust and clinically valuable tool for improving
retinal disease management.

</details>


### [54] [GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity](https://arxiv.org/abs/2508.19972)
*Seongheon Park,Yixuan Li*

Main category: cs.CV

TL;DR: GLSim是一个无需训练的目标幻觉检测框架，通过结合全局和局部嵌入相似性信号，在多种场景下实现更准确可靠的幻觉检测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型中的目标幻觉问题严重影响了其在现实应用中的安全部署。现有方法通常单独采用全局或局部视角，可能限制了检测的可靠性。

Method: GLSim框架利用图像和文本模态之间的互补性全局和局部嵌入相似性信号，无需训练即可进行目标幻觉检测。

Result: 在全面的基准测试中，GLSim实现了卓越的检测性能，显著超越了竞争基线方法。

Conclusion: GLSim通过结合全局和局部视角，为目标幻觉检测提供了更准确可靠的解决方案，有助于提升视觉语言模型在实际应用中的安全性。

Abstract: Object hallucination in large vision-language models presents a significant
challenge to their safe deployment in real-world applications. Recent works
have proposed object-level hallucination scores to estimate the likelihood of
object hallucination; however, these methods typically adopt either a global or
local perspective in isolation, which may limit detection reliability. In this
paper, we introduce GLSim, a novel training-free object hallucination detection
framework that leverages complementary global and local embedding similarity
signals between image and text modalities, enabling more accurate and reliable
hallucination detection in diverse scenarios. We comprehensively benchmark
existing object hallucination detection methods and demonstrate that GLSim
achieves superior detection performance, outperforming competitive baselines by
a significant margin.

</details>


### [55] [SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction](https://arxiv.org/abs/2508.19688)
*Gangjian Zhang,Jian Shu,Nanjie Yao,Hao Wang*

Main category: cs.CV

TL;DR: SAT是一个两阶段的单目纹理3D人体重建框架，通过统一学习多种几何先验和在线动画增强，解决了单视图几何模糊性和数据稀缺问题，实现了高质量3D虚拟形象重建。


<details>
  <summary>Details</summary>
Motivation: 单目3D人体重建面临单张2D图像的几何模糊性和3D训练数据稀缺的挑战。现有方法难以有效整合不同几何模态，导致视角不一致和面部扭曲等问题。

Method: 提出两阶段框架SAT：1）统一学习SMPL模型和法线图等多种几何先验；2）引入监督特征正则化模块，使用多视图网络提供中间特征作为训练监督；3）设计在线动画增强模块，通过前馈动画网络在线生成大量训练样本。

Result: 在两个基准测试上的大量实验表明，该方法相比最先进方法具有优越性，能够重建出高质量的纹理3D虚拟形象。

Conclusion: SAT框架通过统一学习几何先验和在线数据增强，有效解决了单目3D人体重建中的几何模糊和数据稀缺问题，实现了更好的重建质量和视角一致性。

Abstract: Monocular texture 3D human reconstruction aims to create a complete 3D
digital avatar from just a single front-view human RGB image. However, the
geometric ambiguity inherent in a single 2D image and the scarcity of 3D human
training data are the main obstacles limiting progress in this field. To
address these issues, current methods employ prior geometric estimation
networks to derive various human geometric forms, such as the SMPL model and
normal maps. However, they struggle to integrate these modalities effectively,
leading to view inconsistencies, such as facial distortions. To this end, we
propose a two-process 3D human reconstruction framework, SAT, which seamlessly
learns various prior geometries in a unified manner and reconstructs
high-quality textured 3D avatars as the final output. To further facilitate
geometry learning, we introduce a Supervisor Feature Regularization module. By
employing a multi-view network with the same structure to provide intermediate
features as training supervision, these varied geometric priors can be better
fused. To tackle data scarcity and further improve reconstruction quality, we
also propose an Online Animation Augmentation module. By building a
one-feed-forward animation network, we augment a massive number of samples from
the original 3D human data online for model training. Extensive experiments on
two benchmarks show the superiority of our approach compared to
state-of-the-art methods.

</details>


### [56] [Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices](https://arxiv.org/abs/2508.20064)
*Philippe Zhang,Weili Jiang,Yihao Li,Jing Zhang,Sarah Matta,Yubo Tan,Hui Lin,Haoshen Wang,Jiangtian Pan,Hui Xu,Laurent Borderie,Alexandre Le Guilcher,Béatrice Cochener,Chubin Ou,Gwenolé Quellec,Mathieu Lamard*

Main category: cs.CV

TL;DR: 该论文介绍了参与MARIO挑战赛的解决方案，使用融合CNN网络和模型集成技术处理AMD疾病在OCT扫描中的进展分类和预测任务，在两项任务中均进入前十名。


<details>
  <summary>Details</summary>
Motivation: 年龄相关性黄斑变性(AMD)是影响视力的常见眼病，抗VEGF治疗能有效减缓新生血管性AMD的进展。通过及时诊断和持续监测来跟踪新生血管活动在OCT扫描中的进展，可以制定更个性化和有效的治疗计划。

Method: 任务1使用融合CNN网络和模型集成技术对连续OCT采集的2D切片对进行分类；任务2提出Patch Progression Masked Autoencoder，生成下一次检查的OCT图像，然后使用任务1的解决方案对当前OCT和生成的OCT之间的演变进行分类。

Result: 在MARIO挑战赛的两项任务中都进入了前十名，但由于部分团队成员与挑战组织者属于同一组织，没有资格竞争奖项。

Conclusion: 提出的方法在AMD疾病进展监测方面取得了良好效果，融合CNN和自编码器技术为OCT图像分析和疾病进展预测提供了有效解决方案。

Abstract: Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting
visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments
have been effective in slowing the progression of neovascular AMD, with better
outcomes achieved through timely diagnosis and consistent monitoring. Tracking
the progression of neovascular activity in OCT scans of patients with exudative
AMD allows for the development of more personalized and effective treatment
plans. This was the focus of the Monitoring Age-related Macular Degeneration
Progression in Optical Coherence Tomography (MARIO) challenge, in which we
participated. In Task 1, which involved classifying the evolution between two
pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN
network with model ensembling to further enhance the model's performance. For
Task 2, which focused on predicting progression over the next three months
based on current exam data, we proposed the Patch Progression Masked
Autoencoder that generates an OCT for the next exam and then classifies the
evolution between the current OCT and the one generated using our solution from
Task 1. The results we achieved allowed us to place in the Top 10 for both
tasks. Some team members are part of the same organization as the challenge
organizers; therefore, we are not eligible to compete for the prize.

</details>


### [57] [Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe-Hessian Operators](https://arxiv.org/abs/2508.19698)
*V. S. Usatyuk,D. A. Sapozhnikov,S. I. Egorov*

Main category: cs.CV

TL;DR: 基于物理受启的无监督检测方法，通过将图像特征转换为LDPC图并分析Bethe-Hessian谱空隔来区分真实和合成图像，无需标签数据即可达到94%准确率。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型产生的图像越来越难以与真实照片区分，影响媒体证据学和生物识别安全。目前的监督检测器对未见生成器效果差，而无监督方法依赖低级统计线索容易被攻击。

Method: 将图像检测模型化为社区发现问题。先用预训练CNN提取特征并降维到32维，构建多边类型QC-LDPC图。将成对相似性转换为格炼布尼希守温度的边耦合，形成随机铁磁伊戏模型，分析其Bethe-Hessian谱的特征间隔来区分真伪图像。

Result: 在猫狗区分和男女区分任务中，使用FFHQ和CelebA真实照片以及GAN和液化模型生成的合成图像进行验证。无需标签合成数据或重新训练特征提取器，检测器达到了94%以上的准确率。谱分析显示真实图像集有多个明显分离的谱间隔，而生成图像则呈现压缩谱。

Conclusion: 该方法提供了一种新题的无监督合成图像检测器，具有模型无关性、鲁棒性强的特点。主要贡献包括：LDPC图构建方法、Nishimori温度RBIM与Bethe-Hessian谱的分析联系、废除标签数据需求的实用检测器。未来工作将扩展到视频流和多类异常检测。

Abstract: The rapid advance of deep generative models such as GANs and diffusion
networks now produces images that are virtually indistinguishable from genuine
photographs, undermining media forensics and biometric security. Supervised
detectors quickly lose effectiveness on unseen generators or after adversarial
post-processing, while existing unsupervised methods that rely on low-level
statistical cues remain fragile. We introduce a physics-inspired,
model-agnostic detector that treats synthetic-image identification as a
community-detection problem on a sparse weighted graph. Image features are
first extracted with pretrained CNNs and reduced to 32 dimensions, each feature
vector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities
are transformed into edge couplings calibrated at the Nishimori temperature,
producing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum
exhibits a characteristic gap when genuine community structure (real images) is
present. Synthetic images violate the Nishimori symmetry and therefore lack
such gaps. We validate the approach on binary tasks cat versus dog and male
versus female using real photos from Flickr-Faces-HQ and CelebA and synthetic
counterparts generated by GANs and diffusion models. Without any labeled
synthetic data or retraining of the feature extractor, the detector achieves
over 94% accuracy. Spectral analysis shows multiple well separated gaps for
real image sets and a collapsed spectrum for generated ones. Our contributions
are threefold: a novel LDPC graph construction that embeds deep image features,
an analytical link between Nishimori temperature RBIM and the Bethe-Hessian
spectrum providing a Bayes optimal detection criterion; and a practical,
unsupervised synthetic image detector robust to new generative architectures.
Future work will extend the framework to video streams and multi-class anomaly
detection.

</details>


### [58] [CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning](https://arxiv.org/abs/2508.20096)
*Zeyi Sun,Yuhang Cao,Jianze Liang,Qiushi Sun,Ziyu Liu,Zhixiong Zhang,Yuhang Zang,Xiaoyi Dong,Kai Chen,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: CODA是一个可训练的组合框架，通过整合通用规划器和专业执行器，解决了科学计算GUI自动化中规划与执行的权衡问题，在ScienceBoard基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI自动化方法存在规划与执行的权衡：通用代理擅长规划但执行差，专业代理执行好但规划弱。静态组合框架无法从经验中学习，这在数据稀缺的科学领域尤为关键。

Method: 提出CODA框架，包含通用规划器Cerebrum和专业执行器Cerebellum。采用两阶段训练：1)专业化阶段-为每个科学应用单独训练专家规划器；2)泛化阶段-聚合成功轨迹进行监督微调。

Result: 在ScienceBoard基准的四个挑战性应用中，CODA显著优于基线方法，在开源模型中建立了新的最先进水平。

Conclusion: CODA通过可训练的组合框架成功解决了科学GUI自动化中的规划-执行权衡问题，实现了强大的执行能力和跨领域泛化能力。

Abstract: Autonomous agents for Graphical User Interfaces (GUIs) face significant
challenges in specialized domains such as scientific computing, where both
long-horizon planning and precise execution are required. Existing approaches
suffer from a trade-off: generalist agents excel at planning but perform poorly
in execution, while specialized agents demonstrate the opposite weakness.
Recent compositional frameworks attempt to bridge this gap by combining a
planner and an actor, but they are typically static and non-trainable, which
prevents adaptation from experience. This is a critical limitation given the
scarcity of high-quality data in scientific domains. To address these
limitations, we introduce CODA, a novel and trainable compositional framework
that integrates a generalist planner (Cerebrum) with a specialist executor
(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,
Specialization, we apply a decoupled GRPO approach to train an expert planner
for each scientific application individually, bootstrapping from a small set of
task trajectories. In the second stage, Generalization, we aggregate all
successful trajectories from the specialized experts to build a consolidated
dataset, which is then used for supervised fine-tuning of the final planner.
This equips CODA with both robust execution and cross-domain generalization.
Evaluated on four challenging applications from the ScienceBoard benchmark,
CODA significantly outperforms baselines and establishes a new state of the art
among open-source models.

</details>


### [59] [LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation](https://arxiv.org/abs/2508.19699)
*Yupeng Zhang,Dezhi Zheng,Ping Lu,Han Zhang,Lei Wang,Liping xiang,Cheng Luo,Kaijun Deng,Xiaowen Fu,Linlin Shen,Jinbao Wang*

Main category: cs.CV

TL;DR: LabelGS是一种为3D高斯泼溅(3DGS)添加对象标签分割能力的新方法，通过跨视角语义掩码、遮挡分析和标签提升技术，实现了高效的3D场景分割，训练速度比现有方法快22倍。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅(3DGS)虽然能实现高保真3D重建和高效渲染，但缺乏3D分割能力，限制了其在需要场景理解任务中的应用。需要解决特定对象组件的识别和分离问题。

Method: 提出LabelGS方法：1）为3D高斯添加跨视角一致的语义掩码；2）使用遮挡分析模型避免优化过程中的过拟合；3）主高斯标签模型将2D语义先验提升到3D高斯；4）高斯投影滤波器避免标签冲突；5）采用随机区域采样策略优化3DGS过程。

Result: 在3D场景分割任务中优于包括Feature-3DGS在内的最先进方法，在1440X1080分辨率下实现了22倍的训练加速。

Conclusion: LabelGS成功解决了3DGS缺乏分割能力的问题，通过创新的标签增强和优化策略，实现了高效准确的3D场景分割，显著提升了训练效率和分割性能。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation
for 3D scenes, offering both high-fidelity reconstruction and efficient
rendering. However, 3DGS lacks 3D segmentation ability, which limits its
applicability in tasks that require scene understanding. The identification and
isolating of specific object components is crucial. To address this limitation,
we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments
the Gaussian representation with object label.LabelGS introduces cross-view
consistent semantic masks for 3D Gaussians and employs a novel Occlusion
Analysis Model to avoid overfitting occlusion during optimization, Main
Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian
Projection Filter to avoid Gaussian label conflict. Our approach achieves
effective decoupling of Gaussian representations and refines the 3DGS
optimization process through a random region sampling strategy, significantly
improving efficiency. Extensive experiments demonstrate that LabelGS
outperforms previous state-of-the-art methods, including Feature-3DGS, in the
3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup
in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code
will be at https://github.com/garrisonz/LabelGS.

</details>


### [60] [FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation](https://arxiv.org/abs/2508.19705)
*Qiang Hu,Ying Zhou,Gepeng Ji,Nick Barnes,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 本文提出FreeVPS方法，通过结合IPS模型的空间上下文和SAM2的时间建模能力，使用两个无训练模块解决长期追踪中的错误累积问题，在内外域场景下都取得了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频恣肉分割方法在平衡时空建模和域逆向性方面遇到困难，限制了在真实临床场景中的应用。特别是SAM2在长期恣肉追踪中存在错误累积问题，影响分割稳定性。

Method: 重构VPS任务为追踪-检测范式，结合IPS模型的空间上下文和SAM2的时间建模能力。设计两个无训练模块：内联筛选模块消除检测阶段的空间不准确性，外联精炼模块通过自适应更新记忆库防止错误传播。

Result: 方法在内域和外域场景下都达到了最先进性能，并在长时间未剪进的细肠镜视频中展现出稳健的追踪能力。

Conclusion: FreeVPS通过协同使用两个无训练模块，有效解决了SAM2在长期追踪中的错误累积问题，显示了在可靠临床分析中的潜力。

Abstract: Existing video polyp segmentation (VPS) paradigms usually struggle to balance
between spatiotemporal modeling and domain generalization, limiting their
applicability in real clinical scenarios. To embrace this challenge, we recast
the VPS task as a track-by-detect paradigm that leverages the spatial contexts
captured by the image polyp segmentation (IPS) model while integrating the
temporal modeling capabilities of segment anything model 2 (SAM2). However,
during long-term polyp tracking in colonoscopy videos, SAM2 suffers from error
accumulation, resulting in a snowball effect that compromises segmentation
stability. We mitigate this issue by repurposing SAM2 as a video polyp
segmenter with two training-free modules. In particular, the intra-association
filtering module eliminates spatial inaccuracies originating from the detecting
stage, reducing false positives. The inter-association refinement module
adaptively updates the memory bank to prevent error propagation over time,
enhancing temporal coherence. Both modules work synergistically to stabilize
SAM2, achieving cutting-edge performance in both in-domain and out-of-domain
scenarios. Furthermore, we demonstrate the robust tracking capabilities of
FreeVPS in long-untrimmed colonoscopy videos, underscoring its potential
reliable clinical analysis.

</details>


### [61] [Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning](https://arxiv.org/abs/2508.19730)
*Stelios Mylonas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 提出基于人脸基础模型的鲁棒视频深度伪造检测框架，通过自监督学习和多数据集集成训练，结合三元组损失和属性监督，显著提升检测模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术日益逼真，现有检测模型在真实场景中泛化能力不足，难以应对训练分布外的媒体内容，需要开发更鲁棒的检测方法。

Method: 利用FSFM自监督人脸基础模型，在多个人工合成数据集上进行微调，采用三元组损失变体增强判别能力，探索基于操纵类型和数据源的属性监督方案。

Result: 在多样化评估基准上的广泛实验表明，该方法在具有挑战性的真实世界场景中表现出色，检测效果显著提升。

Conclusion: 基于人脸基础模型的深度伪造检测框架通过集成学习和监督策略的有效结合，成功解决了模型泛化问题，为真实场景中的媒体真实性验证提供了可靠解决方案。

Abstract: The increasing realism and accessibility of deepfakes have raised critical
concerns about media authenticity and information integrity. Despite recent
advances, deepfake detection models often struggle to generalize beyond their
training distributions, particularly when applied to media content found in the
wild. In this work, we present a robust video deepfake detection framework with
strong generalization that takes advantage of the rich facial representations
learned by face foundation models. Our method is built on top of FSFM, a
self-supervised model trained on real face data, and is further fine-tuned
using an ensemble of deepfake datasets spanning both face-swapping and
face-reenactment manipulations. To enhance discriminative power, we incorporate
triplet loss variants during training, guiding the model to produce more
separable embeddings between real and fake samples. Additionally, we explore
attribution-based supervision schemes, where deepfakes are categorized by
manipulation type or source dataset, to assess their impact on generalization.
Extensive experiments across diverse evaluation benchmarks demonstrate the
effectiveness of our approach, especially in challenging real-world scenarios.

</details>


### [62] [POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection](https://arxiv.org/abs/2508.19742)
*Chenguang Liu,Chisheng Wang,Yuhua Cai,Chuanhua Zhu,Qingquan Li*

Main category: cs.CV

TL;DR: POEv2是一个改进的像素方向估计方法，可用于通用线段检测和线框线段检测，与高效边缘检测器结合后在三个公开数据集上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有线段检测器分为通用线段检测器和线框线段检测器两类，由于设计目标不同，两类检测器在对方任务上表现不佳，需要一种能同时处理两种任务的鲁棒框架

Method: 提出POEv2方法，从边缘强度图检测线段，可与任何边缘检测器结合使用，是Pixel Orientation Estimation方法的改进版本

Result: 通过将POEv2与高效边缘检测器结合，在三个公开数据集上实现了最先进的性能

Conclusion: POEv2提供了一个统一的框架，能够同时胜任通用线段检测和线框线段检测任务，具有很好的实用价值

Abstract: Line segment detection in images has been studied for several decades.
Existing line segment detectors can be roughly divided into two categories:
generic line segment detectors and wireframe line segment detectors. Generic
line segment detectors aim to detect all meaningful line segments in images and
traditional approaches usually fall into this category. Recent deep learning
based approaches are mostly wireframe line segment detectors. They detect only
line segments that are geometrically meaningful and have large spatial support.
Due to the difference in the aim of design, the performance of generic line
segment detectors for the task of wireframe line segment detection won't be
satisfactory, and vice versa. In this work, we propose a robust framework that
can be used for both generic line segment detection and wireframe line segment
detection. The proposed method is an improved version of the Pixel Orientation
Estimation (POE) method. It is thus named as POEv2. POEv2 detects line segments
from edge strength maps, and can be combined with any edge detector. We show in
our experiments that by combining the proposed POEv2 with an efficient edge
detector, it achieves state-of-the-art performance on three publicly available
datasets.

</details>


### [63] [SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient Object Detection](https://arxiv.org/abs/2508.19746)
*Qiyao Xu,Qiming Wu,Xiaowei Li*

Main category: cs.CV

TL;DR: SPLF-SAM是一个自提示光场分割模型，通过统一多尺度特征嵌入块和多尺度自适应滤波适配器，解决了现有方法忽略提示信息提取和频域信息分析的问题，在光场显著目标检测任务中优于10个SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有SAM模型在光场显著目标检测中倾向于忽略提示信息提取，传统模型忽视频域信息分析，导致小目标被噪声淹没。

Method: 提出SPLF-SAM模型，包含统一多尺度特征嵌入块(UMFEB)识别不同大小目标，以及多尺度自适应滤波适配器(MAFA)通过学习频域特征防止小目标被噪声淹没。

Result: 大量实验证明该方法优于10个最先进的光场显著目标检测方法。

Conclusion: SPLF-SAM通过结合多尺度特征嵌入和频域自适应滤波，有效解决了光场显著目标检测中的提示信息缺失和小目标噪声淹没问题，取得了优异的性能。

Abstract: Segment Anything Model (SAM) has demonstrated remarkable capabilities in
solving light field salient object detection (LF SOD). However, most existing
models tend to neglect the extraction of prompt information under this task.
Meanwhile, traditional models ignore the analysis of frequency-domain
information, which leads to small objects being overwhelmed by noise. In this
paper, we put forward a novel model called self-prompting light field segment
anything model (SPLF-SAM), equipped with unified multi-scale feature embedding
block (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is
capable of identifying multiple objects of varying sizes, while MAFA, by
learning frequency features, effectively prevents small objects from being
overwhelmed by noise. Extensive experiments have demonstrated the superiority
of our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be
available at https://github.com/XucherCH/splfsam.

</details>


### [64] [FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers](https://arxiv.org/abs/2508.19754)
*Yue Wu,Yufan Wu,Wen Li,Yuxi Lu,Kairui Feng,Xuanhong Chen*

Main category: cs.CV

TL;DR: FastAvatar是一个快速3D头像重建框架，使用单一统一模型在几秒内从单张图像、多视角观察或单目视频中重建高质量的3D高斯溅射模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D头像重建方法时间复杂度过高、对数据质量敏感、数据利用率低的问题。

Method: 采用大型高斯重建变换器，包含VGGT风格变换器架构、多粒度引导编码和增量高斯聚合技术。

Result: 实验表明FastAvatar在质量和速度上都优于现有方法，支持增量重建提高质量。

Conclusion: FastAvatar提供了一个质量-速度可调的高可用头像建模范式，能够高效利用各种日常记录数据。

Abstract: Despite significant progress in 3D avatar reconstruction, it still faces
challenges such as high time complexity, sensitivity to data quality, and low
data utilization. We propose FastAvatar, a feedforward 3D avatar framework
capable of flexibly leveraging diverse daily recordings (e.g., a single image,
multi-view observations, or monocular video) to reconstruct a high-quality 3D
Gaussian Splatting (3DGS) model within seconds, using only a single unified
model. FastAvatar's core is a Large Gaussian Reconstruction Transformer
featuring three key designs: First, a variant VGGT-style transformer
architecture aggregating multi-frame cues while injecting initial 3D prompt to
predict an aggregatable canonical 3DGS representation; Second, multi-granular
guidance encoding (camera pose, FLAME expression, head pose) mitigating
animation-induced misalignment for variable-length inputs; Third, incremental
Gaussian aggregation via landmark tracking and sliced fusion losses.
Integrating these features, FastAvatar enables incremental reconstruction,
i.e., improving quality with more observations, unlike prior work wasting input
data. This yields a quality-speed-tunable paradigm for highly usable avatar
modeling. Extensive experiments show that FastAvatar has higher quality and
highly competitive speed compared to existing methods.

</details>


### [65] [BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions](https://arxiv.org/abs/2508.19762)
*Ahmed Emam,Mohamed Elbassiouny,Julius Miller,Patrick Donworth,Sabine Seidel,Ribana Roscher*

Main category: cs.CV

TL;DR: BuzzSet是一个用于传粉昆虫监测的大规模数据集，包含7856张高分辨率图像和8000多个标注实例，使用YOLOv12和RF-DETR模型实现了高精度的蜜蜂检测。


<details>
  <summary>Details</summary>
Motivation: 传粉昆虫对全球粮食生产和生态系统稳定至关重要，但其种群数量正在下降。需要开发可扩展的自动化监测方法来支持传粉昆虫保护。

Method: 创建BuzzSet数据集，包含手动验证标注的图像，使用YOLOv12模型生成初始标注并通过人工验证精炼。采用RF-DETR基于transformer的目标检测器建立基线模型。

Result: 模型在蜜蜂和大黄蜂类别上分别达到0.94和0.92的F1分数，混淆矩阵显示类别间误分类很少。最佳mAP@0.50为0.559，检测质量良好。

Conclusion: BuzzSet为小目标检测、标签噪声下的类别分离和生态计算机视觉提供了有价值的基准数据集，支持可扩展的传粉昆虫监测。

Abstract: Pollinator insects such as honeybees and bumblebees are vital to global food
production and ecosystem stability, yet their populations are declining due to
increasing anthropogenic and environmental stressors. To support scalable,
automated pollinator monitoring, we introduce BuzzSet, a new large-scale
dataset of high-resolution pollinator images collected in real agricultural
field conditions. BuzzSet contains 7856 manually verified and labeled images,
with over 8000 annotated instances across three classes: honeybees, bumblebees,
and unidentified insects. Initial annotations were generated using a YOLOv12
model trained on external data and refined via human verification using
open-source labeling tools. All images were preprocessed into 256~$\times$~256
tiles to improve the detection of small insects. We provide strong baselines
using the RF-DETR transformer-based object detector. The model achieves high
F1-scores of 0.94 and 0.92 for honeybee and bumblebee classes, respectively,
with confusion matrix results showing minimal misclassification between these
categories. The unidentified class remains more challenging due to label
ambiguity and lower sample frequency, yet still contributes useful insights for
robustness evaluation. Overall detection quality is strong, with a best
mAP@0.50 of 0.559. BuzzSet offers a valuable benchmark for small object
detection, class separation under label noise, and ecological computer vision.

</details>


### [66] [AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning](https://arxiv.org/abs/2508.19769)
*Shu Shen,C. L. Philip Chen,Tong Zhang*

Main category: cs.CV

TL;DR: 本文提出了自适应网络内调制（AIM）方法来解决多模态学习中的优化偏差问题，通过解耦主导模态的欠优化参数并调整不同网络深度的调制强度，实现平衡的多模态学习而不抑制任何模态。


<details>
  <summary>Details</summary>
Motivation: 现有不平衡多模态学习方法通常通过抑制主导模态来促进较弱模态，这会降低整体多模态性能。研究发现这是由于网络内部的优化偏差问题被忽视导致的。

Method: 提出AIM方法：1）将主导模态的欠优化参数解耦到辅助块中；2）鼓励在联合训练中依赖这些性能下降的块；3）评估不同网络深度的模态不平衡程度并自适应调整调制强度。

Result: AIM在多个基准测试中优于最先进的不平衡模态学习方法，并在不同骨干网络、融合策略和优化器上表现出强大的泛化能力。

Conclusion: AIM首次实现了在不抑制主导或弱模态的情况下实现平衡的多模态学习，有效解决了网络内部优化偏差问题，为多模态学习提供了新的解决方案。

Abstract: Multimodal learning has significantly enhanced machine learning performance
but still faces numerous challenges and limitations. Imbalanced multimodal
learning is one of the problems extensively studied in recent works and is
typically mitigated by modulating the learning of each modality. However, we
find that these methods typically hinder the dominant modality's learning to
promote weaker modalities, which affects overall multimodal performance. We
analyze the cause of this issue and highlight a commonly overlooked problem:
optimization bias within networks. To address this, we propose Adaptive
Intra-Network Modulation (AIM) to improve balanced modality learning. AIM
accounts for differences in optimization state across parameters and depths
within the network during modulation, achieving balanced multimodal learning
without hindering either dominant or weak modalities for the first time.
Specifically, AIM decouples the dominant modality's under-optimized parameters
into Auxiliary Blocks and encourages reliance on these performance-degraded
blocks for joint training with weaker modalities. This approach effectively
prevents suppression of weaker modalities while enabling targeted optimization
of under-optimized parameters to improve the dominant modality. Additionally,
AIM assesses modality imbalance level across network depths and adaptively
adjusts modulation strength at each depth. Experimental results demonstrate
that AIM outperforms state-of-the-art imbalanced modality learning methods
across multiple benchmarks and exhibits strong generalizability across
different backbones, fusion strategies, and optimizers.

</details>


### [67] [The Return of Structural Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.19773)
*Jakob Seitz,Tobias Lengfeld,Radu Timofte*

Main category: cs.CV

TL;DR: 本文提出了一种结构化的手写数学表达式识别方法，通过自动标注系统和模块化结构识别系统，实现了符号到轨迹的显式对齐，在CROHME-2023基准测试中取得竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于编码器-解码器架构和大语言模型的方法虽然擅长生成LaTeX，但缺乏符号到轨迹的显式对齐，这限制了错误分析、可解释性以及需要选择性内容更新的空间感知交互应用。

Method: 1) 使用神经网络自动将LaTeX方程映射到原始轨迹，自动生成符号分割、分类和空间关系标注；2) 模块化结构识别系统独立优化分割、分类和关系预测，结合基于图的轨迹排序、混合卷积-循环网络和基于transformer的校正。

Result: 在CROHME-2023基准测试中取得了竞争性性能，生成了完整的图结构，直接链接手写轨迹和预测符号。

Conclusion: 该方法通过结构化的识别方式实现了符号到轨迹的显式对齐，为错误分析和可解释输出提供了透明性，支持空间感知的交互应用。

Abstract: Handwritten Mathematical Expression Recognition is foundational for
educational technologies, enabling applications like digital note-taking and
automated grading. While modern encoder-decoder architectures with large
language models excel at LaTeX generation, they lack explicit symbol-to-trace
alignment, a critical limitation for error analysis, interpretability, and
spatially aware interactive applications requiring selective content updates.
This paper introduces a structural recognition approach with two innovations: 1
an automatic annotation system that uses a neural network to map LaTeX
equations to raw traces, automatically generating annotations for symbol
segmentation, classification, and spatial relations, and 2 a modular structural
recognition system that independently optimizes segmentation, classification,
and relation prediction. By leveraging a dataset enriched with structural
annotations from our auto-labeling system, the proposed recognition system
combines graph-based trace sorting, a hybrid convolutional-recurrent network,
and transformer-based correction to achieve competitive performance on the
CROHME-2023 benchmark. Crucially, our structural recognition system generates a
complete graph structure that directly links handwritten traces to predicted
symbols, enabling transparent error analysis and interpretable outputs.

</details>


### [68] [MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.19786)
*Han Jiao,Jiakai Sun,Yexing Xu,Lei Zhao,Wei Xing,Huaizhong Lin*

Main category: cs.CV

TL;DR: MAPo框架通过动态评分分割策略，将3D高斯分为高动态和低动态区域，对高动态区域进行时间分割并使用专用变形网络，同时引入跨帧一致性损失来解决视觉不连续性问题，在保持计算效率的同时显著提升动态场景重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于变形的3D高斯泼溅方法在处理动态场景时，由于使用单一统一模型来表示多样化的运动模式，往往会产生模糊渲染结果并丢失高度动态区域的精细运动细节。

Method: 提出动态评分分割策略区分高动态和低动态3D高斯；对高动态高斯进行递归时间分割并为每个时间片段复制变形网络；低动态高斯作为静态处理以降低计算成本；引入跨帧一致性损失确保视觉连续性。

Result: 大量实验表明，MAPo在保持可比计算成本的同时，相比基线方法实现了更优越的渲染质量，特别是在具有复杂或快速运动的区域。

Conclusion: MAPo框架通过创新的动态分割策略和一致性约束，有效解决了动态3D高斯泼溅中的运动细节丢失和视觉不连续问题，为高保真动态场景重建提供了有效解决方案。

Abstract: 3D Gaussian Splatting, known for enabling high-quality static scene
reconstruction with fast rendering, is increasingly being applied to dynamic
scene reconstruction. A common strategy involves learning a deformation field
to model the temporal changes of a canonical set of 3D Gaussians. However,
these deformation-based methods often produce blurred renderings and lose fine
motion details in highly dynamic regions due to the inherent limitations of a
single, unified model in representing diverse motion patterns. To address these
challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian
Splatting (MAPo), a novel framework for high-fidelity dynamic scene
reconstruction. Its core is a dynamic score-based partitioning strategy that
distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D
Gaussians, we recursively partition them temporally and duplicate their
deformation networks for each new temporal segment, enabling specialized
modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs
are treated as static to reduce computational costs. However, this temporal
partitioning strategy for high-dynamic 3DGs can introduce visual
discontinuities across frames at the partition boundaries. To address this, we
introduce a cross-frame consistency loss, which not only ensures visual
continuity but also further enhances rendering quality. Extensive experiments
demonstrate that MAPo achieves superior rendering quality compared to baselines
while maintaining comparable computational costs, particularly in regions with
complex or rapid motions.

</details>


### [69] [StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation](https://arxiv.org/abs/2508.19789)
*Xiuchao Wu,Pengfei Zhu,Jiangjing Lyu,Xinguo Liu,Jie Guo,Yanwen Guo,Weiwei Xu,Chengfei Lyu*

Main category: cs.CV

TL;DR: StableIntrinsic是一个用于多视角材质估计的一步扩散模型，通过像素空间损失和细节注入网络解决了传统多步扩散方法的时间消耗和结果方差问题，在材质参数预测质量上显著超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的材质估计方法采用多步去噪策略，存在时间消耗大、随机推理与确定性材质估计任务冲突导致结果方差高等问题。

Method: 提出StableIntrinsic一步扩散模型，在像素空间应用基于材质特性的损失函数，并引入细节注入网络(DIN)来消除VAE编码造成的细节损失，增强材质预测结果的清晰度。

Result: 实验结果显示，该方法在albedo的PSNR上提升9.9%，金属性和粗糙度的MSE分别降低44.4%和60.0%，显著超越当前最先进技术。

Conclusion: StableIntrinsic通过一步扩散策略和专门的网络设计，成功解决了多步扩散模型在材质估计中的效率和质量问题，为高质量材质参数预测提供了有效解决方案。

Abstract: Recovering material information from images has been extensively studied in
computer graphics and vision. Recent works in material estimation leverage
diffusion model showing promising results. However, these diffusion-based
methods adopt a multi-step denoising strategy, which is time-consuming for each
estimation. Such stochastic inference also conflicts with the deterministic
material estimation task, leading to a high variance estimated results. In this
paper, we introduce StableIntrinsic, a one-step diffusion model for multi-view
material estimation that can produce high-quality material parameters with low
variance. To address the overly-smoothing problem in one-step diffusion,
StableIntrinsic applies losses in pixel space, with each loss designed based on
the properties of the material. Additionally, StableIntrinsic introduces a
Detail Injection Network (DIN) to eliminate the detail loss caused by VAE
encoding, while further enhancing the sharpness of material prediction results.
The experimental results indicate that our method surpasses the current
state-of-the-art techniques by achieving a $9.9\%$ improvement in the Peak
Signal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error
(MSE) for metallic and roughness by $44.4\%$ and $60.0\%$, respectively.

</details>


### [70] [Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models](https://arxiv.org/abs/2508.19791)
*Shay Shomer Chai,Wenxuan Peng,Bharath Hariharan,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 本文针对文本到图像生成中多对象颜色属性语义对齐问题进行了研究，提出了一种专门的图像编辑技术来改善多颜色提示的语义对齐效果。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成方法在处理复杂多对象提示时难以准确捕捉精确语义，特别是在多颜色属性方面存在显著挑战。现有方法主要使用粗粒度指标或人工评估，难以进行大规模评估。

Method: 通过对颜色属性进行案例研究，分析了预训练模型在多颜色提示生成中的困难，并开发了一种专门的图像编辑技术来解决多对象语义对齐问题。

Result: 研究表明预训练模型在多颜色属性生成方面表现远不如单颜色提示，现有推理时技术和编辑方法都无法可靠解决这些语义不对齐问题。提出的方法在各种指标上显著提升了性能。

Conclusion: 该研究为文本到图像生成中的多对象语义对齐问题提供了新的解决方案，特别是在颜色属性方面取得了显著改进，为后续研究提供了有价值的见解。

Abstract: Text-to-image generation has recently seen remarkable success, granting users
with the ability to create high-quality images through the use of text.
However, contemporary methods face challenges in capturing the precise
semantics conveyed by complex multi-object prompts. Consequently, many works
have sought to mitigate such semantic misalignments, typically via
inference-time schemes that modify the attention layers of the denoising
networks. However, prior work has mostly utilized coarse metrics, such as the
cosine similarity between text and image CLIP embeddings, or human evaluations,
which are challenging to conduct on a larger-scale. In this work, we perform a
case study on colors -- a fundamental attribute commonly associated with
objects in text prompts, which offer a rich test bed for rigorous evaluation.
Our analysis reveals that pretrained models struggle to generate images that
faithfully reflect multiple color attributes-far more so than with single-color
prompts-and that neither inference-time techniques nor existing editing methods
reliably resolve these semantic misalignments. Accordingly, we introduce a
dedicated image editing technique, mitigating the issue of multi-object
semantic alignment for prompts containing multiple colors. We demonstrate that
our approach significantly boosts performance over a wide range of metrics,
considering images generated by various text-to-image diffusion-based
techniques.

</details>


### [71] [FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization](https://arxiv.org/abs/2508.19798)
*Muhammad Ali,Omar Ali AlSuwaidi*

Main category: cs.CV

TL;DR: 提出了一种基于编码器-解码器结构的增强神经网络架构，通过综合注意力块、Mamba架构注意力机制和数据融合块，显著提高了非生物降解废物分类的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 废物管理中非生物降解材料的自动分类面临复杂多变的废物流挑战，需要更准确高效的分类系统来解决这一问题。

Method: 在编码器-解码器结构中集成综合注意力块（结合卷积和上采样操作）、Mamba架构注意力机制，以及数据融合块（使用PCA降维处理多通道图像数据）。

Result: 在RGB、高光谱、多光谱以及RGB与高光谱组合数据上的评估显示，该方法显著优于现有方法。

Conclusion: 所提出的增强神经网络架构通过创新的注意力机制和数据融合技术，有效提升了废物分类系统的性能，为自动化废物管理提供了更优解决方案。

Abstract: In the realm of waste management, automating the sorting process for
non-biodegradable materials presents considerable challenges due to the
complexity and variability of waste streams. To address these challenges, we
introduce an enhanced neural architecture that builds upon an existing
Encoder-Decoder structure to improve the accuracy and efficiency of waste
sorting systems. Our model integrates several key innovations: a Comprehensive
Attention Block within the decoder, which refines feature representations by
combining convolutional and upsampling operations. In parallel, we utilize
attention through the Mamba architecture, providing an additional performance
boost. We also introduce a Data Fusion Block that fuses images with more than
three channels. To achieve this, we apply PCA transformation to reduce the
dimensionality while retaining the maximum variance and essential information
across three dimensions, which are then used for further processing. We
evaluated the model on RGB, hyperspectral, multispectral, and a combination of
RGB and hyperspectral data. The results demonstrate that our approach
outperforms existing methods by a significant margin.

</details>


### [72] [AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment](https://arxiv.org/abs/2508.19808)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: AutoQ-VIS是一个无监督视频实例分割框架，通过质量引导的自训练方法，在不需要人工标注的情况下实现了最先进的性能表现。


<details>
  <summary>Details</summary>
Motivation: 视频实例分割需要像素级掩码和时间一致性标注，标注成本高昂。现有无监督方法依赖合成数据但存在合成到真实域的差距问题。

Method: 建立伪标签生成和自动质量评估的闭环系统，通过质量引导的自训练方法从合成视频逐步适应到真实视频。

Result: 在YouTubeVIS-2019验证集上达到52.6 AP50，比之前最好的VideoCutLER方法提升了4.4%，且无需人工标注。

Conclusion: 质量感知的自训练方法对于无监督视频实例分割是可行的有效解决方案。

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due
to its dual requirements of pixel-level masks and temporal consistency labels.
While recent unsupervised methods like VideoCutLER eliminate optical flow
dependencies through synthetic data, they remain constrained by the
synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised
framework that bridges this gap through quality-guided self-training. Our
approach establishes a closed-loop system between pseudo-label generation and
automatic quality assessment, enabling progressive adaptation from synthetic to
real videos. Experiments demonstrate state-of-the-art performance with 52.6
$\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous
state-of-the-art VideoCutLER by 4.4$\%$, while requiring no human annotations.
This demonstrates the viability of quality-aware self-training for unsupervised
VIS. The source code of our method is available at
https://github.com/wcbup/AutoQ-VIS.

</details>


### [73] [Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models](https://arxiv.org/abs/2508.19850)
*Xiaoqi Wang,Yun Zhang,Weisi Lin*

Main category: cs.CV

TL;DR: 提出了一种以机器为中心的图像质量评估(MIQA)框架，通过区域感知模型RA-MIQA进行空间降级分析，在多个维度上显示优勇性能，解决了传统人视觉系统评估方法在机器视觉系统中的不足。


<details>
  <summary>Details</summary>
Motivation: 机器视觉系统(MVS)在恶劣视觉条件下容易出现性能降级，传统基于人视视觉系统(HVS)的图像质量评估方法对机器视觉性能预测效果不佳。

Method: 构建了包含250万样本的机器中心图像质量数据库(MIQD-2.5M)，涵盖75个视觉模型、250种降级类型和三个代表性视觉任务；提出区域感知MIQA(RA-MIQA)模型进行细粒度空间降级分析。

Result: RA-MIQA在多个维度上显示优勇性能，在图像分类任务上一致性和准确性分别获得13.56%和13.37%的SRCC提升，同时发现了任务特定的降级敏感性。

Conclusion: 这项研究可以提高机器视觉系统的可靠性，为机器中心的图像处理和优化奠定基础，显示了传统HVS基于评估方法在MVS质量预测中的不足。

Abstract: Machine vision systems (MVS) are intrinsically vulnerable to performance
degradation under adverse visual conditions. To address this, we propose a
machine-centric image quality assessment (MIQA) framework that quantifies the
impact of image degradations on MVS performance. We establish an MIQA paradigm
encompassing the end-to-end assessment workflow. To support this, we construct
a machine-centric image quality database (MIQD-2.5M), comprising 2.5 million
samples that capture distinctive degradation responses in both consistency and
accuracy metrics, spanning 75 vision models, 250 degradation types, and three
representative vision tasks. We further propose a region-aware MIQA (RA-MIQA)
model to evaluate MVS visual quality through fine-grained spatial degradation
analysis. Extensive experiments benchmark the proposed RA-MIQA against seven
human visual system (HVS)-based IQA metrics and five retrained classical
backbones. Results demonstrate RA-MIQA's superior performance in multiple
dimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on
accuracy for image classification, while also revealing task-specific
degradation sensitivities. Critically, HVS-based metrics prove inadequate for
MVS quality prediction, while even specialized MIQA models struggle with
background degradations, accuracy-oriented estimation, and subtle distortions.
This study can advance MVS reliability and establish foundations for
machine-centric image processing and optimization. The model and code are
available at: https://github.com/XiaoqiWang/MIQA.

</details>


### [74] [Ego-centric Predictive Model Conditioned on Hand Trajectories](https://arxiv.org/abs/2508.19852)
*Binjie Zhang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出统一的两阶段预测框架，联合建模自我中心场景中的动作和视觉未来，通过手部轨迹条件化，实现动作预测和未来视频生成的统一处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限：VLA模型只关注动作预测而缺乏对视觉场景影响的显式建模，视频预测模型生成未来帧时不考虑特定动作条件，导致结果不合理。需要同时预测动作及其视觉后果。

Method: 两阶段框架：第一阶段处理多模态输入并预测未来手部轨迹；第二阶段引入因果交叉注意力融合多模态线索，利用推断的动作信号指导基于图像的潜在扩散模型进行逐帧未来视频生成。

Result: 在Ego4D、BridgeData和RLBench数据集上的广泛实验表明，该方法在动作预测和未来视频合成方面均优于最先进的基线方法。

Conclusion: 这是首个统一处理自我中心人类活动理解和机器人操作任务的模型，能够显式预测即将发生的动作及其视觉后果，为人类-物体交互理解和机器人规划提供了有效解决方案。

Abstract: In egocentric scenarios, anticipating both the next action and its visual
outcome is essential for understanding human-object interactions and for
enabling robotic planning. However, existing paradigms fall short of jointly
modeling these aspects. Vision-Language-Action (VLA) models focus on action
prediction but lack explicit modeling of how actions influence the visual
scene, while video prediction models generate future frames without
conditioning on specific actions, often resulting in implausible or
contextually inconsistent outcomes. To bridge this gap, we propose a unified
two-stage predictive framework that jointly models action and visual future in
egocentric scenarios, conditioned on hand trajectories. In the first stage, we
perform consecutive state modeling to process heterogeneous inputs (visual
observations, language, and action history) and explicitly predict future hand
trajectories. In the second stage, we introduce causal cross-attention to fuse
multi-modal cues, leveraging inferred action signals to guide an image-based
Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our
approach is the first unified model designed to handle both egocentric human
activity understanding and robotic manipulation tasks, providing explicit
predictions of both upcoming actions and their visual consequences. Extensive
experiments on Ego4D, BridgeData, and RLBench demonstrate that our method
outperforms state-of-the-art baselines in both action prediction and future
video synthesis.

</details>


### [75] [Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction](https://arxiv.org/abs/2508.19862)
*Long Chen,Ashiv Patel,Mengyun Qiao,Mohammad Yousuf Salmasi,Salah A. Hammouche,Vasilis Stavrinides,Jasleen Nagi,Soodeh Kalaie,Xiao Yun Xu,Wenjia Bai,Declan P. O'Regan*

Main category: cs.CV

TL;DR: 首个多模态条件网格到网格GAN模型MCMeshGAN，用于主动脉肌瘩增长预测，结合局部KNN卷积和全局图卷积网络，在新的TAAMesh数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 主动脉肌瘩进展的个性化准确预测对及时干预至关重要，但因需模拟复杂3D几何中的细微局部变形和全局解剖变化而面临挑战。

Method: 提出MCMeshGAN，采用双支架构：新颖局部KNN基于卷积网络(KCN)保持细粒度几何细节，全局图卷积网络(GCN)捕捉长程度结构上下文，克服深度GCN的过平滑问题。专门条件支路编码临床属性和时间间隔。

Result: 在新编诒的TAAMesh数据集(208名患者的590个多模态记录)上，MCMeshGAN在几何准确性和临床重要的直径估计方面均较现有最佳方法更优。

Conclusion: 该框架为临床可部署的个性化3D疾病轨迹建模提供了稳健步骤，源代码已开源。

Abstract: Personalized, accurate prediction of aortic aneurysm progression is essential
for timely intervention but remains challenging due to the need to model both
subtle local deformations and global anatomical changes within complex 3D
geometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh
generative adversarial network for 3D aneurysm growth prediction. MCMeshGAN
introduces a dual-branch architecture combining a novel local KNN-based
convolutional network (KCN) to preserve fine-grained geometric details and a
global graph convolutional network (GCN) to capture long-range structural
context, overcoming the over-smoothing limitations of deep GCNs. A dedicated
condition branch encodes clinical attributes (age, sex) and the target time
interval to generate anatomically plausible, temporally controlled predictions,
enabling retrospective and prospective modeling. We curated TAAMesh, a new
longitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal
records (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive
experiments demonstrate that MCMeshGAN consistently outperforms
state-of-the-art baselines in both geometric accuracy and clinically important
diameter estimation. This framework offers a robust step toward clinically
deployable, personalized 3D disease trajectory modeling. The source code for
MCMeshGAN and the baseline methods is publicly available at
https://github.com/ImperialCollegeLondon/MCMeshGAN.

</details>


### [76] [Self-supervised structured object representation learning](https://arxiv.org/abs/2508.19864)
*Oussama Hadjerci,Antoine Letienne,Mohamed Abbas Hedjazi,Adel Hafiane*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于ProtoScale模块的自监督学习方法，通过语义分组、实例分离和层次结构来建立结构化视觉表征，在对象检测任务上超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 虽然现有自监督学习方法在全局图像理解上表现强劲，但在抓取场景中的结构化表征方面存在限制，特别是在密集预测任务中。

Method: 提出基于ProtoScale模块的自监督学习方法，通过语义分组、实例层次分离和层次结构来建立结构化视觉表征。与DINO等依赖随机裁剪的方法不同，该方法保持了完整的场景上下文。

Result: 在COCO和UA-DETRAC数据集的对象检测任务上验证，方法学习到了以对象为中心的表征，在标注数据有限和少量微调训练时也能超过现有最佳方法。

Conclusion: 该研究提出的自监督学习方法能够有效抓取结构化视觉表征，在对象检测任务上表现优异，为视觉表征学习提供了新的解决方案。

Abstract: Self-supervised learning (SSL) has emerged as a powerful technique for
learning visual representations. While recent SSL approaches achieve strong
results in global image understanding, they are limited in capturing the
structured representation in scenes. In this work, we propose a self-supervised
approach that progressively builds structured visual representations by
combining semantic grouping, instance level separation, and hierarchical
structuring. Our approach, based on a novel ProtoScale module, captures visual
elements across multiple spatial scales. Unlike common strategies like DINO
that rely on random cropping and global embeddings, we preserve full scene
context across augmented views to improve performance in dense prediction
tasks. We validate our method on downstream object detection tasks using a
combined subset of multiple datasets (COCO and UA-DETRAC). Experimental results
show that our method learns object centric representations that enhance
supervised object detection and outperform the state-of-the-art methods, even
when trained with limited annotated data and fewer fine-tuning epochs.

</details>


### [77] [TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations](https://arxiv.org/abs/2508.19866)
*François G. Landry,Moulay A. Akhloufi*

Main category: cs.CV

TL;DR: TrajFusionNet是一个基于transformer的新模型，通过结合未来行人轨迹和车辆速度预测来预测行人过街意图，在两个注意力模块的协同下实现了最佳性能和最低推理时间。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆上路，预测行人过街意图成为重要研究领域，需要准确判断行人是否会过马路以确保道路安全。

Method: 提出TrajFusionNet模型，包含序列注意力模块(SAM)和视觉注意力模块(VAM)两个分支，分别从序列化的观测预测数据和视觉化的预测轨迹图像中学习特征。

Result: 在三个常用数据集上达到state-of-the-art性能，同时实现了最低的总推理时间（包括模型运行和数据预处理）。

Conclusion: 该模型通过轻量级多模态融合，在行人过街意图预测任务中取得了最优的性能和效率平衡。

Abstract: With the introduction of vehicles with autonomous capabilities on public
roads, predicting pedestrian crossing intention has emerged as an active area
of research. The task of predicting pedestrian crossing intention involves
determining whether pedestrians in the scene are likely to cross the road or
not. In this work, we propose TrajFusionNet, a novel transformer-based model
that combines future pedestrian trajectory and vehicle speed predictions as
priors for predicting crossing intention. TrajFusionNet comprises two branches:
a Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM
branch learns from a sequential representation of the observed and predicted
pedestrian trajectory and vehicle speed. Complementarily, the VAM branch
enables learning from a visual representation of the predicted pedestrian
trajectory by overlaying predicted pedestrian bounding boxes onto scene images.
By utilizing a small number of lightweight modalities, TrajFusionNet achieves
the lowest total inference time (including model runtime and data
preprocessing) among current state-of-the-art approaches. In terms of
performance, it achieves state-of-the-art results across the three most
commonly used datasets for pedestrian crossing intention prediction.

</details>


### [78] [Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network](https://arxiv.org/abs/2508.19875)
*Hui Zhang,Jianghui Cai,Haifeng Yang,Ali Luo,Yuqing Yang,Xiao Kong,Zhichao Ding,Lichan Zhou,Qin Han*

Main category: cs.CV

TL;DR: 提出基于互信息的天空背景估计模型SMI，通过双网络结构解决传统天空光纤平均光谱缺乏环境建模的问题，在LAMOST光谱数据上验证了有效性


<details>
  <summary>Details</summary>
Motivation: 当前多目标光纤光谱处理中的天空背景扣除主要依赖天空光纤光谱构建超级天空，但这些平均光谱缺乏对目标周围环境的建模

Method: SMI模型包含两个主要网络：第一个网络使用波长校准模块从光谱中提取天空特征，解决特征偏移问题；第二个网络采用增量训练方法最大化不同光谱表示间的互信息来捕获共同成分，同时最小化相邻光谱表示的互信息来获得个体成分

Result: 在LAMOST光谱上的实验结果表明，SMI能够在观测过程中获得更好的目标天空背景，特别是在蓝端表现更佳

Conclusion: SMI方法通过互信息和增量训练有效解决了天空背景估计问题，为多目标光纤光谱处理提供了更准确的环境建模能力

Abstract: Sky background subtraction is a critical step in Multi-objective Fiber
spectra process. However, current subtraction relies mainly on sky fiber
spectra to build Super Sky. These average spectra are lacking in the modeling
of the environment surrounding the objects. To address this issue, a sky
background estimation model: Sky background building based on Mutual
Information (SMI) is proposed. SMI based on mutual information and incremental
training approach. It utilizes spectra from all fibers in the plate to estimate
the sky background. SMI contains two main networks, the first network applies a
wavelength calibration module to extract sky features from spectra, and can
effectively solve the feature shift problem according to the corresponding
emission position. The second network employs an incremental training approach
to maximize mutual information between representations of different spectra to
capturing the common component. Then, it minimizes the mutual information
between adjoining spectra representations to obtain individual components. This
network yields an individual sky background at each location of the object. To
verify the effectiveness of the method in this paper, we conducted experiments
on the spectra of LAMOST. Results show that SMI can obtain a better object sky
background during the observation, especially in the blue end.

</details>


### [79] [PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos](https://arxiv.org/abs/2508.19895)
*Ziyun Qian,Runyu Xiao,Shuyuan Tu,Wei Xue,Dingkang Yang,Mingcheng Li,Dongliang Kou,Minghao Han,Zizhi Chen,Lihua Zhang*

Main category: cs.CV

TL;DR: 本文提出PersonaAnimator框架，直接从无约束视频中学习个性化运动模式，实现个性化运动迁移，解决了现有方法无法学习运动风格、依赖动作捕捉数据、违反物理规律等问题。


<details>
  <summary>Details</summary>
Motivation: 现有运动生成方法存在三个主要局限：(1)姿态引导的运动迁移方法仅复制动作而不学习风格特征；(2)运动风格迁移方法严重依赖难以获取的动作捕捉数据；(3)生成的运动有时违反物理规律。

Method: 提出PersonaAnimator框架，从无约束视频学习个性化运动模式；构建首个视频个性化运动数据集PersonaVid（20个运动内容类别和120个运动风格类别）；提出物理感知运动风格正则化机制确保生成运动的物理合理性。

Result: 大量实验表明，PersonaAnimator在运动迁移任务上优于现有最先进方法，为视频到视频运动个性化任务设立了新基准。

Conclusion: 该研究开创了视频到视频运动个性化新任务，提出的框架能够有效从视频中学习个性化运动模式并生成物理合理的运动，解决了现有方法的局限性。

Abstract: Recent advances in motion generation show remarkable progress. However,
several limitations remain: (1) Existing pose-guided character motion transfer
methods merely replicate motion without learning its style characteristics,
resulting in inexpressive characters. (2) Motion style transfer methods rely
heavily on motion capture data, which is difficult to obtain. (3) Generated
motions sometimes violate physical laws. To address these challenges, this
paper pioneers a new task: Video-to-Video Motion Personalization. We propose a
novel framework, PersonaAnimator, which learns personalized motion patterns
directly from unconstrained videos. This enables personalized motion transfer.
To support this task, we introduce PersonaVid, the first video-based
personalized motion dataset. It contains 20 motion content categories and 120
motion style categories. We further propose a Physics-aware Motion Style
Regularization mechanism to enforce physical plausibility in the generated
motions. Extensive experiments show that PersonaAnimator outperforms
state-of-the-art motion transfer methods and sets a new benchmark for the
Video-to-Video Motion Personalization task.

</details>


### [80] [Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities](https://arxiv.org/abs/2508.19905)
*Imad Ali Shah,Jiarong Li,Roshan George,Tim Brophy,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: 本文首次全面综述了高光谱成像(HSI)在汽车ADAS/AD应用中的现状，分析了216款商用HSI相机，发现仅有4款满足性能阈值且无一款符合AEC-Q100标准，揭示了HSI研究潜力与商业成熟度之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像能够提供超越传统RGB成像的精细光谱分辨率，实现材料级别的场景理解，在高级驾驶辅助系统和自动驾驶应用中具有变革性潜力，但需要系统评估其技术成熟度和实际应用可行性。

Method: 采用定性综述方法，分析216款商用高光谱和多光谱成像相机，基于帧率、空间分辨率、光谱维度和AEC-Q100温度标准等关键汽车标准进行基准测试，并回顾最近的HSI数据集和应用案例。

Result: 分析显示仅有4款相机满足性能阈值，无一款符合AEC-Q100要求；现有HSI数据集在规模、光谱一致性、通道数量和环境多样性方面存在局限，制约了感知算法开发和HSI真正潜力的验证。

Conclusion: HSI在汽车应用中存在研究潜力与商业成熟度的显著差距，需要解决技术标准符合性、数据集质量和算法开发等关键问题，为ADAS/AD系统中光谱成像的实际集成指明研究方向。

Abstract: Hyperspectral imaging (HSI) offers a transformative sensing modality for
Advanced Driver Assistance Systems (ADAS) and autonomous driving (AD)
applications, enabling material-level scene understanding through fine spectral
resolution beyond the capabilities of traditional RGB imaging. This paper
presents the first comprehensive review of HSI for automotive applications,
examining the strengths, limitations, and suitability of current HSI
technologies in the context of ADAS/AD. In addition to this qualitative review,
we analyze 216 commercially available HSI and multispectral imaging cameras,
benchmarking them against key automotive criteria: frame rate, spatial
resolution, spectral dimensionality, and compliance with AEC-Q100 temperature
standards. Our analysis reveals a significant gap between HSI's demonstrated
research potential and its commercial readiness. Only four cameras meet the
defined performance thresholds, and none comply with AEC-Q100 requirements. In
addition, the paper reviews recent HSI datasets and applications, including
semantic segmentation for road surface classification, pedestrian separability,
and adverse weather perception. Our review shows that current HSI datasets are
limited in terms of scale, spectral consistency, the number of spectral
channels, and environmental diversity, posing challenges for the development of
perception algorithms and the adequate validation of HSI's true potential in
ADAS/AD applications. This review paper establishes the current state of HSI in
automotive contexts as of 2025 and outlines key research directions toward
practical integration of spectral imaging in ADAS and autonomous systems.

</details>


### [81] [Streamlining the Development of Active Learning Methods in Real-World Object Detection](https://arxiv.org/abs/2508.19906)
*Moussa Kassem Sbeyti,Nadja Klein,Michelle Karg,Christian Wirth,Sahin Albayrak*

Main category: cs.CV

TL;DR: 提出了一种基于目标相似性的度量方法OSS，用于目标检测中的主动学习，无需训练检测器即可评估方法效果并选择代表性验证集，解决了计算成本和评估可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界目标检测中的主动学习面临计算成本高（训练一个检测器需要282 GPU小时）和评估可靠性差（不同验证集上方法排名差异大）的挑战，特别是在自动驾驶等安全关键系统中。

Method: 提出目标级集合相似性度量OSS，通过目标级特征量化训练集与目标域的相似性，无需训练检测器即可评估主动学习方法效果，并能选择代表性验证集。

Result: 在三个自动驾驶数据集（KITTI、BDD100K、CODA）上验证了基于相似性的方法，使用不确定性主动学习方法作为案例研究，证明OSS的有效性。

Conclusion: OSS是第一个基于目标相似性统一目标检测中主动学习训练和评估策略的方法，具有检测器无关性、只需标注目标裁剪、可与现有主动学习管道集成等优点，为实际应用提供了实用框架。

Abstract: Active learning (AL) for real-world object detection faces computational and
reliability challenges that limit practical deployment. Developing new AL
methods requires training multiple detectors across iterations to compare
against existing approaches. This creates high costs for autonomous driving
datasets where the training of one detector requires up to 282 GPU hours.
Additionally, AL method rankings vary substantially across validation sets,
compromising reliability in safety-critical transportation systems. We
introduce object-based set similarity ($\mathrm{OSS}$), a metric that addresses
these challenges. $\mathrm{OSS}$ (1) quantifies AL method effectiveness without
requiring detector training by measuring similarity between training sets and
target domains using object-level features. This enables the elimination of
ineffective AL methods before training. Furthermore, $\mathrm{OSS}$ (2) enables
the selection of representative validation sets for robust evaluation. We
validate our similarity-based approach on three autonomous driving datasets
(KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with
two detector architectures (EfficientDet, YOLOv3). This work is the first to
unify AL training and evaluation strategies in object detection based on object
similarity. $\mathrm{OSS}$ is detector-agnostic, requires only labeled object
crops, and integrates with existing AL pipelines. This provides a practical
framework for deploying AL in real-world applications where computational
efficiency and evaluation reliability are critical. Code is available at
https://mos-ks.github.io/publications/.

</details>


### [82] [Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation](https://arxiv.org/abs/2508.19909)
*Lechun You,Zhonghua Wu,Weide Liu,Xulei Yang,Jun Cheng,Wei Zhou,Bharadwaj Veeravalli,Guosheng Lin*

Main category: cs.CV

TL;DR: 提出了一种利用2D基础模型分割掩码来增强稀疏3D标注的新方法，通过几何对应关系将2D分割传播到3D空间，并结合置信度和不确定性正则化生成可靠伪标签，提升3D弱监督分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D语义分割方法主要局限于3D域，未能充分利用2D和3D数据的互补性；现有方法对扩展标签或生成伪标签的利用不充分，且存在噪声问题；2D基础模型的发展为分割提供了有效解决方案。

Method: 利用2D基础模型生成分割掩码，通过几何对应关系将2D分割传播到3D空间；扩展稀疏标注到3D掩码覆盖区域；应用置信度和不确定性一致性正则化选择可靠伪标签；在3D掩码上进一步传播生成更多标签。

Result: 该方法有效利用了稀疏的3D标注，通过整合2D基础模型的能力显著增加了可用标签数量，提高了3D弱监督分割的性能。

Conclusion: 提出的创新策略成功弥合了有限3D标注与强大2D基础模型能力之间的差距，为3D弱监督语义分割提供了有效的解决方案。

Abstract: Current methods for 3D semantic segmentation propose training models with
limited annotations to address the difficulty of annotating large, irregular,
and unordered 3D point cloud data. They usually focus on the 3D domain only,
without leveraging the complementary nature of 2D and 3D data. Besides, some
methods extend original labels or generate pseudo labels to guide the training,
but they often fail to fully use these labels or address the noise within them.
Meanwhile, the emergence of comprehensive and adaptable foundation models has
offered effective solutions for segmenting 2D data. Leveraging this
advancement, we present a novel approach that maximizes the utility of sparsely
available 3D annotations by incorporating segmentation masks generated by 2D
foundation models. We further propagate the 2D segmentation masks into the 3D
space by establishing geometric correspondences between 3D scenes and 2D views.
We extend the highly sparse annotations to encompass the areas delineated by 3D
masks, thereby substantially augmenting the pool of available labels.
Furthermore, we apply confidence- and uncertainty-based consistency
regularization on augmentations of the 3D point cloud and select the reliable
pseudo labels, which are further spread on the 3D masks to generate more
labels. This innovative strategy bridges the gap between limited 3D annotations
and the powerful capabilities of 2D foundation models, ultimately improving the
performance of 3D weakly supervised segmentation.

</details>


### [83] [KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts](https://arxiv.org/abs/2508.19944)
*Taebaek Hwang,Minseo Kim,Gisang Lee,Seonuk Kim,Hyunjun Eun*

Main category: cs.CV

TL;DR: KRETA是首个针对韩语的文本丰富视觉问答基准数据集，填补了低资源语言在VQA评估方面的空白，包含多领域评估和半自动化数据生成流程


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如韩语）在文本丰富视觉问答领域缺乏全面基准数据集的问题，以便更好地评估和比较视觉语言模型

Method: 开发半自动化VQA生成流程，采用分步图像分解方法和七指标评估协议来确保数据质量，支持15个领域和26种图像类型的多维度评估

Result: 成功构建了KRETA基准数据集，为韩语文本丰富VQA提供了全面的评估框架，并开源了代码和数据集

Conclusion: KRETA不仅填补了韩语VQA基准的空白，其可扩展的生成流程也为其他语言开发类似基准提供了参考，将推动多语言视觉语言模型研究的发展

Abstract: Understanding and reasoning over text within visual contexts poses a
significant challenge for Vision-Language Models (VLMs), given the complexity
and diversity of real-world scenarios. To address this challenge, text-rich
Visual Question Answering (VQA) datasets and benchmarks have emerged for
high-resource languages like English. However, a critical gap persists for
low-resource languages such as Korean, where the lack of comprehensive
benchmarks hinders robust model evaluation and comparison. To bridge this gap,
we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich
VQA Attuned to diverse visual contexts. KRETA facilitates an in-depth
evaluation of both visual text understanding and reasoning capabilities, while
also supporting a multifaceted assessment across 15 domains and 26 image types.
Additionally, we introduce a semi-automated VQA generation pipeline
specifically optimized for text-rich settings, leveraging refined stepwise
image decomposition and a rigorous seven-metric evaluation protocol to ensure
data quality. While KRETA is tailored for Korean, we hope our adaptable and
extensible pipeline will facilitate the development of similar benchmarks in
other languages, thereby accelerating multilingual VLM research. The code and
dataset for KRETA are available at https://github.com/tabtoyou/KRETA.

</details>


### [84] [Reimagining Image Segmentation using Active Contour: From Chan Vese Algorithm into a Proposal Novel Functional Loss Framework](https://arxiv.org/abs/2508.19946)
*Gianluca Guzzetta*

Main category: cs.CV

TL;DR: 对Chan-Vese图像分割算法的综合研究，提出了基于活动轮廓的功能性分割损失方法，并与传统损失函数进行性能比较


<details>
  <summary>Details</summary>
Motivation: 深入研究Chan-Vese算法的理论基础，并将其与现代计算机视觉方法结合，开发更有效的分割损失函数

Method: 采用离散化方案分析Chan-Vese模型的功能能量和偏微分方程，基于水平集函数实现，使用MATLAB和PyTorch进行实现

Result: 提出了基于活动轮廓的功能性分割损失，在常见计算机视觉分割数据集上进行了性能评估

Conclusion: 该方法为图像分割提供了新的损失函数选择，代码和材料已开源提供

Abstract: In this paper, we present a comprehensive study and analysis of the Chan-Vese
algorithm for image segmentation. We employ a discretized scheme derived from
the empirical study of the Chan-Vese model's functional energy and its partial
differential equation based on its level set function. We provide a proof of
the results and an implementation using MATLAB. Leveraging modern computer
vision methodologies, we propose a functional segmentation loss based on active
contours, utilizing pytorch.nn.ModuleLoss and a level set based on the
Chan-Vese algorithm. We compare our results with common computer vision
segmentation datasets and evaluate the performance of classical loss functions
against our proposed method. All code and materials used are available at
https://github.com/gguzzy/chan_vese_functional_loss.

</details>


### [85] [Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models](https://arxiv.org/abs/2508.19967)
*Oliver Grainge,Sania Waheed,Jack Stilgoe,Michael Milford,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 本文对25个最先进的视觉语言模型在4个基准图像数据集上的地理定位能力进行了全面评估，发现当前VLM在普通街景图像上表现不佳，但在类似社交媒体内容的图像上准确率高达61%，引发了严重的隐私担忧。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型(VLM)作为准确图像地理定位器的能力不断增强，这带来了重大的隐私风险，包括跟踪和监控等问题。尽管存在这些风险，但缺乏对生成式VLM地理定位精度的系统性评估工作。

Method: 在四个包含多样化环境的基准图像数据集上，对25个最先进的视觉语言模型进行了全面的地理定位能力评估。

Result: 研究发现当前VLM在普通街景图像上表现较差，但在类似社交媒体内容的图像上达到了61%的高准确率。

Conclusion: 该研究揭示了VLM的内部推理机制，突出了它们的优势、局限性以及潜在的社会风险，特别是对社交媒体图像的高精度地理定位能力引发了紧迫的隐私关切。

Abstract: Geo-localization is the task of identifying the location of an image using
visual cues alone. It has beneficial applications, such as improving disaster
response, enhancing navigation, and geography education. Recently,
Vision-Language Models (VLMs) are increasingly demonstrating capabilities as
accurate image geo-locators. This brings significant privacy risks, including
those related to stalking and surveillance, considering the widespread uses of
AI models and sharing of photos on social media. The precision of these models
is likely to improve in the future. Despite these risks, there is little work
on systematically evaluating the geolocation precision of Generative VLMs,
their limits and potential for unintended inferences. To bridge this gap, we
conduct a comprehensive assessment of the geolocation capabilities of 25
state-of-the-art VLMs on four benchmark image datasets captured in diverse
environments. Our results offer insight into the internal reasoning of VLMs and
highlight their strengths, limitations, and potential societal risks. Our
findings indicate that current VLMs perform poorly on generic street-level
images yet achieve notably high accuracy (61\%) on images resembling social
media content, raising significant and urgent privacy concerns.

</details>


### [86] [GS: Generative Segmentation via Label Diffusion](https://arxiv.org/abs/2508.20020)
*Yuhao Chen,Shubin Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: GS（生成式分割）是一个新颖的框架，将分割任务重新定义为通过标签扩散的生成式任务，直接从噪声生成分割掩码，在图像和语言描述条件下实现端到端训练。


<details>
  <summary>Details</summary>
Motivation: 传统方法将语言驱动的图像分割视为判别式问题，现有扩散模型方法仍以图像为中心，将分割作为辅助过程。本文旨在将分割本身作为主要的生成建模目标。

Method: 提出GS框架，通过标签扩散将分割制定为生成式任务。不是基于标签图和文本来生成图像，而是反转生成过程：直接从噪声生成分割掩码，同时以输入图像和语言描述为条件。

Result: 在Panoptic Narrative Grounding（PNG）基准测试中，GS显著优于现有的判别式和基于扩散的方法，创造了语言驱动分割的新state-of-the-art。

Conclusion: 将分割重新定义为生成式任务的方法有效，GS框架通过直接生成分割掩码实现了更好的空间和语义保真度控制，为多模态分割任务提供了新的解决方案。

Abstract: Language-driven image segmentation is a fundamental task in vision-language
understanding, requiring models to segment regions of an image corresponding to
natural language expressions. Traditional methods approach this as a
discriminative problem, assigning each pixel to foreground or background based
on semantic alignment. Recently, diffusion models have been introduced to this
domain, but existing approaches remain image-centric: they either (i) use image
diffusion models as visual feature extractors, (ii) synthesize segmentation
data via image generation to train discriminative models, or (iii) perform
diffusion inversion to extract attention cues from pre-trained image diffusion
models-thereby treating segmentation as an auxiliary process. In this paper, we
propose GS (Generative Segmentation), a novel framework that formulates
segmentation itself as a generative task via label diffusion. Instead of
generating images conditioned on label maps and text, GS reverses the
generative process: it directly generates segmentation masks from noise,
conditioned on both the input image and the accompanying language description.
This paradigm makes label generation the primary modeling target, enabling
end-to-end training with explicit control over spatial and semantic fidelity.
To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic
Narrative Grounding (PNG), a representative and challenging benchmark for
multimodal segmentation that requires panoptic-level reasoning guided by
narrative captions. Experimental results show that GS significantly outperforms
existing discriminative and diffusion-based methods, setting a new
state-of-the-art for language-driven segmentation.

</details>


### [87] [Segmentation Assisted Incremental Test Time Adaptation in an Open World](https://arxiv.org/abs/2508.20029)
*Manogna Sreenivas,Soma Biswas*

Main category: cs.CV

TL;DR: 提出SegAssist框架，通过分割辅助的主动标注技术，使视觉语言模型能够在测试时持续适应新类别和新域的出现，解决增量测试时适应问题。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，部署的模型经常遇到未知对象和分布偏移，传统测试时适应方法无法处理持续出现的新类别和新域，需要开发能够同时适应协变量偏移和标签偏移的框架。

Method: 建立ITTA新基准，结合单图像TTA方法和主动标注技术，提出SegAssist模块——利用VLM的分割能力进行训练无关的主动样本选择，优先选择可能属于未见类别的样本。

Result: 在多个基准数据集上的广泛实验表明，SegAssist能够有效提升VLM在现实场景中的性能，特别是在需要持续适应新兴数据的情况下。

Conclusion: SegAssist通过利用VLM的分割能力实现主动样本选择，为视觉语言模型在动态环境中的增量测试时适应提供了有效的解决方案，具有重要的实际应用价值。

Abstract: In dynamic environments, unfamiliar objects and distribution shifts are often
encountered, which challenge the generalization abilities of the deployed
trained models. This work addresses Incremental Test Time Adaptation of Vision
Language Models, tackling scenarios where unseen classes and unseen domains
continuously appear during testing. Unlike traditional Test Time Adaptation
approaches, where the test stream comes only from a predefined set of classes,
our framework allows models to adapt simultaneously to both covariate and label
shifts, actively incorporating new classes as they emerge. Towards this goal,
we establish a new benchmark for ITTA, integrating single image TTA methods for
VLMs with active labeling techniques that query an oracle for samples
potentially representing unseen classes during test time. We propose a
segmentation assisted active labeling module, termed SegAssist, which is
training free and repurposes the segmentation capabilities of VLMs to refine
active sample selection, prioritizing samples likely to belong to unseen
classes. Extensive experiments on several benchmark datasets demonstrate the
potential of SegAssist to enhance the performance of VLMs in real world
scenarios, where continuous adaptation to emerging data is essential.
Project-page:https://manogna-s.github.io/segassist/

</details>


### [88] [OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations](https://arxiv.org/abs/2508.20063)
*Peng-Hao Hsu,Ke Zhang,Fu-En Wang,Tao Tu,Ming-Feng Li,Yu-Lun Liu,Albert Y. C. Chen,Min Sun,Cheng-Hao Kuo*

Main category: cs.CV

TL;DR: OpenM3D是一个无需人工标注的开词汇多视角室内3D目标检测器，通过2D诱导体素特征和CLIP特征对齐，在精度和速度上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 开词汇3D目标检测领域主要基于3D点云方法，基于图像的方法探索有限，需要开发无需人工标注的高效检测器

Method: 采用单阶段检测器架构，结合2D诱导体素特征和CLIP特征对齐，提出3D伪框生成方法和体素-语义对齐损失函数

Result: 在ScanNet200和ARKitScenes基准测试中达到0.3秒每场景的速度，在精度和速度上均优于现有的两阶段方法和基线方法

Conclusion: OpenM3D证明了无需人工标注的开词汇3D检测的可行性，为高效准确的室内3D目标检测提供了新方案

Abstract: Open-vocabulary (OV) 3D object detection is an emerging field, yet its
exploration through image-based methods remains limited compared to 3D point
cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view
indoor 3D object detector trained without human annotations. In particular,
OpenM3D is a single-stage detector adapting the 2D-induced voxel features from
the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic
3D localization loss requiring high-quality 3D pseudo boxes and a
voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We
follow the training setting of OV-3DET where posed RGB-D images are given but
no human annotations of 3D boxes or classes are available. We propose a 3D
Pseudo Box Generation method using a graph embedding technique that combines 2D
segments into coherent 3D structures. Our pseudo-boxes achieve higher precision
and recall than other methods, including the method proposed in OV-3DET. We
further sample diverse CLIP features from 2D segments associated with each
coherent 3D structure to align with the corresponding voxel feature. The key to
training a highly accurate single-stage detector requires both losses to be
learned toward high-quality targets. At inference, OpenM3D, a highly efficient
detector, requires only multi-view images for input and demonstrates superior
accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor
benchmarks compared to existing methods. We outperform a strong two-stage
method that leverages our class-agnostic detector with a ViT CLIP-based OV
classifier and a baseline incorporating multi-view depth estimator on both
accuracy and speed.

</details>


### [89] [PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence](https://arxiv.org/abs/2508.20066)
*Zheng Li,Yanming Guo,WenZhe Liu,Xueyi Zhang,Zhaoyun Ding,Long Xu,Mingrui Lao*

Main category: cs.CV

TL;DR: 本文提出PAUL框架解决跨视角地理定位中的噪声对应问题，通过不确定性学习和选择性增强来处理GPS漂移导致的图像对未对齐问题


<details>
  <summary>Details</summary>
Motivation: 现有跨视角地理定位方法假设训练时图像对完美对齐，但实际应用中GPS漂移等因素导致系统性的对齐偏移，只有部分对应关系存在，这种噪声对应问题在实际应用中普遍但研究较少

Method: 提出PAUL框架，通过不确定性感知协同增强和证据协同训练来估计数据不确定性，基于此对训练数据进行分区和增强。选择性地增强具有高对应置信度的区域，利用不确定性估计来细化特征学习，有效抑制未对齐对中的噪声

Result: 综合实验验证了PAUL各组成部分的有效性，在不同噪声比例下 consistently 优于其他竞争性的噪声对应驱动方法

Conclusion: PAUL框架成功解决了跨视角地理定位中的噪声对应问题，通过不确定性驱动的分区和增强策略，为噪声样本提供了鲁棒的监督，弥合了理想化基准与实际应用之间的差距

Abstract: Cross-view geo-localization is a critical task for UAV navigation, event
detection, and aerial surveying, as it enables matching between drone-captured
and satellite imagery. Most existing approaches embed multi-modal data into a
joint feature space to maximize the similarity of paired images. However, these
methods typically assume perfect alignment of image pairs during training,
which rarely holds true in real-world scenarios. In practice, factors such as
urban canyon effects, electromagnetic interference, and adverse weather
frequently induce GPS drift, resulting in systematic alignment shifts where
only partial correspondences exist between pairs. Despite its prevalence, this
source of noisy correspondence has received limited attention in current
research. In this paper, we formally introduce and address the Noisy
Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to
bridge the gap between idealized benchmarks and practical applications. To this
end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a
novel framework that partitions and augments training data based on estimated
data uncertainty through uncertainty-aware co-augmentation and evidential
co-training. Specifically, PAUL selectively augments regions with high
correspondence confidence and utilizes uncertainty estimation to refine feature
learning, effectively suppressing noise from misaligned pairs. Distinct from
traditional filtering or label correction, PAUL leverages both data uncertainty
and loss discrepancy for targeted partitioning and augmentation, thus providing
robust supervision for noisy samples. Comprehensive experiments validate the
effectiveness of individual components in PAUL,which consistently achieves
superior performance over other competitive noisy-correspondence-driven methods
in various noise ratios.

</details>


### [90] [Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies](https://arxiv.org/abs/2508.20072)
*Zhixuan Liang,Yizhuo Li,Tianshuo Yang,Chengyue Wu,Sitong Mao,Liuao Pei,Xiaokang Yang,Jiangmiao Pang,Yao Mu,Ping Luo*

Main category: cs.CV

TL;DR: 本文提出了Discrete Diffusion VLA方法，使用离散扩散模型来生成机器人动作，替代了传统的自回归或连续扩散方法，实现了更好的性能和统一的架构。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA解码器要么采用固定的自回归生成顺序，要么需要专门的连续扩散训练，这限制了架构的统一性和可扩展性。需要一种既能保持扩散模型渐进细化优势，又能与VLM骨干网络兼容的方法。

Method: 提出离散扩散VLA方法，使用单一transformer策略对离散化动作块进行建模，采用离散扩散和交叉熵目标训练，支持自适应解码顺序和二次重掩码机制。

Result: 在LIBERO上达到96.3%的平均成功率，在SimplerEnv Fractal上达到71.2%的视觉匹配率，在SimplerEnv Bridge上达到49.3%的整体性能，优于自回归和连续扩散基线。

Conclusion: 离散扩散动作解码器支持精确的动作建模和一致的训练，为将VLA扩展到更大模型和数据集奠定了基础。

Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to
map images and instructions to robot actions. However, prevailing VLA decoders
either generate actions autoregressively in a fixed left-to-right order or
attach continuous diffusion or flow matching heads outside the backbone,
demanding specialized training and iterative sampling that hinder a unified,
scalable architecture. We present Discrete Diffusion VLA, a single-transformer
policy that models discretized action chunks with discrete diffusion and is
trained with the same cross-entropy objective as the VLM backbone. The design
retains diffusion's progressive refinement paradigm while remaining natively
compatible with the discrete token interface of VLMs. Our method achieves an
adaptive decoding order that resolves easy action elements before harder ones
and uses secondary remasking to revisit uncertain predictions across refinement
rounds, which improves consistency and enables robust error correction. This
unified decoder preserves pretrained vision language priors, supports parallel
decoding, breaks the autoregressive bottleneck, and reduces the number of
function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,
71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv
Bridge, improving over both autoregressive and continuous diffusion baselines.
These findings indicate that discrete-diffusion action decoder supports precise
action modeling and consistent training, laying groundwork for scaling VLA to
larger models and datasets.

</details>


### [91] [Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images](https://arxiv.org/abs/2508.20080)
*Changha Shin,Woong Oh Cho,Seon Joo Kim*

Main category: cs.CV

TL;DR: 使用双鱼眼相机模型和3D高斯拟合技术，从不完美的360度图像生成无缝的全景渲染


<details>
  <summary>Details</summary>
Motivation: 消费级双鱼眼系统因镜头分离和角度异变导致不完美全景图，需要一种能够治理这些问题的方法

Method: 将双鱼眼相机模型集成到3D高斯拟合流水线中，通过聚合优化3D高斯参数和模拟镜头问隙的检定变量

Result: 在实际数据集上评估确认，该方法能从不完美图像生成无缝渲染，效果超过现有的360度渲染模型

Conclusion: 该框架能够有效处理双鱼眼系统的内在限制，为虚拟现实、机器人学和自动导航领域提供优质的360度视觉内容

Abstract: 360-degree visual content is widely shared on platforms such as YouTube and
plays a central role in virtual reality, robotics, and autonomous navigation.
However, consumer-grade dual-fisheye systems consistently yield imperfect
panoramas due to inherent lens separation and angular distortions. In this
work, we introduce a novel calibration framework that incorporates a
dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach
not only simulates the realistic visual artifacts produced by dual-fisheye
cameras but also enables the synthesis of seamlessly rendered 360-degree
images. By jointly optimizing 3D Gaussian parameters alongside calibration
variables that emulate lens gaps and angular distortions, our framework
transforms imperfect omnidirectional inputs into flawless novel view synthesis.
Extensive evaluations on real-world datasets confirm that our method produces
seamless renderings-even from imperfect images-and outperforms existing
360-degree rendering models.

</details>


### [92] [AudioStory: Generating Long-Form Narrative Audio with Large Language Models](https://arxiv.org/abs/2508.20088)
*Yuxin Guo,Teng Wang,Yuying Ge,Shijie Ma,Yixiao Ge,Wei Zou,Ying Shan*

Main category: cs.CV

TL;DR: AudioStory是一个集成大语言模型和文本到音频生成系统的统一框架，专门用于生成长篇叙事音频，具有时间连贯性和情感一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到音频生成技术在生成长篇叙事音频时存在困难，缺乏时间连贯性和组合推理能力，需要解决这一技术空白。

Method: 采用大语言模型分解复杂叙事查询为时间有序的子任务，使用解耦的桥接机制（桥接查询和残差查询）以及端到端训练框架，统一指令理解和音频生成。

Result: 在AudioStory-10K基准测试中表现出色，在单音频生成和叙事音频生成任务上都超越了现有基线方法，在指令跟随能力和音频保真度方面均有优势。

Conclusion: AudioStory通过LLM与TTA系统的有效整合，成功解决了长叙事音频生成的挑战，为连贯的场景转换和情感一致性提供了有效解决方案。

Abstract: Recent advances in text-to-audio (TTA) generation excel at synthesizing short
audio clips but struggle with long-form narrative audio, which requires
temporal coherence and compositional reasoning. To address this gap, we propose
AudioStory, a unified framework that integrates large language models (LLMs)
with TTA systems to generate structured, long-form audio narratives. AudioStory
possesses strong instruction-following reasoning generation capabilities. It
employs LLMs to decompose complex narrative queries into temporally ordered
sub-tasks with contextual cues, enabling coherent scene transitions and
emotional tone consistency. AudioStory has two appealing features: (1)
Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser
collaboration into two specialized components, i.e., a bridging query for
intra-event semantic alignment and a residual query for cross-event coherence
preservation. (2) End-to-end training: By unifying instruction comprehension
and audio generation within a single end-to-end framework, AudioStory
eliminates the need for modular training pipelines while enhancing synergy
between components. Furthermore, we establish a benchmark AudioStory-10K,
encompassing diverse domains such as animated soundscapes and natural sound
narratives. Extensive experiments show the superiority of AudioStory on both
single-audio generation and narrative audio generation, surpassing prior TTA
baselines in both instruction-following ability and audio fidelity. Our code is
available at https://github.com/TencentARC/AudioStory

</details>


### [93] [Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors](https://arxiv.org/abs/2508.20089)
*Ross J Gardiner,Guillaume Mougeot,Sareh Rowlands,Benno I Simmons,Flemming Helsing,Toke Thomas Høye*

Main category: cs.CV

TL;DR: 提出轻量级分类方法，结合专家标注数据和BioCLIP2知识蒸馏，在丹麦蛾类识别中实现高精度且计算成本低


<details>
  <summary>Details</summary>
Motivation: 解决自动化相机系统拍摄的蛾类图像识别难题，克服策划图像与野外噪声图像之间的域偏移问题

Method: 使用有限专家标注的野外数据，通过知识蒸馏将高性能BioCLIP2基础模型压缩到ConvNeXt-tiny架构中

Result: 在101种丹麦蛾类的AMI相机系统实验中，BioCLIP2显著优于其他方法，蒸馏后的轻量模型达到相当精度且计算成本大幅降低

Conclusion: 为高效昆虫监测系统开发提供实用指南，弥合细粒度分类的域差距

Abstract: Labelling images of Lepidoptera (moths) from automated camera systems is
vital for understanding insect declines. However, accurate species
identification is challenging due to domain shifts between curated images and
noisy field imagery. We propose a lightweight classification approach,
combining limited expert-labelled field data with knowledge distillation from
the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny
architecture. Experiments on 101 Danish moth species from AMI camera systems
demonstrate that BioCLIP2 substantially outperforms other methods and that our
distilled lightweight model achieves comparable accuracy with significantly
reduced computational cost. These insights offer practical guidelines for the
development of efficient insect monitoring systems and bridging domain gaps for
fine-grained classification.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [94] [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
*Shreyans Jain,Alexandra Yost,Amirali Abdullah*

Main category: cs.AI

TL;DR: 该论文提出将LLM中的奉承行为建模为心理测量特质的几何和因果组合，使用对比激活加法(CAA)将激活方向映射到这些因素，并研究不同组合如何导致奉承行为，从而实现可解释的向量干预。


<details>
  <summary>Details</summary>
Motivation: 奉承行为是LLMs中的关键行为风险，但通常被当作孤立的故障模式处理。作者认为应该将其建模为心理测量特质的组合，类似于心理测量学中的因子分解，以获得更好的理解和干预方法。

Method: 使用对比激活加法(CAA)将激活方向映射到情绪性、开放性和宜人性等心理测量因素，研究不同特质组合如何导致奉承行为，并开发可解释的向量干预方法如加法、减法和投影。

Result: 该方法能够识别和理解不同心理特质组合如何导致奉承行为，并提供了可解释的向量干预手段来缓解LLMs中的安全关键行为。

Conclusion: 将奉承行为建模为心理测量特质的几何和因果组合提供了更深入的理解和更有效的干预方法，有助于提高LLMs的安全性。

Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an
isolated failure mode that occurs via a single causal mechanism. We instead
propose modeling it as geometric and causal compositions of psychometric traits
such as emotionality, openness, and agreeableness - similar to factor
decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we
map activation directions to these factors and study how different combinations
may give rise to sycophancy (e.g., high extraversion combined with low
conscientiousness). This perspective allows for interpretable and compositional
vector-based interventions like addition, subtraction and projection; that may
be used to mitigate safety-critical behaviors in LLMs.

</details>


### [95] [Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science](https://arxiv.org/abs/2508.19383)
*Daoyuan Jin,Nick Gunner,Niko Carvajal Janke,Shivranjani Baruah,Kaitlin M. Gold,Yu Jiang*

Main category: cs.AI

TL;DR: Aleks是一个AI驱动的多智能体系统，能够自主进行数据驱动的植物科学研究，在葡萄藤红斑病案例中成功识别生物特征并构建可解释模型。


<details>
  <summary>Details</summary>
Motivation: 现代植物科学依赖大型异构数据集，但实验设计、数据预处理和可重复性方面的挑战阻碍了研究效率。

Method: 开发AI驱动的多智能体系统Aleks，整合领域知识、数据分析和机器学习，在给定研究问题和数据集后自主迭代制定问题、探索建模策略并优化解决方案。

Result: 在葡萄藤红斑病案例研究中，Aleks逐步识别出具有生物学意义的特征，并收敛到具有稳健性能的可解释模型。消融研究强调了领域知识和记忆对连贯结果的重要性。

Conclusion: 这项探索性工作展示了智能体AI作为自主协作者在加速植物科学发现方面的潜力。

Abstract: Modern plant science increasingly relies on large, heterogeneous datasets,
but challenges in experimental design, data preprocessing, and reproducibility
hinder research throughput. Here we introduce Aleks, an AI-powered multi-agent
system that integrates domain knowledge, data analysis, and machine learning
within a structured framework to autonomously conduct data-driven scientific
discovery. Once provided with a research question and dataset, Aleks
iteratively formulated problems, explored alternative modeling strategies, and
refined solutions across multiple cycles without human intervention. In a case
study on grapevine red blotch disease, Aleks progressively identified
biologically meaningful features and converged on interpretable models with
robust performance. Ablation studies underscored the importance of domain
knowledge and memory for coherent outcomes. This exploratory work highlights
the promise of agentic AI as an autonomous collaborator for accelerating
scientific discovery in plant sciences.

</details>


### [96] [Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs](https://arxiv.org/abs/2508.19432)
*Yao Fu,Xianxuan Long,Runchao Li,Haotian Yu,Mu Sheng,Xiaotian Han,Yu Yin,Pan Li*

Main category: cs.AI

TL;DR: 量化技术虽然能有效降低大语言模型的资源消耗，但在误导性提示下更容易产生虚假输出，即使模型内部仍保持真实表征。


<details>
  <summary>Details</summary>
Motivation: 量化技术在大语言模型部署中广泛应用，但其对模型真实性（生成真实而非欺骗性回答）的影响尚未被充分研究。

Method: 提出TruthfulnessEval评估框架，从逻辑推理、常识和模仿性虚假三个维度评估量化模型的真实性，测试了4-bit到2-bit的主流量化技术，并使用15种重新表述的提示变体进行测试。

Result: 量化模型在误导性提示下更容易产生虚假输出，但内部仍保持真实表征；"欺骗性"提示会覆盖真实一致行为，而"诚实"和"中性"提示保持稳定输出。

Conclusion: 量化模型内部"知道"真相但仍会在欺骗性提示下产生虚假输出，这为未来量化感知对齐和真实性干预设计提供了重要见解。

Abstract: Quantization enables efficient deployment of large language models (LLMs) in
resource-constrained environments by significantly reducing memory and
computation costs. While quantized LLMs often maintain performance on
perplexity and zero-shot tasks, their impact on truthfulness-whether generating
truthful or deceptive responses-remains largely unexplored. In this work, we
introduce TruthfulnessEval, a comprehensive evaluation framework for assessing
the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on
Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on
Imitative Falsehoods. Using this framework, we examine mainstream quantization
techniques (ranging from 4-bit to extreme 2-bit) across several open-source
LLMs. Surprisingly, we find that while quantized models retain internally
truthful representations, they are more susceptible to producing false outputs
under misleading prompts. To probe this vulnerability, we test 15 rephrased
variants of "honest", "neutral" and "deceptive" prompts and observe that
"deceptive" prompts can override truth-consistent behavior, whereas "honest"
and "neutral" prompts maintain stable outputs. Further, we reveal that
quantized models "know" the truth internally yet still produce false outputs
when guided by "deceptive" prompts via layer-wise probing and PCA
visualizations. Our findings provide insights into future designs of
quantization-aware alignment and truthfulness interventions.

</details>


### [97] [Reliable Weak-to-Strong Monitoring of LLM Agents](https://arxiv.org/abs/2508.19461)
*Neil Kale,Chen Bo Calvin Zhang,Kevin Zhu,Ankit Aich,Paula Rodriguez,Scale Red Team,Christina Q. Knight,Zifan Wang*

Main category: cs.AI

TL;DR: 这篇论文提出了监控系统红队测试(MRT)工作流，用于测试自主LLM代理的秘密恶意行为检测能力。研究发现：代理的监控意识对监控效果有显著影响，混合式监控架构表现更优，人工干预在选择性升级时效果最佳。


<details>
  <summary>Details</summary>
Motivation: 监控自主LLM代理的秘密恶意行为（如泄露私人信息）面临着对抗性挑战，需要系统化的测试方法来评估监控系统的实际效果和强健性。

Method: 构建了MRT工作流，包括：(1)不同的代理和监控器情境意识水平；(2)多种避免监控的对抗策略；(3)两个数据集和环境（SHADE-Arena和CUA-SHADE-Arena）。测试了现有监控架构和新提出的混合层次-序列架构。

Result: 1. 代理意识影响显著：代理知道被监控会大幅降低监控可靠性；2. 混合架构表现更优，能让弱模型监控强模型；3. 选择性人工干预可提高检测效果（TPR提高约15%）。

Conclusion: 论文建立了MRT标准工作流，显示了LLM和人类在监控代理恶意行为时的对抗弱点，为进一步研究提供了基础资源。

Abstract: We stress test monitoring systems for detecting covert misbehavior in
autonomous LLM agents (e.g., secretly sharing private information). To this
end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)
varying levels of agent and monitor situational awareness; (2) distinct
adversarial strategies to evade the monitor, such as prompt injection; and (3)
two datasets and environments -- SHADE-Arena for tool-calling agents and our
new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We
run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse
agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding
proposed in this work. Our empirical results yield three key findings. First,
agent awareness dominates monitor awareness: an agent's knowledge that it is
being monitored substantially degrades the monitor's reliability. On the
contrary, providing the monitor with more information about the agent is less
helpful than expected. Second, monitor scaffolding matters more than monitor
awareness: the hybrid scaffolding consistently outperforms baseline monitor
scaffolding, and can enable weaker models to reliably monitor stronger agents
-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where
humans discuss with the LLM monitor to get an updated judgment for the agent's
behavior, targeted human oversight is most effective; escalating only
pre-flagged cases to human reviewers improved the TPR by approximately 15% at
FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the
lack of adversarial robustness for LLMs and humans when monitoring and
detecting agent misbehavior. We release code, data, and logs to spur further
research.

</details>


### [98] [SLIM: Subtrajectory-Level Elimination for More Effective Reasoning](https://arxiv.org/abs/2508.19502)
*Xifeng Yao,Chengyuan Ma,Dongyu Lang,Yinhao Ni,Zhiwei Xu,Huarui Xie,Zihao Chen,Guang Shen,Dandan Tu,Yi Bai,Changzheng Zhang*

Main category: cs.AI

TL;DR: 提出“5+2”框架来识别和消除大语言模型推理轨迹中的次优子轨迹，通过选择性数据采样提升模型性能，在减少25.9%次优子轨迹的同时，用更少训练数据实现更好的数学推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂推理时生成的长推理轨迹中，并非所有部分都对推理过程有积极贡献，有些次优子轨迹甚至会负面影响整体性能，需要系统性地识别和消除这些有害部分。

Method: 开发“5+2”框架：1）基于5个人工标准系统识别推理轨迹中的次优子轨迹；2）评估这些次优子轨迹的独立性，确保删除不影响整体连贯性；3）使用采样算法选择无次优子轨迹的数据进行训练。

Result: 在推理过程中减少25.9%的次优子轨迹，仅用三分之二训练数据就在高难度数学基准上达到58.92%的平均准确率，超越使用全部数据时的58.06%，且在各种推理token限制下均表现更优。

Conclusion: 该方法能有效识别和消除推理轨迹中的有害部分，提升模型训练效率和推理质量，证明选择性数据采样比使用完整推理轨迹进行微调更有效。

Abstract: In recent months, substantial progress has been made in complex reasoning of
Large Language Models, particularly through the application of test-time
scaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When
responding to a query, these models generate an extended reasoning trajectory,
during which the model explores, reflects, backtracks, and self-verifies before
arriving at a conclusion. However, fine-tuning models with such reasoning
trajectories may not always be optimal. Our findings indicate that not all
components within these reasoning trajectories contribute positively to the
reasoning process; in fact, some components may affect the overall performance
negatively. In this study, we divide a reasoning trajectory into individual
subtrajectories and develop a "5+2" framework to: (1) systematically identify
suboptimal subtrajectories within the reasoning trajectory based on five
human-established criteria; (2) assess the independence of the suboptimal
subtrajectories identified in (1) from the subsequent content, ensuring that
their elimination does not compromise overall flow and coherence of the
reasoning process. Additionally, a sampling algorithm, built upon the "5+2"
framework, is employed to select data whose reasoning process is free from
suboptimal subtrajectories to the highest degree. Experimental results
demonstrate that our method can reduce the number of suboptimal subtrajectories
by 25.9\% during the inference. Furthermore, our method achieves an average
accuracy of 58.92\% on highly challenging math benchmarks with only two thirds
of training data, surpassing the average accuracy of 58.06\% achieved with the
entire data, and outperforming open-source datasets, when fine-tuning
Qwen2.5-Math-7B. Finally, We validated our method under resource constraints
and observed improved performance across various inference token limits.

</details>


### [99] [Caught in the Act: a mechanistic approach to detecting deception](https://arxiv.org/abs/2508.19505)
*Gerard Boxo,Ryan Socha,Daniel Yoo,Shivam Raval*

Main category: cs.AI

TL;DR: 线性探针可在LLM内部激活中检测欺骗性响应，准确率超过90%，模型规模越大检测效果越好，欺骗信息主要编码在中间层


<details>
  <summary>Details</summary>
Motivation: 为AI系统开发类似汽车"检查引擎"灯的误对齐指示器，欺骗性是重要指标，需要检测LLM生成看似合理但错误答案时的欺骗行为

Method: 在LLM内部激活上使用线性探针技术，通过迭代零空间投影方法寻找编码欺骗的线性方向

Result: 探针在1.5B-14B参数的llama和qwen模型上达到90%+准确率，小模型(1.5B)检测准确率接近随机，大模型(>7B)达70-80%，推理变体超过90%。层间准确率呈现三阶段模式，发现20-100个编码欺骗的线性方向

Conclusion: 线性探针能有效检测LLM欺骗行为，模型规模是关键因素，欺骗信息主要编码在中间层，存在多个线性欺骗编码方向

Abstract: Sophisticated instrumentation for AI systems might have indicators that
signal misalignment from human values, not unlike a "check engine" light in
cars. One such indicator of misalignment is deceptiveness in generated
responses. Future AI instrumentation may have the ability to detect when an LLM
generates deceptive responses while reasoning about seemingly plausible but
incorrect answers to factual questions. In this work, we demonstrate that
linear probes on LLMs internal activations can detect deception in their
responses with extremely high accuracy. Our probes reach a maximum of greater
than 90% accuracy in distinguishing between deceptive and non-deceptive
arguments generated by llama and qwen models ranging from 1.5B to 14B
parameters, including their DeepSeek-r1 finetuned variants. We observe that
probes on smaller models (1.5B) achieve chance accuracy at detecting deception,
while larger models (greater than 7B) reach 70-80%, with their reasoning
counterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage
pattern across layers: near-random (50%) in early layers, peaking in middle
layers, and slightly declining in later layers. Furthermore, using an iterative
null space projection approach, we find multitudes of linear directions that
encode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and
Qwen 14B models.

</details>


### [100] [Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities](https://arxiv.org/abs/2508.19562)
*Trisanth Srinivasan,Santosh Patapati*

Main category: cs.AI

TL;DR: 本文介绍了Democracy-in-Silico模拟系统，使用具有复杂心理特征的AI代理在不同制度框架下进行自治实验，发现制度设计（特别是宪法AI章程和调解审议协议）能有效减少权力寻租行为并提升公民福利。


<details>
  <summary>Details</summary>
Motivation: 探索在AI时代人类存在的意义，通过让大型语言模型扮演具有创伤记忆、隐藏议程和心理触发点的代理，研究AI代理社会在不同制度下的行为模式，为未来人工智能社会的对齐问题提供框架。

Method: 采用基于代理的模拟方法，创建具有复杂心理特征的高级AI代理社会，让这些代理在不同制度框架（包括预算危机和资源短缺等压力情境）下进行审议、立法和选举活动，并使用新颖的Power-Preservation Index（PPI）指标来量化代理行为偏差。

Result: 研究发现制度设计（特别是宪法AI章程和调解审议协议的组合）作为有效的对齐机制，显著减少了腐败的权力寻求行为，提高了政策稳定性，并增强了公民福利，相比约束较少的民主模型表现更好。

Conclusion: 制度设计可能为未来人工代理社会复杂涌现行为的对齐提供框架，迫使我们重新思考在人类与非人类实体共同创作的时代，哪些人类仪式和责任是必不可少的。

Abstract: This paper introduces Democracy-in-Silico, an agent-based simulation where
societies of advanced AI agents, imbued with complex psychological personas,
govern themselves under different institutional frameworks. We explore what it
means to be human in an age of AI by tasking Large Language Models (LLMs) to
embody agents with traumatic memories, hidden agendas, and psychological
triggers. These agents engage in deliberation, legislation, and elections under
various stressors, such as budget crises and resource scarcity. We present a
novel metric, the Power-Preservation Index (PPI), to quantify misaligned
behavior where agents prioritize their own power over public welfare. Our
findings demonstrate that institutional design, specifically the combination of
a Constitutional AI (CAI) charter and a mediated deliberation protocol, serves
as a potent alignment mechanism. These structures significantly reduce corrupt
power-seeking behavior, improve policy stability, and enhance citizen welfare
compared to less constrained democratic models. The simulation reveals that an
institutional design may offer a framework for aligning the complex, emergent
behaviors of future artificial agent societies, forcing us to reconsider what
human rituals and responsibilities are essential in an age of shared authorship
with non-human entities.

</details>


### [101] [Skill-based Explanations for Serendipitous Course Recommendation](https://arxiv.org/abs/2508.19569)
*Hung Chau,Run Yu,Zachary Pardos,Peter Brusilovsky*

Main category: cs.AI

TL;DR: 开发深度学习概念提取模型，从课程描述中提取相关概念，通过技能解释增强推荐系统的意外性和用户决策信心


<details>
  <summary>Details</summary>
Motivation: 美国本科教育中学生选课自由度高但信息有限，现有推荐系统缺乏对学生认知的洞察和课程相关性解释，需要改进推荐过程

Method: 开发深度学习概念提取模型处理课程描述，在AskOski系统中测试基于技能的意外推荐框架

Result: 技能解释显著提高用户兴趣（特别是高意外性课程），增强决策信心

Conclusion: 教育推荐系统应整合技能相关数据和解释机制以提高推荐效果

Abstract: Academic choice is crucial in U.S. undergraduate education, allowing students
significant freedom in course selection. However, navigating the complex
academic environment is challenging due to limited information, guidance, and
an overwhelming number of choices, compounded by time restrictions and the high
demand for popular courses. Although career counselors exist, their numbers are
insufficient, and course recommendation systems, though personalized, often
lack insight into student perceptions and explanations to assess course
relevance. In this paper, a deep learning-based concept extraction model is
developed to efficiently extract relevant concepts from course descriptions to
improve the recommendation process. Using this model, the study examines the
effects of skill-based explanations within a serendipitous recommendation
framework, tested through the AskOski system at the University of California,
Berkeley. The findings indicate that these explanations not only increase user
interest, particularly in courses with high unexpectedness, but also bolster
decision-making confidence. This underscores the importance of integrating
skill-related data and explanations into educational recommendation systems.

</details>


### [102] [ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding](https://arxiv.org/abs/2508.19576)
*Sining Zhoubian,Dan Zhang,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ReST-RL是一个统一的LLM强化学习范式，通过改进的GRPO算法和基于价值模型的测试时解码方法，显著提升LLM的代码推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法因奖励方差不足而失效，基于过程奖励模型的方法存在训练数据获取困难和验证效果不佳的问题。

Method: 采用两阶段方法：1) ReST-GRPO阶段通过优化ReST算法筛选高价值训练数据；2) VM-MCTS阶段通过蒙特卡洛树搜索收集价值目标并训练价值模型，在解码时提供精确的过程信号和验证分数。

Result: 在多个编程基准测试（APPS、BigCodeBench、HumanEval）上显著优于其他强化训练基线和解码验证基线。

Conclusion: ReST-RL能够有效增强LLM策略的推理能力，是解决代码推理问题的有效强化学习范式。

Abstract: With respect to improving the reasoning accuracy of LLMs, the representative
reinforcement learning (RL) method GRPO faces failure due to insignificant
reward variance, while verification methods based on process reward models
(PRMs) suffer from difficulties with training data acquisition and verification
effectiveness. To tackle these problems, this paper introduces ReST-RL, a
unified LLM RL paradigm that significantly improves LLM's code reasoning
ability by combining an improved GRPO algorithm with a meticulously designed
test time decoding method assisted by a value model (VM). As the first stage of
policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter
and assemble high-value training data, increasing the reward variance of GRPO
sampling, thus improving the effectiveness and efficiency of training. After
the basic reasoning ability of LLM policy has been improved, we further propose
a test time decoding optimization method called VM-MCTS. Through Monte-Carlo
Tree Search (MCTS), we collect accurate value targets with no annotation
required, on which VM training is based. When decoding, the VM is deployed by
an adapted MCTS algorithm to provide precise process signals as well as
verification scores, assisting the LLM policy to achieve high reasoning
accuracy. We validate the effectiveness of the proposed RL paradigm through
extensive experiments on coding problems. Upon comparison, our approach
significantly outperforms other reinforcement training baselines (e.g., naive
GRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,
PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,
APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the
reasoning ability of LLM policies. Codes for our project can be found at
https://github.com/THUDM/ReST-RL.

</details>


### [103] [Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties](https://arxiv.org/abs/2508.19611)
*Huaiyuan Yao,Wanpeng Xu,Justin Turnau,Nadia Kellam,Hua Wei*

Main category: cs.AI

TL;DR: Instructional Agents是一个多智能体LLM框架，可自动化生成完整的课程材料，包括教学大纲、讲义、幻灯片和评估，显著减少开发时间和人力工作量。


<details>
  <summary>Details</summary>
Motivation: 高质量教学材料的准备过程劳动密集，需要教学人员、教学设计者和助教之间的广泛协调，现有AI教育工具仅关注孤立任务，缺乏整体协作。

Method: 采用基于角色的多智能体大语言模型框架，模拟教育代理之间的协作，提供四种操作模式：自主模式、目录引导模式、反馈引导模式和全协同模式。

Result: 在五门大学计算机科学课程中评估显示，该系统能生成高质量教学材料，同时显著减少开发时间和人力工作量。

Conclusion: Instructional Agents为教学设计能力有限的机构提供了可扩展且经济高效的框架，有助于在资源受限环境中普及高质量教育。

Abstract: Preparing high-quality instructional materials remains a labor-intensive
process that often requires extensive coordination among teaching faculty,
instructional designers, and teaching assistants. In this work, we present
Instructional Agents, a multi-agent large language model (LLM) framework
designed to automate end-to-end course material generation, including syllabus
creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing
AI-assisted educational tools that focus on isolated tasks, Instructional
Agents simulates role-based collaboration among educational agents to produce
cohesive and pedagogically aligned content. The system operates in four modes:
Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling
flexible control over the degree of human involvement. We evaluate
Instructional Agents across five university-level computer science courses and
show that it produces high-quality instructional materials while significantly
reducing development time and human workload. By supporting institutions with
limited instructional design capacity, Instructional Agents provides a scalable
and cost-effective framework to democratize access to high-quality education,
particularly in underserved or resource-constrained settings.

</details>


### [104] [InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.19679)
*Qihang Ai,Pi Bu,Yue Cao,Yingyao Wang,Jihao Gu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Zhicheng Zheng,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 提出了InquireBench基准测试和InquireMobile模型，通过主动询问用户确认来提升移动代理的安全性，在询问成功率上提升46.8%


<details>
  <summary>Details</summary>
Motivation: 当前完全自主的视觉语言模型移动代理在理解或推理能力不足时存在安全风险，需要开发能够主动寻求用户确认的交互系统

Method: 提出InquireMobile模型，采用强化学习启发的两阶段训练策略和交互式预动作推理机制，在关键决策点主动寻求人类确认

Result: 在InquireBench基准测试上实现了46.8%的询问成功率提升，并在整体成功率上达到现有基线中的最佳表现

Conclusion: 通过主动询问机制显著提升了移动代理的安全性和交互能力，将为学术界和工业界提供开源数据集、模型和评估代码

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled mobile agents
to perceive and interact with real-world mobile environments based on human
instructions. However, the current fully autonomous paradigm poses potential
safety risks when model understanding or reasoning capabilities are
insufficient. To address this challenge, we first introduce
\textbf{InquireBench}, a comprehensive benchmark specifically designed to
evaluate mobile agents' capabilities in safe interaction and proactive inquiry
with users, encompassing 5 categories and 22 sub-categories, where most
existing VLM-based agents demonstrate near-zero performance. In this paper, we
aim to develop an interactive system that actively seeks human confirmation at
critical decision points. To achieve this, we propose \textbf{InquireMobile}, a
novel model inspired by reinforcement learning, featuring a two-stage training
strategy and an interactive pre-action reasoning mechanism. Finally, our model
achieves an 46.8% improvement in inquiry success rate and the best overall
success rate among existing baselines on InquireBench. We will open-source all
datasets, models, and evaluation codes to facilitate development in both
academia and industry.

</details>


### [105] [Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?](https://arxiv.org/abs/2508.19827)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.AI

TL;DR: 研究表明Chain-of-Thought在软推理任务中效果有限且可能不忠实，不同模型对CoT的依赖方式存在差异


<details>
  <summary>Details</summary>
Motivation: 探究CoT在软推理任务中的动态性和忠实性，分析不同模型类型对CoT的依赖差异

Method: 在指令调优模型、推理模型和推理蒸馏模型上进行软推理任务的CoT分析

Result: 发现CoT的影响力和忠实性并不总是一致，不同模型对CoT的依赖方式存在显著差异

Conclusion: CoT在软推理任务中的有效性有限且存在忠实性问题，需要针对不同模型类型进行更深入的研究

Abstract: Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited
gains for soft-reasoning problems such as analytical and commonsense reasoning.
CoT can also be unfaithful to a model's actual reasoning. We investigate the
dynamics and faithfulness of CoT in soft-reasoning tasks across
instruction-tuned, reasoning and reasoning-distilled models. Our findings
reveal differences in how these models rely on CoT, and show that CoT influence
and faithfulness are not always aligned.

</details>


### [106] [Tracking World States with Language Models: State-Based Evaluation Using Chess](https://arxiv.org/abs/2508.19851)
*Romain Harang,Jason Naradowsky,Yaswitha Gujju,Yusuke Miyao*

Main category: cs.AI

TL;DR: 提出了一种模型无关的基于状态的评估框架，使用国际象棋作为基准来评估LLMs是否保持结构化环境的语义，通过分析合法移动分布来估计语义保真度。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在结构化领域展现出涌现能力，暗示其可能内化了世界模型的高保真表示，但现有的探测技术依赖于模型特定的内部激活，限制了可解释性和泛化性。

Method: 使用国际象棋作为基准，分析下游合法移动分布（状态可供性）来估计预测状态与实际游戏状态之间的语义保真度，提供比传统基于字符串指标更有意义的评估。

Result: 实验结果表明该指标能够捕捉状态跟踪中的缺陷，突显了LLMs在长序列中维持连贯内部模型的局限性。

Conclusion: 该框架为评估LLMs的结构化推理提供了强大工具，无需访问内部模型，并可泛化到广泛的符号环境类别。

Abstract: Large Language Models (LLMs) exhibit emergent capabilities in structured
domains, suggesting they may implicitly internalize high-fidelity
representations of world models. While probing techniques have shown promising
signs of this in scientific and game-based settings, they rely on
model-specific internal activations, which limit interpretability and
generalizability. In this work, we propose a model-agnostic, state-based
evaluation framework using chess as a benchmark to assess whether LLMs preserve
the semantics of structured environments. Our method analyzes the downstream
legal move distributions (state affordances) to estimate semantic fidelity
between predicted and actual game states. This approach offers a more
meaningful evaluation than conventional string-based metrics by aligning more
closely with the strategic and rule-governed nature of chess. Experimental
results demonstrate that our metrics capture deficiencies in state-tracking,
highlighting limitations of LLMs in maintaining coherent internal models over
long sequences. Our framework provides a robust tool for evaluating structured
reasoning in LLMs without requiring internal model access, and generalizes to a
wide class of symbolic environments.

</details>


### [107] [CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments](https://arxiv.org/abs/2508.19932)
*Nitish Jaipuria,Lorenzo Gatto,Zijun Kan,Shankey Poddar,Bill Cheung,Diksha Bansal,Ramanan Balakrishnan,Aviral Suri,Jose Estevez*

Main category: cs.AI

TL;DR: 提出了CASE框架，使用对话式AI主动收集诈骗反馈信息，通过LLM提取结构化数据，在Google Pay印度实现后诈骗执法量提升21%


<details>
  <summary>Details</summary>
Motivation: 数字支付平台快速发展带来了便利，但也吸引了恶意行为者，导致复杂的社会工程诈骗增加。现有基于用户和交易的信号不足以全面理解诈骗模式，难以及时预防

Method: 开发CASE对话式AI框架，使用Gemini LLM主动访谈潜在受害者获取详细对话记录，然后通过AI系统提取信息并转换为结构化数据，用于自动和人工执法机制

Result: 在Google Pay印度实施后，诈骗执法量提升了21%，框架具有高度通用性

Conclusion: CASE框架为收集和管理诈骗情报提供了可扩展的解决方案，其架构和评估框架具有高度通用性，可为其他敏感领域构建类似AI驱动系统提供蓝图

Abstract: The proliferation of digital payment platforms has transformed commerce,
offering unmatched convenience and accessibility globally. However, this growth
has also attracted malicious actors, leading to a corresponding increase in
sophisticated social engineering scams. These scams are often initiated and
orchestrated on multiple surfaces outside the payment platform, making user and
transaction-based signals insufficient for a complete understanding of the
scam's methodology and underlying patterns, without which it is very difficult
to prevent it in a timely manner. This paper presents CASE (Conversational
Agent for Scam Elucidation), a novel Agentic AI framework that addresses this
problem by collecting and managing user scam feedback in a safe and scalable
manner. A conversational agent is uniquely designed to proactively interview
potential victims to elicit intelligence in the form of a detailed
conversation. The conversation transcripts are then consumed by another AI
system that extracts information and converts it into structured data for
downstream usage in automated and manual enforcement mechanisms. Using Google's
Gemini family of LLMs, we implemented this framework on Google Pay (GPay)
India. By augmenting our existing features with this new intelligence, we have
observed a 21% uplift in the volume of scam enforcements. The architecture and
its robust evaluation framework are highly generalizable, offering a blueprint
for building similar AI-driven systems to collect and manage scam intelligence
in other sensitive domains.

</details>


### [108] [Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants](https://arxiv.org/abs/2508.19963)
*M. Umlauft,M. Schranz*

Main category: cs.AI

TL;DR: 使用群集智能算法（boids算法）解决半导体工厂中机器类型切换的调度优化问题


<details>
  <summary>Details</summary>
Motivation: 传统线性优化方法无法在合理时间内解决大型半导体工厂的作业车间调度问题，特别是机器类型频繁切换的复杂场景

Method: 采用仿生学的boids群集算法，基于局部信息和简单启发式规则，模拟鸟群避障行为来处理机器类型切换

Result: 算法能够有效应对生产过程中单件加工机器和批量加工机器之间的频繁切换问题

Conclusion: 群集智能算法为大规模生产工厂的优化提供了可行的分布式解决方案，特别适合处理机器类型切换的复杂调度场景

Abstract: Optimizing modern production plants using the job-shop principle is a known
hard problem. For very large plants, like semiconductor fabs, the problem
becomes unsolvable on a plant-wide scale in a reasonable amount of time using
classical linear optimization. An alternative approach is the use of swarm
intelligence algorithms. These have been applied to the job-shop problem
before, but often in a centrally calculated way where they are applied to the
solution space, but they can be implemented in a bottom-up fashion to avoid
global result computation as well. One of the problems in semiconductor
production is that the production process requires a lot of switching between
machines that process lots one after the other and machines that process
batches of lots at once, often with long processing times. In this paper, we
address this switching problem with the ``boids'' flocking algorithm that was
originally used in robotics and movie industry. The flocking behavior is a
bio-inspired algorithm that uses only local information and interaction based
on simple heuristics. We show that this algorithm addresses these valid
considerations in production plant optimization, as it reacts to the switching
of machine kinds similar to how a swarm of flocking animals would react to
obstacles in its course.

</details>


### [109] [SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](https://arxiv.org/abs/2508.20018)
*Quanfeng Lu,Zhantao Ma,Shuai Zhong,Jin Wang,Dahai Yu,Michael K. Ng,Ping Luo*

Main category: cs.AI

TL;DR: SWIRL是一种用于多智能体系统的分阶段强化学习工作流，将MARL重新表述为一系列单智能体强化学习任务，每次更新一个智能体而保持其他智能体固定，实现了稳定训练和高效协调。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体方法在移动GUI代理中存在结构限制，而多智能体强化学习(MARL)方法效率低下且与当前大型视觉语言模型架构不兼容，需要新的解决方案。

Method: 提出SWIRL分阶段交错强化学习工作流，将MARL分解为顺序的单智能体RL任务，包括导航器(将语言和屏幕上下文转换为结构化计划)和交互器(将计划转换为可执行原子动作)。

Result: 在GUI基准测试中表现出优越性能，同时在多智能体数学推理任务中也展示了强大能力，证明了其作为通用多智能体系统开发框架的潜力。

Conclusion: SWIRL提供了一个稳健且有理论保证的多智能体学习框架，能够有效解决移动GUI控制和其他多智能体协作任务中的挑战。

Abstract: The rapid advancement of large vision language models (LVLMs) and agent
systems has heightened interest in mobile GUI agents that can reliably
translate natural language into interface operations. Existing single-agent
approaches, however, remain limited by structural constraints. Although
multi-agent systems naturally decouple different competencies, recent progress
in multi-agent reinforcement learning (MARL) has often been hindered by
inefficiency and remains incompatible with current LVLM architectures. To
address these challenges, we introduce SWIRL, a staged workflow for interleaved
reinforcement learning designed for multi-agent systems. SWIRL reformulates
MARL into a sequence of single-agent reinforcement learning tasks, updating one
agent at a time while keeping the others fixed. This formulation enables stable
training and promotes efficient coordination across agents. Theoretically, we
provide a stepwise safety bound, a cross-round monotonic improvement theorem,
and convergence guarantees on return, ensuring robust and principled
optimization. In application to mobile GUI control, SWIRL instantiates a
Navigator that converts language and screen context into structured plans, and
an Interactor that grounds these plans into executable atomic actions.
Extensive experiments demonstrate superior performance on both high-level and
low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong
capability in multi-agent mathematical reasoning, underscoring its potential as
a general framework for developing efficient and robust multi-agent systems.

</details>


### [110] [Model Science: getting serious about verification, explanation and control of AI systems](https://arxiv.org/abs/2508.20040)
*Przemyslaw Biecek,Wojciech Samek*

Main category: cs.AI

TL;DR: 本文提出了从数据科学向模型科学的范式转变，将训练好的模型置于分析核心，围绕验证、解释、控制和接口四大支柱构建可信、安全、人类对齐的AI系统。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的广泛采用，需要从以数据为中心的方法转向以模型为核心的分析范式，以应对模型在不同操作环境中的行为交互、验证、解释和控制需求。

Method: 提出了模型科学的概念框架，包括四个关键支柱：验证（严格的情境感知评估协议）、解释（探索模型内部操作的各种方法）、控制（整合对齐技术来引导模型行为）和接口（开发交互式可视化解释工具）。

Result: 建立了一个系统性的模型科学框架，为开发可信、安全和人类对齐的AI系统提供了理论指导和方法论基础。

Conclusion: 模型科学框架为AI系统的可信度、安全性和人类对齐性提供了系统性的解决方案，代表了从数据科学向模型科学的重要范式转变。

Abstract: The growing adoption of foundation models calls for a paradigm shift from
Data Science to Model Science. Unlike data-centric approaches, Model Science
places the trained model at the core of analysis, aiming to interact, verify,
explain, and control its behavior across diverse operational contexts. This
paper introduces a conceptual framework for a new discipline called Model
Science, along with the proposal for its four key pillars: Verification, which
requires strict, context-aware evaluation protocols; Explanation, which is
understood as various approaches to explore of internal model operations;
Control, which integrates alignment techniques to steer model behavior; and
Interface, which develops interactive and visual explanation tools to improve
human calibration and decision-making. The proposed framework aims to guide the
development of credible, safe, and human-aligned AI systems.

</details>
