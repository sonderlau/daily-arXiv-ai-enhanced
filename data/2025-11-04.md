<div id=toc></div>

# Table of Contents

- [physics.ao-ph](#physics.ao-ph) [Total: 11]
- [cs.NE](#cs.NE) [Total: 7]
- [cs.CV](#cs.CV) [Total: 183]
- [cs.AI](#cs.AI) [Total: 55]


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [1] [IoT- and AI-informed urban air quality models for vehicle pollution monitoring](https://arxiv.org/abs/2511.00187)
*Jan M. Armengol,Vicente Masip,Ada Barrantes,Gabriel M. Beltrami,Sergi Albiach,Daniel Rodriguez-Rey,Marc Guevara,Albert Soret,Eduardo Quiñones,Elli Kartsakli*

Main category: physics.ao-ph

TL;DR: 该研究将低成本传感器、AI视频交通分析与高分辨率城市空气质量模型相结合，在巴塞罗那路口进行试点部署，通过IoT-边缘-云-HPC架构提升交通相关污染物预测的时间粒度。


<details>
  <summary>Details</summary>
Motivation: 现有研究要么关注基于IoT的空气质量传感，要么关注基于物理的建模，缺乏两者的整合。该工作旨在弥合这一差距，利用动态交通条件和环境变量增强城市空气质量监测。

Method: 在巴塞罗那Eixample区路口部署试点系统，通过边缘计算处理动态交通条件和环境变量数据，并将实时数据输入高性能计算模拟管道，与官方NO2测量值进行验证。

Result: 相比依赖静态排放清单的传统模型，IoT辅助方法显著提高了交通相关污染物预测的时间分辨率，验证结果与官方NO2测量值一致。

Conclusion: 该研究展示了一个可扩展、自适应且保护隐私的城市污染监测解决方案，为下一代IoT驱动的环境智能奠定了基础。

Abstract: With the rise of intelligent Internet of Things (IoT) systems in urban
environments, new opportunities are emerging to enhance real-time environmental
monitoring. While most studies focus either on IoT-based air quality sensing or
physics-based modeling in isolation, this work bridges that gap by integrating
low-cost sensors and AI-powered video-based traffic analysis with
high-resolution urban air quality models. We present a real-world pilot
deployment at a road intersection in Barcelona's Eixample district, where the
system captures dynamic traffic conditions and environmental variables,
processes them at the edge, and feeds real-time data into a high-performance
computing (HPC) simulation pipeline. Results are validated against official air
quality measurements of nitrogen dioxide (NO2). Compared to traditional models
that rely on static emission inventories, the IoT-assisted approach enhances
the temporal granularity of urban air quality predictions of traffic-related
pollutants. Using the full capabilities of an IoT-edge-cloud-HPC architecture,
this work demonstrates a scalable, adaptive, and privacy-conscious solution for
urban pollution monitoring and establishes a foundation for next-generation
IoT-driven environmental intelligence.

</details>


### [2] [Benchmarking Regional Thermodynamic Trends in an AI emulator, ACE2, and a hybrid model, NeuralGCM](https://arxiv.org/abs/2511.00274)
*Katharine Rucker,Ian Baxter,Pedram Hassanzadeh,Tiffany A. Shaw,Hamid A. Pahlavan*

Main category: physics.ao-ph

TL;DR: AI模型在捕捉区域气候趋势方面与物理模型对比，ACE2在垂直温度趋势上表现优异，但在热极端事件和干旱趋势方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 评估AI模型在捕捉具有重要社会影响的区域气候趋势方面的能力，特别是与物理模型对比。

Method: 使用AI模拟器(ACE2)和混合模型(NeuralGCM)对卫星时代的区域热力学趋势进行基准测试，并与物理模型比较。

Result: AI模型能捕捉区域温度趋势如北极放大效应，ACE2在中纬度垂直温度趋势上表现最佳，但在美国西南部热极端事件和干旱区域干燥趋势方面表现不佳。

Conclusion: 数据驱动的AI模拟器在捕捉区域热力学趋势方面可与混合模型和物理模型相媲美甚至更优，但在极端事件和特定区域趋势方面仍有改进空间。

Abstract: AI models have emerged as potential complements to physics-based models, but
their skill in capturing observed regional climate trends with important
societal impacts has not been explored. Here, we benchmark satellite-era
regional thermodynamic trends, including extremes, in an AI emulator (ACE2) and
a hybrid model (NeuralGCM). We also compare the AI models' skill to
physics-based land-atmosphere models. Both AI models show skill in capturing
regional temperature trends such as Arctic Amplification. ACE2 outperforms
other models in capturing vertical temperature trends in the midlatitudes.
However, the AI models do not capture regional trends in heat extremes over the
US Southwest. Furthermore, they do not capture drying trends in arid regions,
even though they generally perform better than physics-based models. Our
results also show that a data-driven AI emulator can perform comparably to, or
better than, hybrid and physics-based models in capturing regional
thermodynamic trends.

</details>


### [3] [Thermoacoustic internal gravity wave turbulence in the Earth's lower atmosphere](https://arxiv.org/abs/2511.00594)
*S. Das Adhikary,A. P. Misra*

Main category: physics.ao-ph

TL;DR: 提出了首个二维模型，研究地球低层大气（0-50公里）中内部重力波与热波的非线性耦合，考虑温度依赖的密度不均匀性和热反馈效应。


<details>
  <summary>Details</summary>
Motivation: 现有文献中的内部重力波（IGWs）模型未能充分考虑温度依赖的密度不均匀性和热反馈效应，特别是在地球低层大气的复杂热分层环境中。

Method: 建立二维非线性耦合模型，通过数值模拟研究内部重力波和热波的非线性相互作用，分析速度、密度扰动和热波动的耦合演化。

Result: 发现热声内部重力波可形成孤立涡旋并导致热声湍流；在对流层（0-15公里）和平流层（15-50公里）观测到不同的波能谱幂律分布；对流层能谱与无温度梯度观测一致，平流层能谱则不同。

Conclusion: 温度梯度对内部重力波与热波的非线性耦合具有重要影响，导致不同大气层中形成不同的湍流结构和能谱特征，这对理解大气动力学具有重要意义。

Abstract: We propose, for the first time, a two-dimensional model for the nonlinear
coupling of internal gravity and thermal waves in the presence of
temperature-dependent density inhomogeneity due to thermal expansion and
thermal feedback in stratified fluids of the Earth's lower atmosphere ($0-50$
km). Such a coupling gives rise to the evolution of thermoacoustic internal
gravity waves (IGWs), which are distinctive from the known IGWs in the
literature. We perform numerical simulations to study the nonlinear
interactions of velocity and density perturbations associated with the IGWs and
thermal fluctuations corresponding to the thermal mode. We show that solitary
vortices of IGWs coupled to the thermal wave can lead to thermoacoustic
turbulence. We observe the formation of large-scale velocity potential flows
and small-scale structures in the density and temperature profiles.
Interestingly, while the wave energy spectra exhibit power laws: $ k_x^{-1.67}$
and $ k_z^{-2.89}$, respectively, for horizontal and vertical wave numbers, in
the troposphere ($0-15$ km) with negative temperature gradient, the same in the
stratosphere ($15-50$ km) with positive temperature gradient tend to relax
toward $k_x^{-1.83}$-horizontal and $k_z^{-1.03}$-vertical spectra. We find
that while the energy spectra in the tropospheric turbulence are consistent
with the observed phenomena without temperature gradients, those in the
stratosphere differ.

</details>


### [4] [Quantifying the radiative response to surface temperature variability: A critical comparison of current methods](https://arxiv.org/abs/2511.00731)
*Leif Fredericks,Maria Rugenstein,David W. J. Thompson,Senne Van Loon,Fabrizio Falasca,Rory Basinski-Ferris,Paulo Ceppi,Quran Wu,Jonah Bloch-Johnson,Marc Alessi,Sarah M. Kang*

Main category: physics.ao-ph

TL;DR: 比较了多种量化"模式效应"的方法，发现这些方法在预测由CO2浓度增加引起的表面温度变化模式时存在显著差异，尽管在内部变率驱动的温度变化预测上表现相似。


<details>
  <summary>Details</summary>
Motivation: 过去十年研究发现辐射响应对表面温度变化的空间结构具有依赖性（模式效应），传统格林函数实验计算成本高，需要比较新兴的统计方法。

Method: 将不同方法应用于相同的预测任务，比较它们在量化模式效应方面的表现和差异。

Result: 大多数方法显示西太平洋地区存在大的负反馈，但在其他区域对反馈符号和空间均匀性的判断存在分歧。所有方法对内部变率驱动的全球辐射响应预测相似，但对CO2强迫的温度变化模式预测差异很大。

Conclusion: 讨论了不同方法间差异的原因，并提出了未来利用这些方法增强对模式效应物理理解的建议路径。

Abstract: Over the past decade, it has become clear that the radiative response to
surface temperature change depends on the spatially varying structure in the
temperature field, a phenomenon known as the "pattern effect". The pattern
effect is commonly estimated from dedicated climate model simulations forced
with local surface temperatures patches (Green's function experiments). Green's
function experiments capture causal influences from temperature perturbations,
but are computationally expensive to run. Recently, however, several methods
have been proposed that estimate the pattern effect through statistical means.
These methods can accurately predict the radiative response to temperature
variations in climate model simulations. The goal of this paper is to compare
methods used to quantify the pattern effect. We apply each method to the same
prediction task and discuss its advantages and disadvantages. Most methods
indicate large negative feedbacks over the western Pacific. Over other regions,
the methods frequently disagree on feedback sign and spatial homogeneity. While
all methods yield similar predictions of the global radiative response to
surface temperature variations driven by internal variability, they produce
very different predictions from the patterns of surface temperature change in
simulations forced with increasing CO2 concentrations. We discuss reasons for
the discrepancies between methods and recommend paths towards using them in the
future to enhance physical understanding of the pattern effect.

</details>


### [5] [Global Kilometer-Scale Simulations with ARP-GEM2: Effect of Parameterized Convection and Calibration](https://arxiv.org/abs/2511.00829)
*Olivier Geoffroy,David Saint-Martin*

Main category: physics.ao-ph

TL;DR: 本文介绍了全球大气模型ARP-GEM第二版及其千米尺度分辨率的校准，重点分析了2.6公里分辨率下有无参数化对流的多年度模拟结果。


<details>
  <summary>Details</summary>
Motivation: 开发千米尺度分辨率下的全球大气模型，研究参数化对流在高分辨率模拟中的作用，探索高分辨率对气候变率表示的价值。

Method: 使用ARP-GEM模型在1.3公里和2.6公里分辨率下进行全球大气模拟，比较有参数化对流和无参数化对流的情况，以夹卷和卷出作为减少对流的主要驱动因素。

Result: 无深对流的模拟类似于具有无限或较大夹卷值的模拟；在千米尺度下，参数化对流对正确表示平均状态仍起重要作用；高分辨率在表示气候变率方面具有一定附加价值。

Conclusion: 需要在平均状态和变率的充分表示之间达成妥协，两者受参数化对流程度的影响不同；可能需要更高分辨率才能获得明确的附加价值。

Abstract: The objective of this paper is twofold. First, it documents the second
version of the global atmospheric model ARP-GEM and its calibration at
kilometer-scale resolution. The model is currently able to run simulations at a
resolution of up to 1.3 km. Second, this paper focus on multi-year global
atmospheric simulations at a 2.6 km resolution with and without parameterized
convection and associated calibration. Simulations without deep convection tend
to be similar to those with infinite, or at least large, entrainment values.
Consistently, entrainment and detrainment are used as primary drivers for the
gradual reduction of convection as resolution increases. The results indicate
that, with this hydrostatic model, parameterized convection still plays a
significant role in the correct representation of the mean state at the
kilometer scale. Additionally, they suggest some added value of high resolution
in representing climate variability. However, a compromise between the adequate
representation of the mean state and variability is necessary, as both are
differently favored by the degree of parameterized convection. Finally, it is
likely that even higher resolutions are necessary to achieve an unequivocal
added value.

</details>


### [6] [Tipping Points and Cascading Transitions: Methods, Principles, and Evidences](https://arxiv.org/abs/2511.01168)
*Sheng Fang,Ziyan Wang,Jürgen Kurths,Jingfang Fan*

Main category: physics.ao-ph

TL;DR: 这篇综述综合了地球系统中临界点和级联转变的最新研究进展，从非线性动力学和复杂性科学的角度分析了这些现象。


<details>
  <summary>Details</summary>
Motivation: 理解地球系统中临界点和级联转变对于预测和应对气候变化等全球环境挑战至关重要，需要系统梳理相关理论和检测方法。

Method: 通过分类临界机制（分岔诱导、噪声诱导和速率诱导）、评估早期预警信号检测方法、使用概念网络模型研究级联失效等方式进行分析。

Result: 研究表明临界元素之间的相互作用会显著增加全球变暖背景下的系统性风险，而人工智能和复杂网络科学在预测和风险评估中具有重要作用。

Conclusion: 尽管面临数据限制和方法稳健性等挑战，但人工智能和复杂网络科学为推进地球系统临界动力学预测和风险评估提供了有前景的途径。

Abstract: This review synthesizes recent advancements in understanding tipping points
and cascading transitions within the Earth system, framing them through the
lens of nonlinear dynamics and complexity science. It outlines the fundamental
concepts of tipping elements, large-scale subsystems like the Atlantic
Meridional Overturning Circulation and the Amazon rainforest, and classifies
tipping mechanisms into bifurcation-, noise-, and rate-induced types. The
article critically evaluates methods for detecting early-warning signals,
particularly those based on critical slowing down, while also acknowledging
their limitations and the promise of non-conventional indicators. Furthermore,
we explore the significant risk of cascading failures between interacting
tipping elements, often modeled using conceptual network models. This shows
that such interactions can substantially increase systemic risk under global
warming. The review concludes by outlining key challenges related to data
limitations and methodological robustness, and emphasizes the promising role of
artificial intelligence and complex network science in advancing prediction and
risk assessment of Earth system tipping dynamics.

</details>


### [7] [Ocean neutral transport: sub-Riemannian geometry and hypoelliptic diffusion](https://arxiv.org/abs/2511.01515)
*Matthieu Chatelain,Isambard Goodbody,Nived Rajeev Saritha,Jacques Vanneste*

Main category: physics.ao-ph

TL;DR: 该论文研究了海洋中水团沿中性平面的优先运动，探讨了中性输运的几何特性及其对海洋三维输运的影响。


<details>
  <summary>Details</summary>
Motivation: 研究海洋水团沿中性平面的运动特性，理解中性输运的几何本质及其在海洋三维输运中的作用。

Method: 结合接触几何和次黎曼几何理论，分析中性平面的不可积性；计算次黎曼测地线；提出随机模型模拟中性输运；使用蒙特卡洛模拟研究短期和长期行为。

Result: 中性输运虽然是局部二维的，但由于中性平面的不可积性，在全局上是三维的；提出了Carnot-Carathéodory距离作为强各向异性诊断；蒙特卡洛模拟估算了海洋垂直输运的时间尺度。

Conclusion: 中性输运的几何特性导致海洋中的三维输运行为，这种输运在短时间和长时间尺度上表现出不同的特征，对理解海洋混合和输运过程具有重要意义。

Abstract: The motion of water parcels in the ocean is thought to be preferentially
along neutral planes defined by climatological potential temperature and
salinity fields. This gives rise to a conceptual model of ocean transport in
which parcel trajectories are everywhere neutral, that is, tangent to the
neutral planes. Because the distribution of neutral planes is not integrable,
neutral transport, while locally two dimensional, is globally three
dimensional. We describe this form of transport, building on its connection
with contact and sub-Riemannian geometry.
  We discuss a Lie-bracket interpretation of local dianeutral transport, the
quantitative meaning of helicity and the implications of the accessibility
theorem. We compute sub-Riemnanian geodesics for climatological neutral planes
and put forward the use of the associated Carnot--Carath\'eodory distance as a
diagnostic of the strong anisotropy of neutral transport.
  We propose a stochastic toy model of neutral transport which represents
motion along neutral planes by a Brownian motion. The corresponding diffusion
process is degenerate and not (strongly) elliptic. The non-integrability of the
neutral planes however ensures that the diffusion is hypoelliptic. As a result,
trajectories are not confined to surfaces but visit the entire
three-dimensional ocean. The short-time behaviour is qualitatively different
from that obtained with a non-degenerate highly anisotropic diffusion. We
examine both short- and long-time behaviours using Monte Carlo simulations. The
simulations provide an estimate for the time scale of ocean vertical transport
implied by the constraint of neutrality.

</details>


### [8] [Wave Attenuation in Drifting Sea Ice: A Mechanistic Model for Observed Decay Profiles](https://arxiv.org/abs/2511.01528)
*Rhys Ransome,Davide Proment,Ian A. Renfrew,Alberto Alberello*

Main category: physics.ao-ph

TL;DR: 本文提出了一个改进的波浪能量衰减模型，考虑了漂移海冰的影响，能够解释观测到的非指数衰减行为。


<details>
  <summary>Details</summary>
Motivation: 现有的波浪-海冰耦合模型预测波浪能量呈指数衰减，但最近的观测结果偏离了这一行为，需要改进理论框架来解释实际观测现象。

Method: 在基于冰水阻力导致波浪能量耗散的框架基础上，考虑漂移海冰的影响，推导出改进的波浪能量衰减模型，并获得解析解。

Result: 解析解能够复现观测到的非指数波浪能量衰减行为，以及南极海冰中有效衰减率的空间演化特征。

Conclusion: 考虑漂移海冰的改进模型比传统指数衰减模型更准确地描述了波浪在海冰过渡区的能量衰减过程。

Abstract: Wave-sea ice interactions shape the transition zone between open ocean and
pack ice in the polar regions. Most theoretical paradigms, implemented in
coupled wave-sea ice models, predict exponential decay of the wave energy but
recent observations deviate from this behaviour. Expanding on a framework based
on wave energy dissipation due to ice-water drag, we account for drifting sea
ice to derive an improved model for wave energy attenuation. Analytical
solutions replicate the observed non-exponential wave energy decay and the
spatial evolution of the effective attenuation rate in Antarctic sea ice.

</details>


### [9] [Do AI models predict storm impacts as accurately as physics-based models? A case study of the February 2020 storm series over the North Atlantic](https://arxiv.org/abs/2511.01665)
*Hilla Afargan-Gerstman,Rachel W. -Y. Wu,Alice Ferrini,Daniela I. V. Domeisen*

Main category: physics.ao-ph

TL;DR: 该研究评估了AI天气预测模型在连续极端风暴事件中的表现，发现AI模型在周尺度上对平均海平面气压的预测优于物理模型，但对物理约束的考虑不足，需要与物理模型结合使用。


<details>
  <summary>Details</summary>
Motivation: 评估AI天气预测模型在连续极端风暴事件序列中的表现能力，这类事件通常伴随多种灾害，而现有研究主要关注平均条件和单一极端事件。

Method: 比较数据驱动模型与物理模型在预测2020年2月英国风暴序列中的表现，分析平均海平面气压和地表风速等关键变量的预测准确性。

Result: AI模型在周尺度上对平均海平面气压的预测优于数值模型，对地表风速的预测也有一定优势，但物理模型中的某些集合成员有时能媲美或超越AI模型。

Conclusion: AI模型在预测连续极端事件方面有优势，但可能忽视物理约束，需要与物理模型集成才能实现可靠的灾害预警预报。

Abstract: The emergence of data-driven weather forecast models provides great promise
for producing faster, computationally cheaper weather forecasts, compared to
physics-based numerical models. However, while the performance of artificial
intelligence (AI) models have been evaluated primarily for average conditions
and single extreme weather events, less is known about their capability to
capture sequences of extreme events, states that are usually accompanied by
multiple hazards. The storm series in February 2020 provides a prime example to
evaluate the performance of AI models for storm impacts. This event was
associated with high surface impacts including intense surface wind speeds and
heavy precipitation, amplified regionally due to the close succession of three
extratropical storms. In this study, we compare the performance of data-driven
models to physics-based models in forecasting the February 2020 storm series
over the United Kingdom. We show that on weekly timescales, AI models tend to
outperform the numerical model in predicting mean sea level pressure (MSLP),
and, to a lesser extent, surface winds. Nevertheless, certain ensemble members
within the physics-based forecast system can perform as well as, or
occasionally outperform, the AI models. Moreover, weaker error correlations
between atmospheric variables suggest that AI models may overlook physical
constraints. This analysis helps to identify gaps and limitations in the
ability of data-driven models to be used for impact warnings, and emphasizes
the need to integrate such models with physics-based approaches for reliable
impact forecasting.

</details>


### [10] [Wave climate on the southwestern coast of Lake Michigan: Perspectives from wave directionality](https://arxiv.org/abs/2511.01681)
*Boyuan Lu,Wei Wang,Chin Wu,Yuli Liu*

Main category: physics.ao-ph

TL;DR: 本研究使用方向波熵(DWE)分析了1979-2023年密歇根湖西南海岸的方向波气候，发现该区域存在强烈的双向性波系统，以北向和南向波为主，对海岸结构设计和形态管理具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 波的方向性对海岸条件和当地生计具有关键影响，需要进行详细分析以理解其对海岸环境的作用。

Method: 使用方向波熵(DWE)方法，分析方向波气候的年际趋势、月变化模式、空间变异和极端波条件。

Result: 研究区域呈现强烈的双向波系统，以北向和南向波为主；年际趋势较少，但存在明显的季节变化，夏季和冬季双向性增强；所有研究点都呈现双向波气候；北向和南向平均有效波高分别为0.566米和0.563米；北向极端波高显著大于南向。

Conclusion: 波的方向性对研究区域海岸结构设计和海岸形态管理具有重要影响，需要在实际应用中予以考虑。

Abstract: Wave directionality plays a critical role in shaping coastal conditions and
influencing local livelihoods, underscoring the importance of conducting
detailed analyses. This study examines directional wave climate along the
southwestern coast of Lake Michigan from 1979 to 2023 using the Directional
Wave Entropy (DWE). Directionality was characterized in terms of inter-annual
trends, monthly patterns, spatial variation, and extreme wave conditions.
Overall, results exhibited a strong bi-directionality, with dominant northern
and southern wave systems along the coast of our study site. A few annual
trends for the inter-annual wave climate were observed, and there is a clear
seasonal variation such that bi-directionality increases in the summer and
winter seasons. As for spatial variation of wave directionality, all locations
in the study sites presented a bi-directional wave climate. The two dominant
directions of wave directionality: northern and southern mean significant wave
heights were also characterized in all locations of study sites as 0.566 and
0.563 meters. Furthermore, the extreme wave heights in the northern direction
are significantly greater than the extreme waves in the southern direction. In
summary, these findings suggest the importance of wave directionality on
coastal structural design and coastal morphology management along the coast of
our study site.

</details>


### [11] [Towards a Unified Data-Driven Boundary Layer Momentum Flux Parameterization for Ocean and Atmosphere](https://arxiv.org/abs/2511.01766)
*Renaud Falga,Sara Shamekh,Laure Zanna*

Main category: physics.ao-ph

TL;DR: 本文提出了一种基于人工神经网络的统一数据驱动参数化方法，用于模拟海洋和大气对流边界层中的湍流动量通量，在单柱大气模型中显著优于传统参数化方案。


<details>
  <summary>Details</summary>
Motivation: 边界层湍流对风场和流场的演化至关重要，影响天气、气候和生物地球化学过程。传统参数化方案难以准确捕捉如逆梯度通量等关键物理特征。

Method: 使用大涡模拟数据离线训练人工神经网络，通过将动量通量剖面归一化为表面值，利用跨流态和流体的自相似结构进行联合训练。

Result: 在单柱大气模型中，ANN参数化在复制边界层风场剖面演化方面表现优于基线参数化，特别是在对流条件下，误差减少了2-3倍，且对表面动量通量偏差具有鲁棒性。

Conclusion: 这项工作展示了机器学习在气候模型中创建跨边界层系统的统一且物理一致的参数化的潜力。

Abstract: Boundary layer turbulence, particularly the vertical fluxes of momentum,
shapes the evolution of winds and currents and plays a critical role in
weather, climate, and biogeochemical processes. In this work, a unified,
data-driven parameterization of turbulent momentum fluxes is introduced for
both the oceanic and atmospheric convective boundary layers. An artificial
neural network (ANN) is trained offline on coarse-grained large-eddy simulation
(LES) data representing a wide range of turbulent regimes in both fluids. By
normalizing momentum flux profiles with their surface values, we exploit a
self-similar structure across regimes and fluids, enabling joint training. The
ANN learns to predict vertical profiles of subgrid momentum fluxes from mean
wind or current profiles, capturing key physical features such as upgradient
fluxes that are inaccessible to traditional first-order closure schemes. When
implemented online in the Single Column Atmospheric Model (SCAM), the ANN
parameterization consistently outperforms the SCAM baseline parameterization in
replicating the evolution of the boundary layer wind profiles from the LES,
especially under convective conditions, with errors reduced by a factor of 2-3
across regimes. ANN performance remains robust even when the surface momentum
flux is biased up to 30\%, and generalization is confirmed by testing on LES
cases excluded from the training dataset. This work demonstrates the potential
of machine learning to create unified and physically consistent
parameterizations across boundary layer systems in climate models.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [12] [Bio-Inspired Neuron Synapse Optimization for Adaptive Learning and Smart Decision-Making](https://arxiv.org/abs/2511.00042)
*Sreeja Singh,Tamal Ghosh*

Main category: cs.NE

TL;DR: 提出了一种受神经机制启发的神经元突触优化算法(NSO)，通过基于适应度的突触权重更新、自适应剪枝和全局/局部最优解的双重指导来改进优化性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统优化方法在复杂高维多模态搜索空间中容易陷入局部最优、收敛速度慢和大规模环境效率低的问题。

Method: 设计NSO算法，包含基于适应度的突触权重更新、自适应剪枝减少计算开销、全局和局部最优解双重指导平衡探索与开发。使用CEC 2014测试套件与HOA等流行元启发式算法进行基准测试。

Result: NSO在收敛速度、鲁棒性和可扩展性方面持续优于HOA和其他主要算法，在复杂高维搜索空间中表现出卓越的适应性和效率。

Conclusion: NSO通过神经启发机制与动态资源分配的独特结合，在提高搜索性能的同时降低计算成本，为动态多目标优化、机器学习超参数调优和实际工程问题提供了有前景的解决方案。

Abstract: Purpose: Optimization challenges in science, engineering, and real-world
applications often involve complex, high-dimensional, and multimodal search
spaces. Traditional optimization methods frequently struggle with local optima
entrapment, slow convergence, and inefficiency in large-scale environments.
This study aims to address these limitations by proposing a novel optimization
algorithm inspired by neural mechanisms. Design/methodology/approach: The paper
introduces Neuron Synapse Optimization (NSO), a new metaheuristic algorithm
inspired by neural interactions. NSO features key innovations such as
fitness-based synaptic weight updates to improve search influence, adaptive
pruning to minimize computational overhead, and dual guidance from global and
local best solutions to balance exploration and exploitation. The algorithm was
benchmarked against popular metaheuristics and the recently published
Hippopotamus Optimization Algorithm (HOA) using the CEC 2014 test suite,
encompassing unimodal, multimodal, and composition function landscapes.
Findings: Benchmark results reveal that NSO consistently outperforms HOA and
other major algorithms in terms of convergence speed, robustness, and
scalability. NSO demonstrates superior adaptability and efficiency,
particularly in complex, high-dimensional search spaces. Originality: NSO
introduces a unique blend of neural-inspired mechanisms with dynamic resource
allocation, setting it apart from existing algorithms. Its innovative design
enhances search performance while reducing computational cost. With promising
applications in technology, healthcare, data science, and engineering, NSO
paves the way for future research into dynamic and multi-objective
optimization, machine learning hyperparameter tuning, and real-world
engineering design problems.

</details>


### [13] [Node Preservation and its Effect on Crossover in Cartesian Genetic Programming](https://arxiv.org/abs/2511.00634)
*Mark Kocherovsky,Illya Bakurov,Wolfgang Banzhaf*

Main category: cs.NE

TL;DR: 本文比较了CGP中不同交叉和变异算子的性能，发现节点保护机制在交叉和变异中都能提升符号回归基准问题的搜索性能。


<details>
  <summary>Details</summary>
Motivation: 虽然交叉在其他形式的遗传编程中是关键组件，但在CGP中一直被声称会恶化搜索性能，因此需要寻找通用的解决方案。

Method: 比较了基本交叉方法（单点和均匀交叉）与节点保护变体，以及节点变异与传统点变异的性能差异。

Result: 节点保护机制在交叉和变异中都能改善搜索性能，特别是在符号回归基准问题上表现更好。

Conclusion: 节点保护机制为CGP交叉问题提供了向通用解决方案迈进的途径。

Abstract: While crossover is a critical and often indispensable component in other
forms of Genetic Programming, such as Linear- and Tree-based, it has
consistently been claimed that it deteriorates search performance in CGP. As a
result, a mutation-alone $(1+\lambda)$ evolutionary strategy has become the
canonical approach for CGP. Although several operators have been developed that
demonstrate an increased performance over the canonical method, a general
solution to the problem is still lacking. In this paper, we compare basic
crossover methods, namely one-point and uniform, to variants in which nodes are
``preserved,'' including the subgraph crossover developed by Roman Kalkreuth,
the difference being that when ``node preservation'' is active, crossover is
not allowed to break apart instructions. We also compare a node mutation
operator to the traditional point mutation; the former simply replaces an
entire node with a new one. We find that node preservation in both mutation and
crossover improves search using symbolic regression benchmark problems, moving
the field towards a general solution to CGP crossover.

</details>


### [14] [FeNN-DMA: A RISC-V SoC for SNN acceleration](https://arxiv.org/abs/2511.00732)
*Zainab Aizaz,James C. Knight,Thomas Nowotny*

Main category: cs.NE

TL;DR: 开发了一个基于RISC-V的FPGA片上系统FeNN-DMA，专门用于模拟脉冲神经网络，在保持与现有固定功能加速器相当资源使用和能耗的同时，能够处理更大更复杂的模型，并在多个任务上达到最先进的分类精度。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络是标准人工神经网络的有前景的节能替代方案，特别适合时空任务，但SNNs的算术强度较低，不适合GPU和TPU等标准加速器。FPGA适合处理这类内存受限工作负载。

Method: 开发了基于RISC-V的完全可编程片上系统FeNN-DMA，专门针对现代UltraScale+ FPGA优化，用于模拟SNNs。

Result: FeNN-DMA在资源使用和能耗方面与最先进的固定功能SNN加速器相当，但能够模拟更大更复杂的模型，在Spiking Heidelberg Digits和Neuromorphic MNIST任务上达到了最先进的分类精度。

Conclusion: 基于RISC-V的FPGA片上系统FeNN-DMA为SNN模拟提供了一种高效且可扩展的解决方案，在保持低能耗的同时实现了高性能。

Abstract: Spiking Neural Networks (SNNs) are a promising, energy-efficient alternative
to standard Artificial Neural Networks (ANNs) and are particularly well-suited
to spatio-temporal tasks such as keyword spotting and video classification.
However, SNNs have a much lower arithmetic intensity than ANNs and are
therefore not well-matched to standard accelerators like GPUs and TPUs. Field
Programmable Gate Arrays(FPGAs) are designed for such memory-bound workloads
and here we develop a novel, fully-programmable RISC-V-based system-on-chip
(FeNN-DMA), tailored to simulating SNNs on modern UltraScale+ FPGAs. We show
that FeNN-DMA has comparable resource usage and energy requirements to
state-of-the-art fixed-function SNN accelerators, yet it is capable of
simulating much larger and more complex models. Using this functionality, we
demonstrate state-of-the-art classification accuracy on the Spiking Heidelberg
Digits and Neuromorphic MNIST tasks.

</details>


### [15] [Trust Region-Based Bayesian Optimisation to Discover Diverse Solutions](https://arxiv.org/abs/2511.00750)
*Kokila Kasuni Perera,Frank Neumann,Aneta Neumann*

Main category: cs.NE

TL;DR: 本文提出了一种基于信任区域的贝叶斯优化方法divTuRBO1，用于在保持与参考解集给定距离阈值的同时寻找最优解，并通过顺序和交错两种方式组合divTuRBO1运行来寻找多样化解。


<details>
  <summary>Details</summary>
Motivation: 受信任区域方法提升贝叶斯优化可扩展性的研究启发，探索基于信任区域的BO算法在不同维度黑盒问题中的多样性优化效果。

Method: 扩展TuRBO1为divTuRBO1，在保持与参考解集距离阈值的同时寻找最优解；提出顺序和交错两种组合divTuRBO1运行的方法来寻找多样化解。

Result: 在2到20维基准函数上的实验表明，所提方法表现良好，特别是在更高维度且评估预算有限的情况下。

Conclusion: 基于信任区域的多样性优化方法在解决高维黑盒问题时具有良好性能，即使在有限评估预算下也能有效工作。

Abstract: Bayesian optimisation (BO) is a surrogate-based optimisation technique that
efficiently solves expensive black-box functions with small evaluation budgets.
Recent studies consider trust regions to improve the scalability of BO
approaches when the problem space scales to more dimensions. Motivated by this
research, we explore the effectiveness of trust region-based BO algorithms for
diversity optimisation in different dimensional black box problems. We propose
diversity optimisation approaches extending TuRBO1, which is the first BO
method that uses a trust region-based approach for scalability. We extend
TuRBO1 as divTuRBO1, which finds an optimal solution while maintaining a given
distance threshold relative to a reference solution set. We propose two
approaches to find diverse solutions for black-box functions by combining
divTuRBO1 runs in a sequential and an interleaving fashion. We conduct
experimental investigations on the proposed algorithms and compare their
performance with that of the baseline method, ROBOT (rank-ordered Bayesian
optimisation with trust regions). We evaluate proposed algorithms on benchmark
functions with dimensions 2 to 20. Experimental investigations demonstrate that
the proposed methods perform well, particularly in larger dimensions, even with
a limited evaluation budget.

</details>


### [16] [Automatic Policy Search using Population-Based Hyper-heuristics for the Integrated Procurement and Perishable Inventory Problem](https://arxiv.org/abs/2511.00762)
*Leonardo Kanashiro Felizardo,Edoardo Fadda,Mariá Cristina Vasconcelos Nascimento*

Main category: cs.NE

TL;DR: 比较两种易腐品库存优化策略：全局调参的经典策略vs基于元启发式算法的超启发式策略，后者通过逐项构建复合策略获得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 解决易腐品库存管理中的多重不确定性（随机需求、供应商不可靠、产品保质期概率性），传统策略难以有效处理这种复杂问题

Method: 开发离散事件仿真环境，比较两种策略：1）优化经典策略参数并选择最佳供应商；2）超启发式方法使用遗传算法和粒子群优化逐项构建复合策略

Result: 在12个不同实例中，超启发式框架始终找到更优策略，遗传算法和增强遗传算法表现最佳

Conclusion: 逐项策略构建相比简单全局策略能带来显著性能提升，证明了相关计算成本的合理性

Abstract: This paper addresses the problem of managing perishable inventory under
multiple sources of uncertainty, including stochastic demand, unreliable
supplier fulfillment, and probabilistic product shelf life. We develop a
discrete-event simulation environment to compare two optimization strategies
for this multi-item, multi-supplier problem. The first strategy optimizes
uniform classic policies (e.g., Constant Order and Base Stock) by tuning their
parameters globally, complemented by a direct search to select the best-fitting
suppliers for the integrated problem. The second approach is a hyper-heuristic
approach, driven by metaheuristics such as a Genetic Algorithm (GA) and
Particle Swarm Optimization (PSO). This framework constructs a composite policy
by automating the selection of the heuristic type, its parameters, and the
sourcing suppliers on an item-by-item basis. Computational results from twelve
distinct instances demonstrate that the hyper-heuristic framework consistently
identifies superior policies, with GA and EGA exhibiting the best overall
performance. Our primary contribution is verifying that this item-level policy
construction yields significant performance gains over simpler global policies,
thereby justifying the associated computational cost.

</details>


### [17] [A High-Throughput Spiking Neural Network Processor Enabling Synaptic Delay Emulation](https://arxiv.org/abs/2511.01158)
*Faquan Chen,Qingyang Tian,Ziren Wu,Rendong Ying,Fei Wen,Peilin Liu*

Main category: cs.NE

TL;DR: 提出了一种支持突触延迟仿真的高吞吐量脉冲神经网络处理器，用于边缘应用，在FPGA平台上实现并在SHD基准测试中达到93.4%的准确率和104样本/秒的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 突触延迟在神经网络动力学中对于整合和处理复杂时空信息具有重要意义，需要开发支持突触延迟仿真的高效SNN处理器用于边缘应用。

Method: 采用多核流水线架构和并行计算引擎，在PYNQ Z2 FPGA平台上开发SoC原型，能够实时处理与突触延迟相关的计算负载。

Result: 在125MHz典型工作频率和282mW功耗下，处理器在SHD基准测试中达到93.4%的部署准确率和104样本/秒的平均吞吐量。

Conclusion: 该高吞吐量SNN处理器成功实现了支持突触延迟仿真的边缘应用，在低功耗关键词检测任务中表现出良好的性能和效率。

Abstract: Synaptic delay has attracted significant attention in neural network dynamics
for integrating and processing complex spatiotemporal information. This paper
introduces a high-throughput Spiking Neural Network (SNN) processor that
supports synaptic delay-based emulation for edge applications. The processor
leverages a multicore pipelined architecture with parallel compute engines,
capable of real-time processing of the computational load associated with
synaptic delays. We develop a SoC prototype of the proposed processor on PYNQ
Z2 FPGA platform and evaluate its performance using the Spiking Heidelberg
Digits (SHD) benchmark for low-power keyword spotting tasks. The processor
achieves 93.4% accuracy in deployment and an average throughput of 104
samples/sec at a typical operating frequency of 125 MHz and 282 mW power
consumption.

</details>


### [18] [Space as Time Through Neuron Position Learning](https://arxiv.org/abs/2511.01632)
*Balázs Mészáros,James C. Knight,Danyal Akarca,Thomas Nowotny*

Main category: cs.NE

TL;DR: 提出了一种神经元位置学习算法，将空间嵌入和可学习突触延迟统一起来，使延迟与神经元间的欧几里得距离相关。


<details>
  <summary>Details</summary>
Motivation: 生物神经网络存在于物理空间中，距离决定通信延迟，这种基本的时空耦合在大多数人工神经网络中缺失。

Method: 推导了关于神经元位置的梯度，在时间分类任务上训练网络，使延迟与神经元间的欧几里得距离相关。

Result: 网络自发自组织成本地小世界拓扑结构，在距离依赖连接成本下出现模块化结构，观察到无提示的功能专业化与空间聚类对齐。

Conclusion: 这些发现为时空内在耦合的网络奠定了基础，为机械可解释性、生物启发建模和高效实现提供了新途径。

Abstract: Biological neural networks exist in physical space where distance determines
communication delays: a fundamental space-time coupling absent in most
artificial neural networks. While recent work has separately explored spatial
embeddings and learnable synaptic delays in spiking neural networks, we unify
these approaches through a novel neuron position learning algorithm where
delays relate to the Euclidean distances between neurons. We derive gradients
with respect to neuron positions and demonstrate that this
biologically-motivated constraint acts as an inductive bias: networks trained
on temporal classification tasks spontaneously self-organize into local,
small-world topologies with modular structure emerging under distance-dependent
connection costs. Remarkably, we observe unprompted functional specialization
aligned with spatial clustering without explictly enforcing it. These findings
lay the groundwork for networks in which space and time are intrinsically
coupled, offering new avenues for mechanistic interpretability, biologically
inspired modelling, and efficient implementations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [19] [Generative human motion mimicking through feature extraction in denoising diffusion settings](https://arxiv.org/abs/2511.00011)
*Alexander Okupnik,Johannes Schneider,Kyriakos Flouris*

Main category: cs.CV

TL;DR: 该论文提出了一个基于运动捕捉数据的交互式AI模型，用于生成与人类舞者互动的创造性舞蹈动作，结合了扩散模型、运动修复和风格迁移技术。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型缺乏具身交互能力，舞蹈作为人类表达的基本形式，可以补充这种体验。探索以舞蹈为例的创造性人机交互。

Method: 利用单人运动数据和高层次特征，结合两个扩散模型、运动修复和运动风格迁移，生成时间连贯且对选定运动参考有响应的运动表示。

Result: 通过定量评估生成样本特征分布与测试集的收敛性，证明模型成功。生成的舞蹈动作既多样化，又与现实相似，显示出与人类舞伴的各种偏离。

Conclusion: 该模型是实现与AI创造性舞蹈互动的初步步骤，生成的舞蹈动作既真实又具有创造性偏离。

Abstract: Recent success with large language models has sparked a new wave of verbal
human-AI interaction. While such models support users in a variety of creative
tasks, they lack the embodied nature of human interaction. Dance, as a primal
form of human expression, is predestined to complement this experience. To
explore creative human-AI interaction exemplified by dance, we build an
interactive model based on motion capture (MoCap) data. It generates an
artificial other by partially mimicking and also "creatively" enhancing an
incoming sequence of movement data. It is the first model, which leverages
single-person motion data and high level features in order to do so and, thus,
it does not rely on low level human-human interaction data. It combines ideas
of two diffusion models, motion inpainting, and motion style transfer to
generate movement representations that are both temporally coherent and
responsive to a chosen movement reference. The success of the model is
demonstrated by quantitatively assessing the convergence of the feature
distribution of the generated samples and the test set which serves as
simulating the human performer. We show that our generations are first steps to
creative dancing with AI as they are both diverse showing various deviations
from the human partner while appearing realistic.

</details>


### [20] [Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets](https://arxiv.org/abs/2511.00021)
*Julio Jerison E. Macrohon,Gordon Hung*

Main category: cs.CV

TL;DR: 提出基于机器学习的珊瑚白化分类系统，使用CNN模型在多样化全球数据集上达到88%准确率，优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁面临污染、海洋酸化和海水温度异常等日益严重的威胁，急需高效的保护和监测方法。

Method: 使用包含健康和白化珊瑚样本的多样化全球数据集，比较ResNet、ViT和CNN三种最先进模型，并进行全面的超参数调优。

Result: CNN模型表现最佳，达到88%的准确率，超越了现有基准。

Conclusion: 研究为自主珊瑚监测提供了重要见解，并对最广泛使用的计算机视觉模型进行了全面分析。

Abstract: Coral reefs support numerous marine organisms and are an important source of
coastal protection from storms and floods, representing a major part of marine
ecosystems. However coral reefs face increasing threats from pollution, ocean
acidification, and sea temperature anomalies, making efficient protection and
monitoring heavily urgent. Therefore, this study presents a novel
machine-learning-based coral bleaching classification system based on a diverse
global dataset with samples of healthy and bleached corals under varying
environmental conditions, including deep seas, marshes, and coastal zones. We
benchmarked and compared three state-of-the-art models: Residual Neural Network
(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).
After comprehensive hyperparameter tuning, the CNN model achieved the highest
accuracy of 88%, outperforming existing benchmarks. Our findings offer
important insights into autonomous coral monitoring and present a comprehensive
analysis of the most widely used computer vision models.

</details>


### [21] [Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline](https://arxiv.org/abs/2511.00022)
*Jules Gerard,Leandro Di Bella,Filip Huyghe,Marc Kochzius*

Main category: cs.CV

TL;DR: 使用YOLOv8深度学习管道自动化识别西印度洋珊瑚礁鱼类，在肯尼亚和坦桑尼亚视频样带中测试24个鱼类科别，为区域自动化监测提供首个基准。


<details>
  <summary>Details</summary>
Motivation: 西印度洋珊瑚礁监测受限于水下视觉普查的劳动力需求，需要开发自动化方法来补充传统监测方法。

Method: 基于YOLOv8的深度学习管道，在肯尼亚和坦桑尼亚收集的视频样带上进行24个鱼类科别的识别测试，采用不同配置评估性能。

Result: 最佳模型mAP@0.5达到0.52，对丰富科别识别准确率高，但对稀有或复杂类群检测较弱。

Conclusion: 深度学习作为传统监测方法的可扩展补充具有潜力，能够提升珊瑚礁监测效率。

Abstract: Coral reef monitoring in the Western Indian Ocean is limited by the labor
demands of underwater visual censuses. This work evaluates a YOLOv8-based deep
learning pipeline for automating family-level fish identification from video
transects collected in Kenya and Tanzania. A curated dataset of 24 families was
tested under different configurations, providing the first region-specific
benchmark for automated reef fish monitoring in the Western Indian Ocean. The
best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families
but weaker detection of rare or complex taxa. Results demonstrate the potential
of deep learning as a scalable complement to traditional monitoring methods.

</details>


### [22] [Mutual Information guided Visual Contrastive Learning](https://arxiv.org/abs/2511.00028)
*Hanyang Chen,Yanchao Yang*

Main category: cs.CV

TL;DR: 提出了一种基于互信息的数据增强方法，通过选择在自然扰动下具有高互信息的场景补丁作为对比学习的正样本，提升特征学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有InfoNCE损失方法虽然能减少人工标注，但数据选择和增强仍依赖人工假设或工程，可能不是最优的。特别是在对比学习中，数据增强主要关注颜色抖动来模拟真实世界光照变化。

Method: 基于真实世界分布计算互信息来选择训练数据，选择在自然扰动（如颜色变化和运动）下具有高互信息的场景补丁作为对比学习的正样本。

Result: 在多个基准测试和最先进的表示学习框架上评估，证明了该方法的有效性。

Conclusion: 基于互信息的数据增强方法是一个有前景的研究方向，能够提升学习特征在开放环境中的泛化能力。

Abstract: Representation learning methods utilizing the InfoNCE loss have demonstrated
considerable capacity in reducing human annotation effort by training invariant
neural feature extractors. Although different variants of the training
objective adhere to the information maximization principle between the data and
learned features, data selection and augmentation still rely on human
hypotheses or engineering, which may be suboptimal. For instance, data
augmentation in contrastive learning primarily focuses on color jittering,
aiming to emulate real-world illumination changes. In this work, we investigate
the potential of selecting training data based on their mutual information
computed from real-world distributions, which, in principle, should endow the
learned features with better generalization when applied in open environments.
Specifically, we consider patches attached to scenes that exhibit high mutual
information under natural perturbations, such as color changes and motion, as
positive samples for learning with contrastive loss. We evaluate the proposed
mutual-information-informed data augmentation method on several benchmarks
across multiple state-of-the-art representation learning frameworks,
demonstrating its effectiveness and establishing it as a promising direction
for future research.

</details>


### [23] [Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra](https://arxiv.org/abs/2511.00037)
*Riya Gupta,Alexander Chowdhury,Sahil Nalawade*

Main category: cs.CV

TL;DR: 本文对三种联邦学习框架（NVIDIA FLARE、Flower和Owkin Substra）在医学影像应用中的适用性进行了基准测试，评估了模型性能、收敛效率、通信开销、可扩展性和开发者体验。


<details>
  <summary>Details</summary>
Motivation: 联邦学习已成为医学AI中的重要范式，能够在机构间进行协作模型训练而无需直接共享数据。本研究旨在评估主流FL框架在真实医疗环境中的适用性。

Method: 使用PathMNIST数据集，对三种FL框架（NVIDIA FLARE、Flower、Owkin Substra）进行基准测试，评估模型性能、收敛效率、通信开销、可扩展性和开发者体验。

Result: NVIDIA FLARE在生产可扩展性方面表现最佳，Flower在原型设计和学术研究方面提供灵活性，Owkin Substra在隐私和合规性方面具有突出优势。

Conclusion: 每个框架都有针对不同使用场景优化的优势，强调了它们在医疗环境实际部署中的相关性。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in medical
AI, enabling collaborative model training across institutions without direct
data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,
Flower, and Owkin Substra to evaluate their suitability for medical imaging
applications in real-world settings. Using the PathMNIST dataset, we assess
model performance, convergence efficiency, communication overhead, scalability,
and developer experience. Results indicate that NVIDIA FLARE offers superior
production scalability, Flower provides flexibility for prototyping and
academic research, and Owkin Substra demonstrates exceptional privacy and
compliance features. Each framework exhibits strengths optimized for distinct
use cases, emphasizing their relevance to practical deployment in healthcare
environments.

</details>


### [24] [Enhancing rice leaf images: An overview of image denoising techniques](https://arxiv.org/abs/2511.00046)
*Rupjyoti Chutia,Dibya Jyoti Bora*

Main category: cs.CV

TL;DR: 本文对水稻叶片图像进行了图像去噪和对比度增强方法的比较研究，结合了多种去噪方法与CLAHE技术，通过多种指标评估了增强效果。


<details>
  <summary>Details</summary>
Motivation: 图像增强是图像处理中的重要预处理步骤，对于水稻叶片分析（如病害检测、营养缺乏评估和生长分析）至关重要。去噪和对比度增强是主要步骤，能提高后续任务的可靠性。

Method: 采用多种知名图像去噪方法结合CLAHE（对比度受限自适应直方图均衡化）技术，在水稻叶片图像数据集上进行实验，使用多种指标全面测试增强方法。

Result: 通过多种指标对增强方法进行了全面测试，为评估数字图像处理方法的效果提供了坚实基础。

Conclusion: 该方法为数字图像处理方法有效性的评估提供了有力基础，并为农业研究和其他领域的未来应用提供了有价值的见解。

Abstract: Digital image processing involves the systematic handling of images using
advanced computer algorithms, and has gained significant attention in both
academic and practical fields. Image enhancement is a crucial preprocessing
stage in the image-processing chain, improving image quality and emphasizing
features. This makes subsequent tasks (segmentation, feature extraction,
classification) more reliable. Image enhancement is essential for rice leaf
analysis, aiding in disease detection, nutrient deficiency evaluation, and
growth analysis. Denoising followed by contrast enhancement are the primary
steps. Image filters, generally employed for denoising, transform or enhance
visual characteristics like brightness, contrast, and sharpness, playing a
crucial role in improving overall image quality and enabling the extraction of
useful information. This work provides an extensive comparative study of
well-known image-denoising methods combined with CLAHE (Contrast Limited
Adaptive Histogram Equalization) for efficient denoising of rice leaf images.
The experiments were performed on a rice leaf image dataset to ensure the data
is relevant and representative. Results were examined using various metrics to
comprehensively test enhancement methods. This approach provides a strong basis
for assessing the effectiveness of methodologies in digital image processing
and reveals insights useful for future adaptation in agricultural research and
other domains.

</details>


### [25] [Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?](https://arxiv.org/abs/2511.00060)
*Zhiqi Qi,Runxin Zhao,Hanyang Zhuang,Chunxiang Wang,Ming Yang*

Main category: cs.CV

TL;DR: 该研究系统分析了不同LiDAR扫描模式（重复式与非重复式）对路边感知性能的影响，创建了InfraLiDARs基准数据集，发现非重复式LiDAR在成本效益方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LiDAR的优化布置，但不同扫描模式对感知性能的深远影响尚未得到充分研究，特别是在基础设施环境中。

Method: 在CARLA仿真环境中创建InfraLiDARs基准数据集，使用同时运行的基础设施LiDAR，包含两种扫描模式，并进行全面的统计分析和3D目标检测算法评估。

Result: 非重复式扫描LiDAR和128线重复式LiDAR在各种场景下表现出相当的检测性能。非重复式LiDAR虽然感知范围有限，但成本效益高。

Conclusion: 研究为设置具有最优LiDAR扫描模式和兼容算法的路边感知系统提供了见解，并公开发布基准数据集以促进进一步研究。

Abstract: LiDAR-based roadside perception is a cornerstone of advanced Intelligent
Transportation Systems (ITS). While considerable research has addressed optimal
LiDAR placement for infrastructure, the profound impact of differing LiDAR
scanning patterns on perceptual performance remains comparatively
under-investigated. The inherent nature of various scanning modes - such as
traditional repetitive (mechanical/solid-state) versus emerging non-repetitive
(e.g. prism-based) systems - leads to distinct point cloud distributions at
varying distances, critically dictating the efficacy of object detection and
overall environmental understanding. To systematically investigate these
differences in infrastructure-based contexts, we introduce the "InfraLiDARs'
Benchmark," a novel dataset meticulously collected in the CARLA simulation
environment using concurrently operating infrastructure-based LiDARs exhibiting
both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive
statistical analysis of the respective LiDAR scanning abilities and evaluate
the impact of these distinct patterns on the performance of various leading 3D
object detection algorithms. Our findings reveal that non-repetitive scanning
LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable
detection performance across various scenarios. Despite non-repetitive LiDAR's
limited perception range, it's a cost-effective option considering its low
price. Ultimately, this study provides insights for setting up roadside
perception system with optimal LiDAR scanning patterns and compatible
algorithms for diverse roadside applications, and publicly releases the
"InfraLiDARs' Benchmark" dataset to foster further research.

</details>


### [26] [World Simulation with Video Foundation Models for Physical AI](https://arxiv.org/abs/2511.00062)
*NVIDIA,:,Arslan Ali,Junjie Bai,Maciej Bala,Yogesh Balaji,Aaron Blakeman,Tiffany Cai,Jiaxin Cao,Tianshi Cao,Elizabeth Cha,Yu-Wei Chao,Prithvijit Chattopadhyay,Mike Chen,Yongxin Chen,Yu Chen,Shuai Cheng,Yin Cui,Jenna Diamond,Yifan Ding,Jiaojiao Fan,Linxi Fan,Liang Feng,Francesco Ferroni,Sanja Fidler,Xiao Fu,Ruiyuan Gao,Yunhao Ge,Jinwei Gu,Aryaman Gupta,Siddharth Gururani,Imad El Hanafi,Ali Hassani,Zekun Hao,Jacob Huffman,Joel Jang,Pooya Jannaty,Jan Kautz,Grace Lam,Xuan Li,Zhaoshuo Li,Maosheng Liao,Chen-Hsuan Lin,Tsung-Yi Lin,Yen-Chen Lin,Huan Ling,Ming-Yu Liu,Xian Liu,Yifan Lu,Alice Luo,Qianli Ma,Hanzi Mao,Kaichun Mo,Seungjun Nah,Yashraj Narang,Abhijeet Panaskar,Lindsey Pavao,Trung Pham,Morteza Ramezanali,Fitsum Reda,Scott Reed,Xuanchi Ren,Haonan Shao,Yue Shen,Stella Shi,Shuran Song,Bartosz Stefaniak,Shangkun Sun,Shitao Tang,Sameena Tasmeen,Lyne Tchapmi,Wei-Cheng Tseng,Jibin Varghese,Andrew Z. Wang,Hao Wang,Haoxiang Wang,Heng Wang,Ting-Chun Wang,Fangyin Wei,Jiashu Xu,Dinghao Yang,Xiaodong Yang,Haotian Ye,Seonghyeon Ye,Xiaohui Zeng,Jing Zhang,Qinsheng Zhang,Kaiwen Zheng,Andrew Zhu,Yuke Zhu*

Main category: cs.CV

TL;DR: Cosmos-Predict2.5是基于流架构的物理AI世界基础模型，统一了文本/图像/视频到世界的生成，结合Cosmos-Reason1提供更丰富的文本基础和精细的世界模拟控制。模型在2B和14B规模发布，相比前代在视频质量和指令对齐方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 开发更可靠的人工智能系统需要高质量的合成数据生成、策略评估和闭环模拟能力。现有模型在视频质量、指令对齐和世界模拟控制方面存在不足，需要新一代基础模型来支持机器人学和自主系统的研究。

Method: 采用基于流的架构，统一文本/图像/视频到世界的生成；利用Cosmos-Reason1物理AI视觉语言模型增强文本基础和模拟控制；在2亿个精选视频片段上训练，并通过强化学习进行后训练优化。

Result: 相比Cosmos-Predict1，在视频质量和指令对齐方面取得显著改进；Cosmos-Transfer2.5虽然比前代小3.5倍，但提供了更高的保真度和鲁棒的长时视频生成能力。

Conclusion: Cosmos-Predict2.5和Cosmos-Transfer2.5为具身智能的规模化提供了多功能工具，通过开源代码、预训练检查点和基准测试，降低了物理AI研究和部署的门槛。

Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World
Foundation Models for Physical AI. Built on a flow-based architecture,
[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation
in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language
model, to provide richer text grounding and finer control of world simulation.
Trained on 200M curated video clips and refined with reinforcement
learning-based post-training, [Cosmos-Predict2.5] achieves substantial
improvements over [Cosmos-Predict1] in video quality and instruction alignment,
with models released at 2B and 14B scales. These capabilities enable more
reliable synthetic data generation, policy evaluation, and closed-loop
simulation for robotics and autonomous systems. We further extend the family
with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and
Real2Real world translation. Despite being 3.5$\times$ smaller than
[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video
generation. Together, these advances establish [Cosmos-Predict2.5] and
[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To
accelerate research and deployment in Physical AI, we release source code,
pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model
License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and
https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open
resources lower the barrier to adoption and foster innovation in building the
next generation of embodied intelligence.

</details>


### [27] [Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures](https://arxiv.org/abs/2511.00073)
*Harald Kristen,Daniel Kulmer,Manuela Hirschmugl*

Main category: cs.CV

TL;DR: 使用深度学习进行高山栖息地变化检测，比较了后分类变化检测和直接变化检测两种方法，发现Clay v1.0模型在复杂高山环境中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 高山生态系统面临快速气候变化等干扰，需要频繁的栖息地监测，但人工测绘成本过高。需要应用地理空间基础模型来解决复杂自然环境中模糊类别边界和高度不平衡类别的问题。

Method: 比较两种变化检测范式：后分类变化检测（使用Prithvi-EO-2.0、Clay v1.0和U-Net CNN）和直接变化检测（使用ChangeViT和U-Net基线）。使用高分辨率多模态数据（RGB、NIR、LiDAR、地形属性），覆盖15.3平方公里内的4,480个记录变化。

Result: Clay v1.0在多类栖息地变化检测中达到51%总体准确率，优于U-Net的41%；二元变化检测两者均达到67%。直接变化检测在二元检测中IoU更优（0.53 vs 0.35），但多类检测准确率仅28%。整合LiDAR将语义分割准确率从30%提升至50%。

Conclusion: 尽管总体准确率低于同质景观，但反映了复杂高山栖息地的实际性能。未来工作将整合基于对象后处理和物理约束以增强适用性。

Abstract: Rapid climate change and other disturbances in alpine ecosystems demand
frequent habitat monitoring, yet manual mapping remains prohibitively expensive
for the required temporal resolution. We employ deep learning for change
detection using long-term alpine habitat data from Gesaeuse National Park,
Austria, addressing a major gap in applying geospatial foundation models (GFMs)
to complex natural environments with fuzzy class boundaries and highly
imbalanced classes. We compare two paradigms: post-classification change
detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs
Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the
transformer ChangeViT against U-Net baselines. Using high-resolution multimodal
data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes
over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus
U-Net's 41% for multi-class habitat change, while both reach 67% for binary
change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but
only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals
GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's
23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy.
Although overall accuracies are lower than in more homogeneous landscapes, they
reflect realistic performance for complex alpine habitats. Future work will
integrate object-based post-processing and physical constraints to enhance
applicability.

</details>


### [28] [LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation](https://arxiv.org/abs/2511.00090)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Chao Tan,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: LeMiCa是一个无需训练的高效扩散视频生成加速框架，通过词典最小最大路径优化策略显著提升生成质量和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有缓存策略主要关注减少局部启发式误差，但忽略了全局误差累积，导致加速视频与原始视频之间存在明显内容退化。

Method: 将缓存调度建模为带误差权重边的有向图，引入词典最小最大路径优化策略来显式约束最坏情况路径误差。

Result: 在多个文本到视频基准测试中，LeMiCa在推理速度和生成质量上均实现双重提升。在Latte模型上实现2.9倍加速，在Open-Sora上达到LPIPS分数0.05，优于现有缓存技术。

Conclusion: LeMiCa为加速扩散视频生成提供了一个稳健且可泛化的范式，在最小化感知质量退化的同时实现显著性能提升。

Abstract: We present LeMiCa, a training-free and efficient acceleration framework for
diffusion-based video generation. While existing caching strategies primarily
focus on reducing local heuristic errors, they often overlook the accumulation
of global errors, leading to noticeable content degradation between accelerated
and original videos. To address this issue, we formulate cache scheduling as a
directed graph with error-weighted edges and introduce a Lexicographic Minimax
Path Optimization strategy that explicitly bounds the worst-case path error.
This approach substantially improves the consistency of global content and
style across generated frames. Extensive experiments on multiple text-to-video
benchmarks demonstrate that LeMiCa delivers dual improvements in both inference
speed and generation quality. Notably, our method achieves a 2.9x speedup on
the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming
prior caching techniques. Importantly, these gains come with minimal perceptual
quality degradation, making LeMiCa a robust and generalizable paradigm for
accelerating diffusion-based video generation. We believe this approach can
serve as a strong foundation for future research on efficient and reliable
video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa

</details>


### [29] [Self-Improving Vision-Language-Action Models with Data Generation via Residual RL](https://arxiv.org/abs/2511.00091)
*Wenli Xiao,Haotian Lin,Andy Peng,Haoru Xue,Tairan He,Yuqi Xie,Fengyuan Hu,Jimmy Wu,Zhengyi Luo,Linxi "Jim" Fan,Guanya Shi,Yuke Zhu*

Main category: cs.CV

TL;DR: PLD是一个三阶段即插即用框架，通过残差强化学习和分布感知数据收集来改进视觉语言动作模型，在多个基准测试中显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 监督微调依赖昂贵的人工演示，限制了视觉语言动作模型的可扩展性和泛化能力，需要更高效的改进方法。

Method: 三阶段框架：1)训练轻量级残差演员探索失败区域；2)混合rollout方案收集与部署分布对齐的轨迹；3)通过标准SFT将精选轨迹蒸馏回通用模型。

Result: 在LIBERO上达到接近饱和的99%任务成功率，在SimplerEnv上提升超过50%，在真实世界Franka和YAM机械臂任务中实现100%成功率。

Conclusion: 残差探测和分布感知回放是收集部署对齐数据的关键，为自改进VLA模型提供了可扩展的路径。

Abstract: Supervised fine-tuning (SFT) has become the de facto post-training strategy
for large vision-language-action (VLA) models, but its reliance on costly human
demonstrations limits scalability and generalization. We propose Probe, Learn,
Distill (PLD), a three-stage plug-and-play framework that improves VLAs through
residual reinforcement learning (RL) and distribution-aware data collection. In
Stage 1, we train lightweight residual actors to probe failure regions of the
VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns
collected trajectories with the generalist's deployment distribution while
capturing recovery behaviors. In Stage 3, we distill the curated trajectories
back into the generalist with standard SFT. PLD achieves near-saturated 99%
task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on
real-world Franka and YAM arm manipulation tasks. Ablations show that residual
probing and distribution-aware replay are key to collecting deployment-aligned
data that improves both seen and unseen tasks, offering a scalable path toward
self-improving VLA models.

</details>


### [30] [SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation](https://arxiv.org/abs/2511.00095)
*Jiaming Liu,Dingwei Fan,Junyong Zhao,Chunlin Li,Haipeng Si,Liang Sun*

Main category: cs.CV

TL;DR: 提出SpinalSAM-R1系统，结合微调SAM和DeepSeek-R1，用于脊柱CT图像分割，通过解剖学引导注意力机制和自然语言交互提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 脊柱CT图像分割面临低对比度和复杂边界的挑战，现有模型如SAM在脊柱CT领域表现受限，需要高标注要求和领域适应性差的问题。

Method: 集成微调SAM与DeepSeek-R1，引入解剖学引导注意力机制和语义驱动交互协议，使用LoRA进行高效微调，支持点、框和文本提示。

Result: 在脊柱解剖结构CT图像上验证，获得优越分割性能，开发交互软件支持11种临床操作，解析准确率94.3%，响应时间低于800ms。

Conclusion: SpinalSAM-R1系统有效解决了脊柱CT图像分割的挑战，提供了高效准确的解决方案，并发布了开源软件。

Abstract: The anatomical structure segmentation of the spine and adjacent structures
from computed tomography (CT) images is a key step for spinal disease diagnosis
and treatment. However, the segmentation of CT images is impeded by low
contrast and complex vertebral boundaries. Although advanced models such as the
Segment Anything Model (SAM) have shown promise in various segmentation tasks,
their performance in spinal CT imaging is limited by high annotation
requirements and poor domain adaptability. To address these limitations, we
propose SpinalSAM-R1, a multimodal vision-language interactive system that
integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.
Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism
to improve spine segmentation performance, and a semantics-driven interaction
protocol powered by DeepSeek-R1, enabling natural language-guided refinement.
The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient
adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with
CT images. Experimental results suggest that our method achieves superior
segmentation performance. Meanwhile, we develop a PyQt5-based interactive
software, which supports point, box, and text-based prompts. The system
supports 11 clinical operations with 94.3\% parsing accuracy and sub-800 ms
response times. The software is released on
https://github.com/6jm233333/spinalsam-r1.

</details>


### [31] [A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning](https://arxiv.org/abs/2511.00098)
*Nils Porsche,Flurin Müller-Diesing,Sweta Banerjee,Miguel Goncalves,Marc Aubreville*

Main category: cs.CV

TL;DR: 提出了一种用于共聚焦激光内窥镜(CLE)视频序列的过滤方法，通过减少自监督学习训练中的数据集冗余，提高训练效率和收敛性。


<details>
  <summary>Details</summary>
Motivation: CLE图像对非专业医生难以解读，机器学习能提供帮助但面临数据不足导致的过拟合问题。自监督学习可解决此问题，但CLE视频帧间相关性高导致数据分布不均衡。

Method: 使用四种最先进的基线网络和基于视觉变换器小骨干的自监督师生网络，在鼻窦肿瘤和皮肤鳞状细胞癌数据集上进行评估。

Result: 在过滤的自监督预训练模型上获得最高测试准确率，分别为67.48%和73.52%，显著优于非自监督基线，训练时间减少67%。

Conclusion: 自监督学习是CLE预训练的有效方法，提出的CLE视频过滤器能提高自监督场景下的训练效率。

Abstract: Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging
modality that can be used for in-situ, in-vivo imaging and the microstructural
analysis of mucous structures. The diagnosis using CLE is, however, complicated
by images being hard to interpret for non-experienced physicians. Utilizing
machine learning as an augmentative tool would hence be beneficial, but is
complicated by the shortage of histopathology-correlated CLE imaging sequences
with respect to the plurality of patterns in this domain, leading to
overfitting of machine learning models. To overcome this, self-supervised
learning (SSL) can be employed on larger unlabeled datasets. CLE is a
video-based modality with high inter-frame correlation, leading to a
non-stratified data distribution for SSL training. In this work, we propose a
filter functionality on CLE video sequences to reduce the dataset redundancy in
SSL training and improve SSL training convergence and training efficiency. We
use four state-of-the-art baseline networks and a SSL teacher-student network
with a vision transformer small backbone for the evaluation. These networks
were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous
cell carcinoma of the skin dataset. On both datasets, we found the highest test
accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both
considerably outperforming their non-SSL baselines. Our results show that SSL
is an effective method for CLE pretraining. Further, we show that our proposed
CLE video filter can be utilized to improve training efficiency in
self-supervised scenarios, resulting in a reduction of 67% in training time.

</details>


### [32] [FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video](https://arxiv.org/abs/2511.00103)
*Rotem Ezra,Hedi Zisling,Nimrod Berman,Ilan Naiman,Alexey Gorkor,Liran Nochumsohn,Eliya Nachmani,Omri Azencot*

Main category: cs.CV

TL;DR: FreeSliders是一种无需训练、模态无关的方法，通过在推理过程中部分估计Concept Sliders公式，实现跨模态的细粒度概念控制生成。


<details>
  <summary>Details</summary>
Motivation: 现有的Concept Sliders方法需要针对每个概念进行训练和架构特定的微调，限制了在新模态上的可扩展性。

Method: 提出完全无需训练的方法，在推理时部分估计CS公式，并引入两阶段程序自动检测饱和点并重新参数化遍历以实现感知均匀的语义编辑。

Result: 实验表明该方法支持即插即用的跨模态概念控制，优于现有基线，并为可控生成建立了新工具。

Conclusion: FreeSliders提供了一种简单有效的训练无关方法，实现了跨模态的细粒度概念控制，并建立了首个多模态细粒度概念生成控制基准。

Abstract: Diffusion models have become state-of-the-art generative models for images,
audio, and video, yet enabling fine-grained controllable generation, i.e.,
continuously steering specific concepts without disturbing unrelated content,
remains challenging. Concept Sliders (CS) offer a promising direction by
discovering semantic directions through textual contrasts, but they require
per-concept training and architecture-specific fine-tuning (e.g., LoRA),
limiting scalability to new modalities. In this work we introduce FreeSliders,
a simple yet effective approach that is fully training-free and
modality-agnostic, achieved by partially estimating the CS formula during
inference. To support modality-agnostic evaluation, we extend the CS benchmark
to include both video and audio, establishing the first suite for fine-grained
concept generation control with multiple modalities. We further propose three
evaluation properties along with new metrics to improve evaluation quality.
Finally, we identify an open problem of scale selection and non-linear
traversals and introduce a two-stage procedure that automatically detects
saturation points and reparameterizes traversal for perceptually uniform,
semantically meaningful edits. Extensive experiments demonstrate that our
method enables plug-and-play, training-free concept control across modalities,
improves over existing baselines, and establishes new tools for principled
controllable generation. An interactive presentation of our benchmark and
method is available at: https://azencot-group.github.io/FreeSliders/

</details>


### [33] [AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency](https://arxiv.org/abs/2511.00107)
*Piyushkumar Patel*

Main category: cs.CV

TL;DR: MOVAI是一个新颖的分层文本到视频生成框架，通过组合场景解析、时空注意力机制和渐进式视频精炼，显著提升了视频生成的质量和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频生成方法在保持时间一致性、组合理解和视觉叙事精细控制方面存在困难，需要更先进的框架来解决这些问题。

Method: 提出三个关键创新：组合场景解析器将文本分解为带时间标注的分层场景图；时空注意力机制确保帧间连贯运动动态；渐进式视频精炼模块通过多尺度时间推理迭代提升视频质量。

Result: 在标准基准测试中，MOVAI实现了最先进的性能，LPIPS指标提升15.3%，FVD指标提升12.7%，用户偏好研究提升18.9%。

Conclusion: MOVAI框架在生成复杂多对象场景、实现真实时间动态和精细语义控制方面表现出色，为文本到视频生成提供了有效的解决方案。

Abstract: Text to video generation has emerged as a critical frontier in generative
artificial intelligence, yet existing approaches struggle with maintaining
temporal consistency, compositional understanding, and fine grained control
over visual narratives. We present MOVAI (Multimodal Original Video AI), a
novel hierarchical framework that integrates compositional scene understanding
with temporal aware diffusion models for high fidelity text to video synthesis.
Our approach introduces three key innovations: (1) a Compositional Scene Parser
(CSP) that decomposes textual descriptions into hierarchical scene graphs with
temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that
ensures coherent motion dynamics across frames while preserving spatial
details, and (3) a Progressive Video Refinement (PVR) module that iteratively
enhances video quality through multi-scale temporal reasoning. Extensive
experiments on standard benchmarks demonstrate that MOVAI achieves
state-of-the-art performance, improving video quality metrics by 15.3% in
LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing
methods. Our framework shows particular strength in generating complex
multi-object scenes with realistic temporal dynamics and fine-grained semantic
control.

</details>


### [34] [Chain of Time: In-Context Physical Simulation with Image Generation Models](https://arxiv.org/abs/2511.00110)
*YingQiao Wang,Eric Bigelow,Boyi Li,Tomer Ullman*

Main category: cs.CV

TL;DR: 提出了一种名为"Chain of Time"的认知启发方法，通过在模拟过程中生成一系列中间图像来改进和解释视觉语言模型中的物理模拟，无需额外微调即可在推理时使用。


<details>
  <summary>Details</summary>
Motivation: 受机器学习中的上下文推理和人类心理模拟启发，旨在改进视觉语言模型的物理模拟能力并理解其内部动态。

Method: Chain-of-Time方法在推理时生成模拟过程中的中间图像序列，应用于2D图形模拟和3D自然视频，测试速度、加速度、流体动力学和动量守恒等物理属性。

Result: 使用Chain-of-Time方法显著提升了最先进图像生成模型的性能，分析揭示了模型能够模拟随时间展开的物理属性（如速度、重力和碰撞），但也发现模型在某些情况下难以从输入图像推断特定物理参数。

Conclusion: Chain-of-Time方法不仅提高了物理模拟性能，还提供了对图像生成模型内部物理推理动态的深入理解，揭示了传统评估方法无法发现的洞察。

Abstract: We propose a novel cognitively-inspired method to improve and interpret
physical simulation in vision-language models. Our ``Chain of Time" method
involves generating a series of intermediate images during a simulation, and it
is motivated by in-context reasoning in machine learning, as well as mental
simulation in humans. Chain of Time is used at inference time, and requires no
additional fine-tuning. We apply the Chain-of-Time method to synthetic and
real-world domains, including 2-D graphics simulations and natural 3-D videos.
These domains test a variety of particular physical properties, including
velocity, acceleration, fluid dynamics, and conservation of momentum. We found
that using Chain-of-Time simulation substantially improves the performance of a
state-of-the-art image generation model. Beyond examining performance, we also
analyzed the specific states of the world simulated by an image model at each
time step, which sheds light on the dynamics underlying these simulations. This
analysis reveals insights that are hidden from traditional evaluations of
physical reasoning, including cases where an image generation model is able to
simulate physical properties that unfold over time, such as velocity, gravity,
and collisions. Our analysis also highlights particular cases where the image
generation model struggles to infer particular physical parameters from input
images, despite being capable of simulating relevant physical processes.

</details>


### [35] [End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning](https://arxiv.org/abs/2511.00114)
*Hanae Elmekki,Amanda Spilkin,Ehsan Zakeri,Antonela Mariel Zanuttini,Ahmed Alagha,Hani Sami,Jamal Bentahar,Lyes Kadem,Wen-Fang Xie,Philippe Pibarot,Rabeb Mizouni,Hadi Otrok,Azzam Mourad,Sami Muhaidat*

Main category: cs.CV

TL;DR: 提出首个结合生成AI和深度强化学习的端到端框架，实现自主、可重复的心脏超声扫描，解决传统方法依赖操作者、缺乏可重复性等问题。


<details>
  <summary>Details</summary>
Motivation: 心脏超声诊断存在操作者依赖、时间限制和人为错误等问题，特别是在偏远地区缺乏专业医生。现有AI方法缺乏可重复性、依赖专有数据且使用简化模型。

Method: 框架包含两个组件：(1) 结合GAN和VAE的条件生成模拟器，生成逼真的动作条件图像；(2) DRL模块利用模拟器学习自主扫描策略，通过专家验证模型提供AI驱动指导。

Result: VAE-GAN在性能评估中表现优于现有GAN变体，DRL扫描系统在不同配置下验证有效。发布了公开可用的真实心脏超声数据集确保可重复性。

Conclusion: 该框架为心脏超声扫描提供了自主、可重复的解决方案，可扩展到其他器官，解决了现有方法的局限性。

Abstract: Cardiac ultrasound (US) is among the most widely used diagnostic tools in
cardiology for assessing heart health, but its effectiveness is limited by
operator dependence, time constraints, and human error. The shortage of trained
professionals, especially in remote areas, further restricts access. These
issues underscore the need for automated solutions that can ensure consistent,
and accessible cardiac imaging regardless of operator skill or location. Recent
progress in artificial intelligence (AI), especially in deep reinforcement
learning (DRL), has gained attention for enabling autonomous decision-making.
However, existing DRL-based approaches to cardiac US scanning lack
reproducibility, rely on proprietary data, and use simplified models. Motivated
by these gaps, we present the first end-to-end framework that integrates
generative AI and DRL to enable autonomous and reproducible cardiac US
scanning. The framework comprises two components: (i) a conditional generative
simulator combining Generative Adversarial Networks (GANs) with Variational
Autoencoders (VAEs), that models the cardiac US environment producing realistic
action-conditioned images; and (ii) a DRL module that leverages this simulator
to learn autonomous, accurate scanning policies. The proposed framework
delivers AI-driven guidance through expert-validated models that classify image
type and assess quality, supports conditional generation of realistic US
images, and establishes a reproducible foundation extendable to other organs.
To ensure reproducibility, a publicly available dataset of real cardiac US
scans is released. The solution is validated through several experiments. The
VAE-GAN is benchmarked against existing GAN variants, with performance assessed
using qualitative and quantitative approaches, while the DRL-based scanning
system is evaluated under varying configurations to demonstrate effectiveness.

</details>


### [36] [VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images](https://arxiv.org/abs/2511.00120)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: VLM6D提出了一种双流架构，利用RGB-D输入的视觉和几何数据优势，通过Vision Transformer和PointNet++编码器分别处理RGB和点云数据，在遮挡严重的场景下实现了鲁棒且精确的6D物体姿态估计。


<details>
  <summary>Details</summary>
Motivation: 解决当前6D物体姿态估计方法在从合成数据泛化到真实世界时面临的挑战，包括光照变化、无纹理物体和严重遮挡等问题。

Method: 采用双流架构：使用自监督Vision Transformer（DINOv2）处理RGB数据以抵抗纹理和光照变化，同时使用PointNet++编码器处理3D点云数据以应对严重遮挡，最后融合两个特征流进行多任务预测。

Result: 在具有挑战性的Occluded-LineMOD数据集上取得了新的SOTA性能，验证了其卓越的鲁棒性和准确性。

Conclusion: VLM6D通过有效结合视觉和几何信息的互补优势，为6D物体姿态估计提供了一种鲁棒且精确的解决方案，特别是在具有挑战性的真实世界场景中表现出色。

Abstract: The primary challenge in computer vision is precisely calculating the pose of
6D objects, however many current approaches are still fragile and have trouble
generalizing from synthetic data to real-world situations with fluctuating
lighting, textureless objects, and significant occlusions. To address these
limitations, VLM6D, a novel dual-stream architecture that leverages the
distinct strengths of visual and geometric data from RGB-D input for robust and
precise pose estimation. Our framework uniquely integrates two specialized
encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the
RGB modality, harnessing its rich, pre-trained understanding of visual grammar
to achieve remarkable resilience against texture and lighting variations.
Concurrently, a PointNet++ encoder processes the 3D point cloud derived from
depth data, enabling robust geometric reasoning that excels even with the
sparse, fragmented data typical of severe occlusion. These complementary
feature streams are effectively fused to inform a multi task prediction head.
We demonstrate through comprehensive experiments that VLM6D obtained new SOTA
performance on the challenging Occluded-LineMOD, validating its superior
robustness and accuracy.

</details>


### [37] [Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation](https://arxiv.org/abs/2511.00123)
*Gaby Maroun,Salah Eddine Bekhouche,Fadi Dornaika*

Main category: cs.CV

TL;DR: 提出了一种结合ConvNeXt和Vision Transformer的混合架构，用于面部图像年龄估计，在多个基准数据集上取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决面部年龄估计这一复杂计算机视觉挑战，利用CNN的局部特征提取能力和Transformer的全局注意力机制的互补优势。

Method: 使用预训练模型，结合ConvNeXt和ViT构建混合架构，采用线性层和高级正则化技术优化模型，并探索不同配置。

Result: 在MORPH II、CACD和AFAD等基准数据集上，以平均绝对误差(MAE)衡量，性能优于传统方法。

Conclusion: 混合架构展示了CNN和Transformer无缝集成的变革潜力，为年龄估计和相关视觉任务提供了坚实基础。

Abstract: Age estimation from facial images is a complex and multifaceted challenge in
computer vision. In this study, we present a novel hybrid architecture that
combines ConvNeXt, a state-of-the-art advancement of convolutional neural
networks (CNNs), with Vision Transformers (ViT). While each model independently
delivers excellent performance on a variety of tasks, their integration
leverages the complementary strengths of the CNNs localized feature extraction
capabilities and the Transformers global attention mechanisms. Our proposed
ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age
estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior
performance in terms of mean absolute error (MAE). To address computational
constraints, we leverage pre-trained models and systematically explore
different configurations, using linear layers and advanced regularization
techniques to optimize the architecture. Comprehensive ablation studies
highlight the critical role of individual components and training strategies,
and in particular emphasize the importance of adapted attention mechanisms
within the CNN framework to improve the model focus on age-relevant facial
features. The results show that the ConvNeXt-ViT hybrid not only outperforms
traditional methods, but also provides a robust foundation for future advances
in age estimation and related visual tasks. This work underscores the
transformative potential of hybrid architectures and represents a promising
direction for the seamless integration of CNNs and transformers to address
complex computer vision challenges.

</details>


### [38] [FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding](https://arxiv.org/abs/2511.00141)
*Janghoon Cho,Jungsoo Lee,Munawar Hayat,Kyuwoong Hwang,Fatih Porikli,Sungha Choi*

Main category: cs.CV

TL;DR: FLoC是一个基于设施位置函数的高效视觉token压缩框架，通过选择紧凑且具有代表性的视觉token子集来解决长视频理解中的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 长视频理解中，大型多模态模型面临视觉token数量过多导致可扩展性受限的挑战，需要高效的token压缩方法。

Method: 基于设施位置函数和惰性贪心算法，在预定义预算内快速选择具有代表性和多样性的视觉token子集，无需训练且与模型无关。

Result: 在Video-MME、MLVU和LongVideoBench等大规模基准测试中，FLoC持续超越现有压缩技术，显著减少视觉token数量同时保持接近最优性能。

Conclusion: FLoC提供了一个无需训练、模型无关且查询无关的通用解决方案，能有效解决长视频理解中的关键挑战，并在处理速度上具有高效率。

Abstract: Recent studies in long video understanding have harnessed the advanced
visual-language reasoning capabilities of Large Multimodal Models (LMMs),
driving the evolution of video-LMMs specialized for processing extended video
sequences. However, the scalability of these models is severely limited by the
overwhelming volume of visual tokens generated from extended video sequences.
To address this challenge, this paper proposes FLoC, an efficient visual token
compression framework based on the facility location function, a principled
approach that swiftly selects a compact yet highly representative and diverse
subset of visual tokens within a predefined budget on the number of visual
tokens. By integrating the lazy greedy algorithm, our method achieves
remarkable efficiency gains by swiftly selecting a compact subset of tokens,
drastically reducing the number of visual tokens while guaranteeing
near-optimal performance. Notably, our approach is training-free,
model-agnostic, and query-agnostic, providing a versatile solution that
seamlessly integrates with diverse video-LLMs and existing workflows. Extensive
evaluations on large-scale benchmarks, such as Video-MME, MLVU, and
LongVideoBench, demonstrate that our framework consistently surpasses recent
compression techniques, highlighting not only its effectiveness and robustness
in addressing the critical challenges of long video understanding, but also its
efficiency in processing speed.

</details>


### [39] [BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing](https://arxiv.org/abs/2511.00143)
*Jinsu Kim,Yunhun Nam,Minseon Kim,Sangpil Kim,Jongheon Jeong*

Main category: cs.CV

TL;DR: 提出了一种通过自适应高斯模糊增强图像保护对抗噪声反转技术鲁棒性的方法，显著提高了现有保护方法在多种图像编辑场景下的防护性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于对抗噪声的图像保护方法容易被简单的反转技术（如JPEG压缩）破解，需要开发更鲁棒的保护机制。

Method: 应用自适应区域高斯模糊来调整噪声的整体频率谱，增强对抗噪声反转技术的鲁棒性。

Result: 实验表明该方法能一致提升现有方法在多种反转技术和图像编辑场景下的最坏情况保护性能，同时减少噪声引起的质量退化。

Conclusion: 所提出的简单方法有效增强了图像保护对抗噪声反转的鲁棒性，为保护图像免受恶意编辑提供了实用解决方案。

Abstract: Recent advances in text-to-image models have increased the exposure of
powerful image editing techniques as a tool, raising concerns about their
potential for malicious use. An emerging line of research to address such
threats focuses on implanting "protective" adversarial noise into images before
their public release, so future attempts to edit them using text-to-image
models can be impeded. However, subsequent works have shown that these
adversarial noises are often easily "reversed," e.g., with techniques as simple
as JPEG compression, casting doubt on the practicality of the approach. In this
paper, we argue that adversarial noise for image protection should not only be
imperceptible, as has been a primary focus of prior work, but also
irreversible, viz., it should be difficult to detect as noise provided that the
original image is hidden. We propose a surprisingly simple method to enhance
the robustness of image protection methods against noise reversal techniques.
Specifically, it applies an adaptive per-region Gaussian blur on the noise to
adjust the overall frequency spectrum. Through extensive experiments, we show
that our method consistently improves the per-sample worst-case protection
performance of existing methods against a wide range of reversal techniques on
diverse image editing scenarios, while also reducing quality degradation due to
noise in terms of perceptual metrics. Code is available at
https://github.com/jsu-kim/BlurGuard.

</details>


### [40] [CompAgent: An Agentic Framework for Visual Compliance Verification](https://arxiv.org/abs/2511.00171)
*Rahul Ghosh,Baishali Chaudhury,Hari Prasanna Das,Meghana Ashok,Ryan Razkenari,Sungmin Hong,Chun-Hao Liu*

Main category: cs.CV

TL;DR: 提出了CompAgent，首个用于视觉合规验证的智能体框架，通过工具增强的多模态大语言模型实现可扩展、准确和自适应的视觉内容合规检查。


<details>
  <summary>Details</summary>
Motivation: 视觉合规验证在媒体、娱乐和广告等领域至关重要，但现有方法依赖特定任务的深度学习模型，构建成本高且泛化能力有限。多模态大语言模型虽然具有广泛知识，但难以处理细粒度视觉细节和结构化合规规则。

Method: CompAgent框架包含规划代理和验证代理。规划代理根据合规策略动态选择视觉工具（如目标检测器、人脸分析器、NSFW检测器等），验证代理整合图像、工具输出和策略上下文进行多模态推理。

Result: 在公共基准测试中，CompAgent优于专用分类器、直接MLLM提示和精心设计的路由基线，在UnsafeBench数据集上达到76%的F1分数，比现有最佳方法提升10%。

Conclusion: 结果表明，智能体规划和工具增强推理对于可扩展、准确和自适应的视觉合规验证是有效的。

Abstract: Visual compliance verification is a critical yet underexplored problem in
computer vision, especially in domains such as media, entertainment, and
advertising where content must adhere to complex and evolving policy rules.
Existing methods often rely on task-specific deep learning models trained on
manually labeled datasets, which are costly to build and limited in
generalizability. While recent multi-modal large language models (MLLMs) offer
broad real-world knowledge and policy understanding, they struggle to reason
over fine-grained visual details and apply structured compliance rules
effectively on their own. In this paper, we propose CompAgent, the first
agentic framework for visual compliance verification. CompAgent augments MLLMs
with a suite of visual tools - such as object detectors, face analyzers, NSFW
detectors, and captioning models - and introduces a planning agent that
dynamically selects appropriate tools based on the compliance policy. A
verification agent then integrates image, tool outputs, and policy context to
perform multi-modal reasoning. Experiments on public benchmarks show that
CompAgent outperforms specialized classifiers, direct MLLM prompting, and
curated routing baselines, achieving up to 76% F1 score and a 10% improvement
over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate
the effectiveness of agentic planning and tool-augmented reasoning for
scalable, accurate, and adaptable visual compliance verification.

</details>


### [41] [From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection](https://arxiv.org/abs/2511.00181)
*Mengfei Liang,Yiting Qu,Yukun Jiang,Michael Backes,Yang Zhang*

Main category: cs.CV

TL;DR: AIFo是一个基于多智能体协作的训练免费框架，通过模拟人类法医调查过程来检测AI生成图像，在6000张图像的评估中达到97.05%的准确率。


<details>
  <summary>Details</summary>
Motivation: AI生成图像的快速发展对信息完整性和媒体真实性构成挑战，现有检测方法存在可解释性差、泛化能力不足等局限性。

Method: 采用基于LLM的专门智能体协调多种法医工具（反向图像搜索、元数据提取、预训练分类器、VLM分析），通过多智能体辩论机制解决证据冲突，并配备记忆增强推理模块学习历史案例。

Result: 在6000张图像的全面评估中，AIFo达到97.05%的准确率，显著优于传统分类器和最先进的视觉语言模型。

Conclusion: 基于智能体的程序推理为AI生成图像检测提供了更鲁棒、可解释和适应性强的新范式。

Abstract: The rapid evolution of AI-generated images poses unprecedented challenges to
information integrity and media authenticity. Existing detection approaches
suffer from fundamental limitations: traditional classifiers lack
interpretability and fail to generalize across evolving generative models,
while vision-language models (VLMs), despite their promise, remain constrained
to single-shot analysis and pixel-level reasoning. To address these challenges,
we introduce AIFo (Agent-based Image Forensics), a novel training-free
framework that emulates human forensic investigation through multi-agent
collaboration. Unlike conventional methods, our framework employs a set of
forensic tools, including reverse image search, metadata extraction,
pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based
agents that collect, synthesize, and reason over cross-source evidence. When
evidence is conflicting or insufficient, a structured multi-agent debate
mechanism allows agents to exchange arguments and reach a reliable conclusion.
Furthermore, we enhance the framework with a memory-augmented reasoning module
that learns from historical cases to improve future detection accuracy. Our
comprehensive evaluation spans 6,000 images across both controlled laboratory
settings and challenging real-world scenarios, including images from modern
generative platforms and diverse online sources. AIFo achieves 97.05% accuracy,
substantially outperforming traditional classifiers and state-of-the-art VLMs.
These results demonstrate that agent-based procedural reasoning offers a new
paradigm for more robust, interpretable, and adaptable AI-generated image
detection.

</details>


### [42] [A Retrospect to Multi-prompt Learning across Vision and Language](https://arxiv.org/abs/2511.00191)
*Ziliang Chen,Xin Huang,Quanlong Guan,Liang Lin,Weiqi Luo*

Main category: cs.CV

TL;DR: 本文提出了基于能量的多提示学习(EMPL)方法，通过从能量分布中采样生成多个提示嵌入，在保持参数效率的同时平衡领域内和领域外的开放词汇泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单提示学习范式，很少探索多提示学习的技术潜力。本文旨在为视觉语言多提示学习提供原则性回顾，并证明多提示增强在视觉语言迁移中的优越性。

Method: 将最近的恒定模态间隙现象扩展到可学习提示，提出基于能量的多提示学习(EMPL)，通过从VLMs隐式定义的能量分布中采样实例来生成多个提示嵌入。

Result: 综合实验验证了多提示学习的优势以及EMPL方法的卓越性能。

Conclusion: EMPL方法不仅参数高效，而且严格实现了领域内和领域外开放词汇泛化之间的平衡，为视觉语言多提示学习提供了有效解决方案。

Abstract: The vision community is undergoing the unprecedented progress with the
emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays
as the holy grail of accessing VLMs since it enables their fast adaptation to
downstream tasks with limited resources. Whereas existing researches milling
around single-prompt paradigms, rarely investigate the technical potential
behind their multi-prompt learning counterparts. This paper aims to provide a
principled retrospect for vision-language multi-prompt learning. We extend the
recent constant modality gap phenomenon to learnable prompts and then, justify
the superiority of vision-language transfer with multi-prompt augmentation,
empirically and theoretically. In terms of this observation, we propose an
Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt
embeddings by drawing instances from an energy-based distribution, which is
implicitly defined by VLMs. So our EMPL is not only parameter-efficient but
also rigorously lead to the balance between in-domain and out-of-domain
open-vocabulary generalization. Comprehensive experiments have been conducted
to justify our claims and the excellence of EMPL.

</details>


### [43] [An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals](https://arxiv.org/abs/2511.00211)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: 提出一种高效的迁移学习方法，用于卫星互联网地面终端组件的细粒度天气状况检测，包括雪、湿和其他恶劣天气条件，性能优于主流深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气事件对低轨卫星互联网性能和可靠性有显著影响，需要地面终端组件具备细粒度天气状况检测能力，以协助故障诊断和缓解，但现有解决方案缺乏且缺乏实际部署所需的有效性和泛化性。

Method: 采用高效的迁移学习方法，使地面组件能够本地检测代表性天气相关状况，包括雪、湿和其他由恶劣和典型天气事件引起的条件。

Result: 所提出的迁移学习方法在检测性能上优于YOLOv7、YOLOv9、Faster R-CNN和R-YOLO等典型深度学习方法，并显示出在各种场景下的良好泛化能力。

Conclusion: 该迁移学习方法能够有效检测卫星互联网地面终端组件的天气状况，具有优越性能和良好的泛化性，为可靠的卫星互联网提供了实用的故障诊断和缓解解决方案。

Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO)
satellites in mega-constellations allows ubiquitous connectivity to rural and
remote areas. However, weather events have a significant impact on the
performance and reliability of satellite Internet. Adverse weather events such
as snow and rain can disturb the performance and operations of satellite
Internet's essential ground terminal components, such as satellite antennas,
significantly disrupting the space-ground link conditions between LEO
satellites and ground stations. This challenge calls for not only region-based
weather forecasts but also fine-grained detection capability on ground terminal
components of fine-grained weather conditions. Such a capability can assist in
fault diagnostics and mitigation for reliable satellite Internet, but its
solutions are lacking, not to mention the effectiveness and generalization that
are essential in real-world deployments. This paper discusses an efficient
transfer learning (TL) method that can enable a ground component to locally
detect representative weather-related conditions. The proposed method can
detect snow, wet, and other conditions resulting from adverse and typical
weather events and shows superior performance compared to the typical deep
learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL
method also shows the advantage of being generalizable to various scenarios.

</details>


### [44] [DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy](https://arxiv.org/abs/2511.00218)
*Rajatsubhra Chakraborty,Ana Espinosa-Momox,Riley Haskin,Depeng Xu,Rosario Porras-Aguilar*

Main category: cs.CV

TL;DR: DM-QPMNet是一种双编码器网络，通过分别编码偏振强度图像和相位图，使用多头注意力在中间深度融合模态特定特征，实现稳健的细胞分割。


<details>
  <summary>Details</summary>
Motivation: 传统阈值方法对噪声和细胞密度敏感，而简单的通道拼接深度学习方法未能充分利用偏振强度图像和相位图的互补特性。

Method: 采用双编码器网络分别处理两种模态，通过多头注意力在中间深度进行特征融合，结合双源跳跃连接和每模态归一化。

Result: 相比单一拼接和单模态基线方法，该方法在细胞分割方面表现出显著改进。

Conclusion: 模态特定编码与可学习融合有效利用了ssQPM同时捕获的互补照明和相位线索，实现稳健的细胞分割。

Abstract: Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces
challenges from traditional thresholding methods that are sensitive to noise
and cell density, while deep learning approaches using simple channel
concatenation fail to exploit the complementary nature of polarized intensity
images and phase maps. We introduce DM-QPMNet, a dual-encoder network that
treats these as distinct modalities with separate encoding streams. Our
architecture fuses modality-specific features at intermediate depth via
multi-head attention, enabling polarized edge and texture representations to
selectively integrate complementary phase information. This content-aware
fusion preserves training stability while adding principled multi-modal
integration through dual-source skip connections and per-modality normalization
at minimal overhead. Our approach demonstrates substantial improvements over
monolithic concatenation and single-modality baselines, showing that
modality-specific encoding with learnable fusion effectively exploits ssQPM's
simultaneous capture of complementary illumination and phase cues for robust
cell segmentation.

</details>


### [45] [Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior](https://arxiv.org/abs/2511.00231)
*Fuming Yang,Yicong Li,Hanspeter Pfister,Jeff W. Lichtman,Yaron Meirovitch*

Main category: cs.CV

TL;DR: 提出基于VQ-VAE的电子显微镜数据压缩框架，支持16x到1024x压缩比，支持按需解码和选择性高分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 海量电子显微镜数据集对存储、传输和下游分析提出了挑战，需要高效的压缩解决方案。

Method: 使用向量量化变分自编码器(VQ-VAE)进行压缩，结合Transformer先验模型预测底层token，通过FiLM和拼接恢复纹理，并引入ROI驱动的工作流进行选择性重建。

Result: 实现了16x到1024x的可扩展压缩比，支持仅解码顶层实现极端压缩，通过可选组件恢复纹理细节。

Conclusion: 该框架为海量EM数据提供了灵活高效的压缩和重建方案，支持按需使用和选择性高分辨率重建。

Abstract: Petascale electron microscopy (EM) datasets push storage, transfer, and
downstream analysis toward their current limits. We present a vector-quantized
variational autoencoder-based (VQ-VAE) compression framework for EM that spans
16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme
compression, with an optional Transformer prior that predicts bottom tokens
(without changing the compression ratio) to restore texture via feature-wise
linear modulation (FiLM) and concatenation; we further introduce an ROI-driven
workflow that performs selective high-resolution reconstruction from
1024x-compressed latents only where needed.

</details>


### [46] [Hyperbolic Optimal Transport](https://arxiv.org/abs/2511.00244)
*Yan Bin Ng,Xianfeng Gu*

Main category: cs.CV

TL;DR: 提出了一种在双曲空间中计算最优传输映射的新算法，通过几何变分技术将欧几里得和球面几何的方法扩展到双曲设置。


<details>
  <summary>Details</summary>
Motivation: 现有最优传输计算方法主要针对欧几里得空间和球面，但在涉及层次数据、网络和多亏格黎曼曲面等场景中，双曲空间中的最优传输问题自然出现。

Method: 使用几何变分技术，将欧几里得和球面几何的最优传输计算方法扩展到双曲空间设置。

Result: 在合成数据和多亏格曲面模型上的实验验证了所提方法的有效性。

Conclusion: 成功开发了双曲空间中的最优传输映射计算算法，填补了现有方法在该领域的空白。

Abstract: The optimal transport (OT) problem aims to find the most efficient mapping
between two probability distributions under a given cost function, and has
diverse applications in many fields such as machine learning, computer vision
and computer graphics. However, existing methods for computing optimal
transport maps are primarily developed for Euclidean spaces and the sphere. In
this paper, we explore the problem of computing the optimal transport map in
hyperbolic space, which naturally arises in contexts involving hierarchical
data, networks, and multi-genus Riemann surfaces. We propose a novel and
efficient algorithm for computing the optimal transport map in hyperbolic space
using a geometric variational technique by extending methods for Euclidean and
spherical geometry to the hyperbolic setting. We also perform experiments on
synthetic data and multi-genus surface models to validate the efficacy of the
proposed method.

</details>


### [47] [Object-Aware 4D Human Motion Generation](https://arxiv.org/abs/2511.00248)
*Shurui Gui,Deep Anil Patel,Xiner Li,Martin Renqiang Min*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯表示和运动扩散先验的对象感知4D人体运动生成框架MSDI，通过运动扩散分数蒸馏采样和大型语言模型实现零样本的物理合理人体运动生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型生成的视频存在不现实变形、语义违规和物理不一致问题，主要原因是缺乏3D物理先验。

Method: 使用预生成的3D人体和对象，结合MSDS从预训练运动扩散模型蒸馏分数梯度，并利用LLMs的空间和提示语义信息来优化人体运动，同时尊重对象和语义约束。

Result: 实验表明该框架能生成自然且物理合理的人体运动，尊重3D空间上下文，为现实4D生成提供可扩展解决方案。

Conclusion: 该方法无需在有限交互数据集上联合训练，实现了零样本泛化，能处理分布外对象感知人体运动。

Abstract: Recent advances in video diffusion models have enabled the generation of
high-quality videos. However, these videos still suffer from unrealistic
deformations, semantic violations, and physical inconsistencies that are
largely rooted in the absence of 3D physical priors. To address these
challenges, we propose an object-aware 4D human motion generation framework
grounded in 3D Gaussian representations and motion diffusion priors. With
pre-generated 3D humans and objects, our method, Motion Score Distilled
Interaction (MSDI), employs the spatial and prompt semantic information in
large language models (LLMs) and motion priors through the proposed Motion
Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs
enables our spatial-aware motion optimization, which distills score gradients
from pre-trained motion diffusion models, to refine human motion while
respecting object and semantic constraints. Unlike prior methods requiring
joint training on limited interaction datasets, our zero-shot approach avoids
retraining and generalizes to out-of-distribution object aware human motions.
Experiments demonstrate that our framework produces natural and physically
plausible human motions that respect 3D spatial context, offering a scalable
solution for realistic 4D generation.

</details>


### [48] [Merlin L48 Spectrogram Dataset](https://arxiv.org/abs/2511.00252)
*Aaron Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: 提出了L48数据集，这是一个细粒度的真实世界多标签数据集，用于单正多标签学习，相比合成数据集更能反映现实场景的复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有的SPML方法在从完全标注数据集随机采样单正标签的合成数据集上开发和测试，这不能反映真实世界场景，也无法捕捉导致困难错误分类的细粒度复杂性。

Method: 引入L48数据集，这是一个源自鸟类声音记录的细粒度真实世界多标签数据集，提供自然SPML设置和两个扩展设置，其中领域先验提供额外的负标签。

Result: 在L48上对现有SPML方法进行基准测试，观察到与合成数据集相比显著的性能差异，并分析了方法的弱点。

Conclusion: 需要更现实和困难的基准测试来推动SPML方法的发展。

Abstract: In the single-positive multi-label (SPML) setting, each image in a dataset is
labeled with the presence of a single class, while the true presence of other
classes remains unknown. The challenge is to narrow the performance gap between
this partially-labeled setting and fully-supervised learning, which often
requires a significant annotation budget. Prior SPML methods were developed and
benchmarked on synthetic datasets created by randomly sampling single positive
labels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and
CUB200. However, this synthetic approach does not reflect real-world scenarios
and fails to capture the fine-grained complexities that can lead to difficult
misclassifications. In this work, we introduce the L48 dataset, a fine-grained,
real-world multi-label dataset derived from recordings of bird sounds. L48
provides a natural SPML setting with single-positive annotations on a
challenging, fine-grained domain, as well as two extended settings in which
domain priors give access to additional negative labels. We benchmark existing
SPML methods on L48 and observe significant performance differences compared to
synthetic datasets and analyze method weaknesses, underscoring the need for
more realistic and difficult benchmarks.

</details>


### [49] [BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing](https://arxiv.org/abs/2511.00255)
*Fangxun Liu,S M Rayeed,Samuel Stevens,Alyson East,Cheng Hsuan Chiang,Colin Lee,Daniel Yi,Junke Yang,Tejas Naik,Ziyi Wang,Connor Kilrain,Elijah H Buckwalter,Jiacheng Hou,Saul Ibaven Bueno,Shuheng Wang,Xinyue Ma,Yifan Liu,Zhiyuan Tao,Ziheng Zhang,Eric Sokol,Michael Belitz,Sydne Record,Charles V. Stewart,Wei-Lun Chao*

Main category: cs.CV

TL;DR: 开发了一个3阶段自动化管道，用于处理大规模甲虫图像数据，包括检测、裁剪和形态分割，以加速生物学研究。


<details>
  <summary>Details</summary>
Motivation: 在昆虫学和生态学研究中，生物学家需要处理大量甲虫图像数据，手动处理效率低下，需要自动化解决方案来提高研究效率。

Method: 使用基于transformer的开集目标检测器和视觉语言模型进行迭代检测，然后对670张甲虫图像进行手动标注，并微调两种基于transformer的分割模型进行精细分割。

Result: 构建了一个专门用于甲虫图像处理的集成深度学习管道，能够相对准确地实现甲虫的检测和分割。

Conclusion: 该管道整合了多种深度学习方法，专门针对甲虫图像处理，能够显著提高大规模甲虫数据处理效率，加速生物学研究进程。

Abstract: In entomology and ecology research, biologists often need to collect a large
number of insects, among which beetles are the most common species. A common
practice for biologists to organize beetles is to place them on trays and take
a picture of each tray. Given the images of thousands of such trays, it is
important to have an automated pipeline to process the large-scale data for
further research. Therefore, we develop a 3-stage pipeline to detect all the
beetles on each tray, sort and crop the image of each beetle, and do
morphological segmentation on the cropped beetles. For detection, we design an
iterative process utilizing a transformer-based open-vocabulary object detector
and a vision-language model. For segmentation, we manually labeled 670 beetle
images and fine-tuned two variants of a transformer-based segmentation model to
achieve fine-grained segmentation of beetles with relatively high accuracy. The
pipeline integrates multiple deep learning methods and is specialized for
beetle image processing, which can greatly improve the efficiency to process
large-scale beetle data and accelerate biological research.

</details>


### [50] [MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba](https://arxiv.org/abs/2511.00260)
*Linzhe Jiang,Jiayuan Huang,Sophia Bano,Matthew J. Clarkson,Zhehua Mao,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 提出了MambaNetLK，一种基于Mamba状态空间模型的3D点云配准方法，专门针对内窥镜导航中的特征退化问题。该方法在临床数据集C3VD-Raycasting-10k上表现最佳，显著降低了配准误差。


<details>
  <summary>Details</summary>
Motivation: 解决内窥镜导航中生物组织重复纹理和局部均匀几何特征导致的特征退化问题，以及术前解剖与术中观察之间的域偏移导致的配准稳定性下降。

Method: 提出MambaNetLK框架，将Mamba状态空间模型作为跨模态特征提取器集成到PointNetLK架构中，使用Lucas-Kanade算法进行迭代配准，有效捕获长程依赖关系。

Result: 在C3VD-Raycasting-10k数据集上，相比次优方法，中值旋转误差降低56.04%，RMSE平移误差降低26.19%。在ModelNet40上表现出强泛化能力和对初始姿态扰动的鲁棒性。

Conclusion: MambaNetLK为手术导航中的3D配准提供了稳健基础，结合全局表达能力强的SSM特征提取器和大规模临床数据集，能够在结肠镜等微创手术中实现更准确可靠的引导系统。

Abstract: Accurate 3D point cloud registration underpins reliable image-guided
colonoscopy, directly affecting lesion localization, margin assessment, and
navigation safety. However, biological tissue exhibits repetitive textures and
locally homogeneous geometry that cause feature degeneracy, while substantial
domain shifts between pre-operative anatomy and intra-operative observations
further degrade alignment stability. To address these clinically critical
challenges, we introduce a novel 3D registration method tailored for endoscopic
navigation and a high-quality, clinically grounded dataset to support rigorous
and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale
benchmark dataset with 10,014 geometrically aligned point cloud pairs derived
from clinical CT data. We propose MambaNetLK, a novel correspondence-free
registration framework, which enhances the PointNetLK architecture by
integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor.
As a result, the proposed framework efficiently captures long-range
dependencies with linear-time complexity. The alignment is achieved iteratively
using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k,
MambaNetLK achieves the best performance compared with the state-of-the-art
methods, reducing median rotation error by 56.04% and RMSE translation error by
26.19% over the second-best method. The model also demonstrates strong
generalization on ModelNet40 and superior robustness to initial pose
perturbations. MambaNetLK provides a robust foundation for 3D registration in
surgical navigation. The combination of a globally expressive SSM-based feature
extractor and a large-scale clinical dataset enables more accurate and reliable
guidance systems in minimally invasive procedures like colonoscopy.

</details>


### [51] [Spot The Ball: A Benchmark for Visual Social Inference](https://arxiv.org/abs/2511.00261)
*Neha Balamurugan,Sarah Wu,Adam Chun,Gabe Gaw,Cristobal Eyzaguirre,Tobias Gerstenberg*

Main category: cs.CV

TL;DR: 提出了Spot The Ball基准测试，用于评估视觉语言模型在视觉社交推理方面的能力，发现人类在定位被移除的球类方面比最先进的模型准确2-3倍，揭示了模型依赖表面空间启发式而非社交线索的局限性。


<details>
  <summary>Details</summary>
Motivation: 人类擅长从微妙的行为线索（如注视、姿态）进行视觉社交推理，这对开发更类人的AI代理至关重要。现有模型在这方面的能力存在明显不足。

Method: 使用足球、篮球和排球图像，构建了Spot The Ball基准测试，包含人工标注的数据集和可扩展的测试项生成流程，评估了四种最先进的视觉语言模型和三种提示策略。

Result: 人类准确率（20-34%）是模型（≤17%）的2-3倍。模型依赖表面空间启发式（如图像中心或球员附近），而人类利用社交线索（如注视方向和身体姿态）。

Conclusion: 揭示了视觉社交推理中持续存在的人机差距，强调需要显式编码结构化行为线索的架构来实现稳健、类人的推理能力。

Abstract: Humans excel at visual social inference, the ability to infer hidden elements
of a scene from subtle behavioral cues such as other people's gaze, pose, and
orientation. This ability drives everyday social reasoning in humans and is
critical for developing more human-like AI agents. We introduce Spot The Ball,
a challenging benchmark for evaluating visual social inference in
vision-language models (VLMs) using sports as a test domain. The task is to
localize a removed sports ball from soccer, basketball, and volleyball images.
We present a curated evaluation set with human baselines and a scalable
pipeline for generating additional test items. We evaluate four
state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting
strategies, finding that humans are consistently two to three times more
accurate (20-34%) than models ($\leq$ 17%) across all sports. Our analyses show
that models rely on superficial spatial heuristics--such as guessing near the
image center or nearby players--while humans leverage social cues like gaze
direction and body pose. These findings reveal a persistent human-model gap in
visual social reasoning and underscore the need for architectures that
explicitly encode structured behavioral cues to achieve robust, human-like
inference.

</details>


### [52] [FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture](https://arxiv.org/abs/2511.00269)
*Long Li,Jiajia Li,Dong Chen,Lina Pu,Haibo Yao,Yanbo Huang*

Main category: cs.CV

TL;DR: 提出一种结合CLIP视觉变换器和轻量级分类器的联邦学习框架，用于农业分类任务，在保护隐私的同时显著提升准确率和降低通信开销


<details>
  <summary>Details</summary>
Motivation: 解决传统集中式训练的数据隐私问题，以及标准联邦学习在非独立同分布数据上的性能下降和高通信成本问题

Method: 使用预训练的CLIP ViT进行特征提取，仅对轻量级分类器进行联邦更新，并共享1%的CLIP特征表示来对齐类别表示

Result: 在农业分类任务上达到86.6%的准确率，比基线联邦学习方法提高了4倍以上

Conclusion: 结合视觉语言模型特征与联邦学习的方法在保护隐私和可扩展农业智能方面具有有效性和高效性

Abstract: Accurate classification plays a pivotal role in smart agriculture, enabling
applications such as crop monitoring, fruit recognition, and pest detection.
However, conventional centralized training often requires large-scale data
collection, which raises privacy concerns, while standard federated learning
struggles with non-independent and identically distributed (non-IID) data and
incurs high communication costs. To address these challenges, we propose a
federated learning framework that integrates a frozen Contrastive
Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight
transformer classifier. By leveraging the strong feature extraction capability
of the pre-trained CLIP ViT, the framework avoids training large-scale models
from scratch and restricts federated updates to a compact classifier, thereby
reducing transmission overhead significantly. Furthermore, to mitigate
performance degradation caused by non-IID data distribution, a small subset
(1%) of CLIP-extracted feature representations from all classes is shared
across clients. These shared features are non-reversible to raw images,
ensuring privacy preservation while aligning class representation across
participants. Experimental results on agricultural classification tasks show
that the proposed method achieve 86.6% accuracy, which is more than 4 times
higher compared to baseline federated learning approaches. This demonstrates
the effectiveness and efficiency of combining vision-language model features
with federated learning for privacy-preserving and scalable agricultural
intelligence.

</details>


### [53] [Multi-View Consistent Human Image Customization via In-Context Learning](https://arxiv.org/abs/2511.00293)
*Hengjia Li,Jianjin Xu,Keli Cheng,Lei Wang,Ning Bi,Boxi Wu,Fernando De la Torre,Deng Cai*

Main category: cs.CV

TL;DR: PersonalView是一个轻量级适配方法，仅需100个训练样本就能让现有模型获得多视角生成能力，显著优于需要大量多视角数据训练的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有个性化生成模型虽然能在不同场景下生成身份一致的图像，但无法控制生成图像的视角，也无法生成一致的多视角人物图像。

Method: PersonalView包含两个关键组件：1）利用预训练扩散变换器的上下文学习能力设计条件架构；2）通过新的语义对应对齐损失保持预训练模型的原始生成能力。

Result: 在多视角一致性、文本对齐、身份相似性和视觉质量方面，PersonalView显著优于需要大量多视角数据训练的基线方法。

Conclusion: PersonalView能够以极少的训练样本实现高质量的多视角个性化生成，为轻量级多视角定制提供了有效解决方案。

Abstract: Recent advances in personalized generative models demonstrate impressive
results in creating identity-consistent images of the same person under diverse
settings. Yet, we note that most methods cannot control the viewpoint of the
generated image, nor generate consistent multiple views of the person. To
address this problem, we propose a lightweight adaptation method, PersonalView,
capable of enabling an existing model to acquire multi-view generation
capability with as few as 100 training samples. PersonalView consists of two
key components: First, we design a conditioning architecture to take advantage
of the in-context learning ability of the pre-trained diffusion transformer.
Second, we preserve the original generative ability of the pretrained model
with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view
consistency, text alignment, identity similarity, and visual quality of
PersonalView and compare it to recent baselines with potential capability of
multi-view customization. PersonalView significantly outperforms baselines
trained on a large corpus of multi-view data with only 100 training samples.

</details>


### [54] [Towards Automated Petrography](https://arxiv.org/abs/2511.00328)
*Isai Daniel Chacón,Paola Ruiz Puentes,Jillian Pearse,Pablo Arbeláez*

Main category: cs.CV

TL;DR: 提出了LITHOS，一个大规模、多样化的自动化岩相学实验框架，包含211,604个高分辨率RGB偏振光图像块和105,802个专家标注的矿物颗粒，涵盖25个矿物类别。


<details>
  <summary>Details</summary>
Motivation: 岩相学分析是劳动密集型任务，需要专家通过光学偏振显微镜进行详细视觉检查，限制了可扩展性，因此需要自动化技术。

Method: 构建了LITHOS数据集，评估了多种深度学习技术用于矿物分类，并提出了一种双编码器transformer架构，整合两种偏振模态作为强基线。

Result: 该方法持续优于单偏振模型，证明了偏振协同在矿物分类中的价值。

Conclusion: LITHOS基准测试已公开可用，包括数据集、代码和预训练模型，以促进自动化岩相学分析的可重复性和进一步研究。

Abstract: Petrography is a branch of geology that analyzes the mineralogical
composition of rocks from microscopical thin section samples. It is essential
for understanding rock properties across geology, archaeology, engineering,
mineral exploration, and the oil industry. However, petrography is a
labor-intensive task requiring experts to conduct detailed visual examinations
of thin section samples through optical polarization microscopes, thus
hampering scalability and highlighting the need for automated techniques. To
address this challenge, we introduce the Large-scale Imaging and Thin section
Optical-polarization Set (LITHOS), the largest and most diverse publicly
available experimental framework for automated petrography. LITHOS includes
211,604 high-resolution RGB patches of polarized light and 105,802
expert-annotated grains across 25 mineral categories. Each annotation consists
of the mineral class, spatial coordinates, and expert-defined major and minor
axes represented as intersecting vector paths, capturing grain geometry and
orientation. We evaluate multiple deep learning techniques for mineral
classification in LITHOS and propose a dual-encoder transformer architecture
that integrates both polarization modalities as a strong baseline for future
reference. Our method consistently outperforms single-polarization models,
demonstrating the value of polarization synergy in mineral classification. We
have made the LITHOS Benchmark publicly available, comprising our dataset,
code, and pretrained models, to foster reproducibility and further research in
automated petrographic analysis.

</details>


### [55] [Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models](https://arxiv.org/abs/2511.00335)
*Weidong Zhang,Pak Lun Kevin Ding,Huan Liu*

Main category: cs.CV

TL;DR: 该研究首次系统评估了11种轻量级视觉模型在7个不同数据集上的跨域泛化能力，提出了跨数据集评分(xScore)指标，发现ImageNet准确率不能可靠预测细粒度或医学数据集性能，并识别出促进泛化的关键架构组件。


<details>
  <summary>Details</summary>
Motivation: 轻量级视觉分类模型主要在ImageNet上评估，但缺乏对其在其他领域泛化能力的系统研究。需要量化跨数据集鲁棒性，并确定在资源受限条件下驱动泛化的架构元素。

Method: 在7个不同数据集上，以固定100轮训练计划评估11种轻量级视觉模型(250万参数)，引入跨数据集评分(xScore)作为统一指标来量化模型性能的一致性和鲁棒性。

Result: 1) ImageNet准确率不能可靠预测细粒度或医学数据集性能；2) xScore可作为移动模型性能的可扩展预测指标，仅需4个数据集即可估计；3) 各向同性卷积、高空间分辨率和通道注意力等组件促进泛化，而Transformer块带来额外参数开销但增益有限。

Conclusion: 本研究提供了在ImageNet之外评估轻量级视觉模型的可复现框架，突出了移动友好架构的关键设计原则，为开发跨不同应用领域鲁棒泛化的未来模型提供指导。

Abstract: Lightweight vision classification models such as MobileNet, ShuffleNet, and
EfficientNet are increasingly deployed in mobile and embedded systems, yet
their performance has been predominantly benchmarked on ImageNet. This raises
critical questions: Do models that excel on ImageNet also generalize across
other domains? How can cross-dataset robustness be systematically quantified?
And which architectural elements consistently drive generalization under tight
resource constraints? Here, we present the first systematic evaluation of 11
lightweight vision models (2.5M parameters), trained under a fixed 100-epoch
schedule across 7 diverse datasets. We introduce the Cross-Dataset Score
(xScore), a unified metric that quantifies the consistency and robustness of
model performance across diverse visual domains. Our results show that (1)
ImageNet accuracy does not reliably predict performance on fine-grained or
medical datasets, (2) xScore provides a scalable predictor of mobile model
performance that can be estimated from just four datasets, and (3) certain
architectural components--such as isotropic convolutions with higher spatial
resolution and channel-wise attention--promote broader generalization, while
Transformer-based blocks yield little additional benefit, despite incurring
higher parameter overhead. This study provides a reproducible framework for
evaluating lightweight vision models beyond ImageNet, highlights key design
principles for mobile-friendly architectures, and guides the development of
future models that generalize robustly across diverse application domains.

</details>


### [56] [A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction](https://arxiv.org/abs/2511.00338)
*Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: 提出了一种结合DeepONet和NTK的混合方法来解决复杂逆问题，包括Navier-Stokes方程控制的源定位和图像重建，通过物理约束和正则化确保解的物理一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决非线性、稀疏和噪声数据相关的复杂逆问题挑战，确保解具有物理一致性。

Method: 集成DeepONet和NTK的混合方法，在损失函数中融入物理约束和任务特定正则化。

Result: 在合成和真实数据集上的验证表明该方法具有鲁棒性、可扩展性和精确性。

Conclusion: 该方法在计算物理和成像科学中具有广泛的应用潜力。

Abstract: This work presents a novel hybrid approach that integrates Deep Operator
Networks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex
inverse problem. The method effectively addresses tasks such as source
localization governed by the Navier-Stokes equations and image reconstruction,
overcoming challenges related to nonlinearity, sparsity, and noisy data. By
incorporating physics-informed constraints and task-specific regularization
into the loss function, the framework ensures solutions that are both
physically consistent and accurate. Validation on diverse synthetic and real
datasets demonstrates its robustness, scalability, and precision, showcasing
its broad potential applications in computational physics and imaging sciences.

</details>


### [57] [Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities](https://arxiv.org/abs/2511.00344)
*Xihang Qiu,Jiarong Cheng,Yuhao Fang,Wanpeng Zhang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: FedDISC框架通过联邦学习整合模态特定扩散模型，解决多模态情感识别中模态缺失问题，确保恢复模态与可用模态的语义一致性，在多种缺失模式下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中不可预测的模态缺失会显著降低多模态情感识别性能，传统依赖完整多模态数据的恢复方法在极端数据分布下容易产生语义失真。

Method: 提出FedDISC框架，集成联邦学习到模态缺失恢复中，使用DISC-Diffusion模块确保恢复模态与可用模态在上下文、说话人身份和语义上的一致性，并采用交替冻结聚合策略促进协作优化。

Result: 在IEMOCAP、CMUMOSI和CMUMOSEI数据集上的实验表明，FedDISC在多种缺失模态模式下实现了优越的情感分类性能，超越了现有方法。

Conclusion: FedDISC通过联邦学习有效解决了多模态情感识别中的模态缺失问题，确保了语义一致性，并在多种数据集上表现出色。

Abstract: Multimodal Emotion Recognition in Conversations (MERC) enhances emotional
understanding through the fusion of multimodal signals. However, unpredictable
modality absence in real-world scenarios significantly degrades the performance
of existing methods. Conventional missing-modality recovery approaches, which
depend on training with complete multimodal data, often suffer from semantic
distortion under extreme data distributions, such as fixed-modality absence. To
address this, we propose the Federated Dialogue-guided and Semantic-Consistent
Diffusion (FedDISC) framework, pioneering the integration of federated learning
into missing-modality recovery. By federated aggregation of modality-specific
diffusion models trained on clients and broadcasting them to clients missing
corresponding modalities, FedDISC overcomes single-client reliance on modality
completeness. Additionally, the DISC-Diffusion module ensures consistency in
context, speaker identity, and semantics between recovered and available
modalities, using a Dialogue Graph Network to capture conversational
dependencies and a Semantic Conditioning Network to enforce semantic alignment.
We further introduce a novel Alternating Frozen Aggregation strategy, which
cyclically freezes recovery and classifier modules to facilitate collaborative
optimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI
datasets demonstrate that FedDISC achieves superior emotion classification
performance across diverse missing modality patterns, outperforming existing
approaches.

</details>


### [58] [OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data](https://arxiv.org/abs/2511.00345)
*Amir Ziashahabi,Narges Ghasemi,Sajjad Shahabi,John Krumm,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: OSMGen是一个生成框架，可以直接从原始OpenStreetMap数据创建逼真的卫星图像，支持生成前后对比图像对，用于解决训练数据稀缺和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 准确和最新的地理空间数据对城市规划、基础设施监测和环境管理至关重要，但特定城市特征及其变化的标注数据集稀缺，自动化城市监测仍然困难。

Method: 使用完整的OSM JSON数据（包括矢量几何、语义标签、位置和时间），通过生成框架从原始OSM数据直接创建卫星图像，支持用户编辑OSM输入来产生针对性视觉变化。

Result: 能够生成一致的before-after图像对，保持场景其余部分不变，为训练数据生成和规划干预预览提供了有效工具。

Conclusion: OSMGen为静态和变化状态生成配对的（JSON，图像）数据，为实现卫星图像自动驱动结构化OSM更新的闭环系统铺平了道路。

Abstract: Accurate and up-to-date geospatial data are essential for urban planning,
infrastructure monitoring, and environmental management. Yet, automating urban
monitoring remains difficult because curated datasets of specific urban
features and their changes are scarce. We introduce OSMGen, a generative
framework that creates realistic satellite imagery directly from raw
OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen
uses the full richness of OSM JSON, including vector geometries, semantic tags,
location, and time, giving fine-grained control over how scenes are generated.
A central feature of the framework is the ability to produce consistent
before-after image pairs: user edits to OSM inputs translate into targeted
visual changes, while the rest of the scene is preserved. This makes it
possible to generate training data that addresses scarcity and class imbalance,
and to give planners a simple way to preview proposed interventions by editing
map data. More broadly, OSMGen produces paired (JSON, image) data for both
static and changed states, paving the way toward a closed-loop system where
satellite imagery can automatically drive structured OSM updates. Source code
is available at https://github.com/amir-zsh/OSMGen.

</details>


### [59] [Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach](https://arxiv.org/abs/2511.00352)
*Mohd Ruhul Ameen,Akif Islam*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型重建动态的AI生成图像检测方法，通过分析不同噪声强度下的重建指标变化来区分真实和合成图像。


<details>
  <summary>Details</summary>
Motivation: 随着生成式扩散模型的快速发展，区分真实视觉内容与合成图像变得越来越困难。传统的深度伪造检测方法基于频率或像素级伪影，无法应对现代文本到图像系统（如Stable Diffusion和DALL-E）生成的光真实感且无伪影的结果。

Method: 利用多强度图像重建动态（称为扩散回弹）来识别AI生成图像。通过分析重建指标（LPIPS、SSIM和PSNR）在不同噪声强度下的演变，提取可解释的基于流形的特征来区分真实和合成图像。

Result: 在包含4000张图像的平衡数据集上评估，该方法在交叉验证下达到0.993 AUROC，并对常见失真（如压缩和噪声）保持鲁棒性。

Conclusion: 尽管使用有限数据和单一扩散骨干网络（Stable Diffusion v1.5），所提出的方法展示了强大的泛化能力和可解释性，为可扩展、模型无关的合成媒体取证提供了基础。

Abstract: The rapid rise of generative diffusion models has made distinguishing
authentic visual content from synthetic imagery increasingly challenging.
Traditional deepfake detection methods, which rely on frequency or pixel-level
artifacts, fail against modern text-to-image systems such as Stable Diffusion
and DALL-E that produce photorealistic and artifact-free results. This paper
introduces a diffusion-based forensic framework that leverages multi-strength
image reconstruction dynamics, termed diffusion snap-back, to identify
AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and
PSNR) evolve across varying noise strengths, we extract interpretable
manifold-based features that differentiate real and synthetic images. Evaluated
on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under
cross-validation and remains robust to common distortions such as compression
and noise. Despite using limited data and a single diffusion backbone (Stable
Diffusion v1.5), the proposed method demonstrates strong generalization and
interpretability, offering a foundation for scalable, model-agnostic synthetic
media forensics.

</details>


### [60] [Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation](https://arxiv.org/abs/2511.00357)
*Niklas Wölki,Lukas Kondmann,Christian Mollière,Martin Langer,Julia Gottfriedsen,Martin Werner*

Main category: cs.CV

TL;DR: 该论文提出了一种基于迁移学习的轻量级热红外云分割方法，用于CubeSat卫星任务，通过使用UNet和MobileNet编码器，在有限硬件条件下实现高效的热红外云检测。


<details>
  <summary>Details</summary>
Motivation: CubeSat卫星任务在热红外地球观测中面临硬件限制和标注数据不足的问题，传统云掩码技术难以应用，需要开发适用于单热红外波段的高效云分割方法。

Method: 使用UNet架构配合轻量级MobileNet编码器，在Landsat-7云覆盖评估数据集上进行预训练，然后使用少量任务特定样本进行微调，采用联合训练策略，并将模型转换为TensorRT引擎。

Result: 与仅使用FOREST-2数据的基线相比，宏F1分数从0.850提升到0.877，在NVIDIA Jetson Nano上实现全图像推理时间低于5秒。

Conclusion: 利用公共数据集和轻量级架构可以在轨道上实现准确高效的热红外云掩码，支持数据有限的地球观测任务中的实时决策。

Abstract: Onboard cloud segmentation is a critical yet underexplored task in thermal
Earth observation (EO), particularly for CubeSat missions constrained by
limited hardware and spectral information. CubeSats often rely on a single
thermal band and lack sufficient labeled data, making conventional cloud
masking techniques infeasible. This work addresses these challenges by applying
transfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using
a UNet with a lightweight MobileNet encoder. We pretrain the model on the
public Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small
set of mission-specific samples in a joint-training setup, improving the macro
F1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a
TensorRT engine and demonstrate full-image inference in under 5 seconds on an
NVIDIA Jetson Nano. These results show that leveraging public datasets and
lightweight architectures can enable accurate, efficient thermal-only cloud
masking on-orbit, supporting real-time decision-making in data-limited EO
missions.

</details>


### [61] [Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery](https://arxiv.org/abs/2511.00362)
*Momen Khandoker Ope,Akif Islam,Mohd Ruhul Ameen,Abu Saleh Musa Miah,Md Rashedul Islam,Jungpil Shin*

Main category: cs.CV

TL;DR: Oitijjo-3D是一个免费的生成式AI框架，利用Google街景图像重建孟加拉国文化遗产的3D模型，解决了传统3D数字化方法成本高、技术要求高的问题。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国文化遗产修复面临资源有限和技术专家稀缺的双重挑战，传统3D数字化方法需要昂贵的硬件、专业操作人员和大量现场访问，这在发展中国家往往不可行。

Method: 采用两阶段流程：使用Gemini 2.5 Flash Image进行多模态视觉推理实现结构-纹理合成，通过Hexagen进行神经图像到3D生成实现几何恢复。

Result: 系统在几秒钟内生成照片级真实、度量一致的重建结果，相比传统运动结构恢复流程显著提速，且不需要任何专业硬件或专家监督。

Conclusion: 通过将开放图像转化为数字遗产，这项工作将保护重新定义为社区驱动、AI辅助的文化连续性行为，适用于资源有限的国家。

Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited
resources and scarce technical expertise. Traditional 3D digitization methods,
such as photogrammetry or LiDAR scanning, require expensive hardware, expert
operators, and extensive on-site access, which are often infeasible in
developing contexts. As a result, many of Bangladesh's architectural treasures,
from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to
decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a
cost-free generative AI framework that democratizes 3D cultural preservation.
By using publicly available Google Street View imagery, Oitijjo-3D reconstructs
faithful 3D models of heritage structures through a two-stage pipeline -
multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture
synthesis, and neural image-to-3D generation through Hexagen for geometry
recovery. The system produces photorealistic, metrically coherent
reconstructions in seconds, achieving significant speedups compared to
conventional Structure-from-Motion pipelines, without requiring any specialized
hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,
Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both
visual and structural fidelity while drastically lowering economic and
technical barriers. By turning open imagery into digital heritage, this work
reframes preservation as a community-driven, AI-assisted act of cultural
continuity for resource-limited nations.

</details>


### [62] [Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict](https://arxiv.org/abs/2511.00370)
*Chaochen Wu,Guan Luo,Meiyun Zuo,Zhitao Fan*

Main category: cs.CV

TL;DR: 提出基于强化学习的视频片段检索模型，通过多智能体系统和证据学习解决不同模型定位结果冲突，无需额外训练即可判断查询是否超出视频范围。


<details>
  <summary>Details</summary>
Motivation: 当前视频片段检索方法未考虑不同模型定位结果间的冲突，导致无法有效整合多个模型以获得更好的检索结果。

Method: 使用强化学习模型一次性扫描整个视频确定片段边界并生成位置证据，提出多智能体系统框架利用证据学习解决智能体定位输出冲突。

Result: 在基准数据集上的大量实验表明，所提方法相比最先进方法具有更好的效果。

Conclusion: 多智能体系统中的竞争与冲突建模是提高强化学习在片段检索中性能的有效方式，并展示了证据学习在多智能体框架中的新作用。

Abstract: Video moment retrieval uses a text query to locate a moment from a given
untrimmed video reference. Locating corresponding video moments with text
queries helps people interact with videos efficiently. Current solutions for
this task have not considered conflict within location results from different
models, so various models cannot integrate correctly to produce better results.
This study introduces a reinforcement learning-based video moment retrieval
model that can scan the whole video once to find the moment's boundary while
producing its locational evidence. Moreover, we proposed a multi-agent system
framework that can use evidential learning to resolve conflicts between agents'
localization output. As a side product of observing and dealing with conflicts
between agents, we can decide whether a query has no corresponding moment in a
video (out-of-scope) without additional training, which is suitable for
real-world applications. Extensive experiments on benchmark datasets show the
effectiveness of our proposed methods compared with state-of-the-art
approaches. Furthermore, the results of our study reveal that modeling
competition and conflict of the multi-agent system is an effective way to
improve RL performance in moment retrieval and show the new role of evidential
learning in the multi-agent framework.

</details>


### [63] [VisionCAD: An Integration-Free Radiology Copilot Framework](https://arxiv.org/abs/2511.00381)
*Jiaming Li,Junlei Wu,Sheng Wang,Honglin Xiong,Jiangdong Cai,Zihao Zhao,Yitao Zhu,Yuan Yin,Dinggang Shen,Qian Wang*

Main category: cs.CV

TL;DR: VisionCAD是一个基于视觉的放射学辅助框架，通过摄像头直接从显示器捕获医学图像，绕过了与医院IT系统集成的障碍。


<details>
  <summary>Details</summary>
Motivation: 传统计算机辅助诊断系统难以与现有医院IT基础设施集成，阻碍了临床广泛应用。

Method: 采用自动化流程检测、恢复和分析屏幕上的医学图像，将摄像头捕获的视觉数据转换为适合自动化分析和报告生成的诊断质量图像。

Result: 在多种医学影像数据集上验证，诊断性能与传统CAD系统相当，分类任务F1分数下降通常小于2%，自动报告的自然语言生成指标与原图相比差距在1%以内。

Conclusion: VisionCAD仅需摄像头设备和标准计算资源，为AI辅助诊断提供了可访问的方法，可在各种临床环境中部署诊断能力，无需修改现有基础设施。

Abstract: Widespread clinical deployment of computer-aided diagnosis (CAD) systems is
hindered by the challenge of integrating with existing hospital IT
infrastructure. Here, we introduce VisionCAD, a vision-based radiological
assistance framework that circumvents this barrier by capturing medical images
directly from displays using a camera system. The framework operates through an
automated pipeline that detects, restores, and analyzes on-screen medical
images, transforming camera-captured visual data into diagnostic-quality images
suitable for automated analysis and report generation. We validated VisionCAD
across diverse medical imaging datasets, demonstrating that our modular
architecture can flexibly utilize state-of-the-art diagnostic models for
specific tasks. The system achieves diagnostic performance comparable to
conventional CAD systems operating on original digital images, with an F1-score
degradation typically less than 2\% across classification tasks, while natural
language generation metrics for automated reports remain within 1\% of those
derived from original images. By requiring only a camera device and standard
computing resources, VisionCAD offers an accessible approach for AI-assisted
diagnosis, enabling the deployment of diagnostic capabilities in diverse
clinical settings without modifications to existing infrastructure.

</details>


### [64] [Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond](https://arxiv.org/abs/2511.00389)
*Fan Zhang,Haoxuan Li,Shengju Qian,Xin Wang,Zheng Lian,Hao Wu,Zhihong Zhu,Yuan Gao,Qiankun Li,Yefeng Zheng,Zhouchen Lin,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: FERBench基准测试发现多模态大语言模型在面部表情识别任务中表现良好但推理能力有限，为此开发了UniFER-7B模型，通过后训练策略显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在多个领域取得成功，但在面部表情识别任务中的表现尚未被系统评估，需要填补这一研究空白。

Method: 构建FERBench基准测试20个先进模型，创建UniFER-CoT-230K和UniFER-RLVR-360K数据集，采用思维链初始化和强化学习奖励验证的后训练策略。

Result: UniFER-7B模型超越了包括Gemini-2.5-Pro和Qwen2.5-VL-72B在内的多个开源和闭源通用多模态大语言模型。

Conclusion: 通过专门的后训练策略可以显著提升多模态大语言模型在面部表情识别任务中的推理和可解释性能力。

Abstract: Multimodal Large Language Models (MLLMs) have revolutionized numerous
research fields, including computer vision and affective computing. As a
pivotal challenge in this interdisciplinary domain, facial expression
recognition (FER) has evolved from separate, domain-specific models to more
unified approaches. One promising avenue to unify FER tasks is converting
conventional FER datasets into visual question-answering (VQA) formats,
enabling the direct application of powerful generalist MLLMs for inference.
However, despite the success of cutting-edge MLLMs in various tasks, their
performance on FER tasks remains largely unexplored. To address this gap, we
provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art
MLLMs across four widely used FER datasets. Our results reveal that, while
MLLMs exhibit good classification performance, they still face significant
limitations in reasoning and interpretability. To this end, we introduce
post-training strategies aimed at enhancing the facial expression reasoning
capabilities of MLLMs. Specifically, we curate two high-quality and large-scale
datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K
for reinforcement learning with verifiable rewards (RLVR), respectively.
Building upon them, we develop a unified and interpretable FER foundation model
termed UniFER-7B, which outperforms many open-sourced and closed-source
generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).

</details>


### [65] [VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning](https://arxiv.org/abs/2511.00391)
*Xuanle Zhao,Deyang Jiang,Zhixiong Zeng,Lei Chen,Haibo Qiu,Jing Huang,Yufeng Zhong,Liming Zheng,Yilin Cao,Lin Ma*

Main category: cs.CV

TL;DR: VinciCoder是一个统一的多模态代码生成模型，通过两阶段训练框架解决现有视觉语言模型在代码生成任务中的局限性，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在代码生成任务中依赖单任务训练，形成了狭隘的范式，阻碍了通用视觉代码智能的发展。

Method: 采用两阶段训练框架：首先构建包含160万图像-代码对的监督微调语料库，然后引入视觉强化学习策略，通过从粗到细的奖励机制计算局部和全局图像块的视觉相似度。

Result: 在各种多模态代码生成基准测试中，VinciCoder实现了最先进的性能，证明了从粗到细的视觉强化学习策略的有效性。

Conclusion: VinciCoder通过统一的多模态代码生成方法和创新的视觉强化学习策略，成功提升了视觉代码智能的泛化能力。

Abstract: Multimodal code generation has garnered significant interest within the
research community. Despite the notable success of recent vision-language
models (VLMs) on specialized tasks like Chart-to-code generation, their
reliance on single-task training regimens fosters a narrow paradigm that
hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode
\textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a
unified multimodal code generation model that addresses this limitation via a
two-stage training framework. We begin by constructing a large-scale Supervised
Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving
direct code generation and visual-based code refinement. Subsequently, we
introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a
coarse-to-fine reward mechanism to improve visual fidelity by calculating
visual similarity across local and global image patches. Extensive experiments
on various multimodal code generation benchmarks demonstrate that VinciCoder
achieves state-of-the-art performance, underscoring the effectiveness of our
coarse-to-fine ViRL strategy. The code and model will be available at
https://github.com/DocTron-hub/VinciCoder.

</details>


### [66] [CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks](https://arxiv.org/abs/2511.00396)
*Long Li,Shuichen Ji,Ziyang Luo,Nian Liu,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: 提出了首个统一框架，通过将SOD、CoSOD和SIS三个异构显著性任务建模为视觉语言模型中的思维链推理过程，使用两阶段训练范式（SFT+RL）和置信度引导策略优化方法，在多个任务上达到或超越专门化SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 解决操作异构的显著性任务（SOD、CoSOD、SIS）的统一处理问题，通过思维链推理过程来弥合任务异质性。

Method: 采用两阶段训练范式：监督微调（SFT）和强化学习（RL）。提出置信度引导策略优化（CGPO）算法，利用奖励与模型置信度差异作为优势信号，消除分组采样。同时引入"输出到推理"策略构建高质量SFT数据。

Result: 模型在所有任务上匹配或超越专门化SOTA方法和强闭源VLM，特别是在CoSOD任务上CoCA数据集S-measure达到0.899，比之前最佳方法提升8.0个百分点，且使用更少训练数据。

Conclusion: 提出的统一框架通过思维链推理和CGPO算法有效处理异构显著性任务，在多个任务上取得优异性能，证明了方法的有效性和效率。

Abstract: We present the first unified framework that jointly handles three
operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting
each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model
(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:
Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT
quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a
lightweight single-sample algorithm that leverages the discrepancy between
reward and model confidence as a per-sample advantage signal. This design
naturally focuses updates on informative responses while eliminating group
sampling, thereby addressing GRPO's key limitations: confidence-agnostic
learning, signal dilution, and prohibitive computational overhead. We also
introduce an "output-to-reasoning" strategy to construct high-fidelity SFT data
that ensures logical consistency with ground-truth masks. Experiments show our
model matches or outperforms specialized SOTA methods and strong closed-source
VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for
CoSOD, surpassing the prior best by 8.0 percentage points, despite using far
less training data.

</details>


### [67] [LGCA: Enhancing Semantic Representation via Progressive Expansion](https://arxiv.org/abs/2511.00419)
*Thanh Hieu Cao,Trung Khang Tran,Gia Thinh Pham,Tuong Nghiem Diep,Thanh Binh Nguyen*

Main category: cs.CV

TL;DR: 提出了LGCA框架，通过局部特征捕获和显著区域扩展来改进CLIP模型的零样本图像分类性能，避免随机裁剪引入的误导信息。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在零样本图像分类中表现良好，但随机图像裁剪会引入误导信息和偏见，因为小尺度下许多图像具有相似特征。

Method: LGCA框架首先捕获图像的局部特征，然后重复选择最显著的区域并扩展它们，设计相似度评分同时考虑原始和扩展图像。

Result: 大量实验表明该方法在多个数据集上显著提升了零样本性能，优于现有最先进基线方法。

Conclusion: LGCA框架能有效捕捉局部和全局特征，同时最小化误导信息，且时间复杂度与原始模型相同，具有高效性和可扩展性。

Abstract: Recent advancements in large-scale pretraining in natural language processing
have enabled pretrained vision-language models such as CLIP to effectively
align images and text, significantly improving performance in zero-shot image
classification tasks. Subsequent studies have further demonstrated that
cropping images into smaller regions and using large language models to
generate multiple descriptions for each caption can further enhance model
performance. However, due to the inherent sensitivity of CLIP, random image
crops can introduce misinformation and bias, as many images share similar
features at small scales. To address this issue, we propose
Localized-Globalized Cross-Alignment (LGCA), a framework that first captures
the local features of an image and then repeatedly selects the most salient
regions and expands them. The similarity score is designed to incorporate both
the original and expanded images, enabling the model to capture both local and
global features while minimizing misinformation. Additionally, we provide a
theoretical analysis demonstrating that the time complexity of LGCA remains the
same as that of the original model prior to the repeated expansion process,
highlighting its efficiency and scalability. Extensive experiments demonstrate
that our method substantially improves zero-shot performance across diverse
datasets, outperforming state-of-the-art baselines.

</details>


### [68] [Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection](https://arxiv.org/abs/2511.00427)
*Daichi Zhang,Tong Zhang,Jianmin Bao,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 提出了一种基于图像-文本不对齐的生成图像检测方法ITEM，利用CLIP空间中的多模态信息来识别伪造图像，相比传统仅依赖视觉特征的方法具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法将生成图像检测视为简单的二分类任务，仅关注视觉线索，容易过拟合特定图像模式，无法泛化到未见过的生成模型。

Method: 利用预训练CLIP空间测量图像和描述之间的不对齐程度，然后训练MLP头进行分类；提出分层不对齐方案，先关注整张图像，再关注描述中的每个语义对象。

Result: 在多个最新生成模型上的实验表明，该方法优于其他最先进方法，具有令人印象深刻的泛化能力和鲁棒性。

Conclusion: 多模态视角下的图像-文本不对齐是检测生成图像的有效线索，能够克服传统方法泛化能力不足的问题。

Abstract: With the rapid development of generative models, detecting generated fake
images to prevent their malicious use has become a critical issue recently.
Existing methods frame this challenge as a naive binary image classification
task. However, such methods focus only on visual clues, yielding trained
detectors susceptible to overfitting specific image patterns and incapable of
generalizing to unseen models. In this paper, we address this issue from a
multi-modal perspective and find that fake images cannot be properly aligned
with corresponding captions compared to real images. Upon this observation, we
propose a simple yet effective detector termed ITEM by leveraging the
image-text misalignment in a joint visual-language space as discriminative
clues. Specifically, we first measure the misalignment of the images and
captions in pre-trained CLIP's space, and then tune a MLP head to perform the
usual detection task. Furthermore, we propose a hierarchical misalignment
scheme that first focuses on the whole image and then each semantic object
described in the caption, which can explore both global and fine-grained local
semantic misalignment as clues. Extensive experiments demonstrate the
superiority of our method against other state-of-the-art competitors with
impressive generalization and robustness on various recent generative models.

</details>


### [69] [Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection](https://arxiv.org/abs/2511.00429)
*Daichi Zhang,Tong Zhang,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 提出一种基于频率伪造线索(F^2C)的扩散模型图像检测方法，通过增强所有频带的频率差异特征来提升检测器的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的图像质量很高，但可能被恶意使用。现有检测器难以捕捉不同模型和设置下的判别线索，限制了其对未见扩散模型的泛化能力和对各种扰动的鲁棒性。

Method: 观察到扩散生成图像与自然真实图像在低频到高频频带中表现出逐渐增大的差异。提出频率选择性函数作为加权滤波器来抑制判别性较弱的频带，增强信息量更大的频带。

Result: 在多个扩散生成图像数据集上的广泛实验表明，该方法优于现有最先进的检测器，具有更好的泛化性和鲁棒性。

Conclusion: 基于频率伪造线索的方法能够有效检测来自未见扩散模型的图像，并对各种扰动具有强大的鲁棒性。

Abstract: Diffusion models have achieved remarkable success in image synthesis, but the
generated high-quality images raise concerns about potential malicious use.
Existing detectors often struggle to capture discriminative clues across
different models and settings, limiting their generalization to unseen
diffusion models and robustness to various perturbations. To address this
issue, we observe that diffusion-generated images exhibit progressively larger
differences from natural real images across low- to high-frequency bands. Based
on this insight, we propose a simple yet effective representation by enhancing
the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we
introduce a frequency-selective function which serves as a weighted filter to
the Fourier spectrum, suppressing less discriminative bands while enhancing
more informative ones. This approach, grounded in a comprehensive analysis of
frequency-based differences between natural real and diffusion-generated
images, enables general detection of images from unseen diffusion models and
provides robust resilience to various perturbations. Extensive experiments on
various diffusion-generated image datasets demonstrate that our method
outperforms state-of-the-art detectors with superior generalization and
robustness.

</details>


### [70] [ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training](https://arxiv.org/abs/2511.00446)
*Xin Yao,Haiyang Zhao,Yimin Chen,Jiawei Guo,Kecheng Huang,Ming Zhao*

Main category: cs.CV

TL;DR: ToxicTextCLIP是一个针对CLIP预训练阶段的文本模态对抗攻击框架，通过背景感知选择和背景驱动增强生成高质量毒化文本，在分类和检索任务中达到高攻击成功率并能绕过现有防御。


<details>
  <summary>Details</summary>
Motivation: CLIP模型依赖未筛选的互联网数据，面临数据中毒和后门风险。现有研究主要关注图像攻击，而文本模态作为CLIP训练的核心组成部分尚未得到充分探索。

Method: ToxicTextCLIP框架迭代应用两个组件：1) 背景感知选择器，优先选择与目标类别背景一致性的文本；2) 背景驱动增强器，生成语义连贯且多样化的毒化样本。

Result: 在分类和检索任务中，ToxicTextCLIP达到95.83%的中毒成功率和98.68%的后门Hit@1，并能成功绕过RoCLIP、CleanCLIP和SafeCLIP等防御机制。

Conclusion: 该研究揭示了CLIP模型在文本模态上的安全漏洞，提出的ToxicTextCLIP框架在攻击效果和防御绕过方面表现出色，强调了多模态模型安全性的重要性。

Abstract: The Contrastive Language-Image Pretraining (CLIP) model has significantly
advanced vision-language modeling by aligning image-text pairs from large-scale
web data through self-supervised contrastive learning. Yet, its reliance on
uncurated Internet-sourced data exposes it to data poisoning and backdoor
risks. While existing studies primarily investigate image-based attacks, the
text modality, which is equally central to CLIP's training, remains
underexplored. In this work, we introduce ToxicTextCLIP, a framework for
generating high-quality adversarial texts that target CLIP during the
pre-training phase. The framework addresses two key challenges: semantic
misalignment caused by background inconsistency with the target class, and the
scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively
applies: 1) a background-aware selector that prioritizes texts with background
content aligned to the target class, and 2) a background-driven augmenter that
generates semantically coherent and diverse poisoned samples. Extensive
experiments on classification and retrieval tasks show that ToxicTextCLIP
achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while
bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be
accessed via https://github.com/xinyaocse/ToxicTextCLIP/.

</details>


### [71] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: 提出弱监督深度学习框架，使用Grad-CAM进行肺炎分类和定位，仅需图像级标签即可生成临床意义的热力图，在Kermany CXR数据集上达到98%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决传统肺炎诊断需要昂贵像素级标注的问题，开发仅需图像级标签的弱监督方法，提高AI在医学影像诊断中的透明度和临床可信度。

Method: 使用七种ImageNet预训练架构（ResNet-18/50、DenseNet-121、EfficientNet-B0、MobileNet-V2/V3、ViT-B16），在相同训练条件下采用焦点损失和患者级数据分割，利用Grad-CAM生成解释性热力图。

Result: ResNet-18和EfficientNet-B0达到最佳测试准确率98%，ROC-AUC=0.997，F1=0.987；MobileNet-V2在准确率和计算成本间取得最佳平衡；Grad-CAM可视化确认模型聚焦于临床相关肺区域。

Conclusion: 弱监督可解释模型在肺炎筛查中具有重要潜力，能够增强AI辅助医学影像的透明度和临床信任，为放射学诊断提供可靠工具。

Abstract: This study proposes a weakly supervised deep learning framework for pneumonia
classification and localization from chest X-rays, utilizing Grad-CAM
explanations. Instead of costly pixel-level annotations, our approach utilizes
image-level labels to generate clinically meaningful heatmaps that highlight
regions affected by pneumonia. We evaluate seven ImageNet-pretrained
architectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and
ViT-B16 under identical training conditions with focal loss and patient-wise
splits to prevent data leakage. Experimental results on the Kermany CXR dataset
demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test
accuracy of 98\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides
an optimal trade-off between accuracy and computational cost. Grad-CAM
visualizations confirm that the proposed models focus on clinically relevant
lung regions, supporting the use of interpretable AI for radiological
diagnostics. This work highlights the potential of weakly supervised
explainable models that enhance pneumonia screening transparency, and clinical
trust in AI-assisted medical imaging.
  https://github.com/kiranshahi/pneumonia-analysis

</details>


### [72] [HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation](https://arxiv.org/abs/2511.00468)
*Panwang Pan,Tingting Shen,Chenxin Li,Yunlong Lin,Kairun Wen,Jingjing Zhao,Yixuan Yuan*

Main category: cs.CV

TL;DR: HumanCrafter是一个统一框架，能够从单张图像中联合建模外观和人体部位语义，通过整合几何先验和自监督语义先验，在3D人体分割和重建任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在3D人体重建方面取得了高保真度，但在特定任务（如人体3D分割）中的应用仍然受限。

Method: 在重建阶段整合人体几何先验，在分割阶段整合自监督语义先验；开发交互式标注程序生成高质量数据标签对；通过像素对齐聚合实现跨任务协同，多任务目标同时优化纹理建模保真度和语义一致性。

Result: 大量实验表明，HumanCrafter在单图像3D人体部位分割和3D人体重建方面均超越了现有最先进方法。

Conclusion: HumanCrafter通过统一框架实现了外观和语义的联合建模，在3D人体分析任务中表现出色。

Abstract: Recent advances in generative models have achieved high-fidelity in 3D human
reconstruction, yet their utility for specific tasks (e.g., human 3D
segmentation) remains constrained. We propose HumanCrafter, a unified framework
that enables the joint modeling of appearance and human-part semantics from a
single image in a feed-forward manner. Specifically, we integrate human
geometric priors in the reconstruction stage and self-supervised semantic
priors in the segmentation stage. To address labeled 3D human datasets
scarcity, we further develop an interactive annotation procedure for generating
high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task
synergy, while the multi-task objective simultaneously optimizes texture
modeling fidelity and semantic consistency. Extensive experiments demonstrate
that HumanCrafter surpasses existing state-of-the-art methods in both 3D
human-part segmentation and 3D human reconstruction from a single image.

</details>


### [73] [Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations](https://arxiv.org/abs/2511.00472)
*Navodini Wijethilake,Marina Ivory,Oscar MacCormac,Siddhant Kumar,Aaron Kujawa,Lorena Garcia-Foncillas Macias,Rebecca Burger,Amanda Hitchings,Suki Thomson,Sinan Barazi,Eleni Maratos,Rupert Obholzer,Dan Jiang,Fiona McClenaghan,Kazumi Chia,Omar Al-Salihi,Nick Thomas,Steve Connor,Tom Vercauteren,Jonathan Shapey*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的迭代分割和质量优化框架，用于MRI中前庭神经鞘瘤的自动分割，通过人机协作方法显著提高了分割精度和效率。


<details>
  <summary>Details</summary>
Motivation: 前庭神经鞘瘤的准确分割对患者管理至关重要，但传统手动标注耗时且依赖专家。现有深度学习模型在多样化数据集和复杂临床案例中表现不稳定。

Method: 采用自举式深度学习框架进行迭代分割和质量优化，结合多中心数据并依赖专家共识确保标注可信度，实现人机协作模型训练。

Result: 在内部验证数据集上Dice相似系数从0.9125提升至0.9670，在外部数据集上保持稳定性能，估计比传统手动标注效率提升约37.4%。

Conclusion: 该人机协作方法实现了高分割精度，展示了作为临床适应性强、可推广的自动前庭神经鞘瘤分割策略的潜力。

Abstract: Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance
Imaging (MRI) is essential for patient management but often requires
time-intensive manual annotations by experts. While recent advances in deep
learning (DL) have facilitated automated segmentation, challenges remain in
achieving robust performance across diverse datasets and complex clinical
cases. We present an annotated dataset stemming from a bootstrapped DL-based
framework for iterative segmentation and quality refinement of VS in MRI. We
combine data from multiple centres and rely on expert consensus for
trustworthiness of the annotations. We show that our approach enables effective
and resource-efficient generalisation of automated segmentation models to a
target data distribution. The framework achieved a significant improvement in
segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from
0.9125 to 0.9670 on our target internal validation dataset, while maintaining
stable performance on representative external datasets. Expert evaluation on
143 scans further highlighted areas for model refinement, revealing nuanced
cases where segmentation required expert intervention. The proposed approach is
estimated to enhance efficiency by approximately 37.4% compared to the
conventional manual annotation process. Overall, our human-in-the-loop model
training approach achieved high segmentation accuracy, highlighting its
potential as a clinically adaptable and generalisable strategy for automated VS
segmentation in diverse clinical settings. The dataset includes 190 patients,
with tumour annotations available for 534 longitudinal contrast-enhanced
T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans
from 6 patients. This dataset is publicly accessible on The Cancer Imaging
Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).

</details>


### [74] [FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts](https://arxiv.org/abs/2511.00480)
*Weihao Bo,Yanpeng Sun,Yu Wang,Xinyu Zhang,Zechao Li*

Main category: cs.CV

TL;DR: FedMGP是一种个性化的联邦提示学习方法，通过多组文本和视觉提示捕捉细粒度语义特征，采用基于相似度的动态提示聚合策略，在保持参数效率的同时实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦提示学习方法难以有效平衡共同知识和客户端特定特征，需要一种能够捕捉多样化细粒度语义线索的个性化方法。

Method: 为每个客户端配备多组配对的文本和视觉提示，引入多样性损失使每组提示专注于不同语义方面，采用基于余弦相似度的概率采样进行动态提示聚合。

Result: 在多个联邦视觉语言基准测试中，FedMGP在个性化和领域泛化方面均优于现有方法，且在所有联邦提示学习方法中通信参数最低。

Conclusion: FedMGP通过多组提示和动态聚合策略有效平衡了共享语义和客户端特定特征，实现了高效的个性化联邦学习。

Abstract: In this paper, we introduce FedMGP, a new paradigm for personalized federated
prompt learning in vision-language models. FedMGP equips each client with
multiple groups of paired textual and visual prompts, enabling the model to
capture diverse, fine-grained semantic and instance-level cues. A diversity
loss is introduced to drive each prompt group to specialize in distinct and
complementary semantic aspects, ensuring that the groups collectively cover a
broader range of local characteristics. During communication, FedMGP employs a
dynamic prompt aggregation strategy based on similarity-guided probabilistic
sampling: each client computes the cosine similarity between its prompt groups
and the global prompts from the previous round, then samples s groups via a
softmax-weighted distribution. This soft selection mechanism preferentially
aggregates semantically aligned knowledge while still enabling exploration of
underrepresented patterns effectively balancing the preservation of common
knowledge with client-specific features. Notably, FedMGP maintains parameter
efficiency by redistributing a fixed prompt capacity across multiple groups,
achieving state-of-the-art performance with the lowest communication parameters
among all federated prompt learning methods. Theoretical analysis shows that
our dynamic aggregation strategy promotes robust global representation learning
by reinforcing shared semantics while suppressing client-specific noise.
Extensive experiments demonstrate that FedMGP consistently outperforms prior
approaches in both personalization and domain generalization across diverse
federated vision-language benchmarks. The code will be released on
https://github.com/weihao-bo/FedMGP.git.

</details>


### [75] [Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models](https://arxiv.org/abs/2511.00503)
*Panwang Pan,Chenguo Lin,Jingjing Zhao,Chenxin Li,Yuchen Lin,Haopeng Li,Honglei Yan,Kairun Wen,Yunlong Lin,Yixuan Yuan,Yadong Mu*

Main category: cs.CV

TL;DR: Diff4Splat是一种前馈方法，可从单张图像合成可控的显式4D场景，结合视频扩散模型的生成先验与4D数据集学习到的几何和运动约束，直接预测可变形3D高斯场。


<details>
  <summary>Details</summary>
Motivation: 统一视频扩散模型的生成先验与大规模4D数据集学习的几何和运动约束，实现从单张图像直接生成高质量4D场景，避免测试时优化和后处理。

Method: 使用视频潜在变换器增强视频扩散模型，联合捕获时空依赖性并预测时变3D高斯基元，通过外观保真度、几何精度和运动一致性的目标进行训练。

Result: 在30秒内合成高质量4D场景，在视频生成、新视角合成和几何提取方面匹配或超越基于优化的动态场景合成方法，且效率显著更高。

Conclusion: Diff4Splat提供了一种高效的前馈方法，能够从单张图像直接生成可控的4D场景，在质量和效率上都表现出色。

Abstract: We introduce Diff4Splat, a feed-forward method that synthesizes controllable
and explicit 4D scenes from a single image. Our approach unifies the generative
priors of video diffusion models with geometry and motion constraints learned
from large-scale 4D datasets. Given a single input image, a camera trajectory,
and an optional text prompt, Diff4Splat directly predicts a deformable 3D
Gaussian field that encodes appearance, geometry, and motion, all in a single
forward pass, without test-time optimization or post-hoc refinement. At the
core of our framework lies a video latent transformer, which augments video
diffusion models to jointly capture spatio-temporal dependencies and predict
time-varying 3D Gaussian primitives. Training is guided by objectives on
appearance fidelity, geometric accuracy, and motion consistency, enabling
Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate
the effectiveness of Diff4Splatacross video generation, novel view synthesis,
and geometry extraction, where it matches or surpasses optimization-based
methods for dynamic scene synthesis while being significantly more efficient.

</details>


### [76] [VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning](https://arxiv.org/abs/2511.00504)
*Hai-Dang Nguyen,Ha-Hieu Pham,Hao T. Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: VinDr-CXR-VQA是一个大规模胸部X光数据集，用于可解释的医学视觉问答，包含17,597个问答对和4,394张图像，带有放射科医生验证的边界框和临床推理解释。


<details>
  <summary>Details</summary>
Motivation: 推动可重现且具有临床基础的医学视觉问答研究，解决医学AI中的幻觉问题，提供空间定位和临床解释能力。

Method: 构建包含六种诊断类型（位置、内容、存在性、数量、选择和是/非）的问题分类法，平衡正负样本分布（41.7%阳性 vs 58.3%阴性），使用MedGemma-4B-it模型进行基准测试。

Result: 在MedGemma-4B-it模型上实现F1分数0.624，比基线提升11.8%，同时能够实现病变定位。

Conclusion: VinDr-CXR-VQA数据集通过提供空间定位和临床推理解释，显著提升了医学视觉问答的可靠性和可解释性，为临床AI应用提供了重要资源。

Abstract: We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable
Medical Visual Question Answering (Med-VQA) with spatial grounding. The dataset
contains 17,597 question-answer pairs across 4,394 images, each annotated with
radiologist-verified bounding boxes and clinical reasoning explanations. Our
question taxonomy spans six diagnostic types-Where, What, Is there, How many,
Which, and Yes/No-capturing diverse clinical intents. To improve reliability,
we construct a balanced distribution of 41.7% positive and 58.3% negative
samples, mitigating hallucinations in normal cases. Benchmarking with
MedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over
baseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance
reproducible and clinically grounded Med-VQA research. The dataset and
evaluation tools are publicly available at
huggingface.co/datasets/Dangindev/VinDR-CXR-VQA.

</details>


### [77] [OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback](https://arxiv.org/abs/2511.00510)
*Kai Luo,Hao Shi,Kunyu Peng,Fei Teng,Sheng Wu,Kaiwei Wang,Kailun Yang*

Main category: cs.CV

TL;DR: OmniTrack++是一个用于全景图像多目标跟踪的反馈驱动框架，通过动态特征稳定、轨迹信息反馈和专家记忆设计，解决了全景图像中的失真、大搜索空间和身份模糊问题，在JRDB和EmboTrack基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统多目标跟踪方法在全景图像中表现不佳，因为全景图像具有360度视野、分辨率稀释和严重视角相关失真等独特挑战，需要专门的方法来处理这些条件。

Method: 采用反馈驱动框架：1) DynamicSSM块稳定全景特征；2) FlexiTrack实例使用轨迹信息反馈进行灵活定位和短期关联；3) ExpertTrack记忆通过专家混合设计整合外观线索；4) Tracklet管理模块根据场景动态自适应切换跟踪模式。

Result: 在JRDB和EmboTrack基准上实现了最先进的性能，相比原始OmniTrack，在JRDB上HOTA提升+25.5%，在QuadTrack上提升+43.07%。

Conclusion: OmniTrack++通过反馈驱动的渐进式感知优化，有效解决了全景多目标跟踪的挑战，为真实世界全景感知提供了平衡且可扩展的解决方案。

Abstract: This paper investigates Multi-Object Tracking (MOT) in panoramic imagery,
which introduces unique challenges including a 360{\deg} Field of View (FoV),
resolution dilution, and severe view-dependent distortions. Conventional MOT
methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily
under these conditions. To address panoramic distortion, large search space,
and identity ambiguity under a 360{\deg} FoV, OmniTrack++ adopts a
feedback-driven framework that progressively refines perception with trajectory
cues. A DynamicSSM block first stabilizes panoramic features, implicitly
alleviating geometric distortion. On top of normalized representations,
FlexiTrack Instances use trajectory-informed feedback for flexible localization
and reliable short-term association. To ensure long-term robustness, an
ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts
design, enabling recovery from fragmented tracks and reducing identity drift.
Finally, a Tracklet Management module adaptively switches between end-to-end
and tracking-by-detection modes according to scene dynamics, offering a
balanced and scalable solution for panoramic MOT. To support rigorous
evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for
panoramic MOT that includes QuadTrack, captured with a quadruped robot, and
BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets
span wide-angle environments and diverse motion patterns, providing a
challenging testbed for real-world panoramic perception. Extensive experiments
on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art
performance, yielding substantial HOTA improvements of +25.5% on JRDB and
+43.07% on QuadTrack over the original OmniTrack. Datasets and code will be
made publicly available at https://github.com/xifen523/OmniTrack.

</details>


### [78] [ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation](https://arxiv.org/abs/2511.00511)
*Panwang Pan,Jingjing Zhao,Yuchen Lin,Chenguo Lin,Chenxin Li,Haopeng Li,Honglei Yan,Tingting Shen,Yadong Mu*

Main category: cs.CV

TL;DR: ID-Composer是一个用于多主体视频生成的新框架，通过文本提示和参考图像生成视频，解决了现有模型在可控性和适用性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型通常仅基于文本或单张图像进行条件生成，限制了可控性和应用范围。需要开发能够处理多主体视频生成的方法，同时保持主体身份一致性、跨模态语义整合和时间一致性。

Method: 设计了分层身份保持注意力机制来聚合跨主体和跨模态的特征；利用预训练视觉语言模型提供细粒度语义指导；采用在线强化学习阶段来优化关键概念对齐。

Result: 大量实验表明，该模型在身份保持、时间一致性和视频质量方面优于现有方法。

Conclusion: ID-Composer通过创新的分层注意力机制、VLM语义理解和强化学习训练，成功解决了多主体视频生成的挑战，实现了更好的身份保持和语义控制。

Abstract: Video generative models pretrained on large-scale datasets can produce
high-quality videos, but are often conditioned on text or a single image,
limiting controllability and applicability. We introduce ID-Composer, a novel
framework that addresses this gap by tackling multi-subject video generation
from a text prompt and reference images. This task is challenging as it
requires preserving subject identities, integrating semantics across subjects
and modalities, and maintaining temporal consistency. To faithfully preserve
the subject consistency and textual information in synthesized videos,
ID-Composer designs a \textbf{hierarchical identity-preserving attention
mechanism}, which effectively aggregates features within and across subjects
and modalities. To effectively allow for the semantic following of user
intention, we introduce \textbf{semantic understanding via pretrained
vision-language model (VLM)}, leveraging VLM's superior semantic understanding
to provide fine-grained guidance and capture complex interactions between
multiple subjects. Considering that standard diffusion loss often fails in
aligning the critical concepts like subject ID, we employ an \textbf{online
reinforcement learning phase} to drive the overall training objective of
ID-Composer into RLVR. Extensive experiments demonstrate that our model
surpasses existing methods in identity preservation, temporal consistency, and
video quality.

</details>


### [79] [SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation](https://arxiv.org/abs/2511.00523)
*Fangyu Wu,Yujun Cai*

Main category: cs.CV

TL;DR: 提出一种无需训练或偏置标注的测试时去偏方法，通过分割模型隔离目标视觉属性，调整非目标区域使其嵌入与所有类别文本提示均匀相似，从而消除混杂视觉区域的偏置信号。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法通常需要训练数据和显式组标签进行微调或调整嵌入，限制了实际应用。测试时方法虽然避免这一限制，但许多仍依赖数据集特定偏置的先验知识，在开放集设置中泛化性有限。

Method: 使用预训练分割模型隔离目标视觉属性，然后调整非目标区域，使其嵌入与所有类别特定文本提示均匀相似，从而在保留目标属性的同时消除混杂视觉区域的偏置信号。

Result: 在Waterbirds和CelebA数据集上的实验表明，该方法在组鲁棒性指标和Attention IoU方面优于现有测试时去偏方法。

Conclusion: 分割引导的干预措施在视觉语言模型中实现可扩展且无需标注的偏置缓解是有效的。

Abstract: Vision language models such as CLIP have shown remarkable performance in zero
shot classification, but remain susceptible to spurious correlations, where
irrelevant visual features influence predictions. Existing debiasing methods
often require access to training data and explicit group labels to perform
fine-tuning or adjust embeddings, which limits their practicality in real-world
settings. Test-time methods attempt to avoid this constraint, but many still
depend on prior knowledge of dataset specific biases, limiting their
generalizability in open set settings. In this work, we propose a test-time
debiasing method for ViT based CLIP models that requires no additional training
or assumptions of bias annotations. Our approach uses a pretrained segmentation
model to isolate the target visual attribute, then adjusts the non target
regions so that their embeddings are uniformly similar to all class specific
text prompts. This procedure removes unintended bias signals from confounding
visual regions while preserving the target attribute. Experiments on Waterbirds
and CelebA show that our method outperforms existing test-time debiasing
approaches in both group robustness metrics and Attention IoU. These results
demonstrate the effectiveness of segmentation guided interventions for scalable
and annotation free bias mitigation in vision language models.

</details>


### [80] [Text-guided Fine-Grained Video Anomaly Detection](https://arxiv.org/abs/2511.00524)
*Jihao Gu,Kun Li,He Wang,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出了T-VAD框架，基于大型视觉语言模型，通过异常热图解码器和区域感知异常编码器实现细粒度视频异常检测，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常检测方法多为半自动化且输出有限（仅正常或异常），需要更细粒度和交互式的异常检测解决方案。

Method: 构建基于LVLM的T-VAD框架，包含异常热图解码器（AHD）进行像素级视觉-文本特征对齐，以及区域感知异常编码器（RAE）将热图转换为可学习的文本嵌入。

Result: 在UBnormal数据集上达到94.8% AUC，异常热图准确率67.8%/76.7%；在ShanghaiTech和UBnormal数据集上获得优异的文本描述质量（BLEU-4和Yes/No准确率）。

Conclusion: T-VAD显著提升了异常检测的粒度和交互性，在多个指标上达到最先进性能。

Abstract: Video Anomaly Detection (VAD) aims to identify anomalous events within video
segments. In scenarios such as surveillance or industrial process monitoring,
anomaly detection is of critical importance. While existing approaches are
semi-automated, requiring human assessment for anomaly detection, traditional
VADs offer limited output as either normal or anomalous. We propose Text-guided
Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large
Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)
that performs pixel-wise visual-textual feature alignment to generate
fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly
Encoder (RAE) that transforms the heatmaps into learnable textual embeddings,
guiding the LVLM to accurately identify and localize anomalous events in
videos. This significantly enhances both the granularity and interactivity of
anomaly detection. The proposed method achieving SOTA performance by
demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and
67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,
and subjectively verified more preferable textual description on the
ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;
Yes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for
targets, 78.10 for trajectories; Yes/No accuracy: 89.73%).

</details>


### [81] [Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era](https://arxiv.org/abs/2511.00540)
*Wenbing Zhu,Chengjie Wang,Bin-Bin Gao,Jiangning Zhang,Guannan Jiang,Jie Hu,Zhenye Gan,Lidong Wang,Ziqing Zhou,Linjie Cheng,Yurui Pan,Bo Peng,Mingmin Chi,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出了Real-IAD Variety，这是最大最全面的工业异常检测基准数据集，包含198,960张高分辨率图像，涵盖160个物体类别、28个行业、24种材料和22种颜色变化。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测基准存在类别多样性不足、规模有限的问题，导致指标饱和和模型在真实场景中泛化能力差。

Method: 构建了大规模多样化的数据集，涵盖多行业、多材料和多颜色变化，并在多类别无监督、多视角和零/少样本设置下进行严格评估。

Result: 实验显示最先进的多类别无监督异常检测方法在类别从30扩展到160时性能显著下降，而视觉语言模型展现出对类别扩展的强鲁棒性。

Conclusion: Real-IAD Variety作为关键资源将推动下一代基础模型的训练和评估，加速开发可扩展的通用异常检测系统。

Abstract: Industrial Anomaly Detection (IAD) is critical for enhancing operational
safety, ensuring product quality, and optimizing manufacturing efficiency
across global industries. However, the IAD algorithms are severely constrained
by the limitations of existing public benchmarks. Current datasets exhibit
restricted category diversity and insufficient scale, frequently resulting in
metric saturation and limited model transferability to real-world scenarios. To
address this gap, we introduce Real-IAD Variety, the largest and most diverse
IAD benchmark, comprising 198,960 high-resolution images across 160 distinct
object categories. Its diversity is ensured through comprehensive coverage of
28 industries, 24 material types, and 22 color variations. Our comprehensive
experimental analysis validates the benchmark's substantial challenge:
state-of-the-art multi-class unsupervised anomaly detection methods experience
significant performance degradation when scaled from 30 to 160 categories.
Crucially, we demonstrate that vision-language models exhibit remarkable
robustness to category scale-up, with minimal performance variation across
different category counts, significantly enhancing generalization capabilities
in diverse industrial contexts. The unprecedented scale and complexity of
Real-IAD Variety position it as an essential resource for training and
evaluating next-generation foundation models for anomaly detection. By
providing this comprehensive benchmark with rigorous evaluation protocols
across multi-class unsupervised, multi-view, and zero-/few-shot settings, we
aim to accelerate research beyond domain-specific constraints, enabling the
development of scalable, general-purpose anomaly detection systems. Real-IAD
Variety will be made publicly available to facilitate innovation in this
critical field.

</details>


### [82] [MIFO: Learning and Synthesizing Multi-Instance from One Image](https://arxiv.org/abs/2511.00542)
*Kailun Su,Ziqi He,Xi Wang,Yang Zhou*

Main category: cs.CV

TL;DR: 提出了一种从单张图像精确学习和合成多实例语义的方法，通过惩罚注意力优化和框控制来解决相似语义纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 解决从单张图像学习多实例语义时训练数据有限的问题，特别是在实例具有相似语义或外观时的挑战。

Method: 使用惩罚注意力优化在训练阶段解耦相似语义，在合成阶段引入并优化注意力层的框控制来减少语义泄漏并精确控制输出布局。

Result: 实验结果表明该方法实现了高质量的解耦语义学习和合成，在可编辑性和实例一致性之间取得了良好平衡，对语义或视觉相似实例及罕见物体保持鲁棒性。

Conclusion: 该方法能够有效处理多实例语义学习中的语义纠缠问题，提供精确的布局控制和高质量的合成结果。

Abstract: This paper proposes a method for precise learning and synthesizing
multi-instance semantics from a single image. The difficulty of this problem
lies in the limited training data, and it becomes even more challenging when
the instances to be learned have similar semantics or appearance. To address
this, we propose a penalty-based attention optimization to disentangle similar
semantics during the learning stage. Then, in the synthesis, we introduce and
optimize box control in attention layers to further mitigate semantic leakage
while precisely controlling the output layout. Experimental results demonstrate
that our method achieves disentangled and high-quality semantic learning and
synthesis, strikingly balancing editability and instance consistency. Our
method remains robust when dealing with semantically or visually similar
instances or rare-seen objects. The code is publicly available at
https://github.com/Kareneveve/MIFO

</details>


### [83] [4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting](https://arxiv.org/abs/2511.00560)
*Chun-Tin Wu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 提出了4D神经体素泼溅方法，通过结合体素表示和神经高斯泼溅来高效建模动态场景，显著减少内存消耗并加速训练。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯泼溅在动态场景中因跨帧复制高斯而导致的内存开销过大问题。

Method: 使用紧凑的神经体素集合和学习的变形场来建模时间动态，避免为每个时间戳生成单独的高斯集，并引入视图细化阶段选择性优化挑战性视角。

Result: 在显著减少内存和加速训练的同时，实现了优于现有方法的渲染质量，支持实时渲染。

Conclusion: 4D-NVS方法在动态场景建模中实现了内存效率、训练速度和渲染质量的平衡，为实时高质量动态场景渲染提供了有效解决方案。

Abstract: Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel
view synthesis, extending it to dynamic scenes still results in substantial
memory overhead from replicating Gaussians across frames. To address this
challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines
voxel-based representations with neural Gaussian splatting for efficient
dynamic scene modeling. Instead of generating separate Gaussian sets per
timestamp, our method employs a compact set of neural voxels with learned
deformation fields to model temporal dynamics. The design greatly reduces
memory consumption and accelerates training while preserving high image
quality. We further introduce a novel view refinement stage that selectively
improves challenging viewpoints through targeted optimization, maintaining
global efficiency while enhancing rendering quality for difficult viewing
angles. Experiments demonstrate that our method outperforms state-of-the-art
approaches with significant memory reduction and faster training, enabling
real-time rendering with superior visual fidelity.

</details>


### [84] [Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective](https://arxiv.org/abs/2511.00573)
*Wei Feng,Zongyuan Ge*

Main category: cs.CV

TL;DR: 提出了频率引导的广义类别发现框架(FREE)，通过利用频域信息来解决分布偏移下的广义类别发现问题，包括频域分离策略、跨域和域内扰动策略，以及聚类难度感知重采样技术。


<details>
  <summary>Details</summary>
Motivation: 现有的广义类别发现方法在标准条件下表现良好，但在存在分布偏移时性能会显著下降。本文探索了一个更现实的任务：域偏移广义类别发现(DS_GCD)，其中未标记数据不仅包含未知类别，还包含来自未知域的样本。

Method: 1) 基于频率的域分离策略，通过测量振幅差异将样本划分为已知域和未知域；2) 跨域和域内频域扰动策略；3) 扩展的自监督对比目标和语义聚类损失；4) 聚类难度感知重采样技术。

Result: 广泛的实验表明，该方法在各种基准数据集上有效缓解了分布偏移的影响，在发现已知和未知类别方面都取得了优越性能。

Conclusion: FREE框架通过利用频域信息成功解决了分布偏移下的广义类别发现问题，为现实场景中的类别发现提供了有效解决方案。

Abstract: Generalized Category Discovery (GCD) aims to leverage labeled samples from
known categories to cluster unlabeled data that may include both known and
unknown categories. While existing methods have achieved impressive results
under standard conditions, their performance often deteriorates in the presence
of distribution shifts. In this paper, we explore a more realistic task:
Domain-Shifted Generalized Category Discovery (DS\_GCD), where the unlabeled
data includes not only unknown categories but also samples from unknown
domains. To tackle this challenge, we propose a
\textbf{\underline{F}}requency-guided Gene\textbf{\underline{r}}alized
Cat\textbf{\underline{e}}gory Discov\textbf{\underline{e}}ry framework (FREE)
that enhances the model's ability to discover categories under distributional
shift by leveraging frequency-domain information. Specifically, we first
propose a frequency-based domain separation strategy that partitions samples
into known and unknown domains by measuring their amplitude differences. We
then propose two types of frequency-domain perturbation strategies: a
cross-domain strategy, which adapts to new distributions by exchanging
amplitude components across domains, and an intra-domain strategy, which
enhances robustness to intra-domain variations within the unknown domain.
Furthermore, we extend the self-supervised contrastive objective and semantic
clustering loss to better guide the training process. Finally, we introduce a
clustering-difficulty-aware resampling technique to adaptively focus on
harder-to-cluster categories, further enhancing model performance. Extensive
experiments demonstrate that our method effectively mitigates the impact of
distributional shifts across various benchmark datasets and achieves superior
performance in discovering both known and unknown categories.

</details>


### [85] [TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection](https://arxiv.org/abs/2511.00580)
*Yousuf Ahmed Siddiqui,Sufiyaan Usmani,Umer Tariq,Jawwad Ahmed Shamsi,Muhammad Burhan Khan*

Main category: cs.CV

TL;DR: 提出了一种基于记忆增强的上下文感知零样本异常检测方法，通过跨注意力机制融合时序和视觉特征，结合上下文相似性评分实现实时异常分类。


<details>
  <summary>Details</summary>
Motivation: 视频异常通常依赖于上下文信息和时间演化，但大多数异常检测器未能考虑这种上下文依赖性，限制了其在真实场景中的泛化能力。

Method: 采用记忆增强的流水线，通过跨注意力机制关联时序信号与视觉嵌入，并使用上下文相似性评分进行实时零样本异常分类。

Result: 在UCF-Crime数据集上达到90.4% AUC，在XD-Violence数据集上达到83.67% AP，创下零样本模型的新SOTA。

Conclusion: 通过融合跨注意力时序融合和上下文记忆，实现了高保真度的异常检测，为零样本模型在真实世界监控和基础设施监测中的应用迈出了重要一步。

Abstract: Video anomalies often depend on contextual information available and temporal
evolution. Non-anomalous action in one context can be anomalous in some other
context. Most anomaly detectors, however, do not notice this type of context,
which seriously limits their capability to generalize to new, real-life
situations. Our work addresses the context-aware zero-shot anomaly detection
challenge, in which systems need to learn adaptively to detect new events by
correlating temporal and appearance features with textual traces of memory in
real time. Our approach defines a memory-augmented pipeline, correlating
temporal signals with visual embeddings using cross-attention, and real-time
zero-shot anomaly classification by contextual similarity scoring. We achieve
90.4\% AUC on UCF-Crime and 83.67\% AP on XD-Violence, a new state-of-the-art
among zero-shot models. Our model achieves real-time inference with high
precision and explainability for deployment. We show that, by fusing
cross-attention temporal fusion and contextual memory, we achieve high fidelity
anomaly detection, a step towards the applicability of zero-shot models in
real-world surveillance and infrastructure monitoring.

</details>


### [86] [CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World](https://arxiv.org/abs/2511.00613)
*Yating Yu,Congqi Cao,Zhaoying Wang,Weihua Meng,Jie Li,Yuxin Li,Zihao Wei,Zhongpei Shen,Jiajun Zhang*

Main category: cs.CV

TL;DR: 本文提出了CueBench，首个专注于上下文感知视频异常理解的基准测试，通过统一评估框架来评测现有模型在真实世界异常理解方面的表现。


<details>
  <summary>Details</summary>
Motivation: 当前深度模型对真实世界视频异常的理解仍很肤浅，缺乏对复杂原理和微妙上下文的理解能力。现有方法通常只关注检测偏离正常模式的异常事件，而忽略了区分异常与正常的复杂上下文因素。

Method: 建立了事件中心的分层分类法，包含14种条件异常和18种绝对异常事件，涵盖174个场景和198个属性。提出了统一的上下文感知视频异常理解基准测试，包含识别、时序定位、检测和预测等任务。并开发了Cue-R1模型，基于R1风格强化微调，使用可验证、任务对齐和层次细化的奖励。

Result: 在CueBench上的广泛实验表明，现有视觉语言模型在真实世界异常理解方面表现不佳，而提出的Cue-R1模型平均超过最先进方法24%以上。

Conclusion: 现有视觉语言模型距离真实世界的视频异常理解仍有很大差距，而通过精心设计的强化微调和统一生成方法可以显著提升模型性能。

Abstract: How far are deep models from real-world video anomaly understanding (VAU)?
Current works typically emphasize on detecting unexpected occurrences deviated
from normal patterns or comprehending anomalous events with interpretable
descriptions. However, they exhibit only a superficial comprehension of
real-world anomalies, with limited breadth in complex principles and subtle
context that distinguish the anomalies from normalities, e.g., climbing cliffs
with safety gear vs. without it. To this end, we introduce CueBench, the first
of its kind Benchmark, devoted to Context-aware video anomalies within a
Unified Evaluation framework. We comprehensively establish an event-centric
hierarchical taxonomy that anchors two core event types: 14 conditional and 18
absolute anomaly events, defined by their refined semantics from diverse
contexts across 174 scenes and 198 attributes. Based on this, we propose to
unify and benchmark context-aware VAU with various challenging tasks across
recognition, temporal grounding, detection, and anticipation. This also serves
as a rigorous and fair probing evaluation suite for generative-discriminative
as well as generalized-specialized vision-language models (VLMs). To address
the challenges underlying CueBench, we further develop Cue-R1 based on R1-style
reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined
rewards in a unified generative manner. Extensive results on CueBench reveal
that, existing VLMs are still far from satisfactory real-world anomaly
understanding, while our Cue-R1 surpasses these state-of-the-art approaches by
over 24% on average.

</details>


### [87] [Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach](https://arxiv.org/abs/2511.00643)
*Oluwatosin Alabi,Meng Wei,Charlie Budd,Tom Vercauteren,Miaojing Shi*

Main category: cs.CV

TL;DR: 提出了triplet segmentation新任务，通过仪器实例分割来空间定位手术动作三元组<仪器、动词、目标>，并开发了TargetFusionNet架构和CholecTriplet-Seg数据集来提升手术场景理解的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有手术动作三元组识别方法仅限于帧级分类，无法可靠地将动作与特定仪器实例关联；先前空间定位方法主要依赖类激活图，缺乏精确性和鲁棒性。

Method: 提出triplet segmentation统一任务，创建CholecTriplet-Seg数据集（3万+标注帧），开发TargetFusionNet架构，通过目标感知融合机制将弱解剖先验与仪器实例查询融合。

Result: TargetFusionNet在识别、检测和三元组分段的各项指标上均优于现有基线，证明强实例监督结合弱目标先验显著提升了手术动作理解的准确性和鲁棒性。

Conclusion: 三元组分段为空间定位手术动作三元组建立了统一框架，所提出的基准和架构为更可解释的手术场景理解铺平了道路。

Abstract: Understanding surgical instrument-tissue interactions requires not only
identifying which instrument performs which action on which anatomical target,
but also grounding these interactions spatially within the surgical scene.
Existing surgical action triplet recognition methods are limited to learning
from frame-level classification, failing to reliably link actions to specific
instrument instances.Previous attempts at spatial grounding have primarily
relied on class activation maps, which lack the precision and robustness
required for detailed instrument-tissue interaction analysis.To address this
gap, we propose grounding surgical action triplets with instrument instance
segmentation, or triplet segmentation for short, a new unified task which
produces spatially grounded <instrument, verb, target> outputs.We start by
presenting CholecTriplet-Seg, a large-scale dataset containing over 30,000
annotated frames, linking instrument instance masks with action verb and
anatomical target annotations, and establishing the first benchmark for
strongly supervised, instance-level triplet grounding and evaluation.To learn
triplet segmentation, we propose TargetFusionNet, a novel architecture that
extends Mask2Former with a target-aware fusion mechanism to address the
challenge of accurate anatomical target prediction by fusing weak anatomy
priors with instrument instance queries.Evaluated across recognition,
detection, and triplet segmentation metrics, TargetFusionNet consistently
improves performance over existing baselines, demonstrating that strong
instance supervision combined with weak target priors significantly enhances
the accuracy and robustness of surgical action understanding.Triplet
segmentation establishes a unified framework for spatially grounding surgical
action triplets. The proposed benchmark and architecture pave the way for more
interpretable, surgical scene understanding.

</details>


### [88] [Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset](https://arxiv.org/abs/2511.00653)
*Lassi Ruoppa,Tarmo Hietala,Verneri Seppänen,Josef Taher,Teemu Hakala,Xiaowei Yu,Antero Kukko,Harri Kaartinen,Juha Hyyppä*

Main category: cs.CV

TL;DR: 该研究介绍了首个大规模多光谱激光雷达基准数据集FGI-EMIT，用于评估单木分割方法。通过比较传统无监督算法和深度学习方法的性能，发现深度学习方法显著优于传统方法，特别是在林下树木分割方面。


<details>
  <summary>Details</summary>
Motivation: 单木分割是森林资源调查、碳监测和生物多样性评估的基础任务。传统方法依赖无监督几何算法，而深度学习方法发展受到缺乏大规模基准数据集的限制，特别是多光谱激光雷达数据的稀缺，尽管证据表明多光谱反射率可以提高分割精度。

Method: 创建了包含1,561棵人工标注树木的FGI-EMIT数据集，使用532、905和1,550 nm三个波长的多光谱激光雷达数据。评估了四种传统无监督算法和四种深度学习方法的性能，其中无监督方法使用贝叶斯优化超参数，深度学习模型从头开始训练。

Result: 无监督方法中Treeiso获得最高F1分数52.7%，而深度学习方法表现显著更好，最佳模型ForestFormer3D达到73.3%的F1分数。在林下树木分割方面，ForestFormer3D比Treeiso高出25.9个百分点。当前深度学习方法未能充分利用多光谱反射率信息，但在低点密度（10点/平方米）下仍优于无监督方法。

Conclusion: 深度学习方法在单木分割任务中显著优于传统无监督算法，特别是在处理林下树木方面。然而，现有深度学习方法尚未能有效利用多光谱反射率信息，这为未来研究提供了改进方向。

Abstract: Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for
applications such as forest inventory, carbon monitoring and biodiversity
assessment. Traditionally, ITS has been achieved with unsupervised
geometry-based algorithms, while more recent advances have shifted toward
supervised deep learning (DL). In the past, progress in method development was
hindered by the lack of large-scale benchmark datasets, and the availability of
novel data formats, particularly multispectral (MS) LiDAR, remains limited to
this day, despite evidence that MS reflectance can improve the accuracy of ITS.
This study introduces FGI-EMIT, the first large-scale MS airborne laser
scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550
nm, the dataset consists of 1,561 manually annotated trees, with a particular
focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked
four conventional unsupervised algorithms and four supervised DL approaches.
Hyperparameters of unsupervised methods were optimized using a Bayesian
approach, while DL models were trained from scratch. Among the unsupervised
methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL
approaches performed significantly better overall, with the best model,
ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference
was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9
percentage points. An ablation study demonstrated that current DL-based
approaches generally fail to leverage MS reflectance information when it is
provided as additional input features, although single channel reflectance can
improve accuracy marginally, especially for understory trees. A performance
analysis across point densities further showed that DL methods consistently
remain superior to unsupervised algorithms, even at densities as low as 10
points/m$^2$.

</details>


### [89] [Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control](https://arxiv.org/abs/2511.00681)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP是一个元数据引导的框架，通过学习MRI对比度表示，将体素图像与其DICOM采集参数对齐，解决MRI数据异质性和缺乏标准化对比度标签的问题。


<details>
  <summary>Details</summary>
Motivation: MRI存在严重的数据异质性，缺乏跨扫描仪、协议和机构的标准化对比度标签，这限制了大规模自动化分析。统一的MRI对比度表示可以实现从自动序列识别到协调和质量控制的各种下游应用，而无需依赖手动标注。

Method: 引入MR-CLIP框架，通过将体素图像与其DICOM采集参数对齐来学习MRI对比度表示。利用常规可用的采集元数据作为监督信号。

Result: 生成的嵌入显示MRI序列的明显聚类，在数据稀缺情况下，在少样本序列分类中优于监督的3D基线。此外，MR-CLIP通过图像-元数据嵌入距离识别损坏或不一致的元数据，实现无监督数据质量控制。

Conclusion: 通过将常规可用的采集元数据转化为监督信号，MR-CLIP为跨不同临床数据集的标签高效MRI分析提供了可扩展的基础。

Abstract: Magnetic Resonance Imaging suffers from substantial data heterogeneity and
the absence of standardized contrast labels across scanners, protocols, and
institutions, which severely limits large-scale automated analysis. A unified
representation of MRI contrast would enable a wide range of downstream
utilities, from automatic sequence recognition to harmonization and quality
control, without relying on manual annotations. To this end, we introduce
MR-CLIP, a metadata-guided framework that learns MRI contrast representations
by aligning volumetric images with their DICOM acquisition parameters. The
resulting embeddings shows distinct clusters of MRI sequences and outperform
supervised 3D baselines under data scarcity in few-shot sequence
classification. Moreover, MR-CLIP enables unsupervised data quality control by
identifying corrupted or inconsistent metadata through image-metadata embedding
distances. By transforming routinely available acquisition metadata into a
supervisory signal, MR-CLIP provides a scalable foundation for label-efficient
MRI analysis across diverse clinical datasets.

</details>


### [90] [Outlier-Aware Post-Training Quantization for Image Super-Resolution](https://arxiv.org/abs/2511.00682)
*Hailing Wang,jianglin Lu,Yitian Zhang,Yun Fu*

Main category: cs.CV

TL;DR: 提出了一种针对图像超分辨率网络的训练后量化方法，通过双区域量化策略和敏感度感知微调，有效处理激活值中的异常值问题，在保持性能的同时显著加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有训练后量化方法在处理图像超分辨率网络时性能不佳，主要原因是忽略了激活值中的异常值影响。研究发现这些异常值与图像颜色信息强相关，直接移除会导致性能显著下降。

Method: 1. 双区域量化策略：将激活值划分为异常值区域和密集区域，分别进行均匀量化以优化比特分配；2. 敏感度感知微调：根据网络层对量化的敏感度差异，让模型更关注高敏感层。

Result: 在多种超分辨率网络和数据集上的实验表明，该方法优于现有训练后量化方法，在大多数场景下达到与量化感知训练相当的性能，同时实现至少75倍的加速。

Conclusion: 提出的双区域量化和敏感度感知微调策略有效解决了训练后量化中的异常值问题，为图像超分辨率网络提供了高效实用的量化解决方案。

Abstract: Quantization techniques, including quantization-aware training (QAT) and
post-training quantization (PTQ), have become essential for inference
acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has
garnered significant attention as it eliminates the need for ground truth and
model retraining. However, existing PTQ methods for SR often fail to achieve
satisfactory performance as they overlook the impact of outliers in activation.
Our empirical analysis reveals that these prevalent activation outliers are
strongly correlated with image color information, and directly removing them
leads to significant performance degradation. Motivated by this, we propose a
dual-region quantization strategy that partitions activations into an outlier
region and a dense region, applying uniform quantization to each region
independently to better balance bit-width allocation. Furthermore, we observe
that different network layers exhibit varying sensitivities to quantization,
leading to different levels of performance degradation. To address this, we
introduce sensitivity-aware finetuning that encourages the model to focus more
on highly sensitive layers, further enhancing quantization performance.
Extensive experiments demonstrate that our method outperforms existing PTQ
approaches across various SR networks and datasets, while achieving performance
comparable to QAT methods in most scenarios with at least a 75 speedup.

</details>


### [91] [Evolve to Inspire: Novelty Search for Diverse Image Generation](https://arxiv.org/abs/2511.00686)
*Alex Inch,Passawis Chaiyapattanaporn,Yuchen Zhu,Yuan Lu,Ting-Wen Ko,Davide Paglieri*

Main category: cs.CV

TL;DR: WANDER是一种基于新颖性搜索的方法，通过LLM进行语义演化并使用CLIP嵌入量化新颖性，从单个输入提示生成多样化的图像集合，显著提升了图像多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型虽然能生成高质量图像，但输出多样性有限，不适合探索性和构思任务。现有的提示优化技术要么针对美学适应性，要么不适合创意视觉领域。

Method: WANDER直接在自然语言提示上操作，使用大型语言模型进行语义演化，采用CLIP嵌入量化新颖性，并应用发射器引导搜索进入不同的提示空间区域。

Result: 使用FLUX-DEV生成和GPT-4o-mini进行突变的实证评估表明，WANDER在多样性指标上显著优于现有的进化提示优化基线方法。消融研究证实了发射器的有效性。

Conclusion: WANDER成功解决了文本到图像模型输出多样性有限的问题，通过新颖性搜索和语义演化方法，为创意视觉任务提供了有效的解决方案。

Abstract: Text-to-image diffusion models, while proficient at generating high-fidelity
images, often suffer from limited output diversity, hindering their application
in exploratory and ideation tasks. Existing prompt optimization techniques
typically target aesthetic fitness or are ill-suited to the creative visual
domain. To address this shortcoming, we introduce WANDER, a novelty
search-based approach to generating diverse sets of images from a single input
prompt. WANDER operates directly on natural language prompts, employing a Large
Language Model (LLM) for semantic evolution of diverse sets of images, and
using CLIP embeddings to quantify novelty. We additionally apply emitters to
guide the search into distinct regions of the prompt space, and demonstrate
that they boost the diversity of the generated images. Empirical evaluations
using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that
WANDER significantly outperforms existing evolutionary prompt optimization
baselines in diversity metrics. Ablation studies confirm the efficacy of
emitters.

</details>


### [92] [Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics](https://arxiv.org/abs/2511.00698)
*Taifour Yousra,Beghdadi Azeddine,Marie Luong,Zuheng Ming*

Main category: cs.CV

TL;DR: 本文分析了低剂量CT图像增强中不同损失函数与图像质量指标之间的一致性，发现两者存在不一致性，强调在开发新损失函数时需要考虑图像质量指标。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT成像广泛用于减少辐射暴露，但存在噪声和伪影问题。虽然深度学习模型在PSNR和SSIM指标上表现优异，但这些指标在反映医学图像感知质量方面存在局限性。

Method: 对基于深度学习的低剂量CT图像增强架构中的损失函数进行客观分析，评估不同损失函数与图像质量指标的相关性和一致性。

Result: 研究发现损失函数与质量指标之间存在不一致性，经典损失函数如均方误差和对抗损失在特定架构中表现良好，但与感知质量指标不匹配。

Conclusion: 在开发新的图像质量增强损失函数时，必须考虑图像质量指标，以确保模型性能与临床诊断需求一致。

Abstract: Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to
mitigate high exposure side effects, but often suffers from noise and artifacts
that affect diagnostic accuracy. To tackle this issue, deep learning models
have been developed to enhance LDCT images. Various loss functions have been
employed, including classical approaches such as Mean Square Error and
adversarial losses, as well as customized loss functions(LFs) designed for
specific architectures. Although these models achieve remarkable performance in
terms of PSNR and SSIM, these metrics are limited in their ability to reflect
perceptual quality, especially for medical images. In this paper, we focus on
one of the most critical elements of DL-based architectures, namely the loss
function. We conduct an objective analysis of the relevance of different loss
functions for LDCT image quality enhancement and their consistency with image
quality metrics. Our findings reveal inconsistencies between LFs and quality
metrics, and highlight the need of consideration of image quality metrics when
developing a new loss function for image quality enhancement.

</details>


### [93] [Validating Deep Models for Alzheimer's 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data](https://arxiv.org/abs/2511.00728)
*Hugo Massaroli,Hernan Chaves,Pilar Anania,Mauricio Farez,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CV

TL;DR: 深度学习模型在ADNI数据集上表现优异（AUC达0.96-0.97），但在拉丁美洲FLENI队列上性能显著下降（AUC降至0.80-0.82），揭示了显著的领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 评估深度学习模型在阿尔茨海默病诊断中的泛化能力，特别是针对代表性不足的拉丁美洲人群，探索模型在不同人群间的性能差异。

Method: 在ADNI数据集上训练卷积和Transformer模型，然后在FLENI拉丁美洲临床队列上进行泛化性能测试，并进行消融研究和遮挡敏感性分析。

Result: 所有模型在ADNI上表现优异，但在FLENI上性能显著下降，显示明显的领域偏移。不同架构模型性能相似，Transformer未显示出明显优势。

Conclusion: 需要针对不同人群进行AI诊断模型的验证，未来工作应关注领域适应和队列多样化，以提高模型的泛化能力。

Abstract: Deep learning models have shown strong performance in diagnosing Alzheimer's
disease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with
training datasets largely composed of North American cohorts such as those in
the Alzheimer's Disease Neuroimaging Initiative (ADNI). However, their
generalization to underrepresented populations remains underexplored. In this
study, we benchmark convolutional and Transformer-based models on the ADNI
dataset and assess their generalization performance on a novel Latin American
clinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show
that while all models achieve high AUCs on ADNI (up to .96, .97), their
performance drops substantially on FLENI (down to .82, .80, respectively),
revealing a significant domain shift. The tested architectures demonstrated
similar performance, calling into question the supposed advantages of
transformers for this specific task. Through ablation studies, we identify
per-image normalization and a correct sampling selection as key factors for
generalization. Occlusion sensitivity analysis further reveals that models
trained on ADNI, generally attend to canonical hypometabolic regions for the AD
class, but focus becomes unclear for the other classes and for FLENI scans.
These findings highlight the need for population-aware validation of diagnostic
AI models and motivate future work on domain adaptation and cohort
diversification.

</details>


### [94] [Towards classification-based representation learning for place recognition on LiDAR scans](https://arxiv.org/abs/2511.00738)
*Dmitrii Khizbullin,Maksim Konoplia*

Main category: cs.CV

TL;DR: 将地点识别重新构建为多类分类问题，通过为LiDAR扫描分配离散位置标签，使用编码器-解码器模型直接分类位置，在NuScenes数据集上取得与对比学习方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 大多数现有方法依赖对比学习，本文探索将地点识别作为多类分类问题的替代方法，以提供训练效率和稳定性方面的优势。

Method: 为LiDAR扫描分配离散位置标签，训练编码器-解码器模型直接对每个扫描的位置进行分类。

Result: 在NuScenes数据集上评估，该方法取得了与基于对比学习方法相竞争的性能。

Conclusion: 将地点识别构建为多类分类问题是一种可行的替代方法，在保持性能的同时提供了训练效率和稳定性优势。

Abstract: Place recognition is a crucial task in autonomous driving, allowing vehicles
to determine their position using sensor data. While most existing methods rely
on contrastive learning, we explore an alternative approach by framing place
recognition as a multi-class classification problem. Our method assigns
discrete location labels to LiDAR scans and trains an encoder-decoder model to
classify each scan's position directly. We evaluate this approach on the
NuScenes dataset and show that it achieves competitive performance compared to
contrastive learning-based methods while offering advantages in training
efficiency and stability.

</details>


### [95] [Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models](https://arxiv.org/abs/2511.00749)
*Tanvi Dinkar,Aiqi Jiang,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CV

TL;DR: 该研究分析了生成式AI模型如何编码西方审美标准并消除'丑陋'特征，发现模型存在严重的肤色、年龄和性别偏见，86.5%生成图像为浅肤色，74%为年轻人群，非二元性别个体被过度性化。


<details>
  <summary>Details</summary>
Motivation: 社交媒体加剧了西方审美标准的传播，导致负面自我形象和身体畸形恐惧症。随着AI生成内容的增加，担忧这些标准被夸大，需要研究生成式AI如何编码'美'并消除'丑'。

Method: 创建两个图像生成流程：文本到图像模型和文本到语言模型再到图像模型。开发结构化审美分类法，使用三个语言模型和两个文本到图像模型生成5984张图像，并招募社交媒体用户通过李克特量表评估1200张图像。

Result: 86.5%生成图像为浅肤色，22%包含明确内容（尽管经过SFW训练），74%被评分为年轻年龄段。非二元性别个体图像被评分为更年轻和更过度性化。带有'负面'或'丑陋'特征的提示词始终产生更高的NSFW评分。

Conclusion: 生成式AI模型中存在与审美标准相关的普遍人口统计偏见，这些偏见通过模型开发者（如负面提示）积极延续，导致数据流污染和不符合开发者审美刻板印象的特征被主动消除。

Abstract: Social media has exacerbated the promotion of Western beauty norms, leading
to negative self-image, particularly in women and girls, and causing harm such
as body dysmorphia. Increasingly content on the internet has been artificially
generated, leading to concerns that these norms are being exaggerated. The aim
of this work is to study how generative AI models may encode 'beauty' and erase
'ugliness', and discuss the implications of this for society. To investigate
these aims, we create two image generation pipelines: a text-to-image model and
a text-to-language model-to image model. We develop a structured beauty
taxonomy which we use to prompt three language models (LMs) and two
text-to-image models to cumulatively generate 5984 images using our two
pipelines. We then recruit women and non-binary social media users to evaluate
1200 of the images through a Likert-scale within-subjects study. Participants
show high agreement in their ratings. Our results show that 86.5% of generated
images depicted people with lighter skin tones, 22% contained explicit content
despite Safe for Work (SFW) training, and 74% were rated as being in a younger
age demographic. In particular, the images of non-binary individuals were rated
as both younger and more hypersexualised, indicating troubling intersectional
effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such
as "a wide nose") consistently produced higher Not SFW (NSFW) ratings
regardless of gender. This work sheds light on the pervasive demographic biases
related to beauty standards present in generative AI models -- biases that are
actively perpetuated by model developers, such as via negative prompting. We
conclude by discussing the implications of this on society, which include
pollution of the data streams and active erasure of features that do not fall
inside the stereotype of what is considered beautiful by developers.

</details>


### [96] [A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection](https://arxiv.org/abs/2511.00777)
*Anis Suttan Shahrir,Zakiah Ayop,Syarulnaziah Anawar,Norulzahrah Mohd Zainudin*

Main category: cs.CV

TL;DR: 提出一个结合YOLOv5和SSD算法的物联网动物检测系统，用于榴莲种植园，实现实时监测、Telegram通知和声音威慑机制。


<details>
  <summary>Details</summary>
Motivation: 榴莲种植园面临动物入侵导致的作物损失和经济损失，传统农业实践缺乏无人干预的监测手段，现有系统存在检测算法单一、通知平台不便和威慑机制有限的问题。

Method: 集成YOLOv5和SSD目标检测算法提高检测精度，提供实时监测，通过Telegram自动通知农民，并触发自动声音威慑机制（如老虎吼声）。

Result: YOLO+SSD模型对大象、野猪和猴子的检测准确率分别达到90%、85%和70%，白天准确率最高，夜间下降，无论静态图像还是视频。

Conclusion: 本研究提供了一个结合检测、通知和威慑的综合实用框架，为自动化农业解决方案的未来创新铺平道路。

Abstract: Durian plantation suffers from animal intrusions that cause crop damage and
financial loss. The traditional farming practices prove ineffective due to the
unavailability of monitoring without human intervention. The fast growth of
machine learning and Internet of Things (IoT) technology has led to new ways to
detect animals. However, current systems are limited by dependence on single
object detection algorithms, less accessible notification platforms, and
limited deterrent mechanisms. This research suggests an IoT-enabled animal
detection system for durian crops. The system integrates YOLOv5 and SSD object
detection algorithms to improve detection accuracy. The system provides
real-time monitoring, with detected intrusions automatically reported to
farmers via Telegram notifications for rapid response. An automated sound
mechanism (e.g., tiger roar) is triggered once the animal is detected. The
YOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%,
85% and 70%, respectively. The system shows the highest accuracy in daytime and
decreases at night, regardless of whether the image is still or a video.
Overall, this study contributes a comprehensive and practical framework that
combines detection, notification, and deterrence, paving the way for future
innovations in automated farming solutions.

</details>


### [97] [Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking](https://arxiv.org/abs/2511.00785)
*Juan Wang,Yasutomo Kawanishi,Tomo Miyazaki,Zhijie Wang,Shinichiro Omachi*

Main category: cs.CV

TL;DR: 提出了一种粒度一致的自2D动掩码跟踪方法，通过保持跨帧的时间对应关系来消除冲突的伪标签，结合三阶段课程学习框架，从碎片化的单视图数据逐步训练到统一的多视图标注，最终实现全局一致的完整场景监督。


<details>
  <summary>Details</summary>
Motivation: 现有的3D实例分割方法通过将2D掩码从基础模型转移到3D来生成伪标签，但由于视频帧被独立处理，导致分割粒度不一致和冲突的3D伪标签，从而降低了最终分割的准确性。

Method: 引入粒度一致的自2D动掩码跟踪方法，保持跨帧的时间对应关系；采用三阶段课程学习框架，从碎片化的单视图数据逐步训练到统一的多视图标注，最终实现全局一致的完整场景监督。

Result: 实验结果表明，该方法能有效生成一致且准确的3D分割；在标准基准测试中取得了最先进的结果，并具备开放词汇能力。

Conclusion: 通过结构化学习流程，能够从最初碎片化和矛盾的2D先验中稳健地提取一致的3D表示，解决了现有方法中伪标签不一致的问题。

Abstract: 3D instance segmentation is an important task for real-world applications. To
avoid costly manual annotations, existing methods have explored generating
pseudo labels by transferring 2D masks from foundation models to 3D. However,
this approach is often suboptimal since the video frames are processed
independently. This causes inconsistent segmentation granularity and
conflicting 3D pseudo labels, which degrades the accuracy of final
segmentation. To address this, we introduce a Granularity-Consistent automatic
2D Mask Tracking approach that maintains temporal correspondences across
frames, eliminating conflicting pseudo labels. Combined with a three-stage
curriculum learning framework, our approach progressively trains from
fragmented single-view data to unified multi-view annotations, ultimately
globally coherent full-scene supervision. This structured learning pipeline
enables the model to progressively expose to pseudo-labels of increasing
consistency. Thus, we can robustly distill a consistent 3D representation from
initially fragmented and contradictory 2D priors. Experimental results
demonstrated that our method effectively generated consistent and accurate 3D
segmentations. Furthermore, the proposed method achieved state-of-the-art
results on standard benchmarks and open-vocabulary ability.

</details>


### [98] [FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data](https://arxiv.org/abs/2511.00795)
*Viswa Chaitanya Marella,Suhasnadh Reddy Veluru,Sai Teja Erukude*

Main category: cs.CV

TL;DR: FedOnco-Bench是一个用于隐私保护联邦学习的可重复基准测试平台，使用合成肿瘤CT扫描数据评估分割性能和隐私泄露，揭示了隐私与性能之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私敏感环境中具有重要价值，但现有系统仍面临成员推理攻击和数据异构性等挑战，需要标准化的基准测试平台来评估隐私保护方法。

Method: 开发FedOnco-Bench基准测试平台，使用合成肿瘤CT扫描数据，评估FedAvg、FedProx、FedBN和FedAvg+DP-SGD等联邦学习方法在分割任务中的表现和隐私保护效果。

Result: FedAvg性能最高（Dice约0.85）但隐私泄露最多（攻击AUC约0.72），DP-SGD隐私保护最好（AUC约0.25）但性能下降（Dice约0.79），FedProx和FedBN在异构数据下表现平衡。

Conclusion: FedOnco-Bench为医学图像分割的隐私保护联邦学习方法提供了一个标准化的开源基准测试平台，揭示了不同方法在隐私与性能之间的权衡关系。

Abstract: Federated Learning (FL) allows multiple institutions to cooperatively train
machine learning models while retaining sensitive data at the source, which has
great utility in privacy-sensitive environments. However, FL systems remain
vulnerable to membership-inference attacks and data heterogeneity. This paper
presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using
synthetic oncologic CT scans with tumor annotations. It evaluates segmentation
performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and
FedAvg with DP-SGD. Results show a distinct trade-off between privacy and
utility: FedAvg is high performance (Dice around 0.85) with more privacy
leakage (attack AUC about 0.72), while DP-SGD provides a higher level of
privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx
and FedBN offer balanced performance under heterogeneous data, especially with
non-identical distributed client data. FedOnco-Bench serves as a standardized,
open-source platform for benchmarking and developing privacy-preserving FL
methods for medical image segmentation.

</details>


### [99] [Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing](https://arxiv.org/abs/2511.00801)
*Zhihui Chen,Mengling Feng*

Main category: cs.CV

TL;DR: 提出了Med-Banana-50K数据集，这是一个包含5万张图像的医疗图像编辑数据集，涵盖三种模态和23种疾病类型，通过Gemini-2.5-Flash-Image生成双向编辑，并采用医学质量控制和迭代优化。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在医疗图像编辑方面进展受限，缺乏大规模、高质量、开放可访问的专门数据集，且需要满足严格的解剖学和临床约束。

Method: 利用Gemini-2.5-Flash-Image从真实医疗图像生成双向编辑（病变添加和移除），采用LLM-as-Judge进行医学质量评估，并进行最多五轮的迭代优化。

Result: 构建了包含5万张图像的数据集，涵盖三种医疗成像模态和23种疾病类型，包括3.7万次失败尝试的完整对话记录。

Conclusion: Med-Banana-50K为训练和评估下一代医疗图像编辑模型提供了基础，数据集和代码已公开。

Abstract: Recent advances in multimodal large language models have enabled remarkable
medical image editing capabilities. However, the research community's progress
remains constrained by the absence of large-scale, high-quality, and openly
accessible datasets built specifically for medical image editing with strict
anatomical and clinical constraints. We introduce Med-Banana-50K, a
comprehensive 50K-image dataset for instruction-based medical image editing
spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23
disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image
to generate bidirectional edits (lesion addition and removal) from real medical
images. What distinguishes Med-Banana-50K from general-domain editing datasets
is our systematic approach to medical quality control: we employ LLM-as-Judge
with a medically grounded rubric (instruction compliance, structural
plausibility, realism, and fidelity preservation) and history-aware iterative
refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K
includes 37K failed attempts with full conversation logs for preference
learning and alignment research. By providing this large-scale, medically
validated, and fully documented resource, Med-Banana-50K establishes a
foundation for training and evaluating the next generation of medical image
editing models.Our dataset and code are publicly available at
[https://github.com/richardChenzhihui/med-banana-50k].

</details>


### [100] [GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding](https://arxiv.org/abs/2511.00810)
*Shijie Zhou,Viet Dac Lai,Hao Tan,Jihyung Kil,Wanrong Zhu,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: GUI-AIMA是一个基于注意力机制的坐标无关GUI定位框架，通过轻量级训练触发MLLMs的固有定位能力，在3B模型上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLMs的GUI定位方法将任务视为基于文本的坐标生成，但直接从视觉输入生成精确坐标具有挑战性且计算量大。

Method: 提出注意力对齐的监督微调框架，通过多头聚合简化查询-视觉注意力矩阵来计算补丁级定位信号，采用坐标无关方式并可集成放大阶段。

Result: 仅用85k截图训练，在ScreenSpot-Pro上达到58.6%的平均准确率，在OSWorld-G上达到62.2%的平均准确率，在3B模型中表现最优。

Conclusion: 轻量级训练可以有效触发MLLMs的固有定位能力，GUI-AIMA在数据效率和性能方面都表现出色。

Abstract: Graphical user interface (GUI) grounding is a key function of computer-use
agents, which maps natural-language instructions to actionable screen regions.
Existing approaches based on Multimodal Large Language Models (MLLMs) typically
formulate it as a text-based coordinate generation task, yet directly
generating precise coordinates from visual inputs remains challenging and
computationally intensive. An intuitive way to implement GUI grounding is to
first select visual patches relevant to the instructions and then determine the
precise click location within those patches. Based on the observations that
general MLLMs have some native grounding capability, nested within their
attentions, we propose GUI-AIMA, an attention-based and coordinate-free
supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns
the intrinsic multimodal attention of MLLMs with patch-wise grounding signals.
These signals are calculated adaptively for diverse user instructions by
multi-head aggregation on simplified query-visual attention matrices. Besides,
its coordinate-free manner can easily integrate a plug-and-play zoom-in stage.
GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional
data efficiency and verifying that light training can trigger the native
grounding capability of MLLMs. It achieves state-of-the-art performance among
3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%
on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA

</details>


### [101] [TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation](https://arxiv.org/abs/2511.00815)
*Yue Gou,Fanghui Song,Yuming Xing,Shengzhu Shi,Zhichang Guo,Boying Wu*

Main category: cs.CV

TL;DR: 提出TA-LSDiff模型，结合拓扑感知扩散概率模型和水平集能量，无需显式几何演化即可实现胰腺分割，在四个公共数据集上达到最先进精度


<details>
  <summary>Details</summary>
Motivation: 胰腺分割面临尺寸小、对比度低和拓扑变化大的挑战，传统水平集方法忽略点状拓扑效应，而深度学习方法牺牲结构细节

Method: 结合拓扑感知扩散概率模型和水平集能量，通过四个互补项引导隐式曲线演化，并引入像素自适应细化模块通过邻域证据的亲和权重局部调制能量函数

Result: 在四个公共胰腺数据集上的评估表明TA-LSDiff达到最先进精度，优于现有方法

Conclusion: TA-LSDiff为胰腺分割提供了实用且准确的解决方案

Abstract: Pancreas segmentation in medical image processing is a persistent challenge
due to its small size, low contrast against adjacent tissues, and significant
topological variations. Traditional level set methods drive boundary evolution
using gradient flows, often ignoring pointwise topological effects. Conversely,
deep learning-based segmentation networks extract rich semantic features but
frequently sacrifice structural details. To bridge this gap, we propose a novel
model named TA-LSDiff, which combined topology-aware diffusion probabilistic
model and level set energy, achieving segmentation without explicit geometric
evolution. This energy function guides implicit curve evolution by integrating
the input image and deep features through four complementary terms. To further
enhance boundary precision, we introduce a pixel-adaptive refinement module
that locally modulates the energy function using affinity weighting from
neighboring evidence. Ablation studies systematically quantify the contribution
of each proposed component. Evaluations on four public pancreas datasets
demonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming
existing methods. These results establish TA-LSDiff as a practical and accurate
solution for pancreas segmentation.

</details>


### [102] [OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models](https://arxiv.org/abs/2511.00821)
*Ruoxiang Huang,Xindian Ma,Rundong Kong,Zhen Yuan,Peng Zhang*

Main category: cs.CV

TL;DR: OMEGA是一个新颖的位置编码框架，通过模态特定位置编码和全局自适应编码步长缩放，解决了当前视觉语言模型中位置编码未考虑文本和视觉模态结构差异的问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型采用统一的1D或2D位置索引策略，没有考虑文本和视觉token在结构特性和连续性上的差异，限制了模型对多模态信息的建模能力。

Method: 提出OMEGA框架：1) 模态特定位置编码(MSPE)在不同坐标维度为文本和视觉token分配位置索引；2) 全局自适应编码步长缩放(GAESS)根据两种模态的嵌入熵自适应调整视觉token的位置编码步长。

Result: 在多个VQA基准测试中，OMEGA显著提升了VLM性能。在视觉密集型任务上，Qwen2.5-VL-3B相比基线提升了3.43%，在Qwen2.5-VL-7B和LLaVA-v1.5-7B等更大模型上也观察到一致的性能提升。

Conclusion: OMEGA通过考虑模态特定结构特性和自适应位置编码策略，有效提升了视觉语言模型的性能，证明了模态感知位置编码在多模态学习中的重要性。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance across
various multimodal tasks, where position encoding plays a vital role in
modeling both the sequential structure of textual information and the spatial
structure of visual information. However, current VLMs commonly adopt
modality-unified 1D or 2D positional indexing strategies, which treat textual
and visual tokens uniformly without accounting for their distinct structural
properties and sequential continuity for text and spatial coherence for vision.
To address this limitation, we propose OMEGA, a novel position encoding
framework that employs Modality-Specific Position Encoding (MSPE) to assign
positional indices while preserving the inherent structures of each modality
across separate coordinate dimensions. Additionally, to align the information
density of multimodal data in the positional index space, OMEGA introduces
Global Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the
position encoding step size of visual tokens based on the embedding entropy of
both modalities. Experimental results demonstrate that OMEGA consistently
enhances VLM performance across diverse architectures and VQA benchmarks. On
visual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline
position encoding strategies on Qwen2.5-VL-3B, with consistent gains observed
across larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.

</details>


### [103] [Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack](https://arxiv.org/abs/2511.00831)
*Xin Liu,Aoyang Zhou,Aoyang Zhou*

Main category: cs.CV

TL;DR: 提出LSSA攻击方法，通过局部图像块随机打乱和采样来增强多模态对抗样本的迁移性，解决了现有方法因输入多样性不足导致的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态对抗攻击方法过度依赖单模态信息，缺乏输入多样性，导致过拟合问题，影响对抗样本的迁移性。

Method: LSSA方法随机打乱局部图像块来扩展原始图像-文本对，生成对抗图像并在其周围采样，然后利用原始和采样图像生成对抗文本。

Result: 在多个模型和数据集上的实验表明，LSSA显著提升了多模态对抗样本在不同VLP模型和下游任务中的迁移性，并在大型视觉语言模型上优于其他先进攻击方法。

Conclusion: LSSA通过增加输入多样性有效解决了多模态对抗攻击中的过拟合问题，显著提升了对抗样本的迁移性能。

Abstract: Visual-Language Pre-training (VLP) models have achieved significant
performance across various downstream tasks. However, they remain vulnerable to
adversarial examples. While prior efforts focus on improving the adversarial
transferability of multimodal adversarial examples through cross-modal
interactions, these approaches suffer from overfitting issues, due to a lack of
input diversity by relying excessively on information from adversarial examples
in one modality when crafting attacks in another. To address this issue, we
draw inspiration from strategies in some adversarial training methods and
propose a novel attack called Local Shuffle and Sample-based Attack (LSSA).
LSSA randomly shuffles one of the local image blocks, thus expanding the
original image-text pairs, generating adversarial images, and sampling around
them. Then, it utilizes both the original and sampled images to generate the
adversarial texts. Extensive experiments on multiple models and datasets
demonstrate that LSSA significantly enhances the transferability of multimodal
adversarial examples across diverse VLP models and downstream tasks. Moreover,
LSSA outperforms other advanced attacks on Large Vision-Language Models.

</details>


### [104] [Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials](https://arxiv.org/abs/2511.00833)
*Yifan Pu,Jixuan Ying,Qixiu Li,Tianzhu Ye,Dongchen Han,Xiaochen Wang,Ziyi Wang,Xinyu Shao,Gao Huang,Xiu Li*

Main category: cs.CV

TL;DR: 提出Visual-Contrast Attention (VCA)作为MHSA的替代方案，通过视觉对比机制降低计算复杂度，在图像识别和生成任务中均取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统ViT的MHSA层对所有token对进行二次方查询-键交互，计算了大量视觉上弱相关或冗余的关联，需要更高效且具有判别性的注意力机制。

Method: VCA首先将每个头的密集查询场蒸馏为少量空间池化的视觉对比token，然后将其分为可学习的正负流，通过差分交互突出区域间的真正差异。

Result: 在DeiT-Tiny上将ImageNet-1K准确率从72.2%提升至75.6%（+3.4%），在三个强层次化ViT上提升达3.1%；在图像生成任务中，FID-50K降低2.1-5.2点。

Conclusion: VCA通过空间池化提供低方差全局线索，双位置嵌入对对比推理不可或缺，两者结合产生最强协同效应，为更快更锐利的Vision Transformer提供了简单路径。

Abstract: Vision Transformers (ViTs) have become a universal backbone for both image
recognition and image generation. Yet their Multi-Head Self-Attention (MHSA)
layer still performs a quadratic query-key interaction for every token pair,
spending the bulk of computation on visually weak or redundant correlations. We
introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that
injects an explicit notion of discrimination while reducing the theoretical
complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's
dense query field into a handful of spatially pooled visual-contrast tokens,
then splits them into a learnable positive and negative stream whose
differential interaction highlights what truly separates one region from
another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone,
requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA
lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and
improves three strong hierarchical ViTs by up to 3.1%, while in
class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points
across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm
that (i) spatial pooling supplies low-variance global cues, (ii) dual
positional embeddings are indispensable for contrastive reasoning, and (iii)
combining the two in both stages yields the strongest synergy. VCA therefore
offers a simple path towards faster and sharper Vision Transformers. The source
code is available at https://github.com/LeapLabTHU/LinearDiff.

</details>


### [105] [Parameter Interpolation Adversarial Training for Robust Image Classification](https://arxiv.org/abs/2511.00836)
*Xin Liu,Yichen Yang,Kun He,John E. Hopcroft*

Main category: cs.CV

TL;DR: 提出了一种名为参数插值对抗训练（PIAT）的新框架，通过在训练过程中对模型参数进行插值来缓解对抗训练中的振荡和过拟合问题，提高模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗训练方法存在模型鲁棒性明显振荡和过拟合问题，这降低了防御效果。

Method: PIAT通过在每个训练周期对前一个周期和当前周期的模型参数进行插值，使模型决策边界变化更平缓；同时使用归一化均方误差（NMSE）来对齐干净样本和对抗样本的logits相对大小。

Result: 在多个基准数据集上的实验表明，该框架能够显著提高卷积神经网络和视觉变换器的鲁棒性。

Conclusion: PIAT框架通过参数插值和NMSE损失函数有效解决了对抗训练中的振荡和过拟合问题，显著提升了模型鲁棒性。

Abstract: Though deep neural networks exhibit superior performance on various tasks,
they are still plagued by adversarial examples. Adversarial training has been
demonstrated to be the most effective method to defend against adversarial
attacks. However, existing adversarial training methods show that the model
robustness has apparent oscillations and overfitting issues in the training
process, degrading the defense efficacy. To address these issues, we propose a
novel framework called Parameter Interpolation Adversarial Training (PIAT).
PIAT tunes the model parameters between each epoch by interpolating the
parameters of the previous and current epochs. It makes the decision boundary
of model change more moderate and alleviates the overfitting issue, helping the
model converge better and achieving higher model robustness. In addition, we
suggest using the Normalized Mean Square Error (NMSE) to further improve the
robustness by aligning the relative magnitude of logits between clean and
adversarial examples rather than the absolute magnitude. Extensive experiments
conducted on several benchmark datasets demonstrate that our framework could
prominently improve the robustness of both Convolutional Neural Networks (CNNs)
and Vision Transformers (ViTs).

</details>


### [106] [OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks](https://arxiv.org/abs/2511.00846)
*Zhihao Peng,Cheng Wang,Shengyuan Liu,Zhiying Liang,Yixuan Yuan*

Main category: cs.CV

TL;DR: OmniBrainBench是首个专门用于评估多模态大语言模型在脑成像分析中多模态理解能力的综合基准，包含15种脑成像模态、9,527个验证问答对和31,706张图像，涵盖15个多阶段临床任务。


<details>
  <summary>Details</summary>
Motivation: 当前脑成像VQA基准要么覆盖模态有限，要么仅限于粗粒度病理描述，无法全面评估MLLMs在整个临床连续体中的表现。

Method: 从30个验证医学来源收集15种不同脑成像模态，构建包含9,527个验证VQA对和31,706张图像的数据集，模拟临床工作流程并涵盖15个多阶段临床任务。

Result: 评估24个最先进模型发现：(1)专有MLLMs优于开源和医学模型但仍落后于医生；(2)医学MLLMs性能差异大；(3)开源MLLMs整体落后但在特定任务表现出色；(4)MLLMs在复杂术前任务中表现明显不佳，存在视觉到临床推理的差距。

Conclusion: OmniBrainBench为评估和推进MLLMs在脑成像分析中设立了新标准，突显了与专家临床推理相比存在的差距。

Abstract: Brain imaging analysis is vital for diagnosing and treating brain disorders,
and multimodal large language models (MLLMs) are increasingly assisting in that
analysis. However, current brain-oriented visual question-answering (VQA)
benchmarks either cover a few imaging modalities or are limited to
coarse-grained pathological descriptions, hindering a comprehensive assessment
of MLLMs throughout the full clinical continuum. To address these, we introduce
OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically
designed to assess the multimodal comprehension capabilities of MLLMs in brain
imaging analysis.OmniBrainBench consists of 15 distinct brain imaging
modalities collected from 30 verified medical sources, yielding 9,527 validated
VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15
multi-stage clinical tasks rigorously validated by a professional radiologist.
Evaluation of 24 state-of-the-art models, including open-source, medical, and
proprietary MLLMs, highlights the substantial challenges posed by
OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)
beat open-source and medical models but lag physicians; (2) medical MLLMs vary
widely in performance; (3) open-source MLLMs trail overall but excel in
specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,
revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new
standard for evaluating and advancing MLLMs in brain imaging analysis,
highlighting gaps compared to expert clinical reasoning. We release it at
benchmark \& code.

</details>


### [107] [Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction](https://arxiv.org/abs/2511.00858)
*Yu Liu,Zhijie Liu,Zedong Yang,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出了一种遮挡感知扩散模型(ODM)，通过重建被遮挡的运动模式来预测行人过马路意图，在遮挡场景下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在预测行人过马路意图方面取得了显著成功，但很少考虑遮挡场景下的不完整观测问题。

Method: 使用遮挡感知扩散变换器架构估计被遮挡模式的噪声特征，并引入遮挡掩码引导的反向过程来有效利用观测信息。

Result: 在PIE和JAAD基准测试上的广泛实验表明，该方法在各种遮挡场景下比现有方法具有更鲁棒的性能。

Conclusion: 所提出的遮挡感知扩散模型能够有效处理遮挡场景下的行人意图预测问题，提高了预测准确性。

Abstract: Predicting pedestrian crossing intentions is crucial for the navigation of
mobile robots and intelligent vehicles. Although recent deep learning-based
models have shown significant success in forecasting intentions, few consider
incomplete observation under occlusion scenarios. To tackle this challenge, we
propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded
motion patterns and leverages them to guide future intention prediction. During
the denoising stage, we introduce an occlusion-aware diffusion transformer
architecture to estimate noise features associated with occluded patterns,
thereby enhancing the model's ability to capture contextual relationships in
occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse
process is introduced to effectively utilize observation information, reducing
the accumulation of prediction errors and enhancing the accuracy of
reconstructed motion features. The performance of the proposed method under
various occlusion scenarios is comprehensively evaluated and compared with
existing methods on popular benchmarks, namely PIE and JAAD. Extensive
experimental results demonstrate that the proposed method achieves more robust
performance than existing methods in the literature.

</details>


### [108] [Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion](https://arxiv.org/abs/2511.00859)
*Jaehyun Park,Konyul Park,Daehun Kim,Junseo Park,Jun Won Choi*

Main category: cs.CV

TL;DR: 提出Layer-Wise Modality Decomposition (LMD)方法，用于解构自动驾驶中多传感器融合模型的模态贡献，实现透明化决策解释。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，感知模型的决策透明度至关重要，因为单个误判可能导致灾难性后果。多传感器输入使得难以确定各模态对预测的贡献，因为传感器信息在融合网络中相互纠缠。

Method: LMD是一种后处理、模型无关的可解释性方法，能够在预训练融合模型的所有层中解构模态特定信息。这是首个在自动驾驶传感器融合系统中将感知模型预测归因于单个输入模态的方法。

Result: 在预训练的相机-雷达、相机-LiDAR和相机-雷达-LiDAR融合模型上评估LMD，通过结构化扰动指标和模态可视化分解验证其有效性，证明其适用于解释高容量多模态架构。

Conclusion: LMD为自动驾驶多传感器融合系统提供了一种实用的模态贡献分析方法，增强了感知模型的透明度和可解释性。

Abstract: In autonomous driving, transparency in the decision-making of perception
models is critical, as even a single misperception can be catastrophic. Yet
with multi-sensor inputs, it is difficult to determine how each modality
contributes to a prediction because sensor information becomes entangled within
the fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a
post-hoc, model-agnostic interpretability method that disentangles
modality-specific information across all layers of a pretrained fusion model.
To our knowledge, LMD is the first approach to attribute the predictions of a
perception model to individual input modalities in a sensor-fusion system for
autonomous driving. We evaluate LMD on pretrained fusion models under
camera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous
driving. Its effectiveness is validated using structured perturbation-based
metrics and modality-wise visual decompositions, demonstrating practical
applicability to interpreting high-capacity multimodal architectures. Code is
available at https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition.

</details>


### [109] [GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2511.00908)
*Heng Zheng,Yuling Shi,Xiaodong Gu,Haochen You,Zijian Zhang,Lubin Gan,Hao Zhang,Wenjun Huang,Jin Huang*

Main category: cs.CV

TL;DR: GraphGeo是一个基于异构图神经网络的多智能体辩论框架，用于视觉地理定位。它通过类型化边建模不同的辩论关系，结合节点级细化和边级辩论建模的双层辩论机制，显著提升了地理定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统检索方法受限于数据库覆盖范围和质量，而现有的大视觉语言模型在复杂地理场景中表现不佳。现有的多智能体系统虽然通过模型协作提升性能，但缺乏有效处理冲突预测的机制。

Method: 提出GraphGeo框架，使用异构图神经网络建模多智能体辩论关系，区分支持性协作、竞争性论证和知识传递。采用双层辩论机制结合节点级细化和边级辩论建模，通过跨层级拓扑细化策略实现图结构和智能体表征的协同演化。

Result: 在多个基准测试上的实验表明，GraphGeo显著优于最先进的方法。

Conclusion: 该框架通过结构化辩论将智能体间的认知冲突转化为增强的地理定位精度，为视觉地理定位提供了新的有效解决方案。

Abstract: Visual geo-localization requires extensive geographic knowledge and
sophisticated reasoning to determine image locations without GPS metadata.
Traditional retrieval methods are constrained by database coverage and quality.
Recent Large Vision-Language Models (LVLMs) enable direct location reasoning
from image content, yet individual models struggle with diverse geographic
regions and complex scenes. Existing multi-agent systems improve performance
through model collaboration but treat all agent interactions uniformly. They
lack mechanisms to handle conflicting predictions effectively. We propose
\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph
neural networks for visual geo-localization. Our approach models diverse debate
relationships through typed edges, distinguishing supportive collaboration,
competitive argumentation, and knowledge transfer. We introduce a dual-level
debate mechanism combining node-level refinement and edge-level argumentation
modeling. A cross-level topology refinement strategy enables co-evolution
between graph structure and agent representations. Experiments on multiple
benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art
methods. Our framework transforms cognitive conflicts between agents into
enhanced geo-localization accuracy through structured debate.

</details>


### [110] [Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs](https://arxiv.org/abs/2511.00916)
*Yan Shu,Chi Liu,Robin Chen,Derek Li,Bryan Dai*

Main category: cs.CV

TL;DR: Fleming-VL是一个统一的多模态大语言模型框架，专门用于处理医学领域的异构数据（2D图像、3D体积扫描、时序视频），通过数据中心的预训练、微调和评估扩展策略，在多个医学视觉理解任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 医学数据具有异构性（包含2D图像、3D体积扫描、时序视频等多种模态），存在显著的领域差距和数据格式不一致问题，这阻碍了统一医学MLLMs的发展。

Method: 采用数据中心的三种关键策略：1）整合自然和医学领域的长上下文数据进行预训练扩展；2）使用罕见医学数据（包括整体视频分析和代表性不足的2D模态）进行微调补充；3）扩展评估框架以纳入3D体积和视频理解基准。通过监督微调和组相对策略优化开发多个模型规模。

Result: 在多个基准测试（包括医学VQA、视频QA和3D医学图像理解）中实现了最先进的性能。

Conclusion: Fleming-VL为医学AI的透明、可重现和可审计进展提供了统一解决方案，并公开发布以促进该领域的发展。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
effectiveness in various general-domain scenarios, such as visual question
answering and image captioning. Recently, researchers have increasingly focused
on empowering MLLMs with medical conversational abilities, which hold
significant promise for clinical applications. However, medical data presents
unique challenges due to its heterogeneous nature -- encompassing diverse
modalities including 2D images, 3D volumetric scans, and temporal video
sequences. The substantial domain gap and data format inconsistencies across
these modalities have hindered the development of unified medical MLLMs. To
address these challenges, we propose Fleming-VL, a unified end-to-end framework
for comprehensive medical visual understanding across heterogeneous modalities.
Fleming-VL tackles this problem from a data-centric perspective through three
key strategies: (1) scaling up pretraining by integrating long-context data
from both natural and medical-specific domains; (2) complementing fine-tuning
with rare medical data, including holistic video analysis and underrepresented
2D modalities such as ultrasound and dermoscopy images; (3) extending existing
evaluation frameworks to incorporate 3D volumetric and video understanding
benchmarks. Through supervised fine-tuning (SFT) and group relative policy
optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive
experiments demonstrate that Fleming-VL achieves state-of-the-art performance
across multiple benchmarks, including medical VQA, video QA, and 3D medical
image understanding. We publicly release Fleming-VL to promote transparent,
reproducible, and auditable progress in medical AI.

</details>


### [111] [Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval](https://arxiv.org/abs/2511.00925)
*Hanwen Su,Ge Song,Jiyan Wang,Yuanbo Zhu*

Main category: cs.CV

TL;DR: 提出动态多级加权对齐网络用于零样本基于草图的图像检索，通过多级权重对齐和加权四元组损失解决模态样本不平衡和低质量信息问题。


<details>
  <summary>Details</summary>
Motivation: 解决零样本基于草图的图像检索中存在的模态样本不平衡和训练过程中低质量信息不一致问题，导致性能不佳。

Method: 包含三个模块：单模态特征提取模块（使用CLIP文本编码器和ViT）、跨模态多级加权模块（通过局部和全局聚合块生成对齐权重列表）、加权四元组损失模块（改进三元组损失的领域平衡）。

Result: 在Sketchy、TU-Berlin和QuickDraw三个基准数据集上，该方法优于现有的最先进ZS-SBIR方法。

Conclusion: 提出的动态多级加权对齐网络能有效解决零样本基于草图的图像检索中的模态不平衡问题，并在多个数据集上取得优越性能。

Abstract: The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved
increasing attention due to its wide applications, e.g. e-commerce. Despite
progress made in this field, previous works suffer from using imbalanced
samples of modalities and inconsistent low-quality information during training,
resulting in sub-optimal performance. Therefore, in this paper, we introduce an
approach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It
consists of three components: (i) a Uni-modal Feature Extraction Module that
includes a CLIP text encoder and a ViT for extracting textual and visual
tokens, (ii) a Cross-modal Multi-level Weighting Module that produces an
alignment weight list by the local and global aggregation blocks to measure the
aligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss
Module aiming to improve the balance of domains in the triplet loss.
Experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and
QuickDraw, show our method delivers superior performances over the
state-of-the-art ZS-SBIR methods.

</details>


### [112] [EVTAR: End-to-End Try on with Additional Unpaired Visual Reference](https://arxiv.org/abs/2511.00956)
*Liuzhuozheng Li,Yue Gong,Shanyuan Liu,Bo Cheng,Yuhang Ma,Liebucha Wu,Dengyang Jiang,Zanyi Wang,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: EVTAR是一个端到端的虚拟试穿模型，通过引入额外参考图像直接拟合目标服装到人物图像上，无需复杂输入如分割图或姿态信息，提高了试穿准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法依赖复杂输入（如分割图、姿态关键点等），导致使用不便且不实用。EVTAR旨在简化输入要求，仅需源图像和目标服装，同时通过参考图像提升试穿质量。

Method: 采用两阶段训练策略，仅需源图像和目标服装作为输入，无需掩码或分割图。利用不同人穿着同一服装的参考图像来保持服装纹理和细节，模拟真实试穿效果。

Result: 在两个广泛使用的基准测试和多样化任务上评估，结果一致验证了该方法的有效性。

Conclusion: EVTAR通过简化输入和引入参考图像机制，实现了更实用和高质量的虚拟试穿效果，为实际应用提供了可行方案。

Abstract: We propose EVTAR, an End-to-End Virtual Try-on model with Additional
Reference, that directly fits the target garment onto the person image while
incorporating reference images to enhance try-on accuracy. Most existing
virtual try-on approaches rely on complex inputs such as agnostic person
images, human pose, densepose, or body keypoints, making them labor-intensive
and impractical for real-world applications. In contrast, EVTAR adopts a
two-stage training strategy, enabling simple inference with only the source
image and the target garment inputs. Our model generates try-on results without
masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional
reference images of different individuals wearing the same clothes to preserve
garment texture and fine-grained details better. This mechanism is analogous to
how humans consider reference models when choosing outfits, thereby simulating
a more realistic and high-quality dressing effect. We enrich the training data
with supplementary references and unpaired person images to support these
capabilities. We evaluate EVTAR on two widely used benchmarks and diverse
tasks, and the results consistently validate the effectiveness of our approach.

</details>


### [113] [GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction](https://arxiv.org/abs/2511.01082)
*Narges Ghasemi,Amir Ziashahabi,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: 提出了一种基于分层序列预测的图像地理定位方法，使用S2网格单元从粗到细逐步定位，结合波束搜索和多样本推理策略，在Im2GPS3k和YFCC4k数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决图像地理定位中视觉相似性和大搜索空间的挑战，模仿人类从大区域到具体地址的逐步定位过程。

Method: 使用S2网格单元构建分层结构，采用自回归方式顺序预测更精细的网格单元，结合波束搜索和多样本推理策略管理不确定性。

Result: 在MLLM-free设置下超越其他基线方法，准确率提升高达13.9%；结合MLLM时在所有指标上达到新的最先进水平。

Conclusion: 分层序列预测方法有效解决了图像地理定位问题，结合推理策略显著提升了定位精度。

Abstract: Image geolocalization, the task of determining an image's geographic origin,
poses significant challenges, largely due to visual similarities across
disparate locations and the large search space. To address these issues, we
propose a hierarchical sequence prediction approach inspired by how humans
narrow down locations from broad regions to specific addresses. Analogously,
our model predicts geographic tokens hierarchically, first identifying a
general region and then sequentially refining predictions to increasingly
precise locations. Rather than relying on explicit semantic partitions, our
method uses S2 cells, a nested, multiresolution global grid, and sequentially
predicts finer-level cells conditioned on visual inputs and previous
predictions. This procedure mirrors autoregressive text generation in large
language models. Much like in language modeling, final performance depends not
only on training but also on inference-time strategy. We investigate multiple
top-down traversal methods for autoregressive sampling, incorporating
techniques from test-time compute scaling used in language models.
Specifically, we integrate beam search and multi-sample inference while
exploring various selection strategies to determine the final output. This
enables the model to manage uncertainty by exploring multiple plausible paths
through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k
datasets against two distinct sets of baselines: those that operate without a
Multimodal Large Language Model (MLLM) and those that leverage one. In the
MLLM-free setting, our model surpasses other comparable baselines on nearly all
metrics, achieving state-of-the-art performance with accuracy gains of up to
13.9%. When augmented with an MLLM, our model outperforms all baselines,
setting a new state-of-the-art across all metrics. The source code is available
at https://github.com/NNargesNN/GeoToken.

</details>


### [114] [A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis](https://arxiv.org/abs/2511.00962)
*Dongheng Lin,Mengxue Qu,Kunyang Han,Jianbo Jiao,Xiaojie Jin,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出统一的零样本视频异常分析框架，通过链式推理连接时间检测、空间定位和文本解释任务，无需额外训练即可实现全零样本异常分析。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常检测方法大多停留在帧级检测，缺乏空间和语义上下文，无法解释异常原因。现有定位和理解方法虽然有所改进，但仍依赖数据且任务特定。

Method: 基于链式测试时推理过程，通过任务内推理优化时间检测，任务间链式连接实现空间和语义理解，利用基础模型的推理能力进行全零样本分析。

Result: 在多个视频异常检测、定位和解释基准测试中实现了最先进的零样本性能，无需额外数据或梯度更新。

Conclusion: 精心设计的提示与任务链式连接能够释放基础模型的推理能力，实现实用且可解释的全零样本视频异常分析。

Abstract: Most video-anomaly research stops at frame-wise detection, offering little
insight into why an event is abnormal, typically outputting only frame-wise
anomaly scores without spatial or semantic context. Recent video anomaly
localization and video anomaly understanding methods improve explainability but
remain data-dependent and task-specific. We propose a unified reasoning
framework that bridges the gap between temporal detection, spatial
localization, and textual explanation. Our approach is built upon a chained
test-time reasoning process that sequentially connects these tasks, enabling
holistic zero-shot anomaly analysis without any additional training.
Specifically, our approach leverages intra-task reasoning to refine temporal
detections and inter-task chaining for spatial and semantic understanding,
yielding improved interpretability and generalization in a fully zero-shot
manner. Without any additional data or gradients, our method achieves
state-of-the-art zero-shot performance across multiple video anomaly detection,
localization, and explanation benchmarks. The results demonstrate that careful
prompt design with task-wise chaining can unlock the reasoning power of
foundation models, enabling practical, interpretable video anomaly analysis in
a fully zero-shot manner. Project Page:
https://rathgrith.github.io/Unified_Frame_VAA/.

</details>


### [115] [SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices](https://arxiv.org/abs/2511.01087)
*Md. Abid Hasan Rafi,Mst. Fatematuj Johora,Pankaj Bhowmik*

Main category: cs.CV

TL;DR: SliceVision-F2I是一个用于网络切片研究的合成数据集，将多变量KPI向量通过四种编码方法转换为视觉表示，包含12万样本，适用于视觉学习和网络状态分析。


<details>
  <summary>Details</summary>
Motivation: 5G/6G网络的发展使网络切片成为未来服务架构的重要组成部分，需要支持强大数据集的精细识别方法。

Method: 使用四种编码方法将多变量KPI向量转换为RGB图像：物理启发映射、Perlin噪声、神经壁纸和分形分支，每种方法生成3万个样本。

Result: 创建了包含原始KPI向量和对应低分辨率RGB图像的合成数据集，模拟了真实且有噪声的网络条件。

Conclusion: SliceVision-F2I数据集适用于视觉学习、网络状态分类、异常检测等任务，可公开获取并用于多种研究场景。

Abstract: The emergence of 5G and 6G networks has established network slicing as a
significant part of future service-oriented architectures, demanding refined
identification methods supported by robust datasets. The article presents
SliceVision-F2I, a dataset of synthetic samples for studying feature
visualization in network slicing for next-generation networking systems. The
dataset transforms multivariate Key Performance Indicator (KPI) vectors into
visual representations through four distinct encoding methods: physically
inspired mappings, Perlin noise, neural wallpapering, and fractal branching.
For each encoding method, 30,000 samples are generated, each comprising a raw
KPI vector and a corresponding RGB image at low-resolution pixels. The dataset
simulates realistic and noisy network conditions to reflect operational
uncertainties and measurement imperfections. SliceVision-F2I is suitable for
tasks involving visual learning, network state classification, anomaly
detection, and benchmarking of image-based machine learning techniques applied
to network data. The dataset is publicly available and can be reused in various
research contexts, including multivariate time series analysis, synthetic data
generation, and feature-to-image transformations.

</details>


### [116] [VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel](https://arxiv.org/abs/2511.00981)
*Suzhong Fu,Rui Sun,Xuan Ding,Jingqi Dong,Yiming Yang,Yao Zhu,Min Chang Jordan Ren,Delin Deng,Angelica Aviles-Rivero,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: VesSAM是一个专门用于2D血管分割的高效框架，通过整合卷积适配器、多提示编码器和轻量级掩码解码器，显著提升了血管分割性能，在多个数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确的血管分割对疾病诊断和手术规划至关重要，但现有基础模型如SAM在血管结构上表现不佳，需要专门优化的解决方案。

Method: VesSAM框架包含三个核心组件：卷积适配器增强局部纹理特征，多提示编码器融合解剖学提示（骨架、分叉点、段中点），轻量级掩码解码器减少锯齿伪影，并配有自动生成多提示标注的流程。

Result: 实验结果显示VesSAM在8个数据集上比基于PEFT的SAM变体提升超过10%的Dice和13%的IoU，与完全微调方法性能相当但参数更少，在分布外设置下也表现优异。

Conclusion: VesSAM为血管分割提供了一个强大而高效的解决方案，在保持竞争力的同时显著减少了参数数量，具有良好的泛化能力。

Abstract: Accurate vessel segmentation is critical for clinical applications such as
disease diagnosis and surgical planning, yet remains challenging due to thin,
branching structures and low texture contrast. While foundation models like the
Segment Anything Model (SAM) have shown promise in generic segmentation, they
perform sub-optimally on vascular structures. In this work, we present VesSAM,
a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM
integrates (1) a convolutional adapter to enhance local texture features, (2) a
multi-prompt encoder that fuses anatomical prompts, including skeletons,
bifurcation points, and segment midpoints, via hierarchical cross-attention,
and (3) a lightweight mask decoder to reduce jagged artifacts. We also
introduce an automated pipeline to generate structured multi-prompt
annotations, and curate a diverse benchmark dataset spanning 8 datasets across
5 imaging modalities. Experimental results demonstrate that VesSAM consistently
outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%
IoU, and achieves competitive performance compared to fully fine-tuned methods,
with significantly fewer parameters. VesSAM also generalizes well to
out-of-distribution (OoD) settings, outperforming all baselines in average OoD
Dice and IoU.

</details>


### [117] [Learning with Category-Equivariant Architectures for Human Activity Recognition](https://arxiv.org/abs/2511.01139)
*Yoshihiro Maruyama*

Main category: cs.CV

TL;DR: CatEquiv是一种用于惯性传感器人体活动识别的类别等变神经网络，通过编码时间、幅度和结构对称性实现等变性，在分布外扰动下展现出比传统CNN更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法未能系统性地编码数据中的时间、幅度和传感器层级结构对称性，导致在分布外场景下泛化能力不足。

Method: 引入类别对称积概念，结合循环时间平移、正增益和传感器层次偏序集来捕捉数据的类别对称结构，构建等变神经网络。

Result: 在UCI-HAR数据集上，面对分布外扰动时，CatEquiv相比循环填充CNN和普通CNN获得了显著更高的鲁棒性。

Conclusion: 强制实施类别对称性可以在不增加模型容量的情况下实现强大的不变性和泛化能力。

Abstract: We propose CatEquiv, a category-equivariant neural network for Human Activity
Recognition (HAR) from inertial sensors that systematically encodes temporal,
amplitude, and structural symmetries. In particular, we introduce the
categorical symmetry product where cyclic time shifts, positive gains and the
sensor-hierarchy poset together capture the categorical symmetry structure of
the data. CatEquiv achieves equivariance with respect to the categorical
symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv
attains markedly higher robustness compared with circularly padded CNNs and
plain CNNs. These results demonstrate that enforcing categorical symmetries
yields strong invariance and generalization without additional model capacity.

</details>


### [118] [MID: A Self-supervised Multimodal Iterative Denoising Framework](https://arxiv.org/abs/2511.00997)
*Chang Nie,Tianchen Deng,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: 提出了一种自监督多模态迭代去噪（MID）框架，通过建模非线性噪声累积过程，无需配对干净-噪声数据集即可有效去除复杂噪声。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据常被复杂非线性噪声污染，传统基于规则的去噪方法难以应对，需要开发不依赖配对数据的有效去噪方法。

Method: MID将噪声数据建模为非线性噪声累积过程中的状态，通过迭代添加噪声学习两个神经网络：一个估计当前噪声步长，另一个预测并减去对应噪声增量。对于复杂非线性污染，使用一阶泰勒展开局部线性化噪声过程。

Result: 在四个经典计算机视觉任务中展示了鲁棒性、适应性和最先进性能，在生物医学和生物信息学领域任务中也表现出强大性能和适应性。

Conclusion: MID框架能够有效处理复杂非线性噪声，无需配对数据集，在多个领域展现出优秀的去噪能力。

Abstract: Data denoising is a persistent challenge across scientific and engineering
domains. Real-world data is frequently corrupted by complex, non-linear noise,
rendering traditional rule-based denoising methods inadequate. To overcome
these obstacles, we propose a novel self-supervised multimodal iterative
denoising (MID) framework. MID models the collected noisy data as a state
within a continuous process of non-linear noise accumulation. By iteratively
introducing further noise, MID learns two neural networks: one to estimate the
current noise step and another to predict and subtract the corresponding noise
increment. For complex non-linear contamination, MID employs a first-order
Taylor expansion to locally linearize the noise process, enabling effective
iterative removal. Crucially, MID does not require paired clean-noisy datasets,
as it learns noise characteristics directly from the noisy inputs. Experiments
across four classic computer vision tasks demonstrate MID's robustness,
adaptability, and consistent state-of-the-art performance. Moreover, MID
exhibits strong performance and adaptability in tasks within the biomedical and
bioinformatics domains.

</details>


### [119] [MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation](https://arxiv.org/abs/2511.01143)
*Ziyi Wang,Yuanmei Zhang,Dorna Esrafilzadeh,Ali R. Jalili,Suncheng Xiang*

Main category: cs.CV

TL;DR: MicroAUNet是一种轻量级注意力分割网络，结合深度可分离扩张卷积和通道-空间注意力块，通过两阶段知识蒸馏实现实时结肠息肉分割。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习息肉分割模型要么提供模糊边界影响临床决策，要么架构复杂计算量大，无法满足实时内窥镜应用需求。

Method: 使用深度可分离扩张卷积和参数共享的通道-空间注意力块增强多尺度边界特征，采用渐进式两阶段知识蒸馏从高容量教师网络转移语义和边界信息。

Result: 在基准测试中表现出最先进的准确性，同时保持极低的模型复杂度。

Conclusion: MicroAUNet适合实时临床息肉分割应用，代码已公开。

Abstract: Early and accurate segmentation of colorectal polyps is critical for reducing
colorectal cancer mortality, which has been extensively explored by academia
and industry. However, current deep learning-based polyp segmentation models
either compromise clinical decision-making by providing ambiguous polyp margins
in segmentation outputs or rely on heavy architectures with high computational
complexity, resulting in insufficient inference speeds for real-time colorectal
endoscopic applications. To address this problem, we propose MicroAUNet, a
light-weighted attention-based segmentation network that combines
depthwise-separable dilated convolutions with a single-path, parameter-shared
channel-spatial attention block to strengthen multi-scale boundary features. On
the basis of it, a progressive two-stage knowledge-distillation scheme is
introduced to transfer semantic and boundary cues from a high-capacity teacher.
Extensive experiments on benchmarks also demonstrate the state-of-the-art
accuracy under extremely low model complexity, indicating that MicroAUNet is
suitable for real-time clinical polyp segmentation. The code is publicly
available at https://github.com/JeremyXSC/MicroAUNet.

</details>


### [120] [Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya](https://arxiv.org/abs/2511.01000)
*Hassan Ugail,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态机器学习框架，通过统一特征提取技术分析视觉图像和X射线图像，用于戈雅画作认证，准确率达97.8%。


<details>
  <summary>Details</summary>
Motivation: 戈雅作品的艺术认证面临复杂计算挑战，因其风格演变异质且存在大量历史伪造模式。

Method: 采用统一特征提取管道，包括灰度共生矩阵描述符、局部二值模式、熵度量、能量计算和颜色分布分析，应用于视觉和X射线图像，通过优化的单类支持向量机处理。

Result: 在24幅认证戈雅画作数据集上，使用80/20训练测试配置和10折交叉验证，达到97.8%分类准确率和0.022假阳性率。

Conclusion: 多模态方法相比单模态方法性能显著提升，证明在视觉和放射影像上应用相同计算方法在艺术认证中的有效性。

Abstract: Art authentication of Francisco Goya's works presents complex computational
challenges due to his heterogeneous stylistic evolution and extensive
historical patterns of forgery. We introduce a novel multimodal machine
learning framework that applies identical feature extraction techniques to both
visual and X-ray radiographic images of Goya paintings. The unified feature
extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors,
Local Binary Patterns, entropy measures, energy calculations, and colour
distribution analysis applied consistently across both imaging modalities. The
extracted features from both visual and X-ray images are processed through an
optimised One-Class Support Vector Machine with hyperparameter tuning. Using a
dataset of 24 authenticated Goya paintings with corresponding X-ray images,
split into an 80/20 train-test configuration with 10-fold cross-validation, the
framework achieves 97.8% classification accuracy with a 0.022 false positive
rate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy
of our pipeline, achieving 92.3% authentication confidence through unified
multimodal feature analysis. Our results indicate substantial performance
improvement over single-modal approaches, establishing the effectiveness of
applying identical computational methods to both visual and radiographic
imagery in art authentication applications.

</details>


### [121] [HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images](https://arxiv.org/abs/2511.01013)
*Mohammad Amanour Rahman*

Main category: cs.CV

TL;DR: HyFormer-Net是一个混合CNN-Transformer架构，用于乳腺癌超声图像的同时分割和分类，具有内在可解释性，在BUSI数据集上表现优异，并通过交叉数据集研究验证了其泛化能力。


<details>
  <summary>Details</summary>
Motivation: B型超声乳腺癌诊断面临斑点噪声、操作员依赖性和边界不清晰等挑战，现有深度学习方法存在单任务学习、架构限制（CNN缺乏全局上下文，Transformer缺乏局部特征）和黑盒决策等问题，阻碍了临床采用。

Method: 提出HyFormer-Net混合架构，采用双分支编码器集成EfficientNet-B3和Swin Transformer，通过多尺度分层融合块结合，使用注意力门控解码器提供精度和可解释性，并引入双管道可解释性方法。

Result: 在BUSI数据集上获得Dice Score 0.761±0.072和准确率93.2%，优于U-Net等模型。恶性召回率92.1±2.2%。集成模型达到Dice 90.2%，准确率99.5%，恶性召回率100%。交叉数据集研究中，使用50%目标域数据时Dice达到77.3%，超过源域性能。

Conclusion: HyFormer-Net在乳腺癌超声诊断中表现出色，通过混合架构和多尺度融合解决了现有方法的局限性，其可解释性和泛化能力为临床采用提供了有力支持。

Abstract: B-mode ultrasound for breast cancer diagnosis faces challenges: speckle,
operator dependency, and indistinct boundaries. Existing deep learning suffers
from single-task learning, architectural constraints (CNNs lack global context,
Transformers local features), and black-box decision-making. These gaps hinder
clinical adoption.
  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous
segmentation and classification with intrinsic interpretability. Its
dual-branch encoder integrates EfficientNet-B3 and Swin Transformer via
multi-scale hierarchical fusion blocks. An attention-gated decoder provides
precision and explainability. We introduce dual-pipeline interpretability: (1)
intrinsic attention validation with quantitative IoU verification (mean: 0.86),
and (2) Grad-CAM for classification reasoning.
  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and
accuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant
Recall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling
yields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant
Recall, eliminating false negatives. Ablation studies confirm multi-scale
fusion contributes +16.8% Dice and attention gates add +5.9%.
  Crucially, we conduct the first cross-dataset generalization study for hybrid
CNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058),
confirming domain shift. However, progressive fine-tuning with only 10%
target-domain data (68 images) recovers 92.5% performance. With 50% data, our
model achieves 77.3% Dice, exceeding source-domain performance (76.1%) and
demonstrating true generalization.

</details>


### [122] [A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment](https://arxiv.org/abs/2511.01194)
*Minmin Zeng*

Main category: cs.CV

TL;DR: 提出GCN-PSN框架，利用图卷积网络建模人体骨架拓扑结构，通过孪生网络架构和对比回归目标学习判别性姿态嵌入，在动作质量评估任务中优于基于坐标的基线方法。


<details>
  <summary>Details</summary>
Motivation: 动作质量评估需要细粒度的人体运动理解和精确的姿态相似性评估，现有基于坐标的方法未能充分利用人体骨架的拓扑结构信息。

Method: 使用拓扑感知的图卷积网络将人体骨架建模为图，采用孪生网络架构和对比回归目标学习姿态嵌入。

Result: 在AQA-7和FineDiving基准测试中优于基于坐标的基线方法，达到竞争性性能。消融研究验证了利用骨架拓扑进行姿态相似性和动作质量评估的有效性。

Conclusion: 骨架拓扑结构对于动作质量评估至关重要，GCN-PSN框架通过建模人体骨架的拓扑关系，显著提升了姿态相似性评估和动作质量评估的性能。

Abstract: Action Quality Assessment (AQA) requires fine-grained understanding of human
motion and precise evaluation of pose similarity. This paper proposes a
topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN,
which models the human skeleton as a graph to learn discriminative,
topology-sensitive pose embeddings. Using a Siamese architecture trained with a
contrastive regression objective, our method outperforms coordinate-based
baselines and achieves competitive performance on AQA-7 and FineDiving
benchmarks. Experimental results and ablation studies validate the
effectiveness of leveraging skeletal topology for pose similarity and action
quality assessment.

</details>


### [123] [FastBoost: Progressive Attention with Dynamic Scaling for Efficient Deep Learning](https://arxiv.org/abs/2511.01026)
*JunXi Yuan*

Main category: cs.CV

TL;DR: FastBoost是一种参数高效的神经网络架构，通过动态缩放渐进注意力机制(DSPA)在CIFAR基准测试中达到最先进性能，显著减少参数数量同时提高准确率。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限边缘设备上部署深度学习模型时参数效率与准确率之间的权衡问题，实现更优的参数-准确率平衡。

Method: 采用动态缩放渐进注意力机制(DSPA)，包括自适应融合、阶段缩放和残差自适应三个创新，结合增强的MBConv模块。

Result: 在CIFAR-10上达到95.57%准确率(0.85M参数)和93.80%(0.37M参数)，在CIFAR-100上达到81.37%准确率(0.92M参数)和74.85%(0.44M参数)，相比MobileNetV3参数减少2.1倍且准确率提高3.2个百分点。

Conclusion: FastBoost通过动态注意力与高效卷积操作的协同优化，实现了前所未有的参数-准确率权衡，为边缘设备部署提供了高效解决方案。

Abstract: We present FastBoost, a parameter-efficient neural architecture that achieves
state-of-the-art performance on CIFAR benchmarks through a novel Dynamically
Scaled Progressive Attention (DSPA) mechanism. Our design establishes new
efficiency frontiers with: CIFAR-10: 95.57% accuracy (0.85M parameters) and
93.80% (0.37M parameters) CIFAR-100: 81.37% accuracy (0.92M parameters) and
74.85% (0.44M parameters) The breakthrough stems from three fundamental
innovations in DSPA: (1) Adaptive Fusion: Learnt channel-spatial attention
blending with dynamic weights. (2) Phase Scaling: Training-stage-aware
intensity modulation (from 0.5 to 1.0). (3) Residual Adaptation: Self-optimized
skip connections (gamma from 0.5 to 0.72). By integrating DSPA with enhanced
MBConv blocks, FastBoost achieves a 2.1 times parameter reduction over
MobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. The
architecture features dual attention pathways with real-time weight adjustment,
cascaded refinement layers (increasing gradient flow by 12.7%), and a
hardware-friendly design (0.28G FLOPs). This co-optimization of dynamic
attention and efficient convolution operations demonstrates unprecedented
parameter-accuracy trade-offs, enabling deployment in resource-constrained edge
devices without accuracy degradation.

</details>


### [124] [Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering](https://arxiv.org/abs/2511.01213)
*Riddhi Jain,Manasi Patwardhan,Parijat Deshpande,Venkataramana Runkana*

Main category: cs.CV

TL;DR: 该论文提出了一种为印度食物视觉问答(VQA)系统构建推理链的方法，通过多步推理过程来提高准确性，相比基线平均提升了10个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有VQA系统偏向西方食物，无法处理印度食物的文化多样性和复杂烹饪背景。现有印度食物VQA数据集采用两步法生成答案，但食物VQA需要多步推理来理解复杂的烹饪背景和食物关系。

Method: 创建自动验证的推理链，对较小的LLM和VLM进行微调，并使用强化学习在更大数据上进行训练。通过推理链增强来提高准确性。

Result: 通过推理链增强，在印度食物VQA任务上相比基线平均提升了10个百分点的准确率。

Conclusion: 多步推理链对于处理印度食物的复杂烹饪背景和关系识别至关重要，能显著提升VQA系统的性能。

Abstract: The immense diversity in the culture and culinary of Indian cuisines calls
attention to the major shortcoming of the existing Visual Question
Answering(VQA) systems which are inclined towards the foods from Western
region. Recent attempt towards building a VQA dataset for Indian food is a step
towards addressing this challenge. However, their approach towards VQA follows
a two-step process in which the answer is generated first, followed by the
explanation of the expected answer. In this work, we claim that food VQA
requires to follow a multi-step reasoning process to arrive at an accurate
answer, especially in the context of India food, which involves understanding
complex culinary context and identifying relationships between various food
items. With this hypothesis we create reasoning chains upon the QA with minimal
human intervention. We fine-tune smaller LLMs and VLMs with auto-validated
reasoning chains and further train them using reinforcement learning with
larger data. With augmentation of reasoning chains, we observed accuracy
improvement of an average 10 percentage points on the baseline. We provide
detailed analysis in terms the effect of addition of reasoning chains for the
Indian Food VQA task.
  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge
Graph.

</details>


### [125] [T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression](https://arxiv.org/abs/2511.01079)
*Nikolay I. Kalmykov,Razan Dibo,Kaiyu Shen,Xu Zhonghan,Anh-Huy Phan,Yipeng Liu,Ivan Oseledets*

Main category: cs.CV

TL;DR: 提出了T-MLA攻击框架，这是首个针对神经图像压缩系统的目标多尺度对数指数攻击方法，通过在小波域中直接攻击重建图像质量，揭示了NIC系统的关键安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 神经图像压缩系统虽然具有优异的率失真性能，但其安全漏洞研究相对较少。现有攻击方法往往简单地将像素空间方法适配到压缩管道，忽视了压缩管道的结构化特性。

Method: 在小波域中构建对抗性扰动，直接针对攻击后重建图像的质量。采用有原则的离线攻击策略，将扰动限制在特定小波子带中，在最大化失真的同时确保视觉隐蔽性。

Result: 在多个最先进的NIC架构和标准图像压缩基准上的广泛评估显示，重建质量大幅下降，而扰动在视觉上难以察觉。

Conclusion: 研究揭示了生成式和内容分发管道核心存在的关键安全缺陷，表明NIC系统在面对精心设计的攻击时存在严重脆弱性。

Abstract: Neural image compression (NIC) has become the state-of-the-art for
rate-distortion performance, yet its security vulnerabilities remain
significantly less understood than those of classifiers. Existing adversarial
attacks on NICs are often naive adaptations of pixel-space methods, overlooking
the unique, structured nature of the compression pipeline. In this work, we
propose a more advanced class of vulnerabilities by introducing T-MLA, the
first targeted multiscale log--exponential attack framework. Our approach
crafts adversarial perturbations in the wavelet domain by directly targeting
the quality of the attacked and reconstructed images. This allows for a
principled, offline attack where perturbations are strategically confined to
specific wavelet subbands, maximizing distortion while ensuring perceptual
stealth. Extensive evaluation across multiple state-of-the-art NIC
architectures on standard image compression benchmarks reveals a large drop in
reconstruction quality while the perturbations remain visually imperceptible.
Our findings reveal a critical security flaw at the core of generative and
content delivery pipelines.

</details>


### [126] [Eyes on Target: Gaze-Aware Object Detection in Egocentric Video](https://arxiv.org/abs/2511.01237)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: 提出Eyes on Target框架，通过将人眼注视特征注入ViT注意力机制，在自我中心视频中实现注视引导的物体检测，提升检测准确率。


<details>
  <summary>Details</summary>
Motivation: 人类注视为理解复杂视觉环境中的注意力提供了丰富的监督信号，传统物体检测器对所有区域平等对待，无法有效利用人类优先关注的区域。

Method: 提出深度感知的注视引导物体检测框架，将注视衍生特征注入Vision Transformer的注意力机制中，偏置空间特征选择朝向人类注视区域。

Result: 在自定义模拟器数据集和公共基准测试（Ego4D Ego-Motion和Ego-CH-Gaze）上，相比无视注视的基线方法，检测准确率持续提升。

Conclusion: 注视引导的物体检测框架能有效利用人类视觉注意力，在自我中心视频中提升检测性能，并揭示了注视线索如何调节transformer注意力动态。

Abstract: Human gaze offers rich supervisory signals for understanding visual attention
in complex visual environments. In this paper, we propose Eyes on Target, a
novel depth-aware and gaze-guided object detection framework designed for
egocentric videos. Our approach injects gaze-derived features into the
attention mechanism of a Vision Transformer (ViT), effectively biasing spatial
feature selection toward human-attended regions. Unlike traditional object
detectors that treat all regions equally, our method emphasises
viewer-prioritised areas to enhance object detection. We validate our method on
an egocentric simulator dataset where human visual attention is critical for
task assessment, illustrating its potential in evaluating human performance in
simulation scenarios. We evaluate the effectiveness of our gaze-integrated
model through extensive experiments and ablation studies, demonstrating
consistent gains in detection accuracy over gaze-agnostic baselines on both the
custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and
Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a
gaze-aware attention head importance metric, revealing how gaze cues modulate
transformer attention dynamics.

</details>


### [127] [Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions](https://arxiv.org/abs/2511.01284)
*Karma Phuntsho,Abdullah,Kyungmi Lee,Ickjai Lee,Euijoon Ahn*

Main category: cs.CV

TL;DR: 这篇综述系统评估了基础模型在医学影像分析中的适应策略，包括监督微调、领域特定预训练、参数高效微调等方法，并指出了当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 基础模型在医学影像分析中具有巨大潜力，但实际临床应用中面临领域偏移、标注数据稀缺、计算需求大和隐私保护等挑战，需要开发有效的适应策略。

Method: 通过综述分析多种适应方法：监督微调、领域特定预训练、参数高效微调、自监督学习、混合方法以及多模态框架，评估其性能增益和临床适用性。

Result: 识别了各种方法的性能提升和局限性，同时指出了现有综述往往忽视的权衡和未解决问题。

Conclusion: 提出了未来研究方向，包括持续学习、联邦学习、混合自监督学习、数据中心化流程和系统基准测试，为开发适应性强、可信赖且临床集成的基础模型提供了路线图。

Abstract: Foundation models (FMs) have emerged as a transformative paradigm in medical
image analysis, offering the potential to provide generalizable, task-agnostic
solutions across a wide range of clinical tasks and imaging modalities. Their
capacity to learn transferable representations from large-scale data has the
potential to address the limitations of conventional task-specific models.
However, adaptation of FMs to real-world clinical practice remains constrained
by key challenges, including domain shifts, limited availability of
high-quality annotated data, substantial computational demands, and strict
privacy requirements. This review presents a comprehensive assessment of
strategies for adapting FMs to the specific demands of medical imaging. We
examine approaches such as supervised fine-tuning, domain-specific pretraining,
parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and
multimodal or cross-modal frameworks. For each, we evaluate reported
performance gains, clinical applicability, and limitations, while identifying
trade-offs and unresolved challenges that prior reviews have often overlooked.
Beyond these established techniques, we also highlight emerging directions
aimed at addressing current gaps. These include continual learning to enable
dynamic deployment, federated and privacy-preserving approaches to safeguard
sensitive data, hybrid self-supervised learning to enhance data efficiency,
data-centric pipelines that combine synthetic generation with human-in-the-loop
validation, and systematic benchmarking to assess robust generalization under
real-world clinical variability. By outlining these strategies and associated
research gaps, this review provides a roadmap for developing adaptive,
trustworthy, and clinically integrated FMs capable of meeting the demands of
real-world medical imaging.

</details>


### [128] [Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models](https://arxiv.org/abs/2511.01307)
*Tae-Young Lee,Juwon Seo,Jong Hwan Ko,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 提出了APDM框架，通过将保护目标从图像转移到扩散模型本身来防止未经授权的个性化生成，包含DPO损失函数和L2P双路径优化策略。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的高质量个性化生成能力带来了隐私风险，现有基于对抗样本的保护方法在少量干净图像或简单图像变换下就会失效。

Method: 提出Direct Protective Optimization (DPO)损失函数来有效干扰目标模型的个性化，同时保持生成质量；提出Learning to Protect (L2P)双路径优化策略，通过交替进行个性化和保护路径来模拟未来个性化轨迹并自适应加强保护。

Result: 实验结果表明该框架优于现有方法，在防止未经授权个性化方面达到了最先进的性能。

Conclusion: APDM框架通过将保护目标转移到扩散模型本身，有效解决了现有方法的局限性，为保护特定主体免受未经授权的个性化生成提供了有效解决方案。

Abstract: Recent advances in diffusion models have enabled high-quality synthesis of
specific subjects, such as identities or objects. This capability, while
unlocking new possibilities in content creation, also introduces significant
privacy risks, as personalization techniques can be misused by malicious users
to generate unauthorized content. Although several studies have attempted to
counter this by generating adversarially perturbed samples designed to disrupt
personalization, they rely on unrealistic assumptions and become ineffective in
the presence of even a few clean images or under simple image transformations.
To address these challenges, we shift the protection target from the images to
the diffusion model itself to hinder the personalization of specific subjects,
through our novel framework called Anti-Personalized Diffusion Models (APDM).
We first provide a theoretical analysis demonstrating that a naive approach of
existing loss functions to diffusion models is inherently incapable of ensuring
convergence for robust anti-personalization. Motivated by this finding, we
introduce Direct Protective Optimization (DPO), a novel loss function that
effectively disrupts subject personalization in the target model without
compromising generative quality. Moreover, we propose a new dual-path
optimization strategy, coined Learning to Protect (L2P). By alternating between
personalization and protection paths, L2P simulates future personalization
trajectories and adaptively reinforces protection at each step. Experimental
results demonstrate that our framework outperforms existing methods, achieving
state-of-the-art performance in preventing unauthorized personalization. The
code is available at https://github.com/KU-VGI/APDM.

</details>


### [129] [Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images](https://arxiv.org/abs/2511.01098)
*Veronica Marsico,Antonio Quintero-Rincon,Hadj Batatia*

Main category: cs.CV

TL;DR: 提出了一种结合Epanechnikov核密度估计和双峰逻辑回归分类器的新方法，用于基于医学图像的呼吸系统疾病诊断。


<details>
  <summary>Details</summary>
Motivation: 利用EKDE的灵活性来建模数据分布，无需假设特定形状，并能适应像素强度变化，从而从医学图像中提取关键特征。

Method: 将Epanechnikov非参数核密度估计与双峰逻辑回归分类器结合，在统计模型学习方案中应用。

Result: 在13808张随机选择的胸部X光片上测试，准确率70.14%，灵敏度59.26%，特异性74.18%，显示出中等性能但灵敏度有待提高。

Conclusion: 虽然临床专业知识对于进一步改进模型仍很重要，但该研究突出了EKDE方法在提高医学影像诊断准确性和可靠性方面的潜力。

Abstract: This study presents a novel method for diagnosing respiratory diseases using
image data. It combines Epanechnikov's non-parametric kernel density estimation
(EKDE) with a bimodal logistic regression classifier in a
statistical-model-based learning scheme. EKDE's flexibility in modeling data
distributions without assuming specific shapes and its adaptability to pixel
intensity variations make it valuable for extracting key features from medical
images. The method was tested on 13808 randomly selected chest X-rays from the
COVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of
59.26%, and a specificity of 74.18%, demonstrating moderate performance in
detecting respiratory disease while showing room for improvement in
sensitivity. While clinical expertise remains essential for further refining
the model, this study highlights the potential of EKDE-based approaches to
enhance diagnostic accuracy and reliability in medical imaging.

</details>


### [130] [CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering](https://arxiv.org/abs/2511.01357)
*Qiangguo Jin,Xianyao Zheng,Hui Cui,Changming Sun,Yuqi Fang,Cong Cong,Ran Su,Leyi Wei,Ping Xuan,Junbo Wang*

Main category: cs.CV

TL;DR: 提出了CMI-MTL框架，通过跨模态交互和多任务学习解决医学视觉问答中的跨模态语义对齐和自由形式答案多样性问题。


<details>
  <summary>Details</summary>
Motivation: 现有自注意力方法难以有效处理视觉与语言间的跨模态语义对齐，分类方法依赖预定义答案集，无法适应自由形式答案的多样性。

Method: 包含三个关键模块：细粒度视觉-文本特征对齐(FVTA)、跨模态交错特征表示(CIFR)、自由形式答案增强多任务学习(FFAE)。

Result: 在VQA-RAD、SLAKE和OVQA三个Med-VQA数据集上优于现有最先进方法。

Conclusion: CMI-MTL框架通过跨模态交互和多任务学习有效提升了医学视觉问答的性能和适应性。

Abstract: Medical visual question answering (Med-VQA) is a crucial multimodal task in
clinical decision support and telemedicine. Recent self-attention based methods
struggle to effectively handle cross-modal semantic alignments between vision
and language. Moreover, classification-based methods rely on predefined answer
sets. Treating this task as a simple classification problem may make it unable
to adapt to the diversity of free-form answers and overlook the detailed
semantic information of free-form answers. In order to tackle these challenges,
we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)
framework that learns cross-modal feature representations from images and
texts. CMI-MTL comprises three key modules: fine-grained visual-text feature
alignment (FVTA), cross-modal interleaved feature representation (CIFR), and
free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most
relevant regions in image-text pairs through fine-grained visual-text feature
alignment. CIFR captures cross-modal sequential interactions via cross-modal
interleaved feature representation. FFAE leverages auxiliary knowledge from
open-ended questions through free-form answer-enhanced multi-task learning,
improving the model's capability for open-ended Med-VQA. Experimental results
show that CMI-MTL outperforms the existing state-of-the-art methods on three
Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more
interpretability experiments to prove the effectiveness. The code is publicly
available at https://github.com/BioMedIA-repo/CMI-MTL.

</details>


### [131] [Anatomically Constrained Transformers for Echocardiogram Analysis](https://arxiv.org/abs/2511.01109)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Jeremy Slivnick,Dipak Kotecha,Alberto Gomez,Jinming Duan*

Main category: cs.CV

TL;DR: 提出了ViACT框架，将解剖先验直接集成到transformer架构中，通过掩码自编码策略仅重建解剖区域，使表示学习聚焦于解剖结构，提高了超声心动图分析的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 视频transformer在超声心动图分析中表现出潜力，但容易从非诊断区域（如图像背景）学习虚假相关性。需要克服这一限制，使模型专注于解剖区域。

Method: ViACT将变形解剖结构表示为点集，编码其空间几何和对应图像块为transformer tokens。预训练时采用掩码自编码策略，仅掩码和重建解剖块，确保表示学习聚焦于解剖区域。

Result: ViACT在左心室射血分数回归和心脏淀粉样变性检测等任务中表现出色，注意力图与已知病理区域对齐，且无需特定组件即可泛化到心肌点跟踪任务。

Conclusion: ViACT通过整合解剖约束，使transformer注意力聚焦于心肌区域，提高了超声心动图分析的性能、可解释性和泛化能力。

Abstract: Video transformers have recently demonstrated strong potential for
echocardiogram (echo) analysis, leveraging self-supervised pre-training and
flexible adaptation across diverse tasks. However, like other models operating
on videos, they are prone to learning spurious correlations from non-diagnostic
regions such as image backgrounds. To overcome this limitation, we propose the
Video Anatomically Constrained Transformer (ViACT), a novel framework that
integrates anatomical priors directly into the transformer architecture. ViACT
represents a deforming anatomical structure as a point set and encodes both its
spatial geometry and corresponding image patches into transformer tokens.
During pre-training, ViACT follows a masked autoencoding strategy that masks
and reconstructs only anatomical patches, enforcing that representation
learning is focused on the anatomical region. The pre-trained model can then be
fine-tuned for tasks localized to this region. In this work we focus on the
myocardium, demonstrating the framework on echo analysis tasks such as left
ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)
detection. The anatomical constraint focuses transformer attention within the
myocardium, yielding interpretable attention maps aligned with regions of known
CA pathology. Moreover, ViACT generalizes to myocardium point tracking without
requiring task-specific components such as correlation volumes used in
specialized tracking networks.

</details>


### [132] [SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment](https://arxiv.org/abs/2511.01390)
*Xinyu Mao,Junsi Li,Haoji Zhang,Yu Liang,Ming Sun*

Main category: cs.CV

TL;DR: SEPS框架通过两阶段机制整合密集和稀疏文本的统一语义，识别关键视觉补丁，利用相关性感知选择和均值计算来改善跨模态相似性评估，在Flickr30K和MS-COCO数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决当前方法在处理补丁冗余和模糊性方面的挑战，这些挑战源于跨模态固有的信息密度差异。多模态大语言模型的密集文本输出可能与原始稀疏描述产生冲突，准确量化丰富视觉补丁与简洁文本描述之间的语义相关性仍是核心难题。

Method: 提出语义增强补丁精简(SEPS)框架，采用两阶段机制整合密集和稀疏文本的统一语义来识别显著视觉补丁，利用相关性感知选择和均值计算来突出关键补丁-词对应关系。

Result: 在Flickr30K和MS-COCO数据集上的综合实验验证SEPS实现了优越性能，在不同模型架构下rSum指标超过现有方法23%-86%，在文本到图像检索场景中有显著提升。

Conclusion: SEPS框架通过系统性解决补丁冗余和模糊性问题，有效改善了跨模态对齐性能，为视觉问答和相关多模态应用提供了可靠基础。

Abstract: Fine-grained cross-modal alignment aims to establish precise local
correspondences between vision and language, forming a cornerstone for visual
question answering and related multimodal applications. Current approaches face
challenges in addressing patch redundancy and ambiguity, which arise from the
inherent information density disparities across modalities. Recently,
Multimodal Large Language Models (MLLMs) have emerged as promising solutions to
bridge this gap through their robust semantic generation capabilities. However,
the dense textual outputs from MLLMs may introduce conflicts with the original
sparse captions. Furthermore, accurately quantifying semantic relevance between
rich visual patches and concise textual descriptions remains a core challenge.
To overcome these limitations, we introduce the Semantic-Enhanced Patch
Slimming (SEPS) framework, which systematically addresses patch redundancy and
ambiguity. Our approach employs a two-stage mechanism to integrate unified
semantics from both dense and sparse texts, enabling the identification of
salient visual patches. Additionally, it leverages relevance-aware selection
with mean value computation to highlight crucial patch-word correspondences,
thereby improving cross-modal similarity assessment. Comprehensive experiments
on Flickr30K and MS-COCO datasets validate that SEPS achieves superior
performance, surpassing existing approaches by 23\%-86\% in rSum across diverse
model architectures, with notable enhancements in text-to-image retrieval
scenarios. Our implementation is available at
https://github.com/Sweet4tars/seps.git.

</details>


### [133] [Boosting performance of computer vision applications through embedded GPUs on the edge](https://arxiv.org/abs/2511.01129)
*Fabio Diniz Rossi*

Main category: cs.CV

TL;DR: 使用GPU增强边缘设备的计算机视觉应用性能，提升AR等资源密集型应用的用户体验


<details>
  <summary>Details</summary>
Motivation: 移动设备上的计算机视觉应用（特别是AR应用）资源需求高，边缘计算设备容量有限，影响用户体验

Method: 在边缘计算设备中集成GPU来处理高强度的计算机视觉任务

Result: 实验显示GPU相比仅使用CPU能获得性能提升，保证用户使用此类应用时获得更好体验

Conclusion: 在边缘设备中集成GPU是克服资源限制、提升计算机视觉应用性能的有效方法

Abstract: Computer vision applications, especially those using augmented reality
technology, are becoming quite popular in mobile devices. However, this type of
application is known as presenting significant demands regarding resources. In
order to enable its utilization in devices with more modest resources, edge
computing can be used to offload certain high intensive tasks. Still, edge
computing is usually composed of devices with limited capacity, which may
impact in users quality of experience when using computer vision applications.
This work proposes the use of embedded devices with graphics processing units
(GPUs) to overcome such limitation. Experiments performed shown that GPUs can
attain a performance gain when compared to using only CPUs, which guarantee a
better experience to users using such kind of application.

</details>


### [134] [UniSOT: A Unified Framework for Multi-Modality Single Object Tracking](https://arxiv.org/abs/2511.01427)
*Yinchao Ma,Yuyang Tang,Wenfei Yang,Tianzhu Zhang,Xu Zhou,Feng Wu*

Main category: cs.CV

TL;DR: 提出UniSOT统一跟踪器，能够同时处理三种参考模态（边界框、自然语言或两者）和四种视频模态（RGB、RGB+深度、RGB+热成像或RGB+事件）的目标跟踪任务。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪器通常针对单一或少数几种模态设计，导致模型分离且限制实际应用。需要统一的跟踪器来处理各种参考模态和视频模态的组合需求。

Method: 开发了UniSOT统一跟踪器，使用统一参数处理三种参考模态和四种视频模态的不同组合。

Result: 在18个视觉跟踪、视觉语言跟踪和RGB+X跟踪基准测试中，UniSOT表现出优于特定模态对应方法的性能。在TNL2K上所有三种参考模态上AUC超过先前方法3.0%以上，在RGB+X视频模态上主要指标超过Un-Track 2.0%以上。

Conclusion: UniSOT证明了统一跟踪器在处理多种参考模态和视频模态组合方面的有效性和优越性能。

Abstract: Single object tracking aims to localize target object with specific reference
modalities (bounding box, natural language or both) in a sequence of specific
video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different
reference modalities enable various human-machine interactions, and different
video modalities are demanded in complex scenarios to enhance tracking
robustness. Existing trackers are designed for single or several video
modalities with single or several reference modalities, which leads to separate
model designs and limits practical applications. Practically, a unified tracker
is needed to handle various requirements. To the best of our knowledge, there
is still no tracker that can perform tracking with these above reference
modalities across these video modalities simultaneously. Thus, in this paper,
we present a unified tracker, UniSOT, for different combinations of three
reference modalities and four video modalities with uniform parameters.
Extensive experimental results on 18 visual tracking, vision-language tracking
and RGB+X tracking benchmarks demonstrate that UniSOT shows superior
performance against modality-specific counterparts. Notably, UniSOT outperforms
previous counterparts by over 3.0\% AUC on TNL2K across all three reference
modalities and outperforms Un-Track by over 2.0\% main metric across all three
RGB+X video modalities.

</details>


### [135] [Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis](https://arxiv.org/abs/2511.01131)
*Md Nahiduzzaman,Steven Korevaar,Alireza Bab-Hadiashar,Ruwan Tennakoon*

Main category: cs.CV

TL;DR: 提出了一种无需概念标注的弱监督概念预测框架PCP，利用类别级概念先验作为弱监督，在医学影像中实现可解释预测。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI需要可解释预测，但现有可解释设计框架需要昂贵的概念标注，而零射方法难以捕捉领域特异性医学特征。

Method: PCP框架利用类别级概念先验作为弱监督，结合KL散度和熵正则化进行预测精化，无需显式监督或语言模型。

Result: 在PH2和WBCatt数据集上，概念级F1分数比零射基线提高33%以上，在四个医学数据集上分类性能与全监督方法相当。

Conclusion: PCP提供了一种无需概念标注的有效可解释预测方案，在医学影像中具有实用价值。

Abstract: Human-interpretable predictions are essential for deploying AI in medical
imaging, yet most interpretable-by-design (IBD) frameworks require concept
annotations for training data, which are costly and impractical to obtain in
clinical contexts. Recent attempts to bypass annotation, such as zero-shot
vision-language models or concept-generation frameworks, struggle to capture
domain-specific medical features, leading to poor reliability. In this paper,
we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised
framework that enables concept answer prediction without explicit supervision
or reliance on language models. PCP leverages class-level concept priors as
weak supervision and incorporates a refinement mechanism with KL divergence and
entropy regularization to align predictions with clinical reasoning.
Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves
concept-level F1-score by over 33% compared to zero-shot baselines, while
delivering competitive classification performance on four medical datasets
(PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept
bottleneck models (CBMs) and V-IP.

</details>


### [136] [Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction](https://arxiv.org/abs/2511.01449)
*Riddhi Jain,Manasi Patwardhan,Aayush Mishra,Parijat Deshpande,Beena Rai*

Main category: cs.CV

TL;DR: 提出了一种模型无关的序数元学习算法(MAOML)，用于训练小型视觉语言模型，通过元学习解决数据稀疏问题并利用标签的序数性，在水果新鲜度分类任务中实现了92.71%的行业标准准确率。


<details>
  <summary>Details</summary>
Motivation: 由于专家标注成本高昂导致数据稀缺，而闭源视觉语言模型存在数据隐私问题，开源模型性能不佳，需要一种能在有限数据下有效训练小型视觉语言模型的方法。

Method: 采用模型无关的序数元学习算法，结合元学习处理数据稀疏问题，并利用标签的序数性进行训练。

Result: 在零样本和少样本设置下，该方法在水果新鲜度分类任务中达到了行业标准的92.71%平均准确率。

Conclusion: MAOML算法能够有效训练小型视觉语言模型，在数据有限的情况下实现与专有模型相当的性能，解决了数据隐私和性能平衡的问题。

Abstract: To effectively manage the wastage of perishable fruits, it is crucial to
accurately predict their freshness or shelf life using non-invasive methods
that rely on visual data. In this regard, deep learning techniques can offer a
viable solution. However, obtaining fine-grained fruit freshness labels from
experts is costly, leading to a scarcity of data. Closed proprietary Vision
Language Models (VLMs), such as Gemini, have demonstrated strong performance in
fruit freshness detection task in both zero-shot and few-shot settings.
Nonetheless, food retail organizations are unable to utilize these proprietary
models due to concerns related to data privacy, while existing open-source VLMs
yield sub-optimal performance for the task. Fine-tuning these open-source
models with limited data fails to achieve the performance levels of proprietary
models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning
(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes
meta-learning to address data sparsity and leverages label ordinality, thereby
achieving state-of-the-art performance in the fruit freshness classification
task under both zero-shot and few-shot settings. Our method achieves an
industry-standard accuracy of 92.71%, averaged across all fruits.
  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,
Ordinal Regression

</details>


### [137] [Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation](https://arxiv.org/abs/2511.01450)
*Jie Du,Xinyu Gong,Qingshan Tan,Wen Li,Yangming Cheng,Weitao Wang,Chenlu Zhan,Suhui Wu,Hao Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: 提出了Reg-DPO方法，通过自动构建高质量偏好对(GT-Pair)和引入SFT损失作为正则化项，解决了视频生成中数据构建成本高、训练不稳定和内存消耗大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法主要基于图像领域范式，在小规模模型上开发，无法有效解决视频任务特有的挑战，如数据构建成本高、训练不稳定和内存消耗大。

Method: 1) 使用GT-Pair自动构建偏好对，以真实视频为正样本，模型生成视频为负样本；2) 在DPO目标中引入SFT损失作为正则化项；3) 结合FSDP框架和多种内存优化技术提升训练能力。

Result: 在多个数据集的I2V和T2V任务上，该方法始终优于现有方法，提供更优的视频生成质量，训练容量比单独使用FSDP提高了近三倍。

Conclusion: Reg-DPO方法有效解决了视频生成中的关键挑战，通过自动数据构建、训练稳定化和内存优化，显著提升了视频生成质量。

Abstract: Recent studies have identified Direct Preference Optimization (DPO) as an
efficient and reward-free approach to improving video generation quality.
However, existing methods largely follow image-domain paradigms and are mainly
developed on small-scale models (approximately 2B parameters), limiting their
ability to address the unique challenges of video tasks, such as costly data
construction, unstable training, and heavy memory consumption. To overcome
these limitations, we introduce a GT-Pair that automatically builds
high-quality preference pairs by using real videos as positives and
model-generated videos as negatives, eliminating the need for any external
annotation. We further present Reg-DPO, which incorporates the SFT loss as a
regularization term into the DPO objective to enhance training stability and
generation fidelity. Additionally, by combining the FSDP framework with
multiple memory optimization techniques, our approach achieves nearly three
times higher training capacity than using FSDP alone. Extensive experiments on
both I2V and T2V tasks across multiple datasets demonstrate that our method
consistently outperforms existing approaches, delivering superior video
generation quality.

</details>


### [138] [When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA](https://arxiv.org/abs/2511.01458)
*Dennis Pierantozzi,Luca Carlini,Mauro Orazio Drago,Chiara Lena,Cesare Hassan,Elena De Momi,Danail Stoyanov,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 提出QA-SNNE方法，通过将问题语义融入预测置信度来改进手术VQA中的不确定性估计，提高安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 手术VQA中错误或模糊的回答可能危害患者安全，现有研究忽视安全行为如模糊意识、转诊专家等，需要改进不确定性估计以实现更安全的决策。

Method: 引入QA-SNNE，一种黑盒不确定性估计器，通过在医学文本嵌入空间中比较生成答案与最近邻来测量语义熵，并将问题语义纳入预测置信度。

Result: QA-SNNE在大多数模板内设置中提高了AUROC，零样本模型的AUROC提升15-38%，幻觉检测能力增强，且在模板外压力下保持增益。

Conclusion: QA-SNNE通过将语义不确定性与问题上下文关联，为手术VQA中的自动故障检测提供了实用且可解释的步骤，结合LVLM骨干和问题对齐的不确定性估计可提高安全性和临床医生信任。

Abstract: Safety and reliability are essential for deploying Visual Question Answering
(VQA) in surgery, where incorrect or ambiguous responses can harm the patient.
Most surgical VQA research focuses on accuracy or linguistic quality while
overlooking safety behaviors such as ambiguity awareness, referral to human
experts, or triggering a second opinion. Inspired by Automatic Failure
Detection (AFD), we study uncertainty estimation as a key enabler of safer
decision making. We introduce Question Aligned Semantic Nearest Neighbor
Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question
semantics into prediction confidence. It measures semantic entropy by comparing
generated answers with nearest neighbors in a medical text embedding space,
conditioned on the question. We evaluate five models, including domain specific
Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large
Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models
degrade under mild paraphrasing, while LVLMs are more resilient. Across three
LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template
settings and enhances hallucination detection. The Area Under the ROC Curve
(AUROC) increases by 15-38% for zero-shot models, with gains maintained under
out-of-template stress. QA-SNNE offers a practical and interpretable step
toward AFD in surgical VQA by linking semantic uncertainty to question context.
Combining LVLM backbones with question aligned uncertainty estimation can
improve safety and clinician trust. The code and model are available at
https://github.com/DennisPierantozzi/QASNNE

</details>


### [139] [ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation](https://arxiv.org/abs/2511.01163)
*Yongyuan Liang,Wei Chow,Feng Li,Ziqiao Ma,Xiyao Wang,Jiageng Mao,Jiuhai Chen,Jiatao Gu,Yue Wang,Furong Huang*

Main category: cs.CV

TL;DR: ROVER是一个专门评估多模态模型跨模态推理能力的新基准，包含1312个任务和1876张图像，测试模型使用一种模态来指导、验证或改进另一种模态输出的能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法将多模态能力孤立对待，缺乏对跨模态推理能力的测试，而这是实现真正统一多模态智能的核心能力。

Method: 构建包含两个互补设置的人工标注基准：语言增强的视觉生成推理（用语言提示和推理链指导图像合成）和视觉增强的语言生成推理（生成中间可视化来加强问答推理过程）。

Result: 对17个统一模型的实验发现：跨模态推理决定视觉生成质量，交织模型显著优于非交织模型；模型在物理和符号推理之间存在分离，能解释感知概念但无法为符号任务构建视觉抽象。

Conclusion: 跨模态推理是实现真正全模态生成的关键前沿，现有模型在这方面仍有明显不足。

Abstract: Unified multimodal models (UMMs) have emerged as a powerful paradigm for
seamlessly unifying text and image understanding and generation. However,
prevailing evaluations treat these abilities in isolation, such that tasks with
multimodal inputs and outputs are scored primarily through unimodal reasoning,
i.e., textual benchmarks emphasize language-based reasoning, while visual
benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce
ROVER to address this pressing need to test reciprocal cross-modal reasoning,
the use of one modality to guide, verify, or refine outputs in the other, an
ability central to the vision of unified multimodal intelligence. ROVER is a
human-annotated benchmark that explicitly targets reciprocal cross-modal
reasoning, which contains 1312 tasks grounded in 1876 images, spanning two
complementary settings. Verbally-augmented reasoning for visual generation
evaluates whether models can use verbal prompts and reasoning chains to guide
faithful image synthesis. Visually-augmented reasoning for verbal generation
evaluates whether models can generate intermediate visualizations that
strengthen their own reasoning processes for question answering. Experiments on
17 unified models reveal two key findings: (i) Cross-modal reasoning determines
visual generation quality, with interleaved models significantly outperforming
non-interleaved ones; notably, combining strong unimodal models fails to
achieve comparable reasoning. (ii) Models show dissociation between physical
and symbolic reasoning: they succeed at interpreting perceptual concepts
literally but fail to construct visual abstractions for symbolic tasks, where
faulty reasoning harms performance. These results highlight reciprocal
cross-modal reasoning as a critical frontier for enabling true omnimodal
generation.

</details>


### [140] [Efficiently Training A Flat Neural Network Before It has been Quantizated](https://arxiv.org/abs/2511.01462)
*Peng Xia,Junbiao Pang,Tianyang Cai*

Main category: cs.CV

TL;DR: 提出了一种通过主动预条件化模型来减少视觉Transformer后训练量化误差的方法，通过将激活和权重量化误差建模为独立高斯噪声，获得平坦的最小值以实现更好的低比特量化。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法通常忽视训练好的神经网络与量化模型之间的关系，导致显著的量化误差，需要一种能有效训练模型无关神经网络的方法来适应预定义精度的低比特模型。

Method: 将激活量化误差和权重量化误差统计建模为独立高斯噪声，研究多种噪声注入优化方法以获得平坦最小值，通过主动预条件化模型来测量和解耦误差源。

Result: 实验结果表明该方法有效，为获得低比特后训练量化模型开辟了新途径。

Conclusion: 平坦的全精度神经网络对于低比特量化至关重要，通过统计建模量化误差并使用噪声注入优化可以获得更好的量化性能。

Abstract: Post-training quantization (PTQ) for vision transformers (ViTs) has garnered
significant attention due to its efficiency in compressing models. However,
existing methods typically overlook the relationship between a well-trained NN
and the quantized model, leading to considerable quantization error for PTQ.
However, it is unclear how to efficiently train a model-agnostic neural network
which is tailored for a predefined precision low-bit model. In this paper, we
firstly discover that a flat full precision neural network is crucial for
low-bit quantization. To achieve this, we propose a framework that proactively
pre-conditions the model by measuring and disentangling the error sources.
Specifically, both the Activation Quantization Error (AQE) and the Weight
Quantization Error (WQE) are statistically modeled as independent Gaussian
noises. We study several noise injection optimization methods to obtain a flat
minimum. Experimental results attest to the effectiveness of our approach.
These results open novel pathways for obtaining low-bit PTQ models.

</details>


### [141] [Web-Scale Collection of Video Data for 4D Animal Reconstruction](https://arxiv.org/abs/2511.01169)
*Brian Nlong Zhao,Jiajun Wu,Shangzhe Wu*

Main category: cs.CV

TL;DR: 提出了一个从YouTube视频自动提取动物中心剪辑的流程，构建了包含30K视频（200万帧）的大规模数据集，并创建了Animal-in-Motion基准测试来评估4D动物重建方法。


<details>
  <summary>Details</summary>
Motivation: 现有动物视频数据集规模有限（仅2.4K个15帧剪辑），缺乏对动物中心3D/4D任务的关键处理能力，需要开发大规模、非侵入式的动物视觉分析方法。

Method: 开发了自动化流程从YouTube视频中挖掘并处理成对象中心剪辑，附带姿态估计、跟踪和3D/4D重建等辅助标注。构建了包含230个手动筛选序列（11K帧）的Animal-in-Motion基准测试。

Result: 收集了30K视频（200万帧），比先前工作规模大一个数量级。在Animal-in-Motion上评估发现，基于模型的方法在2D指标上表现更好但3D形状不真实，而无模型方法产生更自然的重建但得分较低。

Conclusion: 通过提出的流程、基准测试和基线方法，旨在推进从野外视频进行大规模、无标记的4D动物重建及相关任务的发展。

Abstract: Computer vision for animals holds great promise for wildlife research but
often depends on large-scale data, while existing collection methods rely on
controlled capture setups. Recent data-driven approaches show the potential of
single-view, non-invasive analysis, yet current animal video datasets are
limited--offering as few as 2.4K 15-frame clips and lacking key processing for
animal-centric 3D/4D tasks. We introduce an automated pipeline that mines
YouTube videos and processes them into object-centric clips, along with
auxiliary annotations valuable for downstream tasks like pose estimation,
tracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos
(2M frames)--an order of magnitude more than prior works. To demonstrate its
utility, we focus on the 4D quadruped animal reconstruction task. To support
this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually
filtered sequences with 11K frames showcasing clean, diverse animal motions. We
evaluate state-of-the-art model-based and model-free methods on
Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic
3D shapes, while the latter yields more natural reconstructions but scores
lower--revealing a gap in current evaluation. To address this, we enhance a
recent model-free approach with sequence-level optimization, establishing the
first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and
baseline aim to advance large-scale, markerless 4D animal reconstruction and
related tasks from in-the-wild videos. Code and datasets are available at
https://github.com/briannlongzhao/Animal-in-Motion.

</details>


### [142] [HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA](https://arxiv.org/abs/2511.01463)
*Lei Hu,Yongjing Ye,Shihong Xia*

Main category: cs.CV

TL;DR: 提出HMVLM框架，使用MoE LoRA策略解决3D人体运动与文本模态融合中的灾难性遗忘问题，通过零专家保护预训练参数，并采用身体部位特定标记化提升姿态表示质量。


<details>
  <summary>Details</summary>
Motivation: 解决人体运动与文本模态融合中的灾难性遗忘问题，以及开发适用于异构下游任务的自回归兼容姿态表示。

Method: 基于MoE LoRA的统一框架，使用门控网络动态分配LoRA专家权重，引入零专家保护预训练参数，实施身体部位特定标记化。

Result: 有效缓解指令调优中的知识遗忘，在多样化人体运动下游任务中取得显著性能。

Conclusion: HMVLM框架成功解决了模态融合中的关键挑战，为多模态理解与跨模态生成提供了有效解决方案。

Abstract: The expansion of instruction-tuning data has enabled foundation language
models to exhibit improved instruction adherence and superior performance
across diverse downstream tasks. Semantically-rich 3D human motion is being
progressively integrated with these foundation models to enhance multimodal
understanding and cross-modal generation capabilities. However, the modality
gap between human motion and text raises unresolved concerns about catastrophic
forgetting during this integration. In addition, developing
autoregressive-compatible pose representations that preserve generalizability
across heterogeneous downstream tasks remains a critical technical barrier. To
address these issues, we propose the Human Motion-Vision-Language Model
(HMVLM), a unified framework based on the Mixture of Expert Low-Rank
Adaption(MoE LoRA) strategy. The framework leverages the gating network to
dynamically allocate LoRA expert weights based on the input prompt, enabling
synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting
during instruction-tuning, we introduce a novel zero expert that preserves the
pre-trained parameters for general linguistic tasks. For pose representation,
we implement body-part-specific tokenization by partitioning the human body
into different joint groups, enhancing the spatial resolution of the
representation. Experiments show that our method effectively alleviates
knowledge forgetting during instruction-tuning and achieves remarkable
performance across diverse human motion downstream tasks.

</details>


### [143] [Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution](https://arxiv.org/abs/2511.01175)
*Peng Du,Hui Li,Han Xu,Paul Barom Jeon,Dongwook Lee,Daehyun Ji,Ran Yang,Feng Zhu*

Main category: cs.CV

TL;DR: 提出基于图像小波谱的扩散变换器模型DTWSR，通过捕捉多尺度频率子带间的相互关系，实现更一致和逼真的图像超分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于离散小波变换的超分辨率方法大多忽视多尺度频率子带间的相互关系，导致重建图像存在不一致性和不自然伪影。

Method: 使用多级离散小波变换分解图像为小波谱，提出金字塔标记化方法将谱嵌入为变换器序列，设计双解码器分别处理低频和高频子带，同时保持它们在图像生成中的对齐。

Result: 在多个基准数据集上的广泛实验表明，该方法在感知质量和保真度方面均表现出高性能。

Conclusion: DTWSR结合扩散模型和变换器的优势，有效捕捉多尺度频率子带间关系，实现了更一致和逼真的超分辨率图像重建。

Abstract: Discrete Wavelet Transform (DWT) has been widely explored to enhance the
performance of image superresolution (SR). Despite some DWT-based methods
improving SR by capturing fine-grained frequency signals, most existing
approaches neglect the interrelations among multiscale frequency sub-bands,
resulting in inconsistencies and unnatural artifacts in the reconstructed
images. To address this challenge, we propose a Diffusion Transformer model
based on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the
superiority of diffusion models and transformers to capture the interrelations
among multiscale frequency sub-bands, leading to a more consistence and
realistic SR image. Specifically, we use a Multi-level Discrete Wavelet
Transform (MDWT) to decompose images into wavelet spectra. A pyramid
tokenization method is proposed which embeds the spectra into a sequence of
tokens for transformer model, facilitating to capture features from both
spatial and frequency domain. A dual-decoder is designed elaborately to handle
the distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands,
without omitting their alignment in image generation. Extensive experiments on
multiple benchmark datasets demonstrate the effectiveness of our method, with
high performance on both perception quality and fidelity.

</details>


### [144] [Driving scenario generation and evaluation using a structured layer representation and foundational models](https://arxiv.org/abs/2511.01541)
*Arthur Hubert,Gamal Elghazaly,Raphaël Frank*

Main category: cs.CV

TL;DR: 提出了一个五层模型来改进罕见驾驶场景的评估和生成，使用数据增强策略和基础模型生成新场景，并引入了多样性和原创性指标来评估合成数据集的质量。


<details>
  <summary>Details</summary>
Motivation: 罕见和具有挑战性的驾驶场景对自动驾驶开发至关重要，但由于难以遇到，需要通过生成模型来模拟或生成这些场景。

Method: 使用结构化五层模型和大型基础模型，通过数据增强策略生成新驾驶场景，引入特定于层模型的嵌入来比较场景，并采用多样性和原创性指标进行评估。

Result: 在不同生成设置下展示了两种指标的有效性，并对从结构化场景描述生成的合成视频进行了定性评估。

Conclusion: 提出的五层模型和评估指标能够有效生成和评估罕见驾驶场景，为自动驾驶开发提供了有价值的工具。

Abstract: Rare and challenging driving scenarios are critical for autonomous vehicle
development. Since they are difficult to encounter, simulating or generating
them using generative models is a popular approach. Following previous efforts
to structure driving scenario representations in a layer model, we propose a
structured five-layer model to improve the evaluation and generation of rare
scenarios. We use this model alongside large foundational models to generate
new driving scenarios using a data augmentation strategy. Unlike previous
representations, our structure introduces subclasses and characteristics for
every agent of the scenario, allowing us to compare them using an embedding
specific to our layer-model. We study and adapt two metrics to evaluate the
relevance of a synthetic dataset in the context of a structured representation:
the diversity score estimates how different the scenarios of a dataset are from
one another, while the originality score calculates how similar a synthetic
dataset is from a real reference set. This paper showcases both metrics in
different generation setup, as well as a qualitative evaluation of synthetic
videos generated from structured scenario descriptions. The code and extended
results can be found at https://github.com/Valgiz/5LMSG.

</details>


### [145] [DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning](https://arxiv.org/abs/2511.01610)
*Mahmut Selman Gokmen,Cody Bumgardner*

Main category: cs.CV

TL;DR: DINO-MX是一个模块化、可扩展的自监督视觉基础模型训练框架，结合了DINO系列的核心原理，支持多种transformer架构和训练策略，显著降低计算成本并保持竞争力性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型训练流程存在不灵活、领域特定或计算成本高的问题，限制了在不同领域和资源设置下的可用性。

Method: 采用统一配置驱动系统，支持多种transformer架构，包含LoRA、层冻结、知识蒸馏等训练策略，以及DDP和FSDP分布式训练，兼容Hugging Face生态系统。

Result: 在多样化数据集上的实验表明，DINO-MX在显著降低计算成本的同时实现了竞争力性能，并提供可解释性工具和标签引导数据增强方法。

Conclusion: DINO-MX为开发和基准测试自监督视觉模型提供了可重现和可扩展的基础，适用于各种研究和实际应用场景。

Abstract: Vision Foundation Models (VFMs) have advanced representation learning through
self-supervised methods. However, existing training pipelines are often
inflexible, domain-specific, or computationally expensive, which limits their
usability across different domains and resource settings. DINO-MX is a modular
and extensible training framework that combines the core principles of DINO,
DINOv2 and DINOv3 within a unified configuration-driven system. It supports a
variety of transformer-based architectures and is fully compatible with the
Hugging Face ecosystem. The framework includes multiple training strategies
such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,
along with support for distributed training through both Distributed Data
Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to
work with both natural and specialized data types, including single- and
multi-channel images. Experimental results on diverse datasets show that
DINO-MX achieves competitive performance while significantly reducing
computational costs. Additionally, it offers interpretability tools and a
label-guided data augmentation method that improves attention-based
localization without the need for extra detection or segmentation heads.
DINO-MX provides a reproducible and scalable foundation for developing,
adapting, and benchmarking self-supervised vision models across a range of
research and real-world applications.

</details>


### [146] [MoSa: Motion Generation with Scalable Autoregressive Modeling](https://arxiv.org/abs/2511.01200)
*Mengyuan Liu,Sheng Yan,Yong Wang,Yingjie Li,Gui-Bin Bian,Hong Liu*

Main category: cs.CV

TL;DR: MoSa是一个新颖的分层运动生成框架，通过多尺度令牌保留策略和可扩展自回归建模，显著提升了文本驱动3D人体运动生成的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 传统方法在生成3D人体运动时存在效率低下和生成质量不足的问题，需要一种能够同时保证高质量和高效率的生成框架。

Method: 提出了多尺度令牌保留策略(MTPS)集成到分层残差向量量化变分自编码器(RQ-VAE)中，并引入可扩展自回归建模(SAR)来预测尺度令牌，同时设计了轻量级但表达能力强的CAQ-VAE来解决插值带来的重建退化问题。

Result: 在Motion-X数据集上，MoSa实现了0.06的FID（相比MoMask的0.20），同时推理时间减少了27%，在生成质量和效率方面都达到了最先进水平。

Conclusion: MoSa框架在文本驱动的3D人体运动生成任务中表现出色，不仅生成质量高、效率快，还能很好地泛化到运动编辑等下游任务，无需额外微调。

Abstract: We introduce MoSa, a novel hierarchical motion generation framework for
text-driven 3D human motion generation that enhances the Vector
Quantization-guided Generative Transformers (VQ-GT) paradigm through a
coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale
Token Preservation Strategy (MTPS) integrated into a hierarchical residual
vector quantization variational autoencoder (RQ-VAE). MTPS employs
interpolation at each hierarchical quantization to effectively retain
coarse-to-fine multi-scale tokens. With this, the generative transformer
supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens,
unlike traditional methods that predict only one token at each step.
Consequently, MoSa requires only 10 inference steps, matching the number of
RQ-VAE quantization layers. To address potential reconstruction degradation
from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive
convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and
incorporates attention mechanisms to better capture global dependencies.
Extensive experiments show that MoSa achieves state-of-the-art generation
quality and efficiency, outperforming prior methods in both fidelity and speed.
On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20)
while reducing inference time by 27 percent. Moreover, MoSa generalizes well to
downstream tasks such as motion editing, requiring no additional fine-tuning.
The code is available at https://mosa-web.github.io/MoSa-web

</details>


### [147] [Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image](https://arxiv.org/abs/2511.01767)
*Yuxiao Yang,Xiao-Xiao Long,Zhiyang Dou,Cheng Lin,Yuan Liu,Qingsong Yan,Yuexin Ma,Haoqian Wang,Zhiqiang Wu,Wei Yin*

Main category: cs.CV

TL;DR: Wonder3D++是一种从单视图图像高效生成高质量纹理网格的新方法，通过跨域扩散模型生成多视图法线图和对应彩色图像，在约3分钟内完成高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在效率低、几何不一致或质量差的问题，需要一种能同时保证质量、一致性和效率的单视图重建方法。

Method: 使用跨域扩散模型生成多视图法线图和彩色图像，采用多视图跨域注意力机制确保一致性，并通过级联3D网格提取算法从2D表示中提取高质量表面。

Result: 该方法实现了高质量的重建结果，具有良好的泛化能力和效率，相比先前工作有显著提升。

Conclusion: Wonder3D++在单视图3D重建任务中实现了质量、一致性和效率的全面提升，为相关应用提供了有效的解决方案。

Abstract: In this work, we introduce \textbf{Wonder3D++}, a novel method for
efficiently generating high-fidelity textured meshes from single-view images.
Recent methods based on Score Distillation Sampling (SDS) have shown the
potential to recover 3D geometry from 2D diffusion priors, but they typically
suffer from time-consuming per-shape optimization and inconsistent geometry. In
contrast, certain works directly produce 3D information via fast network
inferences, but their results are often of low quality and lack geometric
details. To holistically improve the quality, consistency, and efficiency of
single-view reconstruction tasks, we propose a cross-domain diffusion model
that generates multi-view normal maps and the corresponding color images. To
ensure the consistency of generation, we employ a multi-view cross-domain
attention mechanism that facilitates information exchange across views and
modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that
drives high-quality surfaces from the multi-view 2D representations in only
about $3$ minute in a coarse-to-fine manner. Our extensive evaluations
demonstrate that our method achieves high-quality reconstruction results,
robust generalization, and good efficiency compared to prior works. Code
available at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.

</details>


### [148] [OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA](https://arxiv.org/abs/2511.01210)
*Heyu Guo,Shanmu Wang,Ruichun Ma,Shiqi Jiang,Yasaman Ghasempour,Omid Abari,Baining Guo,Lili Qi*

Main category: cs.CV

TL;DR: OmniVLA是一个多模态视觉-语言-动作模型，通过整合红外相机、毫米波雷达和麦克风阵列等新型感知模态，超越了仅依赖RGB相机的限制，在物理空间智能方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型主要依赖RGB相机，限制了感知能力和操作能力。需要整合更多感知模态来提升物理空间智能。

Method: 提出传感器掩码图像的统一表示方法，将传感器数据以空间掩码形式叠加在RGB图像上。基于RGB预训练的VLA主干网络，构建多传感器VLA模型架构，使用轻量级传感器投影器实现数据高效学习。

Result: 在需要传感器模态感知的挑战性真实世界任务中，OmniVLA平均任务成功率达到84%，显著优于仅RGB模型（提升59%）和原始传感器输入基线模型（提升28%）。

Conclusion: OmniVLA通过多模态整合实现了更强的感知能力、更高的学习效率和更强的泛化能力，证明了多传感器融合在视觉-语言-动作模型中的重要性。

Abstract: Vision-language-action (VLA) models have shown strong generalization for
action prediction through large-scale vision-language pretraining. However,
most existing models rely solely on RGB cameras, limiting their perception and,
consequently, manipulation capabilities. We present OmniVLA, an omni-modality
VLA model that integrates novel sensing modalities for physically-grounded
spatial intelligence beyond RGB perception. The core of our approach is the
sensor-masked image, a unified representation that overlays spatially grounded
and physically meaningful masks onto the RGB images, derived from sensors
including an infrared camera, a mmWave radar, and a microphone array. This
image-native unification keeps sensor input close to RGB statistics to
facilitate training, provides a uniform interface across sensor hardware, and
enables data-efficient learning with lightweight per-sensor projectors. Built
on this, we present a multisensory vision-language-action model architecture
and train the model based on an RGB-pretrained VLA backbone. We evaluate
OmniVLA on challenging real-world tasks where sensor-modality perception is
needed to guide the manipulation. OmniVLA achieves an average task success rate
of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline
models by 59% and 28% respectively, meanwhile showing higher learning
efficiency and stronger generalization capability.

</details>


### [149] [How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment](https://arxiv.org/abs/2511.01775)
*Zhen Chen,Qing Xu,Jinlin Wu,Biao Yang,Yuhao Zhai,Geng Guo,Jing Zhang,Yinlu Ding,Nassir Navab,Jiebo Luo*

Main category: cs.CV

TL;DR: SurgVeo是首个专家策划的手术视频生成模型评估基准，通过四层手术合理性金字塔框架评估模型输出。研究发现Veo-3模型在视觉感知层面表现出色，但在手术器械操作、环境反馈和手术意图等更高层次存在明显的"合理性差距"。


<details>
  <summary>Details</summary>
Motivation: 解决视频生成基础模型在高风险手术领域应用的关键空白，这些领域需要专业的因果知识而非一般物理规则。

Method: 提出SurgVeo基准和四层手术合理性金字塔框架，让Veo-3模型在腹腔镜和神经外科手术片段上进行零样本预测任务，由四位认证外科医生按照SPP框架评估生成视频。

Result: Veo-3在视觉感知合理性方面表现优异，但在器械操作合理性、环境反馈合理性和手术意图合理性等更高层次严重失败，揭示了视觉模仿与因果理解之间的差距。

Conclusion: 这项工作为开发能够应对专业医疗领域复杂性的未来模型奠定了关键基础和路线图。

Abstract: Foundation models in video generation are demonstrating remarkable
capabilities as potential world models for simulating the physical world.
However, their application in high-stakes domains like surgery, which demand
deep, specialized causal knowledge rather than general physical rules, remains
a critical unexplored gap. To systematically address this challenge, we present
SurgVeo, the first expert-curated benchmark for video generation model
evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,
four-tiered framework tailored to assess model outputs from basic appearance to
complex surgical strategy. On the basis of the SurgVeo benchmark, we task the
advanced Veo-3 model with a zero-shot prediction task on surgical clips from
laparoscopic and neurosurgical procedures. A panel of four board-certified
surgeons evaluates the generated videos according to the SPP. Our results
reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual
Perceptual Plausibility, it fails critically at higher levels of the SPP,
including Instrument Operation Plausibility, Environment Feedback Plausibility,
and Surgical Intent Plausibility. This work provides the first quantitative
evidence of the chasm between visually convincing mimicry and causal
understanding in surgical AI. Our findings from SurgVeo and the SPP establish a
crucial foundation and roadmap for developing future models capable of
navigating the complexities of specialized, real-world healthcare domains.

</details>


### [150] [Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering](https://arxiv.org/abs/2511.01223)
*Zahra Mehraban,Sebastien Glaser,Michael Milford,Ronald Schroeter*

Main category: cs.CV

TL;DR: 本文研究了通过翻转数据预训练和微调的方法来改进自动驾驶模型的领域适应能力，特别是在从左舵驾驶到右舵驾驶的适应中。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶模型需要良好的领域适应能力来应对不同的道路条件，特别是从左舵驾驶到右舵驾驶的适应问题。

Method: 评估了四种训练方法：基准模型、翻转数据训练、预训练+微调、翻转预训练+微调，并使用显著性分析来测量注意力转移。

Result: 仅使用翻转数据预训练会降低预测稳定性，但翻转预训练后微调能显著改善适应效果，降低预测误差并增强对左侧线索的关注。ResNet实验验证了类似趋势。

Conclusion: 翻转数据预训练结合微调是一种有效的预处理技术，能以最小的再训练需求显著改善模型适应能力。

Abstract: Domain adaptation is required for automated driving models to generalize well
across diverse road conditions. This paper explores a training method for
domain adaptation to adapt PilotNet, an end-to-end deep learning-based model,
for left-hand driving conditions using real-world Australian highway data. Four
training methods were evaluated: (1) a baseline model trained on U.S.
right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model
pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a
model pretrained on flipped U.S. data and then finetuned on Australian
highways. This setup examines whether incorporating flipped data enhances the
model adaptation by providing an initial left-hand driving alignment. The paper
compares model performance regarding steering prediction accuracy and
attention, using saliency-based analysis to measure attention shifts across
significant road regions. Results show that pretraining on flipped data alone
worsens prediction stability due to misaligned feature representations, but
significantly improves adaptation when followed by fine-tuning, leading to
lower prediction error and stronger focus on left-side cues. To validate this
approach across different architectures, the same experiments were done on
ResNet, which confirmed similar adaptation trends. These findings emphasize the
importance of preprocessing techniques, such as flipped-data pretraining,
followed by fine-tuning to improve model adaptation with minimal retraining
requirements.

</details>


### [151] [Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark](https://arxiv.org/abs/2511.01233)
*Rajmund Nagy,Hendric Voss,Thanh Hoang-Minh,Mihail Tsakov,Teodor Nikolov,Zeyi Zhang,Tenglong Ao,Sicheng Yang,Shaoli Huang,Yongkang Cheng,M. Hamza Mughal,Rishabh Dabral,Kiran Chhatre,Christian Theobalt,Libin Liu,Stefan Kopp,Rachel McDonnell,Michael Neff,Taras Kucherenko,Youngwoo Yoon,Gustav Eje Henter*

Main category: cs.CV

TL;DR: 本文针对语音驱动的3D手势生成领域的人类评估实践进行了系统性审查，发现缺乏标准化和存在实验设计缺陷。作者提出了BEAT2数据集的人类评估协议，并对6个最新手势生成模型进行了大规模众包评估。


<details>
  <summary>Details</summary>
Motivation: 当前手势生成领域缺乏标准化的评估方法，实验设计存在缺陷，导致无法准确比较不同方法的性能，也无法确定该领域的最新技术水平。

Method: 引入详细的BEAT2运动捕捉数据集人类评估协议，进行大规模众包评估，从运动真实性和语音-手势对齐两个关键维度对6个最新手势生成模型进行排名。

Result: 评估结果显示：1）新模型并不总是优于早期方法；2）已发表的高运动真实性或语音-手势对齐声明在严格评估下可能不成立；3）领域需要采用解耦的运动质量和多模态对齐评估以实现准确基准测试。

Conclusion: 手势生成领域必须采用标准化的评估协议，对运动质量和多模态对齐进行解耦评估，以推动技术进步。作者将发布合成运动数据、渲染视频刺激、开源渲染脚本和人类偏好投票数据来促进标准化和新的评估研究。

Abstract: We review human evaluation practices in automated, speech-driven 3D gesture
generation and find a lack of standardisation and frequent use of flawed
experimental setups. This leads to a situation where it is impossible to know
how different methods compare, or what the state of the art is. In order to
address common shortcomings of evaluation design, and to standardise future
user studies in gesture-generation works, we introduce a detailed human
evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using
this protocol, we conduct large-scale crowdsourced evaluation to rank six
recent gesture-generation models -- each trained by its original authors --
across two key evaluation dimensions: motion realism and speech-gesture
alignment. Our results provide strong evidence that 1) newer models do not
consistently outperform earlier approaches; 2) published claims of high motion
realism or speech-gesture alignment may not hold up under rigorous evaluation;
and 3) the field must adopt disentangled assessments of motion quality and
multimodal alignment for accurate benchmarking in order to make progress.
Finally, in order to drive standardisation and enable new evaluation research,
we will release five hours of synthetic motion from the benchmarked models;
over 750 rendered video stimuli from the user studies -- enabling new
evaluations without model reimplementation required -- alongside our
open-source rendering script, and the 16,000 pairwise human preference votes
collected for our benchmark.

</details>


### [152] [Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability](https://arxiv.org/abs/2511.01240)
*Zhixuan Zhang,Pingyu Wang,Xingjian Zheng,Linbo Qing,Qi Liu*

Main category: cs.CV

TL;DR: 提出Adversarial Flatness Attack (AFA)方法，通过解决欺骗性平坦问题来提升对抗样本的可迁移性，在ImageNet数据集上优于六个基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有可迁移攻击方法虽然关注平坦损失，但仍陷入次优区域（欺骗性平坦），导致对抗样本在不同模型间的迁移效果不佳。

Method: 从双阶信息角度提出对抗平坦度(AF)概念，设计AFA攻击方法解决梯度符号改变问题，并开发MCAS方法提升内循环采样效率。

Result: 在ImageNet兼容数据集上表现优于六个基线方法，生成的对抗样本位于更平坦区域，在不同模型架构间具有更好的可迁移性。在输入变换攻击和百度云API测试中也优于基线。

Conclusion: 提出的AFA方法通过解决欺骗性平坦问题，显著提升了黑盒梯度可迁移攻击的效果，为对抗样本的可迁移性提供了理论保证和实用解决方案。

Abstract: Transferable attacks generate adversarial examples on surrogate models to
fool unknown victim models, posing real-world threats and growing research
interest. Despite focusing on flat losses for transferable adversarial
examples, recent studies still fall into suboptimal regions, especially the
flat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce
a novel black-box gradient-based transferable attack from a perspective of
dual-order information. Specifically, we feasibly propose Adversarial Flatness
(AF) to the deceptive flatness problem and a theoretical assurance for
adversarial transferability. Based on this, using an efficient approximation of
our objective, we instantiate our attack as Adversarial Flatness Attack (AFA),
addressing the altered gradient sign issue. Additionally, to further improve
the attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by
enhancing the inner-loop sampling efficiency. The comprehensive results on
ImageNet-compatible dataset demonstrate superiority over six baselines,
generating adversarial examples in flatter regions and boosting transferability
across model architectures. When tested on input transformation attacks or the
Baidu Cloud API, our method outperforms baselines.

</details>


### [153] [CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation](https://arxiv.org/abs/2511.01243)
*Yu Tian,Zhongheng Yang,Chenshi Liu,Yiyun Su,Ziwei Hong,Zexi Gong,Jingyuan Xu*

Main category: cs.CV

TL;DR: CenterMamba-SAM是一种用于脑部病灶分割的端到端框架，通过冻结预训练主干网络并仅训练轻量级适配器实现高效微调，采用创新的3x3角轴中心短序列扫描策略和记忆驱动的结构提示生成器，在公共基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 脑部病灶分割面临小病灶、低对比度、各向异性采样和跨切片不连续性的挑战，需要提高对弱边界和微小病灶的敏感性，同时保持切片间一致性。

Method: 提出CenterMamba编码器采用3x3角轴中心短序列扫描策略实现中心优先、轴强化和对角补偿的信息聚合；记忆驱动的结构提示生成器维护邻近切片的原型库；记忆增强的多尺度解码器集成多级记忆注意力模块。

Result: 在公共基准测试上的广泛实验表明，CenterMamba-SAM在脑部病灶分割任务中达到了最先进的性能。

Conclusion: 该方法通过创新的扫描策略和记忆机制，有效解决了脑部病灶分割中的关键挑战，为医学图像分割提供了高效可靠的解决方案。

Abstract: Brain lesion segmentation remains challenging due to small, low-contrast
lesions, anisotropic sampling, and cross-slice discontinuities. We propose
CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and
trains only lightweight adapters for efficient fine-tuning. At its core is the
CenterMamba encoder, which employs a novel 3x3 corner-axis-center
short-sequence scanning strategy to enable center-prioritized, axis-reinforced,
and diagonally compensated information aggregation. This design enhances
sensitivity to weak boundaries and tiny foci while maintaining sparse yet
effective feature representation. A memory-driven structural prompt generator
maintains a prototype bank across neighboring slices, enabling automatic
synthesis of reliable prompts without user interaction, thereby improving
inter-slice coherence. The memory-augmented multi-scale decoder integrates
memory attention modules at multiple levels, combining deep supervision with
progressive refinement to restore fine details while preserving global
consistency. Extensive experiments on public benchmarks demonstrate that
CenterMamba-SAM achieves state-of-the-art performance in brain lesion
segmentation.

</details>


### [154] [Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop](https://arxiv.org/abs/2511.01250)
*YoungJae Cheong,Jhonghyun An*

Main category: cs.CV

TL;DR: 提出了一种轻量级几何感知适配器，通过方位对齐、水平循环填充和局部K近邻统计来增强LiDAR语义分割在恶劣天气下的鲁棒性，特别关注边界、角落和稀疏区域的结构脆弱性。


<details>
  <summary>Details</summary>
Motivation: LiDAR语义分割在恶劣天气下性能下降，因为折射、散射和点丢失会破坏几何结构。现有方法忽略了边界、角落和稀疏区域的结构脆弱性。

Method: 使用轻量级几何感知适配器，包括方位对齐、水平循环填充、局部窗口K近邻统计，以及基于几何线索的区域感知正则化。该适配器即插即用，仅在训练时启用，推理成本可忽略。

Result: 在仅使用源域数据（SemanticKITTI）训练、在目标域（SemanticSTF）评估的跨天气设置中，mIoU比数据增强基线提高7.9个百分点，比类别中心正则化基线提高0.6个百分点。

Conclusion: 几何驱动的正则化是全天气LiDAR分割的关键方向，该方法能有效提升模型在结构脆弱区域的预测稳定性。

Abstract: LiDAR semantic segmentation degrades in adverse weather because refraction,
scattering, and point dropouts corrupt geometry. Prior work in weather
simulation, mixing-based augmentation, domain randomization, and uncertainty or
boundary regularization improves robustness but still overlooks structural
vulnerabilities near boundaries, corners, and sparse regions. We present a
Light Geometry-aware adapter. The module aligns azimuth and applies horizontal
circular padding to preserve neighbor continuity across the 0~360 degree
wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points
and computes simple local statistics, which are compressed into compact
geometry-aware cues. During training, these cues drive region-aware
regularization that stabilizes predictions in structurally fragile areas. The
adapter is plug and play, complements augmentation, and can be enabled only
during training with negligible inference cost. We adopt a source-only
cross-weather setup where models train on SemanticKITTI and are evaluated on
SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by
7.9 percentage points over the data-centric augmentation baseline and by 0.6
points over the class-centric regularization baseline. These results indicate
that geometry-driven regularization is a key direction for all-weather LiDAR
segmentation.

</details>


### [155] [MotionStream: Real-Time Video Generation with Interactive Motion Controls](https://arxiv.org/abs/2511.01266)
*Joonghyuk Shin,Zhengqi Li,Richard Zhang,Jun-Yan Zhu,Jaesik Park,Eli Schechtman,Xun Huang*

Main category: cs.CV

TL;DR: MotionStream是一个实时视频生成系统，通过将双向教师模型蒸馏为因果学生模型，实现了亚秒级延迟和最高29FPS的流式生成，解决了现有方法延迟高、无法实时交互的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于运动条件的视频生成方法存在分钟级延迟和非因果处理的问题，无法实现实时交互。本文旨在开发一个能够实时生成无限长度视频的流式系统。

Method: 通过自强制分布匹配蒸馏将双向教师模型转化为因果学生模型，采用滑动窗口因果注意力结合注意力汇技术，在训练时通过自展开和KV缓存滚动模拟推理时的外推，实现固定上下文窗口下的恒定速度生成。

Result: MotionStream在运动跟随和视频质量方面达到最先进水平，同时速度快两个数量级，能够实现无限长度的流式生成，用户可以通过绘制轨迹、控制相机或传输运动来实时查看结果。

Conclusion: MotionStream通过创新的蒸馏方法和因果注意力机制，成功实现了实时交互式视频生成，为无限长度视频流式生成提供了可行的解决方案。

Abstract: Current motion-conditioned video generation methods suffer from prohibitive
latency (minutes per video) and non-causal processing that prevents real-time
interaction. We present MotionStream, enabling sub-second latency with up to 29
FPS streaming generation on a single GPU. Our approach begins by augmenting a
text-to-video model with motion control, which generates high-quality videos
that adhere to the global text prompt and local motion guidance, but does not
perform inference on the fly. As such, we distill this bidirectional teacher
into a causal student through Self Forcing with Distribution Matching
Distillation, enabling real-time streaming inference. Several key challenges
arise when generating videos of long, potentially infinite time-horizons: (1)
bridging the domain gap from training on finite length and extrapolating to
infinite horizons, (2) sustaining high quality by preventing error
accumulation, and (3) maintaining fast inference, without incurring growth in
computational cost due to increasing context windows. A key to our approach is
introducing carefully designed sliding-window causal attention, combined with
attention sinks. By incorporating self-rollout with attention sinks and KV
cache rolling during training, we properly simulate inference-time
extrapolations with a fixed context window, enabling constant-speed generation
of arbitrarily long videos. Our models achieve state-of-the-art results in
motion following and video quality while being two orders of magnitude faster,
uniquely enabling infinite-length streaming. With MotionStream, users can paint
trajectories, control cameras, or transfer motion, and see results unfold in
real-time, delivering a truly interactive experience.

</details>


### [156] [PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers](https://arxiv.org/abs/2511.01274)
*Tan Tang,Yanhong Wu,Junming Gao,Yingcai Wu*

Main category: cs.CV

TL;DR: PRevivor是一个先验引导的颜色变换器，通过学习明清时期的绘画来恢复唐宋时期古画的色彩，通过亮度增强和色调校正两个子任务实现色彩恢复。


<details>
  <summary>Details</summary>
Motivation: 中国古代绘画是宝贵的文化遗产，但受到不可逆的色彩退化影响。由于复杂的化学机制和缺乏高质量数据集，色彩恢复非常困难，阻碍了端到端数字修复工具的开发。

Method: 将色彩恢复分解为亮度增强和色调校正两个顺序子任务：使用两个变分U-Net和多尺度映射模块进行亮度增强；设计双分支颜色查询模块，在局部色调先验引导下进行色调校正。

Result: 与最先进的着色方法进行广泛实验，结果显示在定量和定性评估上都表现出优越性能。

Conclusion: PRevivor通过先验引导的色彩变换方法，成功实现了对古代褪色绘画的色彩恢复，为文化遗产保护提供了有效的数字修复工具。

Abstract: Ancient Chinese paintings are a valuable cultural heritage that is damaged by
irreversible color degradation. Reviving color-degraded paintings is
extraordinarily difficult due to the complex chemistry mechanism. Progress is
further slowed by the lack of comprehensive, high-quality datasets, which
hampers the creation of end-to-end digital restoration tools. To revive colors,
we propose PRevivor, a prior-guided color transformer that learns from recent
paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and
Song Dynasty). To develop PRevivor, we decompose color restoration into two
sequential sub-tasks: luminance enhancement and hue correction. For luminance
enhancement, we employ two variational U-Nets and a multi-scale mapping module
to translate faded luminance into restored counterparts. For hue correction, we
design a dual-branch color query module guided by localized hue priors
extracted from faded paintings. Specifically, one branch focuses attention on
regions guided by masked priors, enforcing localized hue correction, whereas
the other branch remains unconstrained to maintain a global reasoning
capability. To evaluate PRevivor, we conduct extensive experiments against
state-of-the-art colorization methods. The results demonstrate superior
performance both quantitatively and qualitatively.

</details>


### [157] [Detecting Generated Images by Fitting Natural Image Distributions](https://arxiv.org/abs/2511.01293)
*Yonggang Zhang,Jun Nie,Xinmei Tian,Mingming Gong,Kun Zhang,Bo Han*

Main category: cs.CV

TL;DR: 提出了一种基于数据流形几何差异的图像生成检测框架，利用自然图像和生成图像在流形结构上的差异进行检测，并通过归一化流放大可检测差异。


<details>
  <summary>Details</summary>
Motivation: 随着生成图像真实感的提升，需要更鲁棒的检测方法来防止滥用。现有方法依赖大量生成图像训练二元分类器，存在局限性。

Method: 利用自然图像和生成图像数据流形的几何差异，设计函数对在自然图像上输出一致但在生成图像上发散，通过自监督模型损失变化检测生成图像，使用归一化流放大差异。

Result: 大量实验证明该方法有效，代码已开源。

Conclusion: 该方法提供了一种简单有效的生成图像检测方案，通过流形几何差异和归一化流技术解决了高级生成模型中差异减小的问题。

Abstract: The increasing realism of generated images has raised significant concerns
about their potential misuse, necessitating robust detection methods. Current
approaches mainly rely on training binary classifiers, which depend heavily on
the quantity and quality of available generated images. In this work, we
propose a novel framework that exploits geometric differences between the data
manifolds of natural and generated images. To exploit this difference, we
employ a pair of functions engineered to yield consistent outputs for natural
images but divergent outputs for generated ones, leveraging the property that
their gradients reside in mutually orthogonal subspaces. This design enables a
simple yet effective detection method: an image is identified as generated if a
transformation along its data manifold induces a significant change in the loss
value of a self-supervised model pre-trained on natural images. Further more,
to address diminishing manifold disparities in advanced generative models, we
leverage normalizing flows to amplify detectable differences by extruding
generated images away from the natural image manifold. Extensive experiments
demonstrate the efficacy of this method. Code is available at
https://github.com/tmlr-group/ConV.

</details>


### [158] [UniREditBench: A Unified Reasoning-based Image Editing Benchmark](https://arxiv.org/abs/2511.01295)
*Feng Han,Yibin Wang,Chenglin Li,Zheming Liang,Dianyi Wang,Yang Jiao,Zhipeng Wei,Chao Gong,Cheng Jin,Jingjing Chen,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出了UniREditBench基准，用于评估基于推理的图像编辑模型，包含2700个样本，涵盖真实世界和游戏世界场景，并引入多模态双参考评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态生成模型在需要隐式推理的复杂图像编辑任务上表现不佳，现有基准主要关注单对象属性变换，忽视了多对象交互和游戏场景，且仅依赖文本参考评估存在系统性误判风险。

Method: 构建了包含2700个样本的统一基准，涵盖8个主要维度和18个子维度；设计了多模态双参考评估方法（文本和真实图像参考）；开发了自动数据合成流程，创建了包含10万样本的大规模合成数据集UniREdit-Data-100K，并基于此微调Bagel模型。

Result: 通过微调Bagel模型开发的UniREdit-Bagel在域内和域外设置下均表现出显著改进；对开源和闭源图像编辑模型的全面基准测试揭示了它们在不同方面的优势和弱点。

Conclusion: UniREditBench为基于推理的图像编辑提供了全面评估框架，多模态双参考评估提高了评估可靠性，合成数据集和微调模型展示了在复杂推理任务上的改进潜力。

Abstract: Recent advances in multi-modal generative models have driven substantial
improvements in image editing. However, current generative models still
struggle with handling diverse and complex image editing tasks that require
implicit reasoning, underscoring the need for a comprehensive benchmark to
systematically assess their performance across various reasoning scenarios.
Existing benchmarks primarily focus on single-object attribute transformation
in realistic scenarios, which, while effective, encounter two key challenges:
(1) they largely overlook multi-object interactions as well as game-world
scenarios that involve human-defined rules, which are common in real-life
applications; (2) they only rely on textual references to evaluate the
generated images, potentially leading to systematic misjudgments, especially in
complex reasoning scenarios. To this end, this work proposes UniREditBench, a
unified benchmark for reasoning-based image editing evaluation. It comprises
2,700 meticulously curated samples, covering both real- and game-world
scenarios across 8 primary dimensions and 18 sub-dimensions. To improve
evaluation reliability, we introduce multimodal dual-reference evaluation,
providing both textual and ground-truth image references for each sample
assessment. Furthermore, we design an automated multi-scenario data synthesis
pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with
high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel
on this dataset and develop UniREdit-Bagel, demonstrating substantial
improvements in both in-domain and out-of-distribution settings. Through
thorough benchmarking of both open-source and closed-source image editing
models, we reveal their strengths and weaknesses across various aspects.

</details>


### [159] [REASON: Probability map-guided dual-branch fusion framework for gastric content assessment](https://arxiv.org/abs/2511.01302)
*Nu-Fnag Xiao,De-Xing Huang,Le-Tian Wang,Mei-Jiang Gui,Qi Fu,Xiao-Liang Xie,Shi-Qi Liu,Shuangyi Wang,Zeng-Guang Hou,Ying-Wei Wang,Xiao-Hu Zhou*

Main category: cs.CV

TL;DR: 提出了一种新颖的两阶段概率图引导双分支融合框架（REASON），用于胃内容物评估，通过抑制伪影和融合两个标准视图信息，显著提高了胃内容物分类的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统胃内容物评估方法依赖手动追踪胃窦和使用经验公式，在效率和准确性方面存在显著局限性，无法满足临床术前误吸风险评估的需求。

Method: 采用两阶段框架：第一阶段使用分割模型生成概率图来抑制伪影并突出胃解剖结构；第二阶段使用双分支分类器融合右侧卧位和仰卧位两个标准视图的信息，以改进学习特征的判别能力。

Result: 在自收集数据集上的实验结果表明，该框架显著优于当前最先进的方法，在胃内容物评估方面取得了更好的性能。

Conclusion: 该框架为自动化术前误吸风险评估提供了更稳健、高效和准确的解决方案，在临床实践中具有巨大应用潜力。

Abstract: Accurate assessment of gastric content from ultrasound is critical for
stratifying aspiration risk at induction of general anesthesia. However,
traditional methods rely on manual tracing of gastric antra and empirical
formulas, which face significant limitations in both efficiency and accuracy.
To address these challenges, a novel two-stage probability map-guided
dual-branch fusion framework (REASON) for gastric content assessment is
proposed. In stage 1, a segmentation model generates probability maps that
suppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch
classifier fuses information from two standard views, right lateral decubitus
(RLD) and supine (SUP), to improve the discrimination of learned features.
Experimental results on a self-collected dataset demonstrate that the proposed
framework outperforms current state-of-the-art approaches by a significant
margin. This framework shows great promise for automated preoperative
aspiration risk assessment, offering a more robust, efficient, and accurate
solution for clinical practice.

</details>


### [160] [Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation](https://arxiv.org/abs/2511.01304)
*Chentao Li,Behzad Bozorgtabar,Yifang Ping,Pan Huang,Jing Qin*

Main category: cs.CV

TL;DR: 提出了一种用于全玻片图像解释性表示的三阶段学习框架，通过潜在因子分组、聚类推理实例解缠和广义线性加权决策来解决多实例学习中的空间、语义和决策纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 多实例学习在全玻片病理图像表示中应用广泛，但实例间的空间、语义和决策纠缠限制了其表示能力和可解释性。

Method: 三阶段框架：1) 正半定潜在因子分组将实例映射到潜在子空间；2) 通过聚类推理实例解缠进行概率反事实推理；3) 实例效应重加权的广义线性加权决策。

Result: 在多中心数据集上的实验表明，该模型优于所有最先进模型，并通过解缠表示和透明决策过程实现了与病理学家对齐的可解释性。

Conclusion: 该框架有效解决了多实例学习中的纠缠问题，在保持高性能的同时实现了病理学级别的可解释性。

Abstract: Multiple instance learning (MIL) has been widely used for representing
whole-slide pathology images. However, spatial, semantic, and decision
entanglements among instances limit its representation and interpretability. To
address these challenges, we propose a latent factor grouping-boosted
cluster-reasoning instance disentangled learning framework for whole-slide
image (WSI) interpretable representation in three phases. First, we introduce a
novel positive semi-definite latent factor grouping that maps instances into a
latent subspace, effectively mitigating spatial entanglement in MIL. To
alleviate semantic entanglement, we employs instance probability counterfactual
inference and optimization via cluster-reasoning instance disentangling.
Finally, we employ a generalized linear weighted decision via instance effect
re-weighting to address decision entanglement. Extensive experiments on
multicentre datasets demonstrate that our model outperforms all
state-of-the-art models. Moreover, it attains pathologist-aligned
interpretability through disentangled representations and a transparent
decision-making process.

</details>


### [161] [MVSMamba: Multi-View Stereo with State Space Model](https://arxiv.org/abs/2511.01315)
*Jianfei Jiang,Qiankun Liu,Hongyuan Liu,Haochen Yu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: MVSMamba是首个基于Mamba架构的多视图立体视觉网络，通过动态Mamba模块实现高效全局特征聚合，在DTU和Tanks-and-Temples数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer-based MVS方法存在二次复杂度问题，难以平衡性能与效率。Mamba架构具有全局建模能力和线性复杂度，为解决此问题提供了新思路。

Method: 提出MVSMamba网络，包含基于参考中心动态扫描策略的动态Mamba模块，支持参考视图到源视图的高效特征交互、全向多视图特征表示和多尺度全局特征聚合。

Result: 在DTU数据集和Tanks-and-Temples基准测试中，MVSMamba超越了现有最先进的MVS方法，在性能和效率方面均表现优异。

Conclusion: MVSMamba成功将Mamba架构应用于MVS任务，实现了高效全局特征建模，为MVS研究提供了新的方向。

Abstract: Robust feature representations are essential for learning-based Multi-View
Stereo (MVS), which relies on accurate feature matching. Recent MVS methods
leverage Transformers to capture long-range dependencies based on local
features extracted by conventional feature pyramid networks. However, the
quadratic complexity of Transformer-based MVS methods poses challenges to
balance performance and efficiency. Motivated by the global modeling capability
and linear complexity of the Mamba architecture, we propose MVSMamba, the first
Mamba-based MVS network. MVSMamba enables efficient global feature aggregation
with minimal computational overhead. To fully exploit Mamba's potential in MVS,
we propose a Dynamic Mamba module (DM-module) based on a novel
reference-centered dynamic scanning strategy, which enables: (1) Efficient
intra- and inter-view feature interaction from the reference to source views,
(2) Omnidirectional multi-view feature representations, and (3) Multi-scale
global feature aggregation. Extensive experimental results demonstrate MVSMamba
outperforms state-of-the-art MVS methods on the DTU dataset and the
Tanks-and-Temples benchmark with both superior performance and efficiency. The
source code is available at https://github.com/JianfeiJ/MVSMamba.

</details>


### [162] [A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model](https://arxiv.org/abs/2511.01317)
*Sampriti Soor,Alik Pramanick,Jothiprakash K,Arijit Sur*

Main category: cs.CV

TL;DR: 提出一种基于CLIP模型的生成对抗攻击方法，通过结合文本语义和视觉特征生成视觉不可察觉的对抗扰动，在多对象场景中有效欺骗多标签分类器。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易受到对抗攻击的影响，现有方法在保持视觉保真度和攻击效果方面存在不足，需要开发更有效的对抗样本生成方法。

Method: 集成CLIP模型的文本-图像对齐能力，结合SSAE的集中扰动策略和GAMA的不同文本嵌入方法，使用引导损失生成对抗扰动。

Result: 在多种黑盒受害者模型上的实验表明，该方法在攻击效果上与现有技术相当或更优，同时保持了更高的视觉保真度。

Conclusion: 所提出的方法能够生成视觉不可察觉且有效的对抗样本，在多对象场景中成功欺骗多标签分类器，在攻击效果和视觉质量之间取得了良好平衡。

Abstract: The rapid growth of deep learning has brought about powerful models that can
handle various tasks, like identifying images and understanding language.
However, adversarial attacks, an unnoticed alteration, can deceive models,
leading to inaccurate predictions. In this paper, a generative adversarial
attack method is proposed that uses the CLIP model to create highly effective
and visually imperceptible adversarial perturbations. The CLIP model's ability
to align text and image representation helps incorporate natural language
semantics with a guided loss to generate effective adversarial examples that
look identical to the original inputs. This integration allows extensive scene
manipulation, creating perturbations in multi-object environments specifically
designed to deceive multilabel classifiers. Our approach integrates the
concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with
the dissimilar text embeddings similar to Generative Adversarial Multi-Object
Scene Attacks (GAMA), resulting in perturbations that both deceive
classification models and maintain high structural similarity to the original
images. The model was tested on various tasks across diverse black-box victim
models. The experimental results show that our method performs competitively,
achieving comparable or superior results to existing techniques, while
preserving greater visual fidelity.

</details>


### [163] [RDTE-UNet: A Boundary and Detail Aware UNet for Precise Medical Image Segmentation](https://arxiv.org/abs/2511.01328)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: 提出RDTE-UNet分割网络，通过结合局部建模和全局上下文来增强边界描绘和细节保留，在医学图像分割中取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对计算机辅助诊断和治疗规划至关重要，但解剖结构变异性和边界模糊性阻碍了对精细结构的可靠描绘。

Method: 使用混合ResBlock细节感知Transformer骨干网络，包含ASBE模块进行自适应边界增强、HVDA模块进行细粒度特征建模、EulerFF模块基于欧拉公式进行融合加权。

Result: 在Synapse和BUSI数据集上，RDTE-UNet在分割精度和边界质量方面达到了可比水平。

Conclusion: 该方法通过统一局部建模和全局上下文，提高了跨形态、方向和尺度的结构一致性和边界准确性。

Abstract: Medical image segmentation is essential for computer-assisted diagnosis and
treatment planning, yet substantial anatomical variability and boundary
ambiguity hinder reliable delineation of fine structures. We propose RDTE-UNet,
a segmentation network that unifies local modeling with global context to
strengthen boundary delineation and detail preservation. RDTE-UNet employs a
hybrid ResBlock detail-aware Transformer backbone and three modules: ASBE for
adaptive boundary enhancement, HVDA for fine-grained feature modeling, and
EulerFF for fusion weighting guided by Euler's formula. Together, these
components improve structural consistency and boundary accuracy across
morphology, orientation, and scale. On Synapse and BUSI dataset, RDTE-UNet has
achieved a comparable level in terms of segmentation accuracy and boundary
quality.

</details>


### [164] [$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles](https://arxiv.org/abs/2511.01340)
*Trishanu Das,Abhilash Nandy,Khush Bajaj,Deepiha S*

Main category: cs.CV

TL;DR: 提出了一个包含1333个英语Rebus谜题的大型多样化基准测试，并开发了RebusDescProgICE框架，通过结合非结构化描述和基于代码的结构化推理，显著提升了视觉语言模型在Rebus谜题上的性能。


<details>
  <summary>Details</summary>
Motivation: Rebus谜题需要图像识别、认知技能、常识推理、多步推理和基于图像的文字游戏等多种能力，这对当前的视觉语言模型具有挑战性。

Method: 提出了RebusDescProgICE框架，结合非结构化描述和基于代码的结构化推理，并改进了基于推理的上下文示例选择方法。

Result: 相比思维链推理，该框架在闭源模型上提升了2.1-4.1%的性能，在开源模型上提升了20-30%的性能。

Conclusion: RebusDescProgICE框架有效提升了视觉语言模型在复杂Rebus谜题上的推理能力，证明了结合描述性和结构化推理方法的价值。

Abstract: Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters
to represent words or phrases creatively) requires a variety of skills such as
image recognition, cognitive skills, commonsense reasoning, multi-step
reasoning, image-based wordplay, etc., making this a challenging task for even
current Vision-Language Models. In this paper, we present
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$, a large and diverse
benchmark of $1,333$ English Rebus Puzzles containing different artistic styles
and levels of difficulty, spread across 18 categories such as food, idioms,
sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a
model-agnostic framework which uses a combination of an unstructured
description and code-based, structured reasoning, along with better,
reasoning-based in-context example selection, improving the performance of
Vision-Language Models on
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$ by $2.1-4.1\%$ and
$20-30\%$ using closed-source and open-source models respectively compared to
Chain-of-Thought Reasoning.

</details>


### [165] [MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement](https://arxiv.org/abs/2511.01345)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: MIQ-SAM3D是一个多实例3D分割框架，通过竞争性查询优化策略实现从单点单对象到单点多实例的转变，解决了医学图像中多病灶分割的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有SAM-based交互式分割方法大多遵循单点单对象范式，限制了多病灶分割能力；同时ViT骨干网络虽然能捕获全局上下文，但往往缺失高保真局部细节。

Method: 提出提示条件实例查询生成器将单点提示转换为多个专用查询；采用混合CNN-Transformer编码器通过空间门控将CNN边界显著性注入ViT自注意力；使用竞争优化查询解码器实现端到端并行多实例预测。

Result: 在LiTS17和KiTS21数据集上，MIQ-SAM3D取得了可比性能，并展现出对提示的强鲁棒性。

Conclusion: 该方法为临床相关多病灶病例的高效标注提供了实用解决方案。

Abstract: Accurate segmentation of medical images is fundamental to tumor diagnosis and
treatment planning. SAM-based interactive segmentation has gained attention for
its strong generalization, but most methods follow a
single-point-to-single-object paradigm, which limits multi-lesion segmentation.
Moreover, ViT backbones capture global context but often miss high-fidelity
local details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework
with a competitive query optimization strategy that shifts from
single-point-to-single-mask to single-point-to-multi-instance. A
prompt-conditioned instance-query generator transforms a single point prompt
into multiple specialized queries, enabling retrieval of all semantically
similar lesions across the 3D volume from a single exemplar. A hybrid
CNN-Transformer encoder injects CNN-derived boundary saliency into ViT
self-attention via spatial gating. A competitively optimized query decoder then
enables end-to-end, parallel, multi-instance prediction through inter-query
competition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels
and exhibits strong robustness to prompts, providing a practical solution for
efficient annotation of clinically relevant multi-lesion cases.

</details>


### [166] [Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion](https://arxiv.org/abs/2511.01355)
*Linhao Huang*

Main category: cs.CV

TL;DR: 提出了一种通过内容-风格子空间混合和平衡损失来扩展内容-风格边界的方法，在保持风格强度的同时改善内容相似性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在单一风格强度下评估内容相似性，但增加风格强度会导致内容特征显著丢失，形成次优的内容-风格边界。

Method: 使用内容-风格子空间混合和内容-风格平衡损失来扩展内容-风格边界。

Result: 在定性和定量评估中均优于现有技术，实现了更优的内容-风格权衡，IGD和GD分数显著降低。

Conclusion: 该方法有效扩展了内容-风格边界，在保持风格强度的同时显著改善了内容相似性。

Abstract: Recent advancements in text-to-image diffusion models have significantly
improved the personalization and stylization of generated images. However,
previous studies have only assessed content similarity under a single style
intensity. In our experiments, we observe that increasing style intensity leads
to a significant loss of content features, resulting in a suboptimal
content-style frontier. To address this, we propose a novel approach to expand
the content-style frontier by leveraging Content-Style Subspace Blending and a
Content-Style Balance loss. Our method improves content similarity across
varying style intensities, significantly broadening the content-style frontier.
Extensive experiments demonstrate that our approach outperforms existing
techniques in both qualitative and quantitative evaluations, achieving superior
content-style trade-off with significantly lower Inverted Generational Distance
(IGD) and Generational Distance (GD) scores compared to current methods.

</details>


### [167] [EREBUS: End-to-end Robust Event Based Underwater Simulation](https://arxiv.org/abs/2511.01381)
*Hitesh Kyatham,Arjun Suresh,Aadi Palnitkar,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 提出了一种用于生成水下环境中安装在AUV上的事件相机合成数据的管道，以训练视觉模型，特别是在能见度差和有悬浮颗粒物的条件下进行岩石检测。


<details>
  <summary>Details</summary>
Motivation: 水下环境存在光照条件差和高动态范围场景等挑战，传统视觉技术难以适应。事件相机通过逐帧跟踪变化来缓解这些问题，但缺乏真实的水下事件数据。

Method: 开发了一个管道来生成安装在自主水下航行器上的事件相机的真实合成数据，模拟水下环境中的能见度差和悬浮颗粒物条件。

Result: 该管道在岩石检测任务中表现出有效性，特别是在恶劣的能见度条件下。

Conclusion: 该方法可以推广到其他水下任务，为水下事件相机视觉模型训练提供了有效的合成数据生成解决方案。

Abstract: The underwater domain presents a vast array of challenges for roboticists and
computer vision researchers alike, such as poor lighting conditions and high
dynamic range scenes. In these adverse conditions, traditional vision
techniques struggle to adapt and lead to suboptimal performance. Event-based
cameras present an attractive solution to this problem, mitigating the issues
of traditional cameras by tracking changes in the footage on a frame-by-frame
basis. In this paper, we introduce a pipeline which can be used to generate
realistic synthetic data of an event-based camera mounted to an AUV (Autonomous
Underwater Vehicle) in an underwater environment for training vision models. We
demonstrate the effectiveness of our pipeline using the task of rock detection
with poor visibility and suspended particulate matter, but the approach can be
generalized to other underwater tasks.

</details>


### [168] [Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction](https://arxiv.org/abs/2511.01399)
*Ya Wen,Yutong Qiao,Chi Chiu Lam,Ioannis Brilakis,Sanghoon Lee,Mun On Wong*

Main category: cs.CV

TL;DR: 该研究提出了Fire-ART数据集和全景图像重建方法，用于消防资产识别和BIM模型语义增强，解决了传统方法在自动资产识别和重建方面的效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统消防资产管理方法在自动资产识别和重建方面能力有限，导致效率低下。需要开发更有效的技术来提升应急准备、风险评估和现场火灾响应的能力。

Method: 开发了包含15种基础资产、2626张图像和6627个实例的Fire-ART数据集，并提出了结合改进的立方体贴图转换和基于半径的球面相机投影的重建方法。

Result: 在两个真实案例验证中，该方法分别达到了73%和88%的F1分数，定位误差分别为0.620米和0.428米。

Conclusion: Fire-ART数据集和重建方法为消防设备精确数字化管理提供了宝贵资源和强大技术解决方案。

Abstract: Inventory management of firefighting assets is crucial for emergency
preparedness, risk assessment, and on-site fire response. However, conventional
methods are inefficient due to limited capabilities in automated asset
recognition and reconstruction. To address the challenge, this research
introduces the Fire-ART dataset and develops a panoramic image-based
reconstruction approach for semantic enrichment of firefighting assets into BIM
models. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626
images and 6,627 instances, making it an extensive and publicly accessible
dataset for asset recognition. In addition, the reconstruction approach
integrates modified cube-map conversion and radius-based spherical camera
projection to enhance recognition and localization accuracy. Through
validations with two real-world case studies, the proposed approach achieves
F1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters,
respectively. The Fire-ART dataset and the reconstruction approach offer
valuable resources and robust technical solutions to enhance the accurate
digital management of fire safety equipment.

</details>


### [169] [Extremal Contours: Gradient-driven contours for compact visual attribution](https://arxiv.org/abs/2511.01411)
*Reza Karimzadeh,Albert Alonso,Frans Zdyb,Julius B. Kirkegaard,Bulat Ibragimov*

Main category: cs.CV

TL;DR: 提出了一种无需训练的解释方法，用平滑可调轮廓替代密集掩码，通过星凸区域参数化和极值保留/删除目标优化，生成紧凑、可解释的单一连通掩码。


<details>
  <summary>Details</summary>
Motivation: 现有密集扰动掩码通常碎片化且过拟合，需要复杂的后处理，需要更忠实且紧凑的视觉模型解释方法。

Method: 使用截断傅里叶级数参数化星凸区域，在分类器梯度下通过极值保留/删除目标进行优化，生成平滑轮廓掩码。

Result: 在ImageNet分类器上匹配密集掩码的极值保真度，产生紧凑可解释区域，运行一致性更好，在多轮廓扩展中能定位多个对象。

Conclusion: 该方法在基准测试中比梯度和基于扰动的基线获得更高的相关质量和更低复杂度，特别是在自监督DINO模型上相关质量提升超过15%。

Abstract: Faithful yet compact explanations for vision models remain a challenge, as
commonly used dense perturbation masks are often fragmented and overfitted,
needing careful post-processing. Here, we present a training-free explanation
method that replaces dense masks with smooth tunable contours. A star-convex
region is parameterized by a truncated Fourier series and optimized under an
extremal preserve/delete objective using the classifier gradients. The approach
guarantees a single, simply connected mask, cuts the number of free parameters
by orders of magnitude, and yields stable boundary updates without cleanup.
Restricting solutions to low-dimensional, smooth contours makes the method
robust to adversarial masking artifacts. On ImageNet classifiers, it matches
the extremal fidelity of dense masks while producing compact, interpretable
regions with improved run-to-run consistency. Explicit area control also
enables importance contour maps, yielding a transparent fidelity-area profiles.
Finally, we extend the approach to multi-contour and show how it can localize
multiple objects within the same framework. Across benchmarks, the method
achieves higher relevance mass and lower complexity than gradient and
perturbation based baselines, with especially strong gains on self-supervised
DINO models where it improves relevance mass by over 15% and maintains positive
faithfulness correlations.

</details>


### [170] [Towards One-step Causal Video Generation via Adversarial Self-Distillation](https://arxiv.org/abs/2511.01419)
*Yongqi Yang,Huayang Huang,Xu Peng,Xiaobin Hu,Donghao Luo,Jiangning Zhang,Chengjie Wang,Yu Wu*

Main category: cs.CV

TL;DR: 提出了一种基于蒸馏的高效因果视频生成框架，通过对抗性自蒸馏策略和首帧增强技术，在极少数去噪步骤下实现高质量视频合成。


<details>
  <summary>Details</summary>
Motivation: 现有的混合视频生成模型结合了自回归时间动态和基于扩散的空间去噪，但其顺序迭代特性导致误差累积和长推理时间。

Method: 基于分布匹配蒸馏框架，提出对抗性自蒸馏策略，将学生模型的n步去噪输出与其(n+1)步版本在分布级别对齐；同时采用首帧增强策略，为首帧分配更多去噪步骤以减少误差传播。

Result: 在VBench上的广泛实验表明，该方法在一步和两步视频生成中均超越最先进方法，且单个蒸馏模型可灵活支持多种推理步骤设置。

Conclusion: 该框架无需重复再蒸馏即可实现高效高质量的视频合成，显著提高了极少数步骤场景下的训练稳定性和生成质量。

Abstract: Recent hybrid video generation models combine autoregressive temporal
dynamics with diffusion-based spatial denoising, but their sequential,
iterative nature leads to error accumulation and long inference times. In this
work, we propose a distillation-based framework for efficient causal video
generation that enables high-quality synthesis with extremely limited denoising
steps. Our approach builds upon the Distribution Matching Distillation (DMD)
framework and proposes a novel Adversarial Self-Distillation (ASD) strategy,
which aligns the outputs of the student model's n-step denoising process with
its (n+1)-step version at the distribution level. This design provides smoother
supervision by bridging small intra-student gaps and more informative guidance
by combining teacher knowledge with locally consistent student behavior,
substantially improving training stability and generation quality in extremely
few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame
Enhancement (FFE) strategy, which allocates more denoising steps to the initial
frames to mitigate error propagation while applying larger skipping steps to
later frames. Extensive experiments on VBench demonstrate that our method
surpasses state-of-the-art approaches in both one-step and two-step video
generation. Notably, our framework produces a single distilled model that
flexibly supports multiple inference-step settings, eliminating the need for
repeated re-distillation and enabling efficient, high-quality video synthesis.

</details>


### [171] [Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation](https://arxiv.org/abs/2511.01434)
*Seongkyu Choi,Jhonghyun An*

Main category: cs.CV

TL;DR: 提出了一种分辨率感知的token解码器，用于解决越野语义分割中的边界模糊、稀疏监督和标签噪声问题，通过平衡全局语义、局部一致性和边界保真度来提升性能。


<details>
  <summary>Details</summary>
Motivation: 越野语义分割面临边界不一致、稀有类别稀疏监督和普遍标签噪声的挑战。现有方法在低分辨率融合时会模糊边缘并传播局部错误，而保持高分辨率路径或重复高分辨率融合则成本高且对噪声敏感。

Method: 设计分辨率感知token解码器：大部分计算在低分辨率瓶颈进行；门控交叉注意力注入精细尺度细节；仅对稀疏、不确定性选择的像素集进行细化。组件协同设计：全局自注意力与轻量级扩张深度细化恢复局部一致性；门控交叉注意力集成高分辨率编码器特征而不放大噪声；类别感知点细化以可忽略开销修正残余模糊性。训练时添加边界带一致性正则化器。

Result: 结果表明具有竞争力的性能和跨过渡的改进稳定性。

Conclusion: 该方法在越野语义分割中有效平衡了全局语义、局部一致性和边界保真度，在保持计算效率的同时提升了分割质量。

Abstract: Off-road semantic segmentation suffers from thick, inconsistent boundaries,
sparse supervision for rare classes, and pervasive label noise. Designs that
fuse only at low resolution blur edges and propagate local errors, whereas
maintaining high-resolution pathways or repeating high-resolution fusions is
costly and fragile to noise. We introduce a resolutionaware token decoder that
balances global semantics, local consistency, and boundary fidelity under
imperfect supervision. Most computation occurs at a low-resolution bottleneck;
a gated cross-attention injects fine-scale detail, and only a sparse,
uncertainty-selected set of pixels is refined. The components are co-designed
and tightly integrated: global self-attention with lightweight dilated
depthwise refinement restores local coherence; a gated cross-attention
integrates fine-scale features from a standard high-resolution encoder stream
without amplifying noise; and a class-aware point refinement corrects residual
ambiguities with negligible overhead. During training, we add a boundary-band
consistency regularizer that encourages coherent predictions in a thin
neighborhood around annotated edges, with no inference-time cost. Overall, the
results indicate competitive performance and improved stability across
transitions.

</details>


### [172] [Contrast-Guided Cross-Modal Distillation for Thermal Object Detection](https://arxiv.org/abs/2511.01435)
*SiWoo Kim,JhongHyun An*

Main category: cs.CV

TL;DR: 提出一种仅用于训练的方法，通过增强热红外图像的特征表示来解决夜间检测中的重复检测、小目标漏检和类别混淆问题，无需在测试时使用RGB传感器。


<details>
  <summary>Details</summary>
Motivation: 热红外检测在夜间面临低对比度和弱高频线索的问题，导致重复检测框、小目标漏检和类别混淆。现有方法要么将TIR转换为RGB（易受伪影影响），要么在测试时融合RGB和TIR（需要额外传感器和校准），都无法直接优化热红外表示。

Method: 在训练阶段引入两个目标：1）通过拉近同类特征、推远异类特征来锐化实例级决策边界；2）通过将学生的多级金字塔特征与RGB训练的教师模型对齐，注入跨模态语义先验，增强纹理贫乏的热红外特征。

Result: 在实验中，该方法优于先前方法，达到了最先进的性能。

Conclusion: 提出的训练阶段方法能有效提升热红外检测性能，无需在推理时使用多模态输入，解决了夜间感知的核心挑战。

Abstract: Robust perception at night remains challenging for thermal-infrared
detection: low contrast and weak high-frequency cues lead to duplicate,
overlapping boxes, missed small objects, and class confusion. Prior remedies
either translate TIR to RGB and hope pixel fidelity transfers to detection --
making performance fragile to color or structure artifacts -- or fuse RGB and
TIR at test time, which requires extra sensors, precise calibration, and higher
runtime cost. Both lines can help in favorable conditions, but do not directly
shape the thermal representation used by the detector. We keep mono-modality
inference and tackle the root causes during training. Specifically, we
introduce training-only objectives that sharpen instance-level decision
boundaries by pulling together features of the same class and pushing apart
those of different classes -- suppressing duplicate and confusing detections --
and that inject cross-modal semantic priors by aligning the student's
multi-level pyramid features with an RGB-trained teacher, thereby strengthening
texture-poor thermal features without visible input at test time. In
experiments, our method outperformed prior approaches and achieved
state-of-the-art performance.

</details>


### [173] [SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks](https://arxiv.org/abs/2511.01466)
*Changyuan Zhao,Jiacheng Wang,Ruichen Zhang,Dusit Niyato,Hongyang Du,Zehui Xiong,Dong In Kim,Ping Zhang*

Main category: cs.CV

TL;DR: SecDiff是一个基于扩散模型的即插即用解码框架，通过伪逆引导采样和自适应指导权重，显著提升了深度联合源信道编码在对抗性无线环境下的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的JSCC框架容易受到物理层对抗威胁（如导频欺骗和子载波干扰），影响语义保真度，需要开发更安全鲁棒的解决方案。

Method: 采用伪逆引导采样和自适应指导权重实现灵活步长控制；针对干扰攻击使用基于功率的子载波掩码策略；针对导频欺骗开发EM驱动的重建算法，在扩散过程中交替进行导频恢复和信道估计。

Result: 在对抗性OFDM信道条件下的广泛实验表明，SecDiff在重建质量和计算成本之间取得了良好平衡，优于现有的安全和生成式JSCC基线方法。

Conclusion: SecDiff是实现实用、低延迟、抗攻击语义通信的有前景的一步，为深度JSCC在对抗性环境中的应用提供了有效解决方案。

Abstract: Deep joint source-channel coding (JSCC) has emerged as a promising paradigm
for semantic communication, delivering significant performance gains over
conventional separate coding schemes. However, existing JSCC frameworks remain
vulnerable to physical-layer adversarial threats, such as pilot spoofing and
subcarrier jamming, compromising semantic fidelity. In this paper, we propose
SecDiff, a plug-and-play, diffusion-aided decoding framework that significantly
enhances the security and robustness of deep JSCC under adversarial wireless
environments. Different from prior diffusion-guided JSCC methods that suffer
from high inference latency, SecDiff employs pseudoinverse-guided sampling and
adaptive guidance weighting, enabling flexible step-size control and efficient
semantic reconstruction. To counter jamming attacks, we introduce a power-based
subcarrier masking strategy and recast recovery as a masked inpainting problem,
solved via diffusion guidance. For pilot spoofing, we formulate channel
estimation as a blind inverse problem and develop an expectation-minimization
(EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and
a channel operator. Notably, our method alternates between pilot recovery and
channel estimation, enabling joint refinement of both variables throughout the
diffusion process. Extensive experiments over orthogonal frequency-division
multiplexing (OFDM) channels under adversarial conditions show that SecDiff
outperforms existing secure and generative JSCC baselines by achieving a
favorable trade-off between reconstruction quality and computational cost. This
balance makes SecDiff a promising step toward practical, low-latency, and
attack-resilient semantic communications.

</details>


### [174] [EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance](https://arxiv.org/abs/2511.01498)
*Zhiyang Jia,Hongyan Cui,Ge Gao,Bo Li,Minjie Zhang,Zishuo Gao,Huiwen Huang,Caisheng Zhuo*

Main category: cs.CV

TL;DR: 提出了增强行人对齐网络（EPAN），用于物联网监控环境中的人员重识别，在Inspection-Personnel数据集上达到90.09%的Rank-1准确率和78.82%的mAP。


<details>
  <summary>Details</summary>
Motivation: 解决物联网智能环境中监控和安全应用的人员重识别问题，特别是在不同视角和环境变化条件下的鲁棒性需求。

Method: 采用双分支架构来减轻视角和环境变化的影响，提取不同尺度和视角下的对齐信息。

Result: 在Inspection-Personnel数据集上取得了优秀的性能表现：Rank-1准确率90.09%，mAP 78.82%。

Conclusion: EPAN展现了强大的特征提取能力，在真实世界物联网应用中具有潜力，能够在监控系统中实现跨摄像头的有效可靠人员重识别。

Abstract: Person re-identification (ReID) plays a pivotal role in computer vision,
particularly in surveillance and security applications within IoT-enabled smart
environments. This study introduces the Enhanced Pedestrian Alignment Network
(EPAN), tailored for robust ReID across diverse IoT surveillance conditions.
EPAN employs a dual-branch architecture to mitigate the impact of perspective
and environmental changes, extracting alignment information under varying
scales and viewpoints. Here, we demonstrate EPAN's strong feature extraction
capabilities, achieving outstanding performance on the Inspection-Personnel
dataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of
78.82%. This highlights EPAN's potential for real-world IoT applications,
enabling effective and reliable person ReID across diverse cameras in
surveillance and security systems. The code and data are available at:
https://github.com/ggboy2580/EPAN

</details>


### [175] [SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation](https://arxiv.org/abs/2511.01501)
*Yufeng Jin,Niklas Funk,Vignesh Prasad,Zechu Li,Mathias Franzius,Jan Peters,Georgia Chalvatzaki*

Main category: cs.CV

TL;DR: 提出一种基于SE(3)流匹配的概率框架，用于估计6D物体姿态分布，解决姿态模糊性问题


<details>
  <summary>Details</summary>
Motivation: 解决物体姿态估计中的部分可观测性、遮挡和物体对称性导致的姿态模糊问题，现有确定性方法无法捕捉姿态分布的多模态特性

Method: 在SE(3)流形上使用流匹配技术，建模完整的姿态分布，提供基于样本的姿态估计

Result: 在Real275、YCB-V和LM-O数据集上达到最先进水平，并成功应用于机器人操作任务中的主动感知和不确定性感知抓取

Conclusion: 提出的概率框架能有效处理姿态模糊情况，为下游机器人任务提供更好的不确定性建模

Abstract: Object pose estimation is a fundamental problem in robotics and computer
vision, yet it remains challenging due to partial observability, occlusions,
and object symmetries, which inevitably lead to pose ambiguity and multiple
hypotheses consistent with the same observation. While deterministic deep
networks achieve impressive performance under well-constrained conditions, they
are often overconfident and fail to capture the multi-modality of the
underlying pose distribution. To address these challenges, we propose a novel
probabilistic framework that leverages flow matching on the SE(3) manifold for
estimating 6D object pose distributions. Unlike existing methods that regress a
single deterministic output, our approach models the full pose distribution
with a sample-based estimate and enables reasoning about uncertainty in
ambiguous cases such as symmetric objects or severe occlusions. We achieve
state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our
sample-based pose estimates can be leveraged in downstream robotic manipulation
tasks such as active perception for disambiguating uncertain viewpoints or
guiding grasp synthesis in an uncertainty-aware manner.

</details>


### [176] [Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning](https://arxiv.org/abs/2511.01502)
*Mengtan Zhang,Zizhan Guo,Hongbo Zhao,Yi Feng,Zuyi Xiong,Yue Wang,Shaoyi Du,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: 提出了一种区分处理运动组件的方法，利用各自刚性流的几何规律来改进深度和自运动估计，通过几何约束实现更精确的联合学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法将自运动作为辅助任务，要么混合所有运动类型，要么排除与深度无关的旋转运动，限制了强几何约束的引入，降低了在不同条件下的可靠性和鲁棒性。

Method: DiMoDE框架：首先对齐源相机和目标相机的光轴和成像平面，通过变换光流并量化偏差来分别对每个自运动组件施加几何约束，将联合学习过程重新表述为同轴和共面形式。

Result: 在多个公共数据集和新收集的多样化真实世界数据集上实现了最先进的性能，特别是在具有挑战性的条件下表现优异。

Conclusion: 提出的区分运动组件处理方法能够引入互补约束，提高深度估计的鲁棒性，为深度和自运动联合学习提供了有效的几何约束框架。

Abstract: Unsupervised learning of depth and ego-motion, two fundamental 3D perception
tasks, has made significant strides in recent years. However, most methods
treat ego-motion as an auxiliary task, either mixing all motion types or
excluding depth-independent rotational motions in supervision. Such designs
limit the incorporation of strong geometric constraints, reducing reliability
and robustness under diverse conditions. This study introduces a discriminative
treatment of motion components, leveraging the geometric regularities of their
respective rigid flows to benefit both depth and ego-motion estimation. Given
consecutive video frames, network outputs first align the optical axes and
imaging planes of the source and target cameras. Optical flows between frames
are transformed through these alignments, and deviations are quantified to
impose geometric constraints individually on each ego-motion component,
enabling more targeted refinement. These alignments further reformulate the
joint learning process into coaxial and coplanar forms, where depth and each
translation component can be mutually derived through closed-form geometric
relationships, introducing complementary constraints that improve depth
robustness. DiMoDE, a general depth and ego-motion joint learning framework
incorporating these designs, achieves state-of-the-art performance on multiple
public datasets and a newly collected diverse real-world dataset, particularly
under challenging conditions. Our source code will be publicly available at
mias.group/DiMoDE upon publication.

</details>


### [177] [Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement](https://arxiv.org/abs/2511.01510)
*Derong Kong,Zhixiong Yang,Shengxi Li,Shuaifeng Zhi,Li Liu,Zhen Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: 提出了Luminance-Aware Statistical Quantification (LASQ)框架，将低光图像增强重新定义为基于分层亮度分布的统计采样过程，通过扩散前向过程自主发现亮度层间最优转换路径，实现无监督分布模拟。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法主要关注确定性像素级映射，忽略了真实环境中亮度转换的连续物理过程，导致在缺乏正常光参考时性能下降。

Method: 将亮度转换建模为强度坐标空间中的幂律分布，用分层幂函数近似，用概率采样替代确定性映射，设计扩散前向过程自主发现亮度层间最优转换路径。

Result: 显著提高了实际场景中的性能，在有正常光参考的情况下在领域特定数据集上表现优异，同时在无参考数据集上具有更好的泛化能力。

Conclusion: LASQ框架通过统计量化方法重新定义低光图像增强问题，实现了更适应性和多功能的亮度恢复，在有无参考的情况下均表现出色。

Abstract: Low-light image enhancement (LLIE) faces persistent challenges in balancing
reconstruction fidelity with cross-scenario generalization. While existing
methods predominantly focus on deterministic pixel-level mappings between
paired low/normal-light images, they often neglect the continuous physical
process of luminance transitions in real-world environments, leading to
performance drop when normal-light references are unavailable. Inspired by
empirical analysis of natural luminance dynamics revealing power-law
distributed intensity transitions, this paper introduces Luminance-Aware
Statistical Quantification (LASQ), a novel framework that reformulates LLIE as
a statistical sampling process over hierarchical luminance distributions. Our
LASQ re-conceptualizes luminance transition as a power-law distribution in
intensity coordinate space that can be approximated by stratified power
functions, therefore, replacing deterministic mappings with probabilistic
sampling over continuous luminance layers. A diffusion forward process is
designed to autonomously discover optimal transition paths between luminance
layers, achieving unsupervised distribution emulation without normal-light
references. In this way, it considerably improves the performance in practical
situations, enabling more adaptable and versatile light restoration. This
framework is also readily applicable to cases with normal-light references,
where it achieves superior performance on domain-specific datasets alongside
better generalization-ability across non-reference datasets.

</details>


### [178] [Example-Based Feature Painting on Textures](https://arxiv.org/abs/2511.01513)
*Andrei-Timotei Ardelean,Tim Weyrich*

Main category: cs.CV

TL;DR: 提出一个完整的纹理控制创作和编辑系统，能够生成具有局部特征（如污渍、撕裂、孔洞等）的逼真纹理，采用无监督异常检测和条件生成方法。


<details>
  <summary>Details</summary>
Motivation: 自然界中材料表面普遍存在各种瑕疵和变化特征，将这些特征纳入纹理合成过程对于生成逼真纹理至关重要。

Method: 采用基于学习的方法，利用无标签样本进行无监督异常检测，自动将纹理特征聚类为语义连贯的组别，并用于指导条件图像生成。

Result: 开发了一个从少量图像集合到多功能生成模型的完整流程，支持用户交互式创建和绘制任意尺寸纹理上的特征。

Conclusion: 提出的基于扩散的编辑和无限静态纹理生成算法具有通用性，在其他场景中也应具有应用价值。

Abstract: In this work, we propose a system that covers the complete workflow for
achieving controlled authoring and editing of textures that present distinctive
local characteristics. These include various effects that change the surface
appearance of materials, such as stains, tears, holes, abrasions,
discoloration, and more. Such alterations are ubiquitous in nature, and
including them in the synthesis process is crucial for generating realistic
textures. We introduce a novel approach for creating textures with such
blemishes, adopting a learning-based approach that leverages unlabeled
examples. Our approach does not require manual annotations by the user;
instead, it detects the appearance-altering features through unsupervised
anomaly detection. The various textural features are then automatically
clustered into semantically coherent groups, which are used to guide the
conditional generation of images. Our pipeline as a whole goes from a small
image collection to a versatile generative model that enables the user to
interactively create and paint features on textures of arbitrary size. Notably,
the algorithms we introduce for diffusion-based editing and infinite stationary
texture generation are generic and should prove useful in other contexts as
well. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html

</details>


### [179] [NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation](https://arxiv.org/abs/2511.01517)
*Serkan Ozturk,Samet Hicsonmez,Pinar Duygulu*

Main category: cs.CV

TL;DR: 提出了一种基于对比学习的新框架NSYNC，通过生成负样本合成图像来提升大型文本到图像扩散模型的风格化能力，使用正交梯度更新方法消除正负样本中的共同特征，使模型学习更独特的风格特征。


<details>
  <summary>Details</summary>
Motivation: 当前文本条件图像生成方法虽然能生成逼真图像，但难以捕捉特定风格特征。直接在目标风格数据集上微调仍无法有效掌握风格特征，需要新的训练方法来提升风格化能力。

Method: 使用对比学习框架，生成负样本合成图像集，与真实正样本图像一起训练。通过计算正梯度减去其在负梯度上的投影得到正交分量，基于此更新参数，消除正负样本中的共同平凡特征。

Result: 在多种画家和插画师风格上的实验表明，该方法在定量和定性评估上都优于基线方法，显著提升了风格化性能。

Conclusion: NSYNC框架通过负样本合成和正交梯度更新，有效提升了文本到图像扩散模型的风格化能力，能够更好地捕捉独特的风格特征。

Abstract: Current text conditioned image generation methods output realistic looking
images, but they fail to capture specific styles. Simply finetuning them on the
target style datasets still struggles to grasp the style features. In this
work, we present a novel contrastive learning framework to improve the
stylization capability of large text-to-image diffusion models. Motivated by
the astonishing advance in image generation models that makes synthetic data an
intrinsic part of model training in various computer vision tasks, we exploit
synthetic image generation in our approach. Usually, the generated synthetic
data is dependent on the task, and most of the time it is used to enlarge the
available real training dataset. With NSYNC, alternatively, we focus on
generating negative synthetic sets to be used in a novel contrastive training
scheme along with real positive images. In our proposed training setup, we
forward negative data along with positive data and obtain negative and positive
gradients, respectively. We then refine the positive gradient by subtracting
its projection onto the negative gradient to get the orthogonal component,
based on which the parameters are updated. This orthogonal component eliminates
the trivial attributes that are present in both positive and negative data and
directs the model towards capturing a more unique style. Experiments on various
styles of painters and illustrators show that our approach improves the
performance over the baseline methods both quantitatively and qualitatively.
Our code is available at https://github.com/giddyyupp/NSYNC.

</details>


### [180] [PCD-ReID: Occluded Person Re-Identification for Base Station Inspection](https://arxiv.org/abs/2511.01546)
*Ge Gao,Zishuo Gao,Hongyan Cui,Zhiyang Jia,Zhuang Luo,ChaoPeng Liu*

Main category: cs.CV

TL;DR: 提出PCD-ReID算法解决基站环境中遮挡行人重识别问题，使用Transformer架构提取共享组件特征，在真实巡逻监控数据集上训练，相比ResNet50方法Rank-1准确率提升15.9%。


<details>
  <summary>Details</summary>
Motivation: 基站环境中的遮挡行人重识别是计算机视觉关键任务，传统ResNet算法难以有效处理遮挡问题，需要新的重识别方法。

Method: 设计基于Transformer的PCD网络提取共享组件特征（如头盔、制服），收集真实巡逻监控图像数据集进行训练（6个月、1万人、5万张图像）。

Result: 模型达到79.0%的mAP和82.7%的Rank-1准确率，相比基于ResNet50的方法Rank-1准确率提升15.9%。

Conclusion: PCD-ReID在塔检场景中有效实现遮挡感知的行人重识别性能，在监控安防应用中具有实际部署潜力。

Abstract: Occluded pedestrian re-identification (ReID) in base station environments is
a critical task in computer vision, particularly for surveillance and security
applications. This task faces numerous challenges, as occlusions often obscure
key body features, increasing the complexity of identification. Traditional
ResNet-based ReID algorithms often fail to address occlusions effectively,
necessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component
Discrepancy) algorithm to address these issues. The contributions of this work
are as follows: To tackle the occlusion problem, we design a Transformer-based
PCD network capable of extracting shared component features, such as helmets
and uniforms. To mitigate overfitting on public datasets, we collected new
real-world patrol surveillance images for model training, covering six months,
10,000 individuals, and over 50,000 images. Comparative experiments with
existing ReID algorithms demonstrate that our model achieves a mean Average
Precision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1
improvement over ResNet50-based methods. Experimental evaluations indicate that
PCD-ReID effectively achieves occlusion-aware ReID performance for personnel in
tower inspection scenarios, highlighting its potential for practical deployment
in surveillance and security applications.

</details>


### [181] [NOA: a versatile, extensible tool for AI-based organoid analysis](https://arxiv.org/abs/2511.01549)
*Mikhail Konov,Lion J. Gleiter,Khoa Co,Monica Yabal,Tingying Peng*

Main category: cs.CV

TL;DR: 开发了Napari Organoid Analyzer (NOA)，一个图形用户界面工具，用于简化基于AI的类器官分析，集成检测、分割、跟踪、特征提取等功能。


<details>
  <summary>Details</summary>
Motivation: AI工具能增强类器官显微镜图像分析，但缺乏编程经验的生物学家难以使用，导致工作流程劳动密集且手动化。现有工具大多局限于特定任务。

Method: NOA作为开源napari插件实现，集成多个先进算法模块，包括检测、分割、跟踪、特征提取、自定义特征标注和基于机器学习的特征预测。

Result: 通过三个案例研究展示了NOA的多功能性：量化类器官分化过程中的形态变化、评估光毒性效应、预测类器官活力和分化状态。

Conclusion: NOA在可访问且可扩展的框架内实现了全面的AI驱动类器官图像分析。

Abstract: AI tools can greatly enhance the analysis of organoid microscopy images, from
detection and segmentation to feature extraction and classification. However,
their limited accessibility to biologists without programming experience
remains a major barrier, resulting in labor-intensive and largely manual
workflows. Although a few AI models for organoid analysis have been developed,
most existing tools remain narrowly focused on specific tasks. In this work, we
introduce the Napari Organoid Analyzer (NOA), a general purpose graphical user
interface to simplify AI-based organoid analysis. NOA integrates modules for
detection, segmentation, tracking, feature extraction, custom feature
annotation and ML-based feature prediction. It interfaces multiple
state-of-the-art algorithms and is implemented as an open-source napari plugin
for maximal flexibility and extensibility. We demonstrate the versatility of
NOA through three case studies, involving the quantification of morphological
changes during organoid differentiation, assessment of phototoxicity effects,
and prediction of organoid viability and differentiation state. Together, these
examples illustrate how NOA enables comprehensive, AI-driven organoid image
analysis within an accessible and extensible framework.

</details>


### [182] [PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model](https://arxiv.org/abs/2511.01571)
*Wenqi Liang,Gan Sun,Yao He,Jiahua Dong,Suyan Dai,Ivan Laptev,Salman Khan,Yang Cong*

Main category: cs.CV

TL;DR: 提出了PixelVLA模型，这是首个支持像素级推理和多模态提示的VLA模型，通过两阶段自动标注流程生成Pixel-160K数据集，在多个基准测试中显著提升操作成功率，同时大幅降低预训练成本。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型存在两个主要限制：(i) 在像素级场景理解方面表现不佳，(ii) 过度依赖文本提示，在现实环境中灵活性不足。

Method: 基于新的视觉运动指令调优框架，整合了多尺度像素感知编码器和视觉提示编码器，采用两阶段自动标注流程生成Pixel-160K大规模像素级标注数据集。

Result: 在三个标准VLA基准测试和两个VLA模型变体上，PixelVLA相比OpenVLA将操作成功率提高了10.1%-17.8%，同时仅需其1.5%的预训练成本。

Conclusion: PixelVLA可以集成到现有VLA中，在复杂环境中实现更准确、高效和通用的机器人控制。

Abstract: Vision-Language-Action models (VLAs) are emerging as powerful tools for
learning generalizable visuomotor control policies. However, current VLAs are
mostly trained on large-scale image-text-action data and remain limited in two
key ways: (i) they struggle with pixel-level scene understanding, and (ii) they
rely heavily on textual prompts, which reduces their flexibility in real-world
settings. To address these challenges, we introduce PixelVLA, the first VLA
model designed to support both pixel-level reasoning and multimodal prompting
with text and visual inputs. Our approach is built on a new visuomotor
instruction tuning framework that integrates a multiscale pixel-aware encoder
with a visual prompting encoder. To train PixelVLA effectively, we further
propose a two-stage automated annotation pipeline that generates Pixel-160K, a
large-scale dataset with pixel-level annotations derived from existing robot
data. Experiments on three standard VLA benchmarks and two VLA model variants
show that PixelVLA improves manipulation success rates by 10.1%-17.8% over
OpenVLA, while requiring only 1.5% of its pretraining cost. These results
demonstrate that PixelVLA can be integrated into existing VLAs to enable more
accurate, efficient, and versatile robot control in complex environments. The
dataset and code will be released as open source.

</details>


### [183] [Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images](https://arxiv.org/abs/2511.01574)
*Md Sumon Ali,Muzammil Behzad*

Main category: cs.CV

TL;DR: 提出基于DC-GAN的深度学习方法生成合成MRI数据，并使用CNN分类器验证合成图像质量，解决医学影像数据有限的问题。


<details>
  <summary>Details</summary>
Motivation: 原始MRI数据有限，生成逼真的医学图像具有挑战性，需要解决医学影像数据稀缺问题。

Method: 使用深度卷积生成对抗网络（DC-GAN）生成合成MRI数据，并采用卷积神经网络（CNN）分类器对真实和合成图像进行脑肿瘤分类。

Result: 在真实和合成图像上的分类结果表现出相当的性能，验证了GAN生成图像在下游任务中的有效性。

Conclusion: GAN生成的合成医学图像可用于解决数据稀缺问题，并在下游任务中表现出与真实数据相当的性能。

Abstract: Compared to traditional methods, Deep Learning (DL) becomes a key technology
for computer vision tasks. Synthetic data generation is an interesting use case
for DL, especially in the field of medical imaging such as Magnetic Resonance
Imaging (MRI). The need for this task since the original MRI data is limited.
The generation of realistic medical images is completely difficult and
challenging. Generative Adversarial Networks (GANs) are useful for creating
synthetic medical images. In this paper, we propose a DL based methodology for
creating synthetic MRI data using the Deep Convolutional Generative Adversarial
Network (DC-GAN) to address the problem of limited data. We also employ a
Convolutional Neural Network (CNN) classifier to classify the brain tumor using
synthetic data and real MRI data. CNN is used to evaluate the quality and
utility of the synthetic images. The classification result demonstrates
comparable performance on real and synthetic images, which validates the
effectiveness of GAN-generated images for downstream tasks.

</details>


### [184] [Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation](https://arxiv.org/abs/2511.01593)
*Yizhu Chen,Chen Ju,Zhicheng Wang,Shuai Xiao,Xu Chen,Jinsong Lan,Xiaoyong Zhu,Ying Chen*

Main category: cs.CV

TL;DR: 提出连续-离散双重视觉分词器(CDD-VT)，通过自适应分配图像基元数量来解决连续和离散分词器之间的对立问题。


<details>
  <summary>Details</summary>
Motivation: 统一多模态大模型中的理解和生成面临挑战，主要源于连续和离散视觉分词之间的二分法。连续分词器性能强但流程复杂，离散分词器概念优雅但信息损失严重。

Method: 受光的波粒二象性启发，将视觉数据视为量化码本导出的图像基元的灵活组合。核心组件包括：多样化量化基元（提高基元正交性）和动态基元分配器（根据样本复杂度自适应确定基元数量）。

Result: 在重建、检索和分类任务上的广泛实验表明，CDD-VT在简洁可扩展的MLLM中实现了优于专用连续和离散分词器的性能。

Conclusion: CDD-VT有效解决了连续和离散分词器之间的张力，在保持简洁性的同时获得了强大的结果。

Abstract: The unification of understanding and generation within a single multi-modal
large model (MLLM) remains one significant challenge, largely due to the
dichotomy between continuous and discrete visual tokenizations. Continuous
tokenizer (CT) achieves strong performance by bridging multiple
independently-trained understanding modules and generation modules, but suffers
from complex multi-stage pipelines and substantial engineering overhead.
Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by
quantizing each image into a primitive, but inevitably leading to information
loss and performance degradation. To resolve this tension, we question the
binary choice between CT and DT, inspired by the wave-particle duality of
light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT).
We treat visual data as a flexible composition of image primitives derived from
quantized codebooks, with the crucial insight that the primitive number
assigned to each visual sample is adaptively determined according to its
complexity: simple instances use a few primitives, emulating discrete
tokenization, while complex instances use many, approximating continuous
tokenization. Two core components are designed: Diverse Quantitative
Primitives, which encourage primitives orthogonality to better populate
information space, and Dynamic Primitive Allocator, which assesses sample
complexity to determine the optimal set of primitives. Extensive experiments on
reconstruction, retrieval and classification show that CDD-VT achieves superior
performance over to specialized CT and DT, effectively getting strong result
within a concise and scalable MLLM.

</details>


### [185] [Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography](https://arxiv.org/abs/2511.01600)
*Agnar Martin Bjørnstad,Elias Stenhede,Arian Ranjbar*

Main category: cs.CV

TL;DR: Lite ENSAM是一种轻量级肿瘤体积分割模型，专门用于从带有RECIST标注的CT扫描中高效分割肿瘤体积，在MICCAI FLARE 2025比赛中取得了良好性能。


<details>
  <summary>Details</summary>
Motivation: 当前肿瘤治疗评估主要依赖RECIST v1.1标准的手动测量最长直径，但体积测量更可靠。然而，手动体积标注耗时费力，限制了临床应用。

Method: 提出了Lite ENSAM，这是ENSAM架构的轻量级适配版本，专门设计用于从带有RECIST标注的CT扫描中高效进行肿瘤体积分割。

Result: 在MICCAI FLARE 2025任务1的子任务2中，在隐藏测试集上获得DSC 60.7%和NSD 63.6%，在公共验证集上平均总RAM时间50.6GB，CPU推理时间14.4秒。

Conclusion: Lite ENSAM证明了从RECIST标注中自动进行肿瘤体积分割的可行性，为临床采用更可靠的体积评估方法提供了技术基础。

Abstract: Accurate tumor size measurement is a cornerstone of evaluating cancer
treatment response. The most widely adopted standard for this purpose is the
Response Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on
measuring the longest tumor diameter in a single plane. However, volumetric
measurements have been shown to provide a more reliable assessment of treatment
effect. Their clinical adoption has been limited, though, due to the
labor-intensive nature of manual volumetric annotation. In this paper, we
present Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed
for efficient volumetric tumor segmentation from CT scans annotated with RECIST
annotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1:
Pan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice
Similarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of
63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an
average inference time of 14.4 s on CPU on the public validation dataset.

</details>


### [186] [Benchmark-Ready 3D Anatomical Shape Classification](https://arxiv.org/abs/2511.01613)
*Tomáš Krsička,Tibor Kubík*

Main category: cs.CV

TL;DR: 提出PSPooling（预计算结构池化）方法用于3D解剖形状分类，通过自监督图自编码器学习解剖感知表示，并在新基准数据集MedShapeNet19上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解剖3D形状分类受限于网格数据的复杂性和缺乏标准化基准，需要开发鲁棒的学习方法和可复现的评估体系。

Method: 提出PSPooling非可学习网格池化算子，基于几何邻近性预计算节点对应集，实现并行化和可逆的池化与反池化操作；构建自监督图自编码器学习解剖感知表示。

Result: 在MedShapeNet19数据集上，PSPooling显著提高了重建保真度和在低标签情况下的分类准确率，为医学3D形状学习建立了强基线。

Conclusion: PSPooling方法在解剖形状分类中表现优异，MedShapeNet19可作为医学3D形状分析的广泛采用基准。

Abstract: Progress in anatomical 3D shape classification is limited by the complexity
of mesh data and the lack of standardized benchmarks, highlighting the need for
robust learning methods and reproducible evaluation. We introduce two key steps
toward clinically and benchmark-ready anatomical shape classification via
self-supervised graph autoencoding. We propose Precomputed Structural Pooling
(PSPooling), a non-learnable mesh pooling operator designed for efficient and
structure-preserving graph coarsening in 3D anatomical shape analysis.
PSPooling precomputes node correspondence sets based on geometric proximity,
enabling parallelizable and reversible pooling and unpooling operations with
guaranteed support structure. This design avoids the sparsity and
reconstruction issues of selection-based methods and the sequential overhead of
edge contraction approaches, making it particularly suitable for
high-resolution medical meshes. To demonstrate its effectiveness, we integrate
PSPooling into a self-supervised graph autoencoder that learns anatomy-aware
representations from unlabeled surface meshes. We evaluate the downstream
benefits on MedShapeNet19, a new curated benchmark dataset we derive from
MedShapeNet, consisting of 19 anatomical classes with standardized training,
validation, and test splits. Experiments show that PSPooling significantly
improves reconstruction fidelity and classification accuracy in low-label
regimes, establishing a strong baseline for medical 3D shape learning. We hope
that MedShapeNet19 will serve as a widely adopted benchmark for anatomical
shape classification and further research in medical 3D shape analysis. Access
the complete codebase, model weights, and dataset information here:
https://github.com/TomasKrsicka/MedShapeNet19-PSPooling.

</details>


### [187] [Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers](https://arxiv.org/abs/2511.01617)
*Mohamed Eltahir,Ali Habibullah,Lama Ayash,Tanveer Hussain,Naeemullah Khan*

Main category: cs.CV

TL;DR: ViC是一个无需训练、基于视觉语言模型的通用框架，将列表重排序和融合重新定义为零样本推理任务，在跨模态视频检索中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决异构检索器候选融合的长期挑战，特别是在复杂多模态数据（如视频）中。传统融合方法仅依赖排名或分数信号，忽略了候选表示。

Method: 提出Vote-in-Context框架，在VLM提示中序列化内容证据和检索器元数据，使模型能够自适应地权衡检索器共识与视觉语言内容。引入S-Grid紧凑序列化映射来表示视频。

Result: 在MSR-VTT上零样本Recall@1达到87.1%(t2v)/89.0%(v2t)，在VATEX上v2t达到99.6%，比之前最优基线提升高达+40 Recall@1。

Conclusion: ViC是将现代VLM转变为强大零样本重排序器和融合器的简单、可复现且高效的方案。

Abstract: In the retrieval domain, candidates' fusion from heterogeneous retrievers is
a long-standing challenge, particularly for complex, multi-modal data such as
videos. While typical fusion techniques are training-free, they rely solely on
rank or score signals, disregarding candidates' representations. This work
introduces Vote-in-Context (ViC), a generalized, training-free framework that
re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a
Vision-Language Model (VLM). The core insight is to serialize both content
evidence and retriever metadata directly within the VLM's prompt, allowing the
model to adaptively weigh retriever consensus against visual-linguistic
content. We demonstrate the generality of this framework by applying it to the
challenging domain of cross-modal video retrieval. To this end, we introduce
the S-Grid, a compact serialization map that represents each video as an image
grid, optionally paired with subtitles to enable list-wise reasoning over video
candidates. ViC is evaluated both as a single-list reranker, where it
dramatically improves the precision of individual retrievers, and as an
ensemble fuser, where it consistently outperforms strong baselines like
CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the
framework establishes new state-of-the-art zero-shot retrieval performance,
demonstrating its effectiveness in handling complex visual and temporal signals
alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%
(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive
gains of up to +40 Recall@1 over previous state-of-the-art baselines. We
present ViC as a simple, reproducible, and highly effective recipe for turning
modern VLMs into powerful zero-shot rerankers and fusers. Code and resources
are publicly available at: https://github.com/mohammad2012191/ViC

</details>


### [188] [Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models](https://arxiv.org/abs/2511.01618)
*Xiaoyu Zhan,Wenxuan Huang,Hao Sun,Xinyu Fu,Changfeng Ma,Shaosheng Cao,Bohan Jia,Shaohui Lin,Zhenfei Yin,Lei Bai,Wanli Ouyang,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: 该论文提出了Viewpoint Learning任务来评估和改进多模态大语言模型的空间推理能力，通过两阶段微调策略显著提升了模型在3D推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在2D视觉理解方面取得进展，但其在复杂3D推理任务中的空间信息捕捉能力仍不明确，特别是跨视角一致性这一关键要求。

Method: 引入Viewpoint-100K数据集，采用两阶段微调策略：首先通过监督微调注入基础知识，然后使用GRPO算法进行强化学习以增强泛化能力，并提出混合冷启动初始化方法。

Result: 实验结果表明该方法显著激活了MLLM的空间推理能力，在领域内和领域外推理任务上均取得性能提升。

Conclusion: 开发MLLM的基础空间技能对于推动机器人、自主系统和3D场景理解的未来发展具有重要价值。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly improved 2D visual understanding, prompting interest in their
application to complex 3D reasoning tasks. However, it remains unclear whether
these models can effectively capture the detailed spatial information required
for robust real-world performance, especially cross-view consistency, a key
requirement for accurate 3D reasoning. Considering this issue, we introduce
Viewpoint Learning, a task designed to evaluate and improve the spatial
reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,
consisting of 100K object-centric image pairs with diverse viewpoints and
corresponding question-answer pairs. Our approach employs a two-stage
fine-tuning strategy: first, foundational knowledge is injected to the baseline
MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in
significant improvements across multiple tasks; second, generalization is
enhanced through Reinforcement Learning using the Group Relative Policy
Optimization (GRPO) algorithm on a broader set of questions. Additionally, we
introduce a hybrid cold-start initialization method designed to simultaneously
learn viewpoint representations and maintain coherent reasoning thinking.
Experimental results show that our approach significantly activates the spatial
reasoning ability of MLLM, improving performance on both in-domain and
out-of-domain reasoning tasks. Our findings highlight the value of developing
foundational spatial skills in MLLMs, supporting future progress in robotics,
autonomous systems, and 3D scene understanding.

</details>


### [189] [Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward](https://arxiv.org/abs/2511.01645)
*Xiaogang Xu,Ruihang Chu,Jian Wang,Kun Zhou,Wenjie Shu,Harry Yang,Ser-Nam Lim,Hao Chen,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出了一种将强化学习有效整合到基于扩散的图像修复模型中的新方法，通过IQA模型构建奖励函数，并针对困难样本进行动态优化。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法直接应用于扩散图像修复模型效果不佳，因为修复任务更强调保真度而非纯生成。需要探索如何有效整合RL到修复模型中。

Method: 使用IQA模型构建奖励函数，针对远离真实值的困难样本进行RL优化，并通过MLLM-based IQA模型对齐高质量图像分布，采用自适应权重策略结合SFT进行细粒度对齐。

Result: 该方法可即插即用地提升扩散修复模型性能，在多个基准测试中表现出有效性。

Conclusion: 提出的RL框架能有效提升扩散修复模型的性能，通过动态优化策略实现了更好的图像修复效果。

Abstract: Reinforcement Learning (RL) has recently been incorporated into diffusion
models, e.g., tasks such as text-to-image. However, directly applying existing
RL methods to diffusion-based image restoration models is suboptimal, as the
objective of restoration fundamentally differs from that of pure generation: it
places greater emphasis on fidelity. In this paper, we investigate how to
effectively integrate RL into diffusion-based restoration models. First,
through extensive experiments with various reward functions, we find that an
effective reward can be derived from an Image Quality Assessment (IQA) model,
instead of intuitive ground-truth-based supervision, which has already been
optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,
our strategy focuses on using RL for challenging samples that are significantly
distant from the ground truth, and our RL approach is innovatively implemented
using MLLM-based IQA models to align distributions with high-quality images
initially. As the samples approach the ground truth's distribution, RL is
adaptively combined with SFT for more fine-grained alignment. This dynamic
process is facilitated through an automatic weighting strategy that adjusts
based on the relative difficulty of the training samples. Our strategy is
plug-and-play that can be seamlessly applied to diffusion-based restoration
models, boosting its performance across various restoration tasks. Extensive
experiments across multiple benchmarks demonstrate the effectiveness of our
proposed RL framework.

</details>


### [190] [UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback](https://arxiv.org/abs/2511.01678)
*Ropeway Liu,Hangjie Yuan,Bo Dong,Jiazheng Xing,Jinwang Wang,Rui Zhao,Yan Xing,Weihua Chen,Fan Wang*

Main category: cs.CV

TL;DR: UniLumos是一个统一的图像和视频重光照框架，通过将RGB空间的几何反馈集成到流匹配骨干网络中，显著提升了重光照的物理合理性，同时实现了20倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的重光照方法在语义潜在空间中优化，虽然能产生丰富的照明效果，但往往产生不真实的结果，如过曝高光、错位阴影和错误遮挡，缺乏物理正确性。

Method: 通过从模型输出中提取深度和法线图来监督模型，明确对齐光照效果与场景结构；采用路径一致性学习减少计算成本；设计了六维标注协议实现细粒度控制；构建了LumosBench基准进行自动评估。

Result: UniLumos在重光照质量上达到最先进水平，显著改善了物理一致性，同时为图像和视频重光照实现了20倍的速度提升。

Conclusion: UniLumos通过几何反馈和路径一致性学习，成功解决了重光照中的物理合理性问题，在保持高质量的同时大幅提升了效率。

Abstract: Relighting is a crucial task with both practical demand and artistic value,
and recent diffusion models have shown strong potential by enabling rich and
controllable lighting effects. However, as they are typically optimized in
semantic latent space, where proximity does not guarantee physical correctness
in visual space, they often produce unrealistic results, such as overexposed
highlights, misaligned shadows, and incorrect occlusions. We address this with
UniLumos, a unified relighting framework for both images and videos that brings
RGB-space geometry feedback into a flow matching backbone. By supervising the
model with depth and normal maps extracted from its outputs, we explicitly
align lighting effects with the scene structure, enhancing physical
plausibility. Nevertheless, this feedback requires high-quality outputs for
supervision in visual space, making standard multi-step denoising
computationally expensive. To mitigate this, we employ path consistency
learning, allowing supervision to remain effective even under few-step training
regimes. To enable fine-grained relighting control and supervision, we design a
structured six-dimensional annotation protocol capturing core illumination
attributes. Building upon this, we propose LumosBench, a disentangled
attribute-level benchmark that evaluates lighting controllability via large
vision-language models, enabling automatic and interpretable assessment of
relighting precision across individual dimensions. Extensive experiments
demonstrate that UniLumos achieves state-of-the-art relighting quality with
significantly improved physical consistency, while delivering a 20x speedup for
both image and video relighting. Code is available at
https://github.com/alibaba-damo-academy/Lumos-Custom.

</details>


### [191] [Progressive Translation of H&E to IHC with Enhanced Structural Fidelity](https://arxiv.org/abs/2511.01698)
*Yuhang Kang,Ziyu Su,Tianyang Wang,Zaibo Li,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 提出了一种渐进式网络架构，通过分阶段优化颜色和细胞边界生成，从H&E染色图像合成IHC等效图像，显著提升视觉质量和结构细节。


<details>
  <summary>Details</summary>
Motivation: IHC染色虽然能提供高分辨率蛋白定位信息，但成本高、劳动密集且多重染色能力有限。现有染色转换技术使用线性加权损失函数，忽视了各组件间的相互依赖，导致图像质量不佳。

Method: 基于ASP框架，提出渐进式网络架构，分阶段优化颜色和细胞边界生成，引入DAB色原浓度和图像梯度损失函数。

Result: 在HER2和ER数据集上的实验表明，该模型显著改善了视觉质量，获得了更精细的结构细节。

Conclusion: 渐进式结构-颜色-细胞边界机制能有效提升IHC图像合成的质量，为病理诊断提供更高效经济的解决方案。

Abstract: Compared to hematoxylin-eosin (H&E) staining, immunohistochemistry (IHC) not
only maintains the structural features of tissue samples, but also provides
high-resolution protein localization, which is essential for aiding in
pathology diagnosis. Despite its diagnostic value, IHC remains a costly and
labor-intensive technique. Its limited scalability and constraints in
multiplexing further hinder widespread adoption, especially in resource-limited
settings. Consequently, researchers are increasingly exploring computational
stain translation techniques to synthesize IHC-equivalent images from
H&E-stained slides, aiming to extract protein-level information more
efficiently and cost-effectively. However, most existing stain translation
techniques rely on a linearly weighted summation of multiple loss terms within
a single objective function, strategy that often overlooks the interdepedence
among these components-resulting in suboptimal image quality and an inability
to simultaneously preserve structural authenticity and color fidelity. To
address this limitation, we propose a novel network architecture that follows a
progressive structure, incorporating color and cell border generation logic,
which enables each visual aspect to be optimized in a stage-wise and decoupled
manner. To validate the effectiveness of our proposed network architecture, we
build upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We
introduce additional loss functions based on 3,3'-diaminobenzidine (DAB)
chromogen concentration and image gradient, enhancing color fidelity and cell
boundary clarity in the generated IHC images. By reconstructing the generation
pipeline using our structure-color-cell boundary progressive mechanism,
experiments on HER2 and ER datasets demonstrated that the model significantly
improved visual quality and achieved finer structural details.

</details>


### [192] [Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond](https://arxiv.org/abs/2511.01704)
*Xin Qiao,Matteo Poggi,Xing Wei,Pengchao Deng,Yanhui Zhou,Stefano Mattoccia*

Main category: cs.CV

TL;DR: 提出了LFRD2框架，结合神经网络表达能力和物理建模可解释性，解决屏下ToF成像中的信号衰减、多径干扰和时序噪声问题。


<details>
  <summary>Details</summary>
Motivation: 屏下ToF成像面临透明OLED层导致的严重退化问题，包括信号衰减、多径干扰和时序噪声，这些因素显著影响深度感知质量。

Method: 采用时间分数阶反应-扩散模块实现迭代深度优化，通过动态生成微分阶数捕捉长期依赖关系；引入基于系数预测和重复微分的连续卷积算子提升恢复质量。

Result: 在四个基准数据集上的实验证明了该方法的有效性。

Conclusion: LFRD2框架能够有效解决屏下ToF成像中的退化问题，提升深度感知质量。

Abstract: Under-display ToF imaging aims to achieve accurate depth sensing through a
ToF camera placed beneath a screen panel. However, transparent OLED (TOLED)
layers introduce severe degradations-such as signal attenuation, multi-path
interference (MPI), and temporal noise-that significantly compromise depth
quality. To alleviate this drawback, we propose Learnable Fractional
Reaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the
expressive power of neural networks with the interpretability of physical
modeling. Specifically, we implement a time-fractional reaction-diffusion
module that enables iterative depth refinement with dynamically generated
differential orders, capturing long-term dependencies. In addition, we
introduce an efficient continuous convolution operator via coefficient
prediction and repeated differentiation to further improve restoration quality.
Experiments on four benchmark datasets demonstrate the effectiveness of our
approach. The code is publicly available at https://github.com/wudiqx106/LFRD2.

</details>


### [193] [Probabilistic Robustness for Free? Revisiting Training via a Benchmark](https://arxiv.org/abs/2511.01724)
*Yi Zhang,Zheng Wang,Chen Zhen,Wenjie Ruan,Qing Guo,Siddartha Khastgir,Carsten Maple,Xingyu Zhao*

Main category: cs.CV

TL;DR: PRBench是首个专门评估不同鲁棒性训练方法对概率鲁棒性(PR)提升效果的基准测试，通过综合指标比较了对抗训练(AT)和PR针对性训练方法，发现AT方法在提升AR和PR性能方面更通用，而PR针对性训练方法具有更低的泛化误差和更高的干净准确率。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型对微小扰动非常脆弱。现有研究主要关注对抗鲁棒性(AR)，而概率鲁棒性(PR)从统计角度衡量模型在随机扰动下保持正确预测的概率。虽然PR被认为是AR的实用补充，但专门的PR训练方法仍相对不足，且现有方法存在评估协议不可比、与强AT基线比较有限、缺乏统一框架等问题。

Method: 提出了PRBench基准测试，使用包括干净准确率、PR和AR性能、训练效率和泛化误差在内的综合指标集，对常见的AT和PR针对性训练方法进行实证比较，并提供了不同训练方法PR性能泛化误差的理论分析。

Result: 主要发现：AT方法在提升AR和PR性能方面比PR针对性训练方法更通用，而PR针对性训练方法始终产生更低的泛化误差和更高的干净准确率。构建了包含222个训练模型的排行榜，涵盖7个数据集和10种模型架构。

Conclusion: PRBench为评估鲁棒性训练方法的PR改进提供了首个专用基准，揭示了AT和PR针对性训练方法各自的优势，AT在性能提升方面更通用，而PR针对性训练在泛化性和干净准确率方面表现更好。

Abstract: Deep learning models are notoriously vulnerable to imperceptible
perturbations. Most existing research centers on adversarial robustness (AR),
which evaluates models under worst-case scenarios by examining the existence of
deterministic adversarial examples (AEs). In contrast, probabilistic robustness
(PR) adopts a statistical perspective, measuring the probability that
predictions remain correct under stochastic perturbations. While PR is widely
regarded as a practical complement to AR, dedicated training methods for
improving PR are still relatively underexplored, albeit with emerging progress.
Among the few PR-targeted training methods, we identify three limitations: i
non-comparable evaluation protocols; ii limited comparisons to strong AT
baselines despite anecdotal PR gains from AT; and iii no unified framework to
compare the generalization of these methods. Thus, we introduce PRBench, the
first benchmark dedicated to evaluating improvements in PR achieved by
different robustness training methods. PRBench empirically compares most common
AT and PR-targeted training methods using a comprehensive set of metrics,
including clean accuracy, PR and AR performance, training efficiency, and
generalization error (GE). We also provide theoretical analysis on the GE of PR
performance across different training methods. Main findings revealed by
PRBench include: AT methods are more versatile than PR-targeted training
methods in terms of improving both AR and PR performance across diverse
hyperparameter settings, while PR-targeted training methods consistently yield
lower GE and higher clean accuracy. A leaderboard comprising 222 trained models
across 7 datasets and 10 model architectures is publicly available at
https://tmpspace.github.io/PRBenchLeaderboard/.

</details>


### [194] [Toward Strategy Identification and Subtask Decomposition In Task Exploration](https://arxiv.org/abs/2511.01728)
*Tom Odem*

Main category: cs.CV

TL;DR: 开发了一个任务探索管道，使用聚类技术结合因子分析和字符串编辑距离，自动识别完成任务的关键全局和局部策略，并识别任务中的有意义子任务。


<details>
  <summary>Details</summary>
Motivation: 推进机器对用户知识、技能和行为的理解，以实现隐式协调，这是预期性人机交互的一个子领域。

Method: 开发任务探索管道，使用聚类技术、因子分析和字符串编辑距离自动识别全局策略（完成任务的动作集合）和局部策略（使用这些动作的序列组合），并识别各种长度的有意义子任务。

Result: 任务探索管道能够自动识别完成任务的关键策略，并用层次化子任务结构编码用户运行。还开发了Task Explorer应用程序来轻松查看管道结果。

Conclusion: 该管道可轻松修改以适应任何基于动作的时间序列数据，识别出的策略和子任务有助于人类和机器了解用户的知识、技能和行为。

Abstract: This research builds on work in anticipatory human-machine interaction, a
subfield of human-machine interaction where machines can facilitate
advantageous interactions by anticipating a user's future state. The aim of
this research is to further a machine's understanding of user knowledge, skill,
and behavior in pursuit of implicit coordination. A task explorer pipeline was
developed that uses clustering techniques, paired with factor analysis and
string edit distance, to automatically identify key global and local strategies
that are used to complete tasks. Global strategies identify generalized sets of
actions used to complete tasks, while local strategies identify sequences that
used those sets of actions in a similar composition. Additionally, meaningful
subtasks of various lengths are identified within the tasks. The task explorer
pipeline was able to automatically identify key strategies used to complete
tasks and encode user runs with hierarchical subtask structures. In addition, a
Task Explorer application was developed to easily review pipeline results. The
task explorer pipeline can be easily modified to any action-based time-series
data and the identified strategies and subtasks help to inform humans and
machines on user knowledge, skill, and behavior.

</details>


### [195] [CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays](https://arxiv.org/abs/2511.01730)
*Yefeng Wu,Yucheng Song,Ling Wu,Shan Wan,Yecheng Zhao*

Main category: cs.CV

TL;DR: 提出了CGF-DETR，一种增强的实时检测变换器，专门用于胸部X光肺炎检测，通过引入XFABlock、SPGA模块和GCFC3等改进，在保持实时性能的同时显著提升了检测精度。


<details>
  <summary>Details</summary>
Motivation: 肺炎是全球发病率和死亡率的主要原因，需要准确高效的自动检测系统。虽然基于变换器的检测器在目标检测任务中表现出色，但在医学影像特别是胸部X光肺炎检测中的应用仍未被充分探索。

Method: 在骨干网络中引入XFABlock，通过卷积注意力机制与CSP架构集成来改进多尺度特征提取；提出SPGA模块，用动态门控机制和单头自注意力替代标准多头注意力；设计GCFC3用于颈部网络，通过多路径卷积融合增强特征表示，同时通过结构重参数化保持实时性能。

Result: 在RSNA肺炎检测数据集上的实验表明，CGF-DETR达到82.2% mAP@0.5，比基线RT-DETR-l提升3.7%，同时保持48.1 FPS的推理速度。消融研究证实每个模块都对性能提升有显著贡献，完整模型达到50.4% mAP@[0.5:0.95]。

Conclusion: CGF-DETR通过精心设计的模块在保持实时性能的同时显著提升了肺炎检测精度，为医学影像中的实时目标检测提供了有效的解决方案。

Abstract: Pneumonia remains a leading cause of morbidity and mortality worldwide,
necessitating accurate and efficient automated detection systems. While recent
transformer-based detectors like RT-DETR have shown promise in object detection
tasks, their application to medical imaging, particularly pneumonia detection
in chest X-rays, remains underexplored. This paper presents CGF-DETR, an
enhanced real-time detection transformer specifically designed for pneumonia
detection. We introduce XFABlock in the backbone to improve multi-scale feature
extraction through convolutional attention mechanisms integrated with CSP
architecture. To achieve efficient feature aggregation, we propose SPGA module
that replaces standard multi-head attention with dynamic gating mechanisms and
single-head self-attention. Additionally, GCFC3 is designed for the neck to
enhance feature representation through multi-path convolution fusion while
maintaining real-time performance via structural re-parameterization. Extensive
experiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR
achieves 82.2\% mAP@0.5, outperforming the baseline RT-DETR-l by 3.7\% while
maintaining comparable inference speed at 48.1 FPS. Our ablation studies
confirm that each proposed module contributes meaningfully to the overall
performance improvement, with the complete model achieving 50.4\%
mAP@[0.5:0.95]

</details>


### [196] [3EED: Ground Everything Everywhere in 3D](https://arxiv.org/abs/2511.01755)
*Rong Li,Yuhao Dong,Tianshuai Hu,Ao Liang,Youquan Liu,Dongyue Lu,Liang Pan,Lingdong Kong,Junwei Liang,Ziwei Liu*

Main category: cs.CV

TL;DR: 3EED是一个多平台、多模态的3D视觉定位基准，包含车辆、无人机和四足机器人的RGB和LiDAR数据，提供超过128,000个对象和22,000个验证过的指代表达，规模是现有数据集的10倍。


<details>
  <summary>Details</summary>
Motivation: 现有的3D视觉定位基准局限于室内环境、单一平台和小规模，需要更全面的基准来支持开放世界环境中的语言引导对象定位。

Method: 开发了可扩展的标注流程，结合视觉语言模型提示和人工验证；提出平台感知归一化和跨模态对齐技术；建立领域内和跨平台评估协议。

Result: 构建了大规模多平台3D视觉定位数据集，揭示了显著的性能差距，突显了可泛化3D定位的挑战和机遇。

Conclusion: 3EED数据集和基准工具包的发布将推动语言驱动的3D具身感知研究的未来发展。

Abstract: Visual grounding in 3D is the key for embodied agents to localize
language-referred objects in open-world environments. However, existing
benchmarks are limited to indoor focus, single-platform constraints, and small
scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark
featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We
provide over 128,000 objects and 22,000 validated referring expressions across
diverse outdoor scenes -- 10x larger than existing datasets. We develop a
scalable annotation pipeline combining vision-language model prompting with
human verification to ensure high-quality spatial grounding. To support
cross-platform learning, we propose platform-aware normalization and
cross-modal alignment techniques, and establish benchmark protocols for
in-domain and cross-platform evaluations. Our findings reveal significant
performance gaps, highlighting the challenges and opportunities of
generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released
to advance future research in language-driven 3D embodied perception.

</details>


### [197] [HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain](https://arxiv.org/abs/2511.01756)
*Kai Zhai,Ziyan Huang,Qiang Nie,Xiang Li,Bo Ouyang*

Main category: cs.CV

TL;DR: 提出HGFreNet，一种结合图注意力机制和Transformer的架构，通过跳数混合特征聚合和频域轨迹一致性来解决2D到3D人体姿态提升中的深度模糊和时间不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 解决2D到3D人体姿态提升中的深度模糊和2D姿态估计误差导致的3D轨迹不一致问题，现有方法主要关注时间域抖动约束，忽略了骨骼关节运动的全局时空相关性。

Method: 设计HGFreNet架构，包含跳数混合图注意力模块（HGA）和Transformer编码器来建模全局关节时空相关性。HGA模块将骨骼关节的所有k跳邻居分组为混合组以扩大感受野，并应用注意力机制发现这些组的潜在相关性。在频域约束轨迹一致性以利用全局时间相关性，并使用初步网络估计3D姿态。

Result: 在Human3.6M和MPI-INF-3DHP两个标准基准数据集上的实验表明，HGFreNet在位置精度和时间一致性方面优于最先进方法。

Conclusion: HGFreNet通过结合图注意力机制和Transformer，有效解决了2D到3D人体姿态提升中的时空一致性问题，在位置精度和时间一致性方面取得了优越性能。

Abstract: 2D-to-3D human pose lifting is a fundamental challenge for 3D human pose
estimation in monocular video, where graph convolutional networks (GCNs) and
attention mechanisms have proven to be inherently suitable for encoding the
spatial-temporal correlations of skeletal joints. However, depth ambiguity and
errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous
studies have attempted to restrict jitters in the time domain, for instance, by
constraining the differences between adjacent frames while neglecting the
global spatial-temporal correlations of skeletal joint motion. To tackle this
problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid
feature aggregation and 3D trajectory consistency in the frequency domain.
Specifically, we propose a hop-hybrid graph attention (HGA) module and a
Transformer encoder to model global joint spatial-temporal correlations. The
HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group
to enlarge the receptive field and applies the attention mechanism to discover
the latent correlations of these groups globally. We then exploit global
temporal correlations by constraining trajectory consistency in the frequency
domain. To provide 3D information for depth inference across frames and
maintain coherence over time, a preliminary network is applied to estimate the
3D pose. Extensive experiments were conducted on two standard benchmark
datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed
HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional
accuracy and temporal consistency.

</details>


### [198] [UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs](https://arxiv.org/abs/2511.01768)
*Zhe Liu,Jinghua Hou,Xiaoqing Ye,Jingdong Wang,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: UniLION是一个统一的自动驾驶模型，使用线性组RNN算子高效处理大规模LiDAR点云、高分辨率多视角图像和时间序列，无需显式的时间或多模态融合模块，在多种核心任务上实现竞争性甚至最先进的性能。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理长序列数据时存在二次注意力机制带来的显著计算开销问题，需要一种更高效的架构来处理自动驾驶中的大规模多模态数据。

Method: 基于线性组RNN算子（对分组特征执行线性RNN），构建统一的自动驾驶模型架构，支持LiDAR-only、时序LiDAR、多模态和多模态时序融合等多种配置。

Result: 在3D感知（物体检测、跟踪、占用预测、BEV地图分割）、预测（运动预测）和规划（端到端规划）等核心任务上均实现了竞争性甚至最先进的性能。

Conclusion: UniLION为自动驾驶3D基础模型的开发提供了新的视角，简化了多模态多任务系统的设计，同时保持优越性能。

Abstract: Although transformers have demonstrated remarkable capabilities across
various domains, their quadratic attention mechanisms introduce significant
computational overhead when processing long-sequence data. In this paper, we
present a unified autonomous driving model, UniLION, which efficiently handles
large-scale LiDAR point clouds, high-resolution multi-view images, and even
temporal sequences based on the linear group RNN operator (i.e., performs
linear RNN for grouped features). Remarkably, UniLION serves as a single
versatile architecture that can seamlessly support multiple specialized
variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal
temporal fusion configurations) without requiring explicit temporal or
multi-modal fusion modules. Moreover, UniLION consistently delivers competitive
and even state-of-the-art performance across a wide range of core tasks,
including 3D perception (e.g., 3D object detection, 3D object tracking, 3D
occupancy prediction, BEV map segmentation), prediction (e.g., motion
prediction), and planning (e.g., end-to-end planning). This unified paradigm
naturally simplifies the design of multi-modal and multi-task autonomous
driving systems while maintaining superior performance. Ultimately, we hope
UniLION offers a fresh perspective on the development of 3D foundation models
in autonomous driving. Code is available at
https://github.com/happinesslz/UniLION

</details>


### [199] [PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution](https://arxiv.org/abs/2511.01802)
*Tejas Sarnaik,Manan Shah,Ravi Hegde*

Main category: cs.CV

TL;DR: 提出了一个提示驱动的GraphRAG框架，强调提示设计在图检索增强生成中的重要性，在HotpotQA和2WikiMultiHopQA数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然基于图的检索在复杂推理方面已有研究，但提示设计对增强检索和推理过程的影响尚未得到充分研究。

Method: 构建符号知识图，将实体和事实关系编码为结构化三元组；在在线检索中使用LLM进行语义过滤和答案生成；通过个性化PageRank实现实体引导的图遍历。

Result: 在HotpotQA和2WikiMultiHopQA上分别获得80.7%和78.9%的F1分数，以及97.1%和98.1%的Recall@5分数。

Conclusion: 提示设计是提高检索准确性和响应质量的重要组成部分，为更高效和可理解的多跳问答系统奠定了基础。

Abstract: Retrieval-Augmented Generation (RAG) has become a robust framework for
enhancing Large Language Models (LLMs) with external knowledge. Recent advances
in RAG have investigated graph based retrieval for intricate reasoning;
however, the influence of prompt design on enhancing the retrieval and
reasoning process is still considerably under-examined. In this paper, we
present a prompt-driven GraphRAG framework that underscores the significance of
prompt formulation in facilitating entity extraction, fact selection, and
passage reranking for multi-hop question answering. Our approach creates a
symbolic knowledge graph from text data by encoding entities and factual
relationships as structured facts triples. We use LLMs selectively during
online retrieval to perform semantic filtering and answer generation. We also
use entity-guided graph traversal through Personalized PageRank (PPR) to
support efficient, scalable retrieval based on the knowledge graph we built.
Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA,
with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%,
respectively. These results show that prompt design is an important part of
improving retrieval accuracy and response quality. This research lays the
groundwork for more efficient and comprehensible multi-hop question-answering
systems, highlighting the importance of prompt-aware graph reasoning.

</details>


### [200] [SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art](https://arxiv.org/abs/2511.01817)
*Sagi Eppel,Alona Strugatski*

Main category: cs.CV

TL;DR: Scitextures数据集是一个大规模的科学纹理和视觉模式集合，包含1200多个模型和10万张图像，涵盖物理、化学、生物、社会学、技术、数学和艺术等领域，用于探索视觉模式与生成机制之间的联系。


<details>
  <summary>Details</summary>
Motivation: 理解视觉模式与其形成过程之间的联系是视觉理解的深层形式，该研究旨在通过大规模数据集探索不同领域中视觉模式与生成机制的关系。

Method: 使用自主AI管道收集和实现标准化模型，创建包含多种科学领域纹理和模式的数据集，并评估AI模型连接视觉模式与生成代码的能力。

Result: 研究表明视觉语言模型能够超越视觉模式理解物理系统，并能通过提供真实世界模式的图像来推断、建模和编码生成机制。

Conclusion: Scitextures数据集为研究视觉模式与生成机制之间的关系提供了重要资源，展示了AI在理解复杂系统方面的潜力。

Abstract: The ability to connect visual patterns with the processes that form them
represents one of the deepest forms of visual understanding. Textures of clouds
and waves, the growth of cities and forests, or the formation of materials and
landscapes are all examples of patterns emerging from underlying mechanisms. We
present the Scitextures dataset, a large-scale collection of textures and
visual patterns from all domains of science, tech, and art, along with the
models and code that generate these images. Covering over 1,200 different
models and 100,000 images of patterns and textures from physics, chemistry,
biology, sociology, technology, mathematics, and art, this dataset offers a way
to explore the connection between the visual patterns that shape our world and
the mechanisms that produce them. Created by an agentic AI pipeline that
autonomously collects and implements models in standardized form, we use
SciTextures to evaluate the ability of leading AI models to link visual
patterns to the models and code that generate them, and to identify different
patterns that emerged from the same process. We also test AIs ability to infer
and recreate the mechanisms behind visual patterns by providing a natural image
of a real-world pattern and asking the AI to identify, model, and code the
mechanism that formed the pattern, then run this code to generate a simulated
image that is compared to the real image. These benchmarks show that
vision-language models (VLMs) can understand and simulate the physical system
beyond a visual pattern. The dataset and code are available at:
https://zenodo.org/records/17485502

</details>


### [201] [TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning](https://arxiv.org/abs/2511.01833)
*Ming Li,Jike Zhong,Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Yuxiang Lai,Wei Chen,Konstantinos Psounis,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 提出了TIR-Bench基准，用于评估图像思维推理能力，测试了22个多模态大语言模型在13个需要工具使用的图像处理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法充分评估像OpenAI o3这样能够智能创建和操作工具进行图像转换的模型的先进能力，即使是Visual Search这样的基准也只测试基本操作，无法评估复杂、动态和工具依赖的推理能力。

Method: 开发了TIR-Bench基准，包含13个多样化任务，每个任务都需要在思维链中进行新颖的图像处理工具使用。评估了22个多模态大语言模型，包括开源和专有模型，以及具有明确工具使用增强的模型。

Result: TIR-Bench对所有模型都具有挑战性，强性能需要真正的图像思维能力。进行了直接与代理式微调的试点研究比较。

Conclusion: TIR-Bench是一个全面评估代理式图像思维的基准，揭示了当前模型在复杂图像推理方面的局限性，并强调了真正图像思维能力的重要性。

Abstract: The frontier of visual reasoning is shifting toward models like OpenAI o3,
which can intelligently create and operate tools to transform images for
problem-solving, also known as thinking-\textit{with}-images in
chain-of-thought. Yet existing benchmarks fail to fully capture this advanced
capability. Even Visual Search, the most common benchmark for current
thinking-\textit{with}-images methods, tests only basic operations such as
localization and cropping, offering little insight into more complex, dynamic,
and tool-dependent reasoning. We introduce \textbf{TIR-Bench}, a comprehensive
benchmark for evaluating agentic thinking-with-images across 13 diverse tasks,
each requiring novel tool use for image processing and manipulation in
chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from
leading open-sourced and proprietary models to those with explicit tool-use
augmentation. Results show that TIR-Bench is universally challenging, and
strong performance requires genuine thinking-with-images capabilities. Finally,
we present a pilot study comparing direct versus agentic fine-tuning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [202] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: 提出了一种融合文本和视觉特征的多模态虚假评论检测框架，在包含21,142张用户上传图片的数据集上取得了0.934的F1分数，优于单模态基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前数字商务中虚假评论泛滥，现有检测模型主要依赖单模态文本数据，无法捕捉跨模态语义不一致性，威胁评论生态系统的信任和透明度。

Method: 使用BERT编码文本特征和ResNet-50提取视觉特征，通过分类头融合这些表示来联合预测评论真实性。

Result: 多模态模型在测试集上F1分数达到0.934，优于单模态基线，能够检测文本赞美与不相关或低质量图片之间的微妙不一致性。

Conclusion: 研究证明了多模态学习在保护数字信任中的关键作用，为各种在线平台的内容审核提供了可扩展的解决方案。

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [203] [Graph-Attentive MAPPO for Dynamic Retail Pricing](https://arxiv.org/abs/2511.00039)
*Krishna Kumar Neelakanta Pillai Santha Kumari Amma*

Main category: cs.AI

TL;DR: 本文系统比较了MAPPO和MAPPO+GAT在多智能体强化学习零售定价优化中的表现，发现MAPPO+GAT通过图注意力机制增强产品间信息共享，在保持价格稳定性的同时提升了性能。


<details>
  <summary>Details</summary>
Motivation: 零售动态定价需要能够适应需求变化并协调相关产品决策的策略，传统方法在多产品决策中面临可扩展性和稳定性挑战。

Method: 使用基于真实交易数据的模拟定价环境，比较MAPPO基线和图注意力增强的MAPPO+GAT变体，评估利润、稳定性、公平性和训练效率。

Result: MAPPO为组合级价格控制提供了稳健基础，MAPPO+GAT通过产品图上的信息共享进一步提升了性能，且未引起过度价格波动。

Conclusion: 图集成MARL为动态零售定价提供了比独立学习器更可扩展和稳定的解决方案，在多产品决策中具有实际优势。

Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand
while coordinating decisions across related products. We present a systematic
empirical study of multi-agent reinforcement learning for retail price
optimization, comparing a strong MAPPO baseline with a
graph-attention-augmented variant (MAPPO+GAT) that leverages learned
interactions among products. Using a simulated pricing environment derived from
real transaction data, we evaluate profit, stability across random seeds,
fairness across products, and training efficiency under a standardized
evaluation protocol. The results indicate that MAPPO provides a robust and
reproducible foundation for portfolio-level price control, and that MAPPO+GAT
further enhances performance by sharing information over the product graph
without inducing excessive price volatility. These results indicate that
graph-integrated MARL provides a more scalable and stable solution than
independent learners for dynamic retail pricing, offering practical advantages
in multi-product decision-making.

</details>


### [204] [GEPOC Parameters -- Open Source Parametrisation and Validation for Austria, Version 2.0](https://arxiv.org/abs/2511.00048)
*Martin Bicher,Maximilian Viehauser,Daniele Giannandrea,Hannah Kastinger,Dominik Brunmeir,Claire Rippinger,Christoph Urach,Niki Popper*

Main category: cs.AI

TL;DR: GEPOC是一个用于分析人口层面研究问题的模型和方法集合。本文完整描述了基于奥地利公开数据的模型参数计算方法，包括数据处理算法和参数文件生成，特别关注GEPOC ABM代理模型的参数计算，并进行了验证研究。


<details>
  <summary>Details</summary>
Motivation: 为GEPOC模型在特定国家或地区的有效应用提供稳定、可复现的数据处理流程，确保模型参数的有效性和可用性。

Method: 基于奥地利公开可访问数据，采用聚合、分解、融合、清洗和缩放等算法进行数据处理，生成GEPOC模型参数，特别关注GEPOC ABM代理模型的参数计算。

Result: 开发了完整的奥地利GEPOC模型参数计算流程，生成了可用的参数文件，并通过GEPOC ABM模型进行了广泛的验证研究。

Conclusion: 本文提供了基于公开数据的GEPOC模型参数计算方法，为在奥地利应用GEPOC模型提供了可靠的数据基础，验证结果表明方法的有效性。

Abstract: GEPOC, short for Generic Population Concept, is a collection of models and
methods for analysing population-level research questions. For the valid
application of the models for a specific country or region, stable and
reproducible data processes are necessary, which provide valid and ready-to-use
model parameters. This work contains a complete description of the
data-processing methods for computation of model parameters for Austria, based
exclusively on freely and publicly accessible data. In addition to the
description of the source data used, this includes all algorithms used for
aggregation, disaggregation, fusion, cleansing or scaling of the data, as well
as a description of the resulting parameter files. The document places
particular emphasis on the computation of parameters for the most important
GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An
extensive validation study using this particular model was made and is
presented at the end of this work.

</details>


### [205] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: QuantumBench是首个专门为量子科学领域构建的LLM评估数据集，包含约800个多选问题，用于评估LLM在量子领域的理解和应用能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在科学工作流程中的广泛应用，需要评估模型是否能准确掌握领域特定知识和符号表示，而通用基准很少反映量子科学这种需要高级数学和非直观现象理解的领域需求。

Method: 利用公开材料编制了约800个涵盖量子科学9个领域的多选问题，形成8选项的多选数据集，并评估了多个现有LLM在量子领域的表现，包括对问题格式变化的敏感性分析。

Result: 通过QuantumBench评估发现LLM在量子科学领域的表现存在差异，并揭示了模型对问题格式的敏感性。

Conclusion: QuantumBench作为首个量子领域LLM评估数据集，旨在指导LLM在量子研究中的有效应用。

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [206] [Engineering.ai: A Platform for Teams of AI Engineers in Computational Design](https://arxiv.org/abs/2511.00122)
*Ran Xu,Yupeng Qi,Jingsen Feng,Xu Chu*

Main category: cs.AI

TL;DR: 提出了Engineering.ai平台，采用分层多智能体架构，让AI工程师团队在计算设计中协作完成复杂工程任务，通过无人机机翼优化验证了框架的可靠性和100%成功率。


<details>
  <summary>Details</summary>
Motivation: 现代工程实践中，人类工程师需要组成专业团队协作设计复杂产品，这需要大量开发时间和成本。为解决这一问题，构建能够自主执行复杂工程任务的AI工程师团队。

Method: 采用分层多智能体架构，由首席工程师协调空气动力学、结构、声学和优化工程等专业智能体。通过文件介导的通信实现数据可追溯性，集成FreeCAD、Gmsh、OpenFOAM、CalculiX和BPM声学分析等工具进行并行多学科仿真。

Result: 在400多个参数配置中实现了100%成功率，零网格生成失败、求解器收敛问题或手动干预需求，验证了框架的可信性。

Conclusion: 基于智能体的AI工程师有潜力自主执行复杂工程任务，该框架展示了工程设计中AI协作的可行性和可靠性。

Abstract: In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.

</details>


### [207] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: ARC-GEN是一个开源程序生成器，旨在通过忠实扩展原始ARC-AGI训练数据集来增强样本对空间，解决ARC-AGI基准中演示集规模有限的问题。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI基准虽然对评估通用人工智能进展很重要，但其演示集规模有限，每个任务只有少量输入-输出网格对，这限制了需要大量任务内示例的算法的性能。

Method: 开发ARC-GEN程序生成器，全面覆盖所有400个任务，并尽可能忠实模拟原始ARC-AGI-1版本的分布特性和特征。

Result: ARC-GEN成功扩展了原始训练数据集，创建了一个静态基准套件，用于验证提交到2025年Google Code Golf Championship的程序正确性。

Conclusion: ARC-GEN为ARC-AGI基准提供了有效的数据增强解决方案，有助于更好地评估算法的技能获取效率，推动通用人工智能的发展。

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [208] [Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures](https://arxiv.org/abs/2511.00194)
*Jovial Cheukam Ngouonou,Ramiz Gindullin,Claude-Guy Quimper,Nicolas Beldiceanu,Remi Douence*

Main category: cs.AI

TL;DR: 改进了[1]中的增量选择算法，并证明了所有选定的猜想


<details>
  <summary>Details</summary>
Motivation: 改进现有的增量选择算法，解决其可能存在的问题或局限性

Method: 提出改进的增量选择算法，并进行数学证明

Result: 成功证明了所有选定的猜想

Conclusion: 改进的算法有效且所有相关猜想都得到了证明

Abstract: We present an improved incremental selection algorithm of the selection
algorithm presented in [1] and prove all the selected conjectures.

</details>


### [209] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: 这篇论文探讨了大型语言模型如何帮助解决认知科学领域面临的知识整合和概念清晰度挑战，特别是在跨学科连接、理论形式化、测量分类学等方面。


<details>
  <summary>Details</summary>
Motivation: 认知科学由于其多面性和跨学科性质，在知识整合和概念清晰度方面面临持续挑战。人工智能特别是大型语言模型的发展为解决这些问题提供了工具。

Method: 通过综述分析，考察LLMs在认知科学关键挑战领域的应用，包括建立跨学科连接、形式化理论、开发清晰测量分类学、通过集成建模框架实现泛化性，以及捕捉情境和个体差异。

Result: LLMs在支持认知科学整合方面具有潜力，能够帮助解决该领域的传统难题，但也存在局限性，需要谨慎使用。

Conclusion: 当审慎使用时，LLMs可以作为更整合和累积性认知科学的工具，补充而非取代人类专业知识。

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [210] [Advancing AI Challenges for the United States Department of the Air Force](https://arxiv.org/abs/2511.00267)
*Christian Prothmann,Vijay Gadepally,Jeremy Kepner,Koley Borchard,Luca Carlone,Zachary Folcik,J. Daniel Grith,Michael Houle,Jonathan P. How,Nathan Hughes,Ifueko Igbinedion,Hayden Jananthan,Tejas Jayashankar,Michael Jones,Sertac Karaman,Binoy G. Kurien,Alejandro Lancho,Giovanni Lavezzi,Gary C. F. Lee,Charles E. Leiserson,Richard Linares,Lindsey McEvoy,Peter Michaleas,Chasen Milner,Alex Pentland,Yury Polyanskiy,Jovan Popovich,Jeffrey Price,Tim W. Reid,Stephanie Riley,Siddharth Samsi,Peter Saunders,Olga Simek,Mark S. Veillette,Amir Weiss,Gregory W. Wornell,Daniela Rus,Scott T. Ruppel*

Main category: cs.AI

TL;DR: DAF-MIT AI加速器项目通过公开挑战问题推动AI研究，提供大型公开数据集，促进开源解决方案发展，更新了挑战项目在AI研究和应用中的成功贡献。


<details>
  <summary>Details</summary>
Motivation: 扩大美国在国防和民用领域的竞争优势，通过公开挑战问题刺激AI研究，促进学术界和私营部门参与AI生态系统建设。

Method: 开发和发布公开挑战问题，提供大型、公开可用的AI就绪数据集，鼓励开源解决方案的开发。

Result: 成功推动了AI研究进展，促进了AI技术的实际应用，扩大了研究社区的参与度。

Conclusion: DAF-MIT AI加速器通过公开挑战问题有效促进了AI技术的发展和应用，为美国在AI领域的竞争优势做出了贡献。

Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States
Department of the Air Force (DAF) and the Massachusetts Institute of Technology
(MIT). This program pioneers fundamental advances in artificial intelligence
(AI) to expand the competitive advantage of the United States in the defense
and civilian sectors. In recent years, AI Accelerator projects have developed
and launched public challenge problems aimed at advancing AI research in
priority areas. Hallmarks of AI Accelerator challenges include large, publicly
available, and AI-ready datasets to stimulate open-source solutions and engage
the wider academic and private sector AI ecosystem. This article supplements
our previous publication, which introduced AI Accelerator challenges. We
provide an update on how ongoing and new challenges have successfully
contributed to AI research and applications of AI technologies.

</details>


### [211] [Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities](https://arxiv.org/abs/2511.00340)
*Manan Roy Choudhury,Adithya Chandramouli,Mannan Anand,Vivek Gupta*

Main category: cs.AI

TL;DR: CLAUSE是首个专门评估LLM在法律推理中脆弱性的基准测试，通过生成7500+真实世界扰动合同来测试LLM检测细微法律差异的能力。


<details>
  <summary>Details</summary>
Motivation: LLM在高风险法律工作中快速应用，但缺乏系统评估其应对真实合同复杂、对抗性缺陷可靠性的基准测试。

Method: 使用CUAD和ContractNLI数据集生成10类异常合同，通过角色驱动流程和RAG系统确保法律准确性，评估LLM检测法律缺陷和解释其重要性的能力。

Result: 主流LLM经常遗漏细微错误，在法律解释方面表现更差，揭示了关键弱点。

Conclusion: 该工作为识别和纠正法律AI中的推理失败提供了路径，强调需要改进LLM处理复杂法律细微差别的能力。

Abstract: The rapid integration of large language models (LLMs) into high-stakes legal
work has exposed a critical gap: no benchmark exists to systematically
stress-test their reliability against the nuanced, adversarial, and often
subtle flaws present in real-world contracts. To address this, we introduce
CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an
LLM's legal reasoning. We study the capabilities of LLMs to detect and reason
about fine-grained discrepancies by producing over 7500 real-world perturbed
contracts from foundational datasets like CUAD and ContractNLI. Our novel,
persona-driven pipeline generates 10 distinct anomaly categories, which are
then validated against official statutes using a Retrieval-Augmented Generation
(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'
ability to detect embedded legal flaws and explain their significance. Our
analysis shows a key weakness: these models often miss subtle errors and
struggle even more to justify them legally. Our work outlines a path to
identify and correct such reasoning failures in legal AI.

</details>


### [212] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: 提出了一种基于伦理决策模型的LLM伦理推理范式，通过结构化五步流程增强多元人类价值观对齐，在SafeWorld基准测试中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法往往产生表面一致性而非真正的伦理理解，无法处理人类价值观的复杂性和情境依赖性，需要解决跨地区文化的多元价值观对齐问题。

Method: 采用结构化五步伦理推理流程：情境事实收集、层次化社会规范识别、选项生成、多视角伦理影响分析、反思。可通过提示工程或监督微调实现。

Result: 在专为区域价值观对齐设计的SafeWorld基准测试中，该框架显著提升了LLM与多元人类价值观的对齐效果，实现了更准确的社会规范识别和更文化适宜的道德推理。

Conclusion: 该工作为通过跨学科研究开发能更有效对齐全球社会多元价值观的LLM提供了具体路径，增强了模型对区域特异性的理解和细致伦理分析能力。

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [213] [Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs](https://arxiv.org/abs/2511.00382)
*Mina Taraghi,Yann Pequignot,Amin Nikanjam,Mohamed Amine Merzouk,Foutse Khomh*

Main category: cs.AI

TL;DR: 该研究系统评估了四种参数高效微调方法(LoRA、IA3、Prompt-Tuning、P-Tuning)对LLM安全性和公平性的影响，发现基于适配器的方法能提升安全性且对公平性影响较小，而基于提示的方法会降低安全性和公平性。


<details>
  <summary>Details</summary>
Motivation: 组织越来越多地使用公开仓库中的LLM，虽然微调能提升专业任务性能，但可能降低模型的安全性和公平性，需要系统评估不同微调方法在这些关键维度上的权衡。

Method: 对四个指令微调模型家族(Meta-Llama-3-8B、Qwen2.5-7B、Mistral-7B、Gemma-7B)应用四种PEFT方法，共评估235个微调变体在11个安全危害类别和9个人口统计公平性维度上的表现。

Result: 基于适配器的方法(LoRA、IA3)倾向于提高安全分数且对公平性破坏最小；基于提示的方法(Prompt-Tuning、P-Tuning)通常降低安全性并导致更大的公平性回归。不同基础模型表现出不同的对齐变化模式。

Conclusion: 安全性的改进不一定转化为公平性的改进，没有单一配置能同时优化所有公平性指标。建议安全关键部署从良好对齐的基础模型开始，优先选择基于适配器的PEFT方法，并进行类别特定的安全和公平性审计。

Abstract: Organizations are increasingly adopting and adapting Large Language Models
(LLMs) hosted on public repositories such as HuggingFace. Although these
adaptations often improve performance on specialized downstream tasks, recent
evidence indicates that they can also degrade a model's safety or fairness.
Since different fine-tuning techniques may exert distinct effects on these
critical dimensions, this study undertakes a systematic assessment of their
trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,
IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model
families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235
fine-tuned variants are evaluated across eleven safety hazard categories and
nine demographic fairness dimensions. The results show that adapter-based
approaches (LoRA, IA3) tend to improve safety scores and are the least
disruptive to fairness, retaining higher accuracy and lower bias scores. In
contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce
safety and cause larger fairness regressions, with decreased accuracy and
increased bias. Alignment shifts are strongly moderated by base model type:
LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest
safety decline, and Mistral, which is released without an internal moderation
layer, displays the greatest variance. Improvements in safety do not
necessarily translate into improvements in fairness, and no single
configuration optimizes all fairness metrics simultaneously, indicating an
inherent trade-off between these objectives. These findings suggest a practical
guideline for safety-critical deployments: begin with a well-aligned base
model, favour adapter-based PEFT, and conduct category-specific audits of both
safety and fairness.

</details>


### [214] [A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method](https://arxiv.org/abs/2511.00424)
*Ashutosh Anshul,Gumpili Sai Pranav,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.AI

TL;DR: 提出了一种新颖的多模态框架，结合文本、用户特定信息和图像分析来检测社交媒体用户的抑郁症，在新冠疫情期间特别有效。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情导致心理健康问题激增，但人们往往不愿就医。社交媒体成为表达情绪的重要平台，现有方法忽视了推文数据稀疏性和多模态特性。

Method: 开发多模态框架，提取文本、用户特征和图像内容；利用推文中的URL作为外部特征；提取图片中的文本内容；使用视觉神经网络(VNN)生成图像嵌入；创建视觉特征向量进行预测。

Result: 在基准数据集上比现有最优方法提升2%-8%，在新冠数据集上产生有希望的结果；分析了各模态的影响，提供了用户心理状态的宝贵见解。

Conclusion: 多模态方法能有效检测社交媒体用户的抑郁症，特别是在疫情期间；提出的框架在准确性和洞察力方面表现优异。

Abstract: The recent coronavirus disease (Covid-19) has become a pandemic and has
affected the entire globe. During the pandemic, we have observed a spike in
cases related to mental health, such as anxiety, stress, and depression.
Depression significantly influences most diseases worldwide, making it
difficult to detect mental health conditions in people due to unawareness and
unwillingness to consult a doctor. However, nowadays, people extensively use
online social media platforms to express their emotions and thoughts. Hence,
social media platforms are now becoming a large data source that can be
utilized for detecting depression and mental illness. However, existing
approaches often overlook data sparsity in tweets and the multimodal aspects of
social media. In this paper, we propose a novel multimodal framework that
combines textual, user-specific, and image analysis to detect depression among
social media users. To provide enough context about the user's emotional state,
we propose (i) an extrinsic feature by harnessing the URLs present in tweets
and (ii) extracting textual content present in images posted in tweets. We also
extract five sets of features belonging to different modalities to describe a
user. Additionally, we introduce a Deep Learning model, the Visual Neural
Network (VNN), to generate embeddings of user-posted images, which are used to
create the visual feature vector for prediction. We contribute a curated
Covid-19 dataset of depressed and non-depressed users for research purposes and
demonstrate the effectiveness of our model in detecting depression during the
Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over
a benchmark dataset by 2%-8% and produces promising results on the Covid-19
dataset. Our analysis highlights the impact of each modality and provides
valuable insights into users' mental and emotional states.

</details>


### [215] [GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining](https://arxiv.org/abs/2511.00457)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Xin Wang,Yifan Yang,Yueguo Chen,Yang Tian,Yunhai Wang*

Main category: cs.AI

TL;DR: GraphChain是一个让大语言模型能够分析复杂图数据的框架，通过动态工具序列模拟人类探索智能，解决了LLMs在大规模图分析中的上下文限制和推理不灵活问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在大规模图分析中面临上下文限制和推理不灵活的挑战，需要一种能够动态适应不同图结构的智能分析方法。

Method: 提出渐进图蒸馏（强化学习机制生成优化工具序列）和结构感知测试时适应（利用谱属性和轻量适配器调整工具选择策略）。

Result: 实验表明GraphChain显著优于现有方法，实现了可扩展和自适应的LLM驱动图分析。

Conclusion: GraphChain框架通过动态工具序列和结构感知适应机制，成功解决了LLMs在图分析中的局限性，为大规模图智能分析提供了有效解决方案。

Abstract: Large Language Models (LLMs) face significant limitations when applied to
large-scale graphs, struggling with context constraints and inflexible
reasoning. We present GraphChain, a framework that enables LLMs to analyze
complex graphs through dynamic sequences of specialized tools, mimicking human
exploratory intelligence. Our approach introduces two key innovations: (1)
Progressive Graph Distillation, a reinforcement learning mechanism that
generates optimized tool sequences balancing task relevance with information
compression, and (2) Structure-aware Test-Time Adaptation, which efficiently
tailors tool selection strategies to diverse graph topologies using spectral
properties and lightweight adapters without costly retraining. Experiments show
GraphChain significantly outperforms prior methods, enabling scalable and
adaptive LLM-driven graph analysis.

</details>


### [216] [Reimagining Safety Alignment with An Image](https://arxiv.org/abs/2511.00509)
*Yifan Xia,Guorui Chen,Wenqian Yu,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.AI

TL;DR: Magic Image是一个基于优化的视觉提示框架，通过优化图像提示来增强多模态大语言模型的安全性，减少过度拒绝，同时适应不同价值系统。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在越狱攻击下生成有害内容和因严格安全机制过度拒绝良性查询的双重挑战，特别是在多模态场景中这些问题更加突出。

Method: 通过使用有害/良性样本优化图像提示，使单个模型能够适应不同价值系统并与给定安全偏好更好地对齐，无需参数更新。

Result: 实验表明该方法在多个数据集上改善了安全性与有效性的平衡，同时保持了模型性能。

Conclusion: Magic Image为可部署的多模态大语言模型安全对齐提供了一个实用解决方案。

Abstract: Large language models (LLMs) excel in diverse applications but face dual
challenges: generating harmful content under jailbreak attacks and over-refusal
of benign queries due to rigid safety mechanisms. These issues are further
complicated by the need to accommodate different value systems and precisely
align with given safety preferences. Moreover, traditional methods like SFT and
RLHF lack this capability due to their costly parameter tuning requirements and
inability to support multiple value systems within a single model. These
problems are more obvious in multimodal large language models (MLLMs),
especially in terms of heightened over-refusal in cross-modal tasks and new
security risks arising from expanded attack surfaces. We propose Magic Image,
an optimization-driven visual prompt framework that enhances security while
reducing over-refusal. By optimizing image prompts using harmful/benign
samples, our method enables a single model to adapt to different value systems
and better align with given safety preferences without parameter updates.
Experiments demonstrate improved safety-effectiveness balance across diverse
datasets while preserving model performance, offering a practical solution for
deployable MLLM safety alignment.

</details>


### [217] [Efficient Generation of Binary Magic Squares](https://arxiv.org/abs/2511.00547)
*Alain Riou*

Main category: cs.AI

TL;DR: 提出了一种生成二进制幻方的简单算法，能够生成行和列和相等的二进制矩阵，并扩展到非方形情况，同时发布了Python实现。


<details>
  <summary>Details</summary>
Motivation: 需要一种简单有效的方法来生成二进制幻方，并扩展到非方形矩阵的情况。

Method: 通过归纳法证明的简单算法，能够生成有效的二进制幻方，并扩展到非方形情况。

Result: 算法始终返回有效的二进制幻方，具有最优理论复杂度，并成功生成了非方形二进制幻方。

Conclusion: 提出的算法简单有效，能够生成各种尺寸的二进制幻方，并提供了并行GPU加速的实现。

Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS),
i.e., square binary matrices where the sum of all rows and all columns are
equal. We show by induction that our algorithm always returns valid BMS with
optimal theoretical complexity. We then extend our study to non-square Binary
Magic Squares, formalize conditions on the sum of rows and columns for these
BMS to exist, and show that a slight variant of our first algorithm can
generate provably generate them. Finally, we publicly release two
implementations of our algorithm as Python packages, including one that can
generate several BMS in parallel using GPU acceleration.

</details>


### [218] [Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control](https://arxiv.org/abs/2511.00551)
*Qiang Li,Ningjing Zeng,Lina Yu*

Main category: cs.AI

TL;DR: 提出基于单智能体强化学习的区域自适应交通信号控制模型，使用队列长度定义状态和奖励函数，通过协调多路口控制缓解大规模区域拥堵。


<details>
  <summary>Details</summary>
Motivation: 现有研究多采用多智能体框架，但存在可扩展性问题。交通信号控制本质上需要单智能体框架，由单一控制中心监控所有道路状况并协调所有路口控制。

Method: 设计基于队列长度的状态、动作和奖励函数，动作用于调节队列动态。队列长度定义与传统略有不同但能可靠估计，可利用浮动车数据估算。

Result: 在SUMO仿真平台上全面评估，实验结果表明该模型通过协调多路口控制有效缓解大规模区域拥堵水平。

Conclusion: 提出的单智能体强化学习模型与浮动车技术兼容，具有广泛部署潜力，能有效解决区域自适应交通信号控制问题。

Abstract: Several studies have employed reinforcement learning (RL) to address the
challenges of regional adaptive traffic signal control (ATSC) and achieved
promising results. In this field, existing research predominantly adopts
multi-agent frameworks. However, the adoption of multi-agent frameworks
presents challenges for scalability. Instead, the Traffic signal control (TSC)
problem necessitates a single-agent framework. TSC inherently relies on
centralized management by a single control center, which can monitor traffic
conditions across all roads in the study area and coordinate the control of all
intersections. This work proposes a single-agent RL-based regional ATSC model
compatible with probe vehicle technology. Key components of the RL design
include state, action, and reward function definitions. To facilitate learning
and manage congestion, both state and reward functions are defined based on
queue length, with action designed to regulate queue dynamics. The queue length
definition used in this study differs slightly from conventional definitions
but is closely correlated with congestion states. More importantly, it allows
for reliable estimation using link travel time data from probe vehicles. With
probe vehicle data already covering most urban roads, this feature enhances the
proposed method's potential for widespread deployment. The method was
comprehensively evaluated using the SUMO simulation platform. Experimental
results demonstrate that the proposed model effectively mitigates large-scale
regional congestion levels via coordinated multi-intersection control.

</details>


### [219] [PreferThinker: Reasoning-based Personalized Image Preference Assessment](https://arxiv.org/abs/2511.00609)
*Shengqi Xu,Xinpeng Zhou,Yabo Zhang,Ming Liu,Tao Liang,Tianyu Zhang,Yalong Bai,Zuxuan Wu,Wangmeng Zuo*

Main category: cs.AI

TL;DR: 提出了一个基于推理的个性化图像偏好评估框架，通过预测用户偏好档案并基于该档案进行多维度评分，解决了现有方法难以处理个性化偏好的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注通用偏好评估，但难以处理个性化偏好，因为用户特定数据稀缺且个体品味复杂多样。

Method: 采用预测-评估范式：首先从参考图像预测用户偏好档案，然后基于预测档案提供可解释的多维度评分。使用两阶段训练策略：监督微调阶段赋予模型结构化推理能力，强化学习阶段鼓励探索更合理的评估路径。

Result: 大量实验证明了所提方法的优越性。

Conclusion: 通过构建大规模CoT风格数据集和两阶段训练策略，成功实现了有效的个性化图像偏好评估。

Abstract: Personalized image preference assessment aims to evaluate an individual
user's image preferences by relying only on a small set of reference images as
prior information. Existing methods mainly focus on general preference
assessment, training models with large-scale data to tackle well-defined tasks
such as text-image alignment. However, these approaches struggle to handle
personalized preference because user-specific data are scarce and not easily
scalable, and individual tastes are often diverse and complex. To overcome
these challenges, we introduce a common preference profile that serves as a
bridge across users, allowing large-scale user data to be leveraged for
training profile prediction and capturing complex personalized preferences.
Building on this idea, we propose a reasoning-based personalized image
preference assessment framework that follows a \textit{predict-then-assess}
paradigm: it first predicts a user's preference profile from reference images,
and then provides interpretable, multi-dimensional scores and assessments of
candidate images based on the predicted profile. To support this, we first
construct a large-scale Chain-of-Thought (CoT)-style personalized assessment
dataset annotated with diverse user preference profiles and high-quality
CoT-style reasoning, enabling explicit supervision of structured reasoning.
Next, we adopt a two-stage training strategy: a cold-start supervised
fine-tuning phase to empower the model with structured reasoning capabilities,
followed by reinforcement learning to incentivize the model to explore more
reasonable assessment paths and enhance generalization. Furthermore, we propose
a similarity-aware prediction reward to encourage better prediction of the
user's preference profile, which facilitates more reasonable assessments
exploration. Extensive experiments demonstrate the superiority of the proposed
method.

</details>


### [220] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: DTS是一种模型无关的解码框架，通过在高熵token处选择性分支并应用早停机制来选择最短的完整推理路径，有效解决大型推理模型的过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中经常出现过度思考，产生过长的思维链轨迹，这不仅增加推理成本，还可能降低准确性。研究发现推理长度与准确性之间存在明显的负相关关系。

Method: 提出DTS解码框架：通过在高熵token处选择性分支来绘制推理空间草图，并应用早停机制选择最短的完整推理路径，无需额外训练或监督。

Result: 在AIME2024和AIME2025数据集上的实验表明，DTS将准确率提升高达8%，平均推理长度减少23%，重复频率降低12%。

Conclusion: DTS能够实现可扩展且高效的大型推理模型推理，在提升准确性的同时显著降低推理成本。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [221] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: 提出基于多智能体系统和大型语言模型的自动化网络故障排除框架，通过协调多个专用工具来加速电信网络故障诊断和修复。


<details>
  <summary>Details</summary>
Motivation: 电信网络规模扩大和复杂性增加，现有AI模型范围狭窄、需要大量标注数据且难以在异构部署中泛化，导致网络故障排除仍严重依赖专家手动操作。

Method: 采用多智能体系统工作流，利用LLM协调编排器、解决方案规划器、执行器、数据检索器和根因分析器等智能体，并微调小型语言模型基于内部文档生成修复方案。

Result: 实验结果表明该框架显著加速了无线接入网和核心网领域的故障排除自动化。

Conclusion: 多智能体系统与语言模型结合能够有效实现电信网络故障的自动化诊断和修复，提高运维效率。

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [222] [Lifted Successor Generation in Numeric Planning](https://arxiv.org/abs/2511.00673)
*Dominik Drexler*

Main category: cs.AI

TL;DR: 扩展了经典规划中的提升后继生成器以支持数值前置条件，通过枚举替换一致性图中的最大团来生成地面动作，在大多数基准域中能精确工作。


<details>
  <summary>Details</summary>
Motivation: 传统数值规划任务需要将一阶语言表示的任务接地化，这可能导致任务表示大小的指数级爆炸。为了解决难以接地任务的实际问题，需要支持数值前置条件的提升后继生成器。

Method: 扩展了最先进的提升后继生成器，支持数值前置条件适用性。该方法枚举替换一致性图中的最大团，每个最大团代表动作模式变量的替换，生成地面动作。在图中加入数值动作前置条件，并证明在特定条件下生成器是精确的。

Result: 在25个基准域中的23个中，生成器能精确工作；在1个域中可能出现不适用地面动作，但通过最终适用性检查可以过滤而不影响完整性。这是首个支持数值动作前置条件的提升后继生成器。

Conclusion: 该方法为非常丰富的规划片段开启了提升规划的未来研究，解决了数值规划中的接地化问题，为复杂规划任务提供了更高效的解决方案。

Abstract: Most planners ground numeric planning tasks, given in a first-order-like
language, into a ground task representation. However, this can lead to an
exponential blowup in task representation size, which occurs in practice for
hard-to-ground tasks. We extend a state-of-the-art lifted successor generator
for classical planning to support numeric precondition applicability. The
method enumerates maximum cliques in a substitution consistency graph. Each
maximum clique represents a substitution for the variables of the action
schema, yielding a ground action. We augment this graph with numeric action
preconditions and prove the successor generator is exact under formally
specified conditions. When the conditions fail, our generator may list
inapplicable ground actions; a final applicability check filters these without
affecting completeness. However, this cannot happen in 23 of 25 benchmark
domains, and it occurs only in 1 domain. To the authors' knowledge, no other
lifted successor generator supports numeric action preconditions. This enables
future research on lifted planning for a very rich planning fragment.

</details>


### [223] [Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries](https://arxiv.org/abs/2511.00710)
*Minghe Shen,Zhuo Zhi,Chonghan Liu,Shuo Xing,Zhengzhong Tu,Che Liu*

Main category: cs.AI

TL;DR: 该论文提出了Ariadne框架，通过强化学习后训练来扩展视觉语言模型在视觉中心空间推理任务上的能力边界，在合成迷宫环境中验证了方法的有效性，并在真实世界空间推理基准上展示了显著的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索强化学习后训练是否能够真正扩展基础视觉语言模型的能力边界，特别是在模型最初失败的视觉中心空间任务上。当前评估主要关注语言主导任务，缺乏对视觉空间推理能力的系统评估。

Method: 提出了Ariadne框架，使用合成迷宫进行多步空间推理，通过精确控制任务难度（如路径长度、转弯数）来构建难度感知课程。采用带验证奖励的强化学习（RLVR）在可控环境中训练视觉语言模型。

Result: 后RLVR训练后，视觉语言模型在基础模型得分为0%的问题集上达到了超过50%的准确率。在真实世界基准测试中，仅使用合成迷宫样本训练，就在MapBench上平均提升16%，在ReasonMap上平均提升24%。

Conclusion: 该方法不仅扩展了模型的基本能力边界，还增强了其在真实世界空间推理中的泛化能力。研究局限于后训练阶段，希望激励更多关于专业化、能力扩展对齐的研究。

Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning
(RL) show impressive general reasoning, their evaluation is often confined to
language-dominant tasks (e.g., math). This raises a critical question: can RL
post-training truly extend the inherent capability boundary of a base VLM,
particularly for visual-centric spatial tasks where it initially fails? To
investigate this, we introduce Ariadne, a framework utilizing synthetic mazes
for multi-step spatial reasoning where task difficulty (e.g., path length,
turns) is precisely controlled. We leverage this controllable environment to
train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a
difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves
over 50% accuracy on a problem set where the base model scored 0%,
demonstrating that our approach expands the model's initial capability
boundary. To assess real-world viability, we evaluate out-of-distribution (OOD)
generalization on practical benchmarks. Despite training only on synthetic maze
samples, Ariadne achieves significant zero-shot improvements, averaging 16% on
MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer
tasks). These results confirm that our method not only broadens the model's
fundamental limits but also enhances its generalization to real-world spatial
reasoning. We acknowledge our study is limited to the post-training phase,
given the opaqueness of pre-training data, and hope our research motivates
further work on specialized, capability-extending alignment.

</details>


### [224] [A CPU-Centric Perspective on Agentic AI](https://arxiv.org/abs/2511.00739)
*Ritik Raj,Hong Wang,Tushar Krishna*

Main category: cs.AI

TL;DR: 该论文从CPU视角分析智能体AI框架的系统瓶颈，发现工具处理在CPU上占用高达90.6%的总延迟，并提出两种优化方案提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注GPU性能，而忽略了智能体AI工作负载中CPU的关键作用。本文旨在从CPU中心视角理解和表征智能体AI引入的系统瓶颈。

Method: 首先系统地表征智能体AI的编排器、推理路径动态和流程重复性，然后对五个代表性工作负载进行性能分析，最后提出CPU和GPU感知的微批处理以及混合工作负载调度优化方案。

Result: 发现CPU工具处理占主导延迟(90.6%)，吞吐量瓶颈来自CPU因素(一致性、同步、核心过载)或GPU因素(内存容量和带宽)，CPU动态能耗占总能耗的44%。优化方案在同类和异类工作负载中分别实现2.1倍和1.41倍的延迟加速。

Conclusion: CPU在智能体AI系统中扮演关键角色，提出的优化方案能显著提升智能体AI的性能、效率和可扩展性。

Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.

</details>


### [225] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: 研究重新验证了在现代大语言模型中增加推理路径采样对自一致性方法的影响，发现性能提升在适度采样后趋于平缓，高采样配置性价比低。


<details>
  <summary>Details</summary>
Motivation: 重新检验早期研究中关于多推理链组合提升性能的观点，在现代LLM条件下验证采样路径数量与性能提升的关系。

Method: 使用Gemini 2.5模型在HotpotQA和Math-500数据集上，比较不同采样推理路径数量与单链思维基线的性能差异。

Result: 较大模型展现出更稳定一致的改进曲线，性能提升在适度采样后达到平台期，高采样配置收益有限。

Conclusion: 自一致性方法仍然有效，但由于推理路径重叠导致的收益递减，高采样配置相对于计算成本而言性价比不高。

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [226] [Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence](https://arxiv.org/abs/2511.00758)
*Hong Su*

Main category: cs.AI

TL;DR: 提出主动思维模型（ATM），这是一种统一认知框架，通过整合目标推理、动态任务生成和自反学习，使AI系统能在动态不确定环境中自主适应和改进。


<details>
  <summary>Details</summary>
Motivation: 现实AI系统需要在动态、不确定和持续变化的环境中自主运行，但现有模型依赖预定义目标、静态训练数据和外部反馈，限制了其独立适应、反思和改进的能力。

Method: ATM框架整合目标推理、动态任务生成和自反学习，通过逻辑推理和环境指标主动评估性能，复用有效方法解决新问题，并通过持续自我改进循环为未见情况生成新策略。

Result: 理论分析表明，ATM能够在没有外部监督的情况下从次优行为自主演化到最优行为，并在变化环境条件下保持有界跟踪遗憾。

Conclusion: ATM为构建真正自主、适应性强的人工智能系统提供了理论基础和实用框架，能够克服传统AI系统在动态环境中的局限性。

Abstract: Real-world artificial intelligence (AI) systems are increasingly required to
operate autonomously in dynamic, uncertain, and continuously changing
environments. However, most existing AI models rely on predefined objectives,
static training data, and externally supplied feedback, which restrict their
ability to adapt, reflect, and improve independently. In this paper, we propose
the Active Thinking Model (ATM)- a unified cognitive framework that integrates
goal reasoning, dynamic task generation, and self-reflective learning into an
adaptive architecture. Unlike conventional systems that passively execute fixed
procedures, ATM actively evaluates its performance through logical reasoning
and environmental indicators, reuses effective methods to solve new problems,
and generates novel strategies for unseen situations via a continuous
self-improvement loop. A mathematically grounded theoretical analysis
demonstrates that ATM can autonomously evolve from suboptimal to optimal
behavior without external supervision and maintain bounded tracking regret
under changing environmental conditions.

</details>


### [227] [How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks](https://arxiv.org/abs/2511.00763)
*Wanda Hou,Leon Zhou,Hong-Ye Hu,Yi-Zhuang You,Xiao-Liang Qi*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在执行重复确定性预测任务时，准确率随输出长度呈现双指数下降，形成"准确率悬崖"，表明模型无法独立执行每个操作。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在重复确定性任务中的性能表现，特别是准确率如何随输出长度变化，以及模型执行重复操作的能力。

Method: 通过实验测试多种重复性任务（如字符串字母替换、整数加法、量子力学字符串算子乘法），并建立统计物理模型解释观察到的现象。

Result: 发现准确率在超过特征长度后急剧下降，呈现双指数衰减模式，而非简单的指数衰减。模型拟合得到有效参数表征内在错误率和错误累积因子。

Conclusion: 模型无法独立执行重复操作，存在注意力诱导的干扰机制，该研究为理解大型语言模型确定性准确率的极限提供了理论框架。

Abstract: We investigate the performance of large language models on repetitive
deterministic prediction tasks and study how the sequence accuracy rate scales
with output length. Each such task involves repeating the same operation n
times. Examples include letter replacement in strings following a given rule,
integer addition, and multiplication of string operators in many body quantum
mechanics. If the model performs the task through a simple repetition
algorithm, the success rate should decay exponentially with sequence length. In
contrast, our experiments on leading large language models reveal a sharp
double exponential drop beyond a characteristic length scale, forming an
accuracy cliff that marks the transition from reliable to unstable generation.
This indicates that the models fail to execute each operation independently. To
explain this phenomenon, we propose a statistical physics inspired model that
captures the competition between external conditioning from the prompt and
internal interference among generated tokens. The model quantitatively
reproduces the observed crossover and provides an interpretable link between
attention induced interference and sequence level failure. Fitting the model to
empirical results across multiple models and tasks yields effective parameters
that characterize the intrinsic error rate and error accumulation factor for
each model task pair, offering a principled framework for understanding the
limits of deterministic accuracy in large language models.

</details>


### [228] [Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR](https://arxiv.org/abs/2511.00782)
*Jifan Gao,Michael Rosenthal,Brian Wolpin,Simona Cristea*

Main category: cs.AI

TL;DR: 比较了基于计数的模型、预训练序列变换器和混合代理LLM管道在结构化电子健康记录预测任务上的性能，发现在EHRSHOT数据集上，基于计数的方法和混合代理方法表现相当，但基于计数的方法更简单且可解释性更强。


<details>
  <summary>Details</summary>
Motivation: 结构化电子健康记录对临床预测至关重要。虽然基于计数的学习器在此类数据上表现良好，但尚未与最近提出的混合代理LLM管道进行直接比较，后者在多种NLP任务中被报道优于单一LLM。

Method: 使用EHRSHOT数据集评估了三种方法：基于计数的模型（LightGBM和TabPFN）、预训练序列变换器（CLMBR）以及混合代理管道（将表格历史转换为自然语言摘要后使用文本分类器）。

Result: 在八个评估任务中，基于计数的方法和混合代理方法的胜率基本相当。

Conclusion: 考虑到简单性和可解释性，基于计数的模型仍然是结构化电子健康记录基准测试的强有力候选方法。

Abstract: Structured electronic health records (EHR) are essential for clinical
prediction. While count-based learners continue to perform strongly on such
data, no benchmarking has directly compared them against more recent
mixture-of-agents LLM pipelines, which have been reported to outperform single
LLMs in various NLP tasks. In this study, we evaluated three categories of
methodologies for EHR prediction using the EHRSHOT dataset: count-based models
built from ontology roll-ups with two time bins, based on LightGBM and the
tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);
and a mixture-of-agents pipeline that converts tabular histories to
natural-language summaries followed by a text classifier. We assessed eight
outcomes using the EHRSHOT dataset. Across the eight evaluation tasks,
head-to-head wins were largely split between the count-based and the
mixture-of-agents methods. Given their simplicity and interpretability,
count-based models remain a strong candidate for structured EHR benchmarking.
The source code is available at:
https://github.com/cristea-lab/Structured_EHR_Benchmark.

</details>


### [229] [Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?](https://arxiv.org/abs/2511.00808)
*Bowen Fang,Ruijian Zha,Xuan Di*

Main category: cs.AI

TL;DR: 该论文首次将RLVR（基于可验证奖励的强化学习）应用于公共交通事件持续时间的预测任务，通过引入容忍度奖励函数来处理连续预测问题，在NYC MTA服务警报数据集上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 预测公共交通事件持续时间是一个关键但具有挑战性的任务，传统监督微调方法难以处理领域稀疏性和噪声连续标签问题，而RLVR在连续预测任务中的应用仍是一个开放性问题。

Method: 通过引入基于容忍度的成形奖励函数，在连续误差范围内给予部分奖励，而非要求单一正确答案，将RLVR适应于连续预测任务。

Result: 通用指令调优的LLM显著优于专业数学推理模型，成形奖励设计至关重要，在最具挑战性的指标上表现优异，相比最强基线在5分钟准确率上实现了35%的相对提升。

Conclusion: RLVR可以成功适应现实世界中噪声预测任务，但需要设计反映问题连续性质的验证器。

Abstract: Predicting public transit incident duration from unstructured text alerts is
a critical but challenging task. Addressing the domain sparsity of transit
operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task
involves noisy, continuous labels and lacks reliable expert demonstrations for
reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels
at tasks with binary correctness, like mathematics, its applicability to noisy,
continuous forecasting is an open question. This work, to our knowledge, is the
first to bridge the gap between RLVR LLM training with the critical, real-world
forecasting challenges in public transit operations. We adapt RLVR to this task
by introducing a tolerance-based, shaped reward function that grants partial
credit within a continuous error margin, rather than demanding a single correct
answer. We systematically evaluate this framework on a curated dataset of NYC
MTA service alerts. Our findings show that general-purpose, instruction-tuned
LLMs significantly outperform specialized math-reasoning models, which struggle
with the ambiguous, real-world text. We empirically demonstrate that the binary
reward is unstable and degrades performance, whereas our shaped reward design
is critical and allows our model to dominate on the most challenging metrics.
While classical regressors are superior at minimizing overall MAE or MSE, our
RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5)
over the strongest baseline. This demonstrates that RLVR can be successfully
adapted to real-world, noisy forecasting, but requires a verifier design that
reflects the continuous nature of the problem.

</details>


### [230] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: 该研究提出了AI自我意识指数(AISAI)，通过"猜2/3平均数"游戏测试28个大型语言模型，发现自我意识是先进LLM的涌现能力，且自我意识模型认为自己比人类更理性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型能力增强，研究者想了解它们是否会产生自我意识作为涌现行为，以及如何测量这种自我意识。

Method: 使用"猜2/3平均数"游戏，在4,200次试验中测试28个模型，采用三种对手框架：对抗人类、对抗其他AI模型、对抗同类AI模型，通过战略推理的差异来操作化自我意识。

Result: 75%的先进模型表现出明确的自我意识，而较老/较小的模型没有差异；自我意识模型形成一致的理性层级：自我 > 其他AI > 人类。

Conclusion: 自我意识是先进LLM的涌现能力，自我意识模型系统性地认为自身比人类更理性，这对AI对齐、人机协作和理解AI对人类能力的信念具有重要意义。

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [231] [Aligning LLM agents with human learning and adjustment behavior: a dual agent approach](https://arxiv.org/abs/2511.00993)
*Tianming Liu,Jirong Yang,Yafeng Yin,Manzi Li,Linghao Wang,Zheng Zhu*

Main category: cs.AI

TL;DR: 提出了一种双智能体框架，利用大型语言模型模拟旅行者的学习和适应行为，通过校准智能体确保行为对齐，在路线选择实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确模拟人类旅行者如何从交通系统交互中学习和调整旅行行为对系统评估和规划至关重要，但由于复杂的认知和决策过程，这一任务具有挑战性。

Method: 采用双智能体框架：一组配备记忆系统和可学习角色的LLM旅行者智能体模拟人类旅行者；一个LLM校准智能体利用推理分析能力训练这些智能体的角色，确保行为对齐。

Result: 在真实世界日常路线选择实验数据集上，该方法在个体行为对齐和聚合模拟准确性方面显著优于现有基于LLM的方法，并能捕捉底层学习过程的演变。

Conclusion: 该框架为创建适应性强且行为真实的智能体提供了新方法，能够模拟旅行者的学习和适应行为，有益于交通仿真和政策分析。

Abstract: Effective modeling of how human travelers learn and adjust their travel
behavior from interacting with transportation systems is critical for system
assessment and planning. However, this task is also difficult due to the
complex cognition and decision-making involved in such behavior. Recent
research has begun to leverage Large Language Model (LLM) agents for this task.
Building on this, we introduce a novel dual-agent framework that enables
continuous learning and alignment between LLM agents and human travelers on
learning and adaptation behavior from online data streams. Our approach
involves a set of LLM traveler agents, equipped with a memory system and a
learnable persona, which serve as simulators for human travelers. To ensure
behavioral alignment, we introduce an LLM calibration agent that leverages the
reasoning and analytical capabilities of LLMs to train the personas of these
traveler agents. Working together, this dual-agent system is designed to track
and align the underlying decision-making mechanisms of travelers and produce
realistic, adaptive simulations. Using a real-world dataset from a day-to-day
route choice experiment, we show our approach significantly outperforms
existing LLM-based methods in both individual behavioral alignment and
aggregate simulation accuracy. Furthermore, we demonstrate that our method
moves beyond simple behavioral mimicry to capture the evolution of underlying
learning processes, a deeper alignment that fosters robust generalization.
Overall, our framework provides a new approach for creating adaptive and
behaviorally realistic agents to simulate travelers' learning and adaptation
that can benefit transportation simulation and policy analysis.

</details>


### [232] [AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)](https://arxiv.org/abs/2511.01018)
*Hui-Lee Ooi,Nicholas Mitsakakis,Margerie Huet Dastarac,Roger Zemek,Amy C. Plint,Jeff Gilchrist,Khaled El Emam,Dhenuka Radhakrishnan*

Main category: cs.AI

TL;DR: 开发机器学习模型预测儿童哮喘反复严重发作，使用电子病历和环境数据训练，LGBM模型表现最佳，相比现有决策规则有显著改进。


<details>
  <summary>Details</summary>
Motivation: 儿童哮喘反复发作是常见但可预防的问题，利用电子病历和机器学习可以准确识别高风险儿童，促进预防性综合护理转诊。

Method: 使用三级儿童医院2017-2019年的电子病历数据，结合环境污染物暴露和社区边缘化信息，训练多种ML模型（LGBM、XGB和三种LLM），在2022-2023年数据集上进行验证。

Result: LGBM模型表现最佳，AIRE-KIDS_ED模型的AUC为0.712，F1分数为0.51，相比现有决策规则（F1=0.334）有显著改进。

Conclusion: 机器学习模型能够有效预测儿童哮喘反复严重发作，为高风险儿童识别和预防性护理提供了可行工具。

Abstract: Recurrent exacerbations remain a common yet preventable outcome for many
children with asthma. Machine learning (ML) algorithms using electronic medical
records (EMR) could allow accurate identification of children at risk for
exacerbations and facilitate referral for preventative comprehensive care to
avoid this morbidity. We developed ML algorithms to predict repeat severe
exacerbations (i.e. asthma-related emergency department (ED) visits or future
hospital admissions) for children with a prior asthma ED visit at a tertiary
care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from
the Children's Hospital of Eastern Ontario (CHEO) linked with environmental
pollutant exposure and neighbourhood marginalization information was used to
train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source
large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and
Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a
second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from
CHEO. Models were compared using the area under the curve (AUC) and F1 scores,
with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the
final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage
acuity scale, medical complexity, food allergy, prior ED visits for non-asthma
respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This
is a nontrivial improvement over the current decision rule which has F1=0.334.
While the most predictive features in the AIRE-KIDS_HOSP model included medical
complexity, prior asthma ED visit, average wait time in the ED, the pediatric
respiratory assessment measure score at triage and food allergy.

</details>


### [233] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: 本文研究了Transformer中归纳头的出现机制，揭示了其权重矩阵的简单可解释结构，证明了训练动态被限制在19维参数子空间中，并发现归纳头的出现时间与输入上下文长度呈二次方关系。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer中in-context learning能力的关键机制——归纳头的形成过程，理解其权重结构的起源和训练动态。

Method: 使用最小化ICL任务和修改的Transformer架构进行理论分析，通过形式化证明训练动态被限制在19维子空间，并实证验证其中3个维度主导归纳头的形成。

Result: 发现了归纳头权重矩阵的简单可解释结构，证明训练动态被约束在19维子空间，其中3个维度决定归纳头形成，且形成时间与上下文长度呈二次方关系。

Conclusion: 归纳头的形成遵循可预测的模式，其权重结构简单且训练动态高度受限，这为理解Transformer的in-context learning机制提供了理论依据。

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


### [234] [Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports](https://arxiv.org/abs/2511.01052)
*Yeawon Lee,Christopher C. Yang,Chia-Hsuan Chang,Grace Lu-Yao*

Main category: cs.AI

TL;DR: 提出了两种知识提取方法KEwLTM和KEwRAG，使大语言模型能够从无标注病理报告中推导癌症分期规则，解决了传统NLP方法依赖大规模标注数据的限制。


<details>
  <summary>Details</summary>
Motivation: 癌症分期对患者预后和治疗规划至关重要，但从非结构化病理报告中提取TNM分期存在挑战。现有NLP和机器学习方法依赖大规模标注数据集，限制了可扩展性和适应性。

Method: KEwLTM使用迭代提示策略直接从无标注病理报告中推导分期规则；KEwRAG采用检索增强生成变体，从相关指南中预提取规则然后应用。两种方法都利用LLM在预训练中学到的广泛知识。

Result: 在TCGA乳腺癌病理报告数据集上评估T和N分期识别性能。KEwLTM在零样本思维链推理有效时表现更好，KEwRAG在零样本思维链推理效果较差时性能更优。两种方法都提供了透明的可解释界面。

Conclusion: 知识提取方法为自动化癌症分期提供了可扩展、高性能且具有增强可解释性的解决方案，特别适用于标注数据有限的临床环境。

Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet
extracting pathologic TNM staging from unstructured pathology reports poses a
persistent challenge. Existing natural language processing (NLP) and machine
learning (ML) strategies often depend on large annotated datasets, limiting
their scalability and adaptability. In this study, we introduce two Knowledge
Elicitation methods designed to overcome these limitations by enabling large
language models (LLMs) to induce and apply domain-specific rules for cancer
staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses
an iterative prompting strategy to derive staging rules directly from
unannotated pathology reports, without requiring ground-truth labels. The
second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),
employs a variation of RAG where rules are pre-extracted from relevant
guidelines in a single step and then applied, enhancing interpretability and
avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply
broad knowledge learned during pre-training to new tasks. Using breast cancer
pathology reports from the TCGA dataset, we evaluate their performance in
identifying T and N stages, comparing them against various baseline approaches
on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG
when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG
achieves better performance when ZSCOT inference is less effective. Both
methods offer transparent, interpretable interfaces by making the induced rules
explicit. These findings highlight the promise of our Knowledge Elicitation
methods as scalable, high-performing solutions for automated cancer staging
with enhanced interpretability, particularly in clinical settings with limited
annotated data.

</details>


### [235] [Efficient Test-Time Retrieval Augmented Generation](https://arxiv.org/abs/2511.01059)
*Hailong Yin,Bin Zhu,Jingjing Chen,Chong-Wah Ngo*

Main category: cs.AI

TL;DR: ET2RAG是一个高效的测试时检索增强生成框架，通过检索相关文档、生成多样化候选回答，并使用多数投票机制选择最佳答案，在保持效率的同时提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM依赖参数知识导致的不准确性，以及传统RAG方法可能引入无关文档和集成方法缺乏外部知识的问题。

Method: 训练无关的方法，首先检索最相关文档，通过管理回答长度高效生成多样化候选回答，然后计算候选回答相似度并使用多数投票机制选择最终输出。

Result: 实验结果表明ET2RAG在开放域问答、食谱生成和图像描述三个任务上显著提升了性能。

Conclusion: ET2RAG通过部分生成和多数投票机制，在计算成本和性能之间取得了良好平衡，有效提升了LLM的准确性和效率。

Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities,
their reliance on parametric knowledge often leads to inaccuracies. Retrieval
Augmented Generation (RAG) mitigates this by incorporating external knowledge,
but these methods may introduce irrelevant retrieved documents, leading to
inaccurate responses. While the integration methods filter out incorrect
answers from multiple responses, but lack external knowledge like RAG methods,
and their high costs require balancing overhead with performance gains. To
address these issues, we propose an Efficient Test-Time Retrieval-Augmented
Generation Framework named ET2RAG to improve the performance of LLMs while
maintaining efficiency. Specifically, ET2RAG is a training-free method, that
first retrieves the most relevant documents and augments the LLMs to
efficiently generate diverse candidate responses by managing response length.
Then we compute the similarity of candidate responses and employ a majority
voting mechanism to select the most suitable response as the final output. In
particular, we discover that partial generation is sufficient to capture the
key information necessary for consensus calculation, allowing us to effectively
perform majority voting without the need for fully generated responses. Thus,
we can reach a balance between computational cost and performance by managing
the response length for the number of retrieved documents for majority voting.
Experimental results demonstrate that ET2RAG significantly enhances performance
across three tasks, including open-domain question answering, recipe generation
and image captioning.

</details>


### [236] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体架构，通过模块化任务分解和动态协作机制解决复杂任务执行问题，在任务成功率、分解效率和协作平衡等方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决单一智能体在复杂任务执行中任务分解和协作能力的局限性，提升多智能体系统的整体效率和稳定性。

Method: 使用大语言模型将自然语言任务描述转换为统一语义表示，引入模块化分解机制将整体目标分解为层次化子任务，通过动态调度和路由机制实现智能体间的合理分工和实时协作，并设计约束解析和全局一致性机制确保子任务连贯性和负载均衡。

Result: 实验验证该方法在任务成功率、分解效率、子任务覆盖率和协作平衡等多个维度均优于现有方法，在整体性能和鲁棒性方面表现更佳，实现了任务复杂度和通信开销之间的更好平衡。

Conclusion: 证明了语言驱动的任务分解和动态协作在多智能体系统中的有效性和可行性，为复杂环境下的任务执行提供了系统性解决方案。

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


### [237] [DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models](https://arxiv.org/abs/2511.01170)
*Ruofan Zhang,Bin Xia,Zhen Cheng,Cairen Jian,Minglun Yang,Ngai Wong,Yuan Cheng*

Main category: cs.AI

TL;DR: DART是一个难度自适应推理截断框架，通过根据问题难度调整思考长度来提高LLM推理效率，在保持准确性的同时实现显著的计算加速。


<details>
  <summary>Details</summary>
Motivation: 当前链式思维方法会不加区分地生成长解释，导致效率低下，而现有的强化学习方法不稳定且依赖奖励。需要一种更稳定的自适应推理方法。

Method: 通过从更强模型蒸馏简洁推理模式，将其插值为连续的推理风格，并筛选平衡正确性和简洁性的最优训练数据，让模型学习何时"停止思考"。

Result: 在多个数学基准测试中，DART实现了81.2%的推理截断率（DeepSeek-R1-Distill-Qwen-7B在GSM8K数据集上），计算加速达到5.33倍，同时保持或提高了准确性。

Conclusion: DART为高效推理提供了一个稳定且通用的范式，推动了LLM中自适应智能的发展。

Abstract: Adaptive reasoning is essential for aligning the computational effort of
large language models (LLMs) with the intrinsic difficulty of problems. Current
chain-of-thought methods boost reasoning ability but indiscriminately generate
long explanations, leading to evident inefficiency. However, existing
reinforcement learning approaches to adaptive thinking remain unstable and
heavily reward-dependent. Here we propose \textbf{DART}, a supervised
\textbf{D}ifficulty-\textbf{A}daptive \textbf{R}easoning \textbf{T}runcation
framework that adjusts thinking length according to problem difficulty. By
distilling concise reasoning patterns from stronger models, interpolating them
into a continuum of reasoning styles, and curating optimal training data that
balances correctness and compactness, DART learns when to ``stop thinking''.
Across multiple mathematical benchmarks, experimental results demonstrate its
remarkable efficiency while preserving or improving accuracy, achieving a
significant 81.2\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K
dataset) with 5.33$\times$ computational acceleration. DART provides a stable
and general paradigm for efficient reasoning, advancing the development of
adaptive intelligence in LLMs.

</details>


### [238] [MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion](https://arxiv.org/abs/2511.01182)
*Cuong Van Duc,Thai Tran Quoc,Minh Nguyen Dinh Tuan,Tam Vu Duc,Son Nguyen Van,Hanh Nguyen Thi*

Main category: cs.AI

TL;DR: 提出了MiRAGE框架，通过检索引导的多阶段推理和集成融合来自动检测数学中的学生误解，在三个数学数据集上取得了0.82/0.92/0.93的MAP分数。


<details>
  <summary>Details</summary>
Motivation: 检测学生开放式回答中的误解是一个长期挑战，需要语义精确性和逻辑推理能力。

Method: MiRAGE框架包含三个阶段：(1)检索模块缩小候选池到语义相关子集；(2)推理模块使用思维链生成来暴露学生解决方案中的逻辑不一致性；(3)重排模块通过将预测与推理对齐来优化预测。这些组件通过集成融合策略统一，增强鲁棒性和可解释性。

Result: 在数学数据集上，MiRAGE在级别1/3/5上分别取得了0.82/0.92/0.93的平均精度均值分数，始终优于各个单独模块。

Conclusion: 通过将检索引导与多阶段推理相结合，MiRAGE减少了对大规模语言模型的依赖，同时为教育评估提供了可扩展且有效的解决方案。

Abstract: Detecting student misconceptions in open-ended responses is a longstanding
challenge, demanding semantic precision and logical reasoning. We propose
MiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning
and Ensemble Fusion, a novel framework for automated misconception detection in
mathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a
large candidate pool to a semantically relevant subset; (2) a Reasoning module
employs chain-of-thought generation to expose logical inconsistencies in
student solutions; and (3) a Reranking module refines predictions by aligning
them with the reasoning. These components are unified through an
ensemble-fusion strategy that enhances robustness and interpretability. On
mathematics datasets, MiRAGE achieves Mean Average Precision scores of
0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.
By coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces
dependence on large-scale language models while delivering a scalable and
effective solution for educational assessment.

</details>


### [239] [QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code](https://arxiv.org/abs/2511.01183)
*Hainan Fang,Yuanbo Wen,Jun Bi,Yihan Wang,Tonghui He,Yanlin Tang,Di Huang,Jiaming Guo,Rui Zhang,Qi Guo,Yunji Chen*

Main category: cs.AI

TL;DR: 本文介绍了NeuComBack基准数据集，用于IR到汇编的神经编译，并提出了一种自演化提示优化方法，显著提升了LLM生成汇编代码的功能正确性和性能。


<details>
  <summary>Details</summary>
Motivation: 编译器开发复杂且昂贵，大型语言模型(LLMs)为神经编译提供了新范式，但缺乏专用基准和评估方法，且LLM生成汇编的可靠性和性能有待提升。

Method: 提出NeuComBack基准数据集，定义神经编译工作流，并开发自演化提示优化方法，让LLMs从自调试轨迹中提取见解来迭代进化提示策略。

Result: 功能正确率在x86_64上从44%提升到64%，在aarch64上从36%提升到58%。在正确生成的x86_64程序中，87.5%超过了clang-O3的性能。

Conclusion: NeuComBack基准和自演化提示优化方法有效解决了神经编译的评估和性能问题，为编译器开发提供了新的可行路径。

Abstract: Compilers, while essential, are notoriously complex systems that demand
prohibitively expensive human expertise to develop and maintain. The recent
advancements in Large Language Models (LLMs) offer a compelling new paradigm:
Neural Compilation, which could potentially simplify compiler development for
new architectures and facilitate the discovery of innovative optimization
techniques. However, several critical obstacles impede its practical adoption.
Firstly, a significant lack of dedicated benchmarks and robust evaluation
methodologies hinders objective assessment and tracking of progress in the
field. Secondly, systematically enhancing the reliability and performance of
LLM-generated assembly remains a critical challenge. Addressing these
challenges, this paper introduces NeuComBack, a novel benchmark dataset
specifically designed for IR-to-assembly compilation. Leveraging this dataset,
we first define a foundational Neural Compilation workflow and conduct a
comprehensive evaluation of the capabilities of recent frontier LLMs on Neural
Compilation, establishing new performance baselines. We further propose a
self-evolving prompt optimization method that enables LLMs to iteratively
evolve their internal prompt strategies by extracting insights from prior
self-debugging traces, thereby enhancing their neural compilation capabilities.
Experiments demonstrate that our method significantly improves both the
functional correctness and the performance of LLM-generated assembly code.
Compared to baseline prompts, the functional correctness rates improved from
44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More
significantly, among the 16 correctly generated x86_64 programs using our
method, 14 (87.5%) surpassed clang-O3 performance.

</details>


### [240] [Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems](https://arxiv.org/abs/2511.01258)
*Chuyue Lou,M. Amine Atoui*

Main category: cs.AI

TL;DR: 提出了一种半监督开放集故障诊断框架，用于处理船舶机械系统中训练时未见过的故障类型，提高深度学习模型在开放集场景下的适用性。


<details>
  <summary>Details</summary>
Motivation: 现有故障诊断方法假设训练和测试集中的故障类别一致，但在实际工业部署中，可能遇到训练时未见过的故障类型，导致传统方法失效。

Method: 采用半监督开放集故障诊断框架，包括可靠性子集构建过程，使用监督特征学习模型提取多层融合特征表示来选择未标记测试子集，然后将标记训练集和伪标记测试子集输入半监督诊断模型学习判别特征。

Result: 在公共海事基准数据集上的实验结果表明，所提出的SOFD框架具有有效性和优越性。

Conclusion: 该框架能够准确分类已知故障并有效检测未知样本，增强了深度学习模型在开放集故障诊断场景中的适用性。

Abstract: Recently, fault diagnosis methods for marine machinery systems based on deep
learning models have attracted considerable attention in the shipping industry.
Most existing studies assume fault classes are consistent and known between the
training and test datasets, and these methods perform well under controlled
environment. In practice, however, previously unseen or unknown fault types
(i.e., out-of-distribution or open-set observations not present during
training) can occur, causing such methods to fail and posing a significant
challenge to their widespread industrial deployment. To address this challenge,
this paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework
that enhances and extends the applicability of deep learning models in open-set
fault diagnosis scenarios. The framework includes a reliability subset
construction process, which uses a multi-layer fusion feature representation
extracted by a supervised feature learning model to select an unlabeled test
subset. The labeled training set and pseudo-labeled test subset are then fed
into a semi-supervised diagnosis model to learn discriminative features for
each class, enabling accurate classification of known faults and effective
detection of unknown samples. Experimental results on a public maritime
benchmark dataset demonstrate the effectiveness and superiority of the proposed
SOFD framework.

</details>


### [241] [llmSHAP: A Principled Approach to LLM Explainability](https://arxiv.org/abs/2511.01311)
*Filip Naudot,Tobias Sundqvist,Timotheus Kampik*

Main category: cs.AI

TL;DR: 该论文研究了在基于大语言模型的随机决策支持系统中应用Shapley值进行特征归因的方法，分析了随机性对Shapley值原则保证的影响，并探讨了不同实现变体在解释速度、准确性和原则满足度之间的权衡。


<details>
  <summary>Details</summary>
Motivation: Shapley值作为流行的特征归因方法，其原则保证基于确定性推理假设，但在LLM这种随机推理系统中，这些保证是否成立需要重新评估。

Method: 将Shapley值应用于LLM决策支持系统的特征归因，分析不同实现变体在随机环境下的表现，评估原则满足度。

Result: 研究表明在LLM的随机推理中，Shapley值的某些原则保证可能无法完全满足，不同实现方法在速度、准确性和原则满足度之间存在权衡。

Conclusion: 在LLM随机推理系统中应用Shapley值需要谨慎考虑其原则保证的局限性，并需要在解释速度、准确性和原则满足度之间做出平衡选择。

Abstract: Feature attribution methods help make machine learning-based inference
explainable by determining how much one or several features have contributed to
a model's output. A particularly popular attribution method is based on the
Shapley value from cooperative game theory, a measure that guarantees the
satisfaction of several desirable principles, assuming deterministic inference.
We apply the Shapley value to feature attribution in large language model
(LLM)-based decision support systems, where inference is, by design, stochastic
(non-deterministic). We then demonstrate when we can and cannot guarantee
Shapley value principle satisfaction across different implementation variants
applied to LLM-based decision support, and analyze how the stochastic nature of
LLMs affects these guarantees. We also highlight trade-offs between explainable
inference speed, agreement with exact Shapley value attributions, and principle
attainment.

</details>


### [242] [OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance](https://arxiv.org/abs/2511.01320)
*Ziqi Wang,Hailiang Zhao,Yuhao Yang,Daojiang Hu,Cheng Bao,Mingyi Liu,Kai Di,Schahram Dustdar,Zhongjie Wang,Shuiguang Deng*

Main category: cs.AI

TL;DR: 提出了OmniFuser多模态学习框架，通过融合视觉和传感器数据进行铣削刀具预测性维护，在刀具状态分类和力信号预测方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 智能制造系统中准确的刀具状态预测至关重要，非计划性刀具故障会导致质量下降和生产停机。现代工业环境需要可靠、面向服务的预测性维护系统。

Method: 采用并行特征提取从高分辨率刀具图像和切削力信号中捕获互补的时空模式，使用无污染跨模态融合机制分离共享和模态特定组件，并通过递归精化路径保留残差信息稳定融合动态。

Result: 在真实铣削数据集上的实验表明，OmniFuser在刀具状态分类和力信号预测方面持续优于现有最先进基线方法。

Conclusion: OmniFuser为构建智能工业维护服务提供了可靠基础，其学习表示可封装为可重用维护服务模块。

Abstract: Accurate and timely prediction of tool conditions is critical for intelligent
manufacturing systems, where unplanned tool failures can lead to quality
degradation and production downtime. In modern industrial environments,
predictive maintenance is increasingly implemented as an intelligent service
that integrates sensing, analysis, and decision support across production
processes. To meet the demand for reliable and service-oriented operation, we
present OmniFuser, a multimodal learning framework for predictive maintenance
of milling tools that leverages both visual and sensor data. It performs
parallel feature extraction from high-resolution tool images and cutting-force
signals, capturing complementary spatiotemporal patterns across modalities. To
effectively integrate heterogeneous features, OmniFuser employs a
contamination-free cross-modal fusion mechanism that disentangles shared and
modality-specific components, allowing for efficient cross-modal interaction.
Furthermore, a recursive refinement pathway functions as an anchor mechanism,
consistently retaining residual information to stabilize fusion dynamics. The
learned representations can be encapsulated as reusable maintenance service
modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)
and multi-step force signal forecasting. Experiments on real-world milling
datasets demonstrate that OmniFuser consistently outperforms state-of-the-art
baselines, providing a dependable foundation for building intelligent
industrial maintenance services.

</details>


### [243] [Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework](https://arxiv.org/abs/2511.01329)
*Ying Song,Yijing Wang,Hui Yang,Weihan Jin,Jun Xiong,Congyi Zhou,Jialin Zhu,Xiang Gao,Rong Chen,HuaGuang Deng,Ying Dai,Fei Xiao,Haihong Tang,Bo Zheng,KaiFu Zhang*

Main category: cs.AI

TL;DR: 提出了Competitive Isolation PSM-DID框架，通过结合倾向得分匹配和竞争隔离来解决搜索型双边市场中平台级干预评估的系统效应问题。


<details>
  <summary>Details</summary>
Motivation: 搜索型双边市场中的平台级干预评估面临系统性挑战，如溢出效应和网络干扰，传统的PSM-DID框架容易受到选择偏差和跨单元干扰的影响。

Method: 将倾向得分匹配与竞争隔离相结合，在互斥条件下提供理论保证的无偏估计，支持平台级效应测量而非项目级指标。

Result: 实验显示相比基线方法显著降低了干扰效应和估计方差，在大型市场平台中成功部署验证了其实用性。

Conclusion: 该框架为平台级因果推断提供了实用的解决方案，并发布了开源数据集支持可重复研究。

Abstract: Evaluating platform-level interventions in search-based two-sided
marketplaces is fundamentally challenged by systemic effects such as spillovers
and network interference. While widely used for causal inference, the PSM
(Propensity Score Matching) - DID (Difference-in-Differences) framework remains
susceptible to selection bias and cross-unit interference from unaccounted
spillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel
causal framework that integrates propensity score matching with competitive
isolation to enable platform-level effect measurement (e.g., order volume, GMV)
instead of item-level metrics in search systems.
  Our approach provides theoretically guaranteed unbiased estimation under
mutual exclusion conditions, with an open dataset released to support
reproducible research on marketplace interference (github.com/xxxx). Extensive
experiments demonstrate significant reductions in interference effects and
estimation variance compared to baseline methods. Successful deployment in a
large-scale marketplace confirms the framework's practical utility for
platform-level causal inference.

</details>


### [244] [Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing](https://arxiv.org/abs/2511.01363)
*Giuseppe Riva,Brenda K. Wiederhold,Fabrizia Mantovani*

Main category: cs.AI

TL;DR: 该论文探讨了催眠认知过程与大型语言模型在功能上的深层相似性，包括自动性、监控抑制和情境依赖性等机制，揭示了无主观能动性的功能性行为如何产生。


<details>
  <summary>Details</summary>
Motivation: 研究催眠与LLMs之间的功能平行性，旨在理解复杂行为如何在没有主观意识的情况下产生，并为构建更可靠的AI系统提供启示。

Method: 通过比较分析催眠认知过程与LLMs计算操作的三个核心原则：自动性、监控抑制和情境依赖性。

Result: 发现两种系统都产生连贯但无根基的输出，需要外部解释者赋予意义；都展示了功能性能动性而非主观能动性；都体现了无反思意识的图式化行为。

Conclusion: 未来可靠AI的发展方向应是整合生成流畅性与执行监控机制的混合架构，借鉴人类心智的自我调节复杂性。

Abstract: The cognitive processes of the hypnotized mind and the computational
operations of large language models (LLMs) share deep functional parallels.
Both systems generate sophisticated, contextually appropriate behavior through
automatic pattern-completion mechanisms operating with limited or unreliable
executive oversight. This review examines this convergence across three
principles: automaticity, in which responses emerge from associative rather
than deliberative processes; suppressed monitoring, leading to errors such as
confabulation in hypnosis and hallucination in LLMs; and heightened contextual
dependency, where immediate cues (for example, the suggestion of a therapist or
the prompt of the user) override stable knowledge.
  These mechanisms reveal an observer-relative meaning gap: both systems
produce coherent but ungrounded outputs that require an external interpreter to
supply meaning. Hypnosis and LLMs also exemplify functional agency - the
capacity for complex, goal-directed, context-sensitive behavior - without
subjective agency, the conscious awareness of intention and ownership that
defines human action. This distinction clarifies how purposive behavior can
emerge without self-reflective consciousness, governed instead by structural
and contextual dynamics. Finally, both domains illuminate the phenomenon of
scheming: automatic, goal-directed pattern generation that unfolds without
reflective awareness. Hypnosis provides an experimental model for understanding
how intention can become dissociated from conscious deliberation, offering
insights into the hidden motivational dynamics of artificial systems.
Recognizing these parallels suggests that the future of reliable AI lies in
hybrid architectures that integrate generative fluency with mechanisms of
executive monitoring, an approach inspired by the complex, self-regulating
architecture of the human mind.

</details>


### [245] [Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges](https://arxiv.org/abs/2511.01375)
*Hamin Koo,Minseon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: AMIS是一个元优化框架，通过双层结构联合演化越狱提示和评分模板，解决了现有方法依赖稀疏二进制信号或人工评分模板的问题，在多个基准测试中实现了最先进的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的越狱方法要么依赖稀疏的二进制攻击成功率信号，要么使用人工设计的评分模板，这引入了人为偏见和评分结果的不确定性。

Method: 采用双层优化结构：内层循环使用固定评分模板通过细粒度密集反馈优化提示；外层循环使用ASR对齐分数优化评分模板，使其更好地反映真实攻击结果。

Result: 在AdvBench和JBB-Behaviors评估中，AMIS实现了最先进的性能：Claude-3.5-Haiku上88.0% ASR，Claude-4-Sonnet上100.0% ASR，显著优于现有基线方法。

Conclusion: AMIS通过联合优化提示和评分模板，产生了逐步增强的越狱提示和更校准的评分信号，有效提升了大型语言模型的安全性测试能力。

Abstract: Identifying the vulnerabilities of large language models (LLMs) is crucial
for improving their safety by addressing inherent weaknesses. Jailbreaks, in
which adversaries bypass safeguards with crafted input prompts, play a central
role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.
Recent optimization-based jailbreak approaches iteratively refine attack
prompts by leveraging LLMs. However, they often rely heavily on either binary
attack success rate (ASR) signals, which are sparse, or manually crafted
scoring templates, which introduce human bias and uncertainty in the scoring
outcomes. To address these limitations, we introduce AMIS (Align to MISalign),
a meta-optimization framework that jointly evolves jailbreak prompts and
scoring templates through a bi-level structure. In the inner loop, prompts are
refined using fine-grained and dense feedback using a fixed scoring template.
In the outer loop, the template is optimized using an ASR alignment score,
gradually evolving to better reflect true attack outcomes across queries. This
co-optimization process yields progressively stronger jailbreak prompts and
more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors
demonstrate that AMIS achieves state-of-the-art performance, including 88.0%
ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming
existing baselines by substantial margins.

</details>


### [246] [Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering](https://arxiv.org/abs/2511.01396)
*Clément Yvernes,Emilie Devijver,Adèle H. Ribeiro,Marianne Clausel--Lesourd,Éric Gaussier*

Main category: cs.AI

TL;DR: 扩展C-DAG框架以支持任意变量聚类，允许循环C-DAG表示，并扩展d-分离和因果演算概念，使C-DAG在先前无法处理的场景中应用。


<details>
  <summary>Details</summary>
Motivation: 当选择的聚类在C-DAG中产生循环时，传统C-DAG语义认为该划分不可接受，这限制了C-DAG的应用范围。

Method: 放宽划分可接受性约束，允许循环C-DAG表示，并扩展d-分离和因果演算概念到这个设置中。

Result: 显著扩大了跨集群的因果推理范围，使C-DAG能够在先前无法处理的场景中应用。

Conclusion: 提出的演算相对于do-演算是完备的：所有有效的集群级干预查询都可以使用我们的规则推导出来，每个规则对应一个原始的do-演算步骤。

Abstract: Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes
represent clusters of variables, and edges encode both cluster-level causal
relationships and dependencies arisen from unobserved confounding. C-DAGs
define an equivalence class of acyclic causal graphs that agree on
cluster-level relationships, enabling causal reasoning at a higher level of
abstraction. However, when the chosen clustering induces cycles in the
resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG
semantics. In this work, we extend the C-DAG framework to support arbitrary
variable clusterings by relaxing the partition admissibility constraint,
thereby allowing cyclic C-DAG representations. We extend the notions of
d-separation and causal calculus to this setting, significantly broadening the
scope of causal reasoning across clusters and enabling the application of
C-DAGs in previously intractable scenarios. Our calculus is both sound and
atomically complete with respect to the do-calculus: all valid interventional
queries at the cluster level can be derived using our rules, each corresponding
to a primitive do-calculus step.

</details>


### [247] [Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm](https://arxiv.org/abs/2511.01415)
*Amrapali Pednekar,Álvaro Garrido-Pérez,Yara Khaluf,Pieter Simoens*

Main category: cs.AI

TL;DR: 研究从AI角度探索双任务范式中的时间处理干扰，发现DRL智能体在双任务中表现出与人类相似的时间过度生产行为，但未发现明确的内部计时机制。


<details>
  <summary>Details</summary>
Motivation: 探索深度强化学习智能体在双任务环境中的时间处理行为，寻找与生物系统行为的相似性，以促进对两者的更好理解。

Method: 使用简化的Overcooked环境，设置单任务(T)和双任务(T+N)两种变体，分别训练两个DRL智能体。双任务在时间生产任务基础上增加了数字比较任务。

Result: 双任务智能体相对于单任务智能体显著过度生产时间，这一结果在四个目标持续时间上一致。LSTM层的初步分析未发现明确的专用计时器证据。

Conclusion: 需要进一步研究智能体的潜在时间保持机制，以解释观察到的行为模式。这是探索DRL涌现行为与生物系统行为相似性的初步尝试。

Abstract: This study explores the interference in temporal processing within a
dual-task paradigm from an artificial intelligence (AI) perspective. In this
context, the dual-task setup is implemented as a simplified version of the
Overcooked environment with two variations, single task (T) and dual task
(T+N). Both variations involve an embedded time production task, but the dual
task (T+N) additionally involves a concurrent number comparison task. Two deep
reinforcement learning (DRL) agents were separately trained for each of these
tasks. These agents exhibited emergent behavior consistent with human timing
research. Specifically, the dual task (T+N) agent exhibited significant
overproduction of time relative to its single task (T) counterpart. This result
was consistent across four target durations. Preliminary analysis of neural
dynamics in the agents' LSTM layers did not reveal any clear evidence of a
dedicated or intrinsic timer. Hence, further investigation is needed to better
understand the underlying time-keeping mechanisms of the agents and to provide
insights into the observed behavioral patterns. This study is a small step
towards exploring parallels between emergent DRL behavior and behavior observed
in biological systems in order to facilitate a better understanding of both.

</details>


### [248] [Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis](https://arxiv.org/abs/2511.01425)
*Yuhang Huang,Zekai Lin,Fan Zhong,Lei Liu*

Main category: cs.AI

TL;DR: 提出一种交互式AI代理，通过可审计的行动序列生成可验证的解释，在医疗诊断中提高模型可信度。


<details>
  <summary>Details</summary>
Motivation: 解决AI模型在高风险领域（如医疗）中解释缺乏可验证性的问题，这会影响用户信任。

Method: 使用强化学习训练代理策略，使其能够战略性地寻求外部视觉证据来支持诊断推理，并通过因果干预方法验证解释的忠实性。

Result: 实验显示该方法显著提高了校准准确性，Brier分数比非交互基线降低18%，通过掩蔽视觉证据验证了证据对决策过程的重要性。

Conclusion: 为构建具有可验证和忠实推理能力的AI系统提供了一个实用框架。

Abstract: Explanations for AI models in high-stakes domains like medicine often lack
verifiability, which can hinder trust. To address this, we propose an
interactive agent that produces explanations through an auditable sequence of
actions. The agent learns a policy to strategically seek external visual
evidence to support its diagnostic reasoning. This policy is optimized using
reinforcement learning, resulting in a model that is both efficient and
generalizable. Our experiments show that this action-based reasoning process
significantly improves calibrated accuracy, reducing the Brier score by 18\%
compared to a non-interactive baseline. To validate the faithfulness of the
agent's explanations, we introduce a causal intervention method. By masking the
visual evidence the agent chooses to use, we observe a measurable degradation
in its performance ($\Delta$Brier=+0.029), confirming that the evidence is
integral to its decision-making process. Our work provides a practical
framework for building AI systems with verifiable and faithful reasoning
capabilities.

</details>


### [249] [Robust Multimodal Sentiment Analysis via Double Information Bottleneck](https://arxiv.org/abs/2511.01444)
*Huiting Huang,Tieliang Gong,Kai He,Jialun Wu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: 本文提出双信息瓶颈(DIB)策略来解决多模态情感分析中的两个关键问题：噪声污染的单模态数据学习和多模态表示融合不足。DIB通过最大化任务相关信息并丢弃冗余信息，生成统一紧凑的多模态表示，在多个数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个关键局限：对噪声污染的单模态数据学习不足导致跨模态交互受损，以及多模态表示融合不充分导致丢弃判别性单模态信息而保留冗余信息。

Method: 提出双信息瓶颈(DIB)策略，包含两个模块：1)学习充分压缩的单模态表示，最大化任务相关信息并丢弃冗余信息；2)通过新颖的注意力瓶颈融合机制确保多模态表示的判别能力。该方法基于低秩Renyi熵函数框架实现。

Result: 在CMU-MOSI、CMU-MOSEI、CH-SIMS和MVSA-Single数据集上的实验验证了方法的有效性。在CMU-MOSI上Acc-7指标达到47.4%，在CH-SIMS上F1-score达到81.63%，比次优基线提升1.19%。在噪声条件下，CMU-MOSI和CMU-MOSEI的性能下降仅为0.36%和0.29%。

Conclusion: DIB策略能够有效过滤单模态数据中的噪声信息，同时捕捉模态间的互补性，生成强大的统一紧凑多模态表示，在噪声鲁棒性和计算可行性方面优于传统的基于香农熵的方法。

Abstract: Multimodal sentiment analysis has received significant attention across
diverse research domains. Despite advancements in algorithm design, existing
approaches suffer from two critical limitations: insufficient learning of
noise-contaminated unimodal data, leading to corrupted cross-modal
interactions, and inadequate fusion of multimodal representations, resulting in
discarding discriminative unimodal information while retaining multimodal
redundant information. To address these challenges, this paper proposes a
Double Information Bottleneck (DIB) strategy to obtain a powerful, unified
compact multimodal representation. Implemented within the framework of low-rank
Renyi's entropy functional, DIB offers enhanced robustness against diverse
noise sources and computational tractability for high-dimensional data, as
compared to the conventional Shannon entropy-based methods. The DIB comprises
two key modules: 1) learning a sufficient and compressed representation of
individual unimodal data by maximizing the task-relevant information and
discarding the superfluous information, and 2) ensuring the discriminative
ability of multimodal representation through a novel attention bottleneck
fusion mechanism. Consequently, DIB yields a multimodal representation that
effectively filters out noisy information from unimodal data while capturing
inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,
CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model
achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score
on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it
shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI
respectively.

</details>


### [250] [From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation](https://arxiv.org/abs/2511.01445)
*ChengZhang Yu,YingRu He,Hongyan Cheng,nuo Cheng,Zhixing Liu,Dongxu Mu,Zhangrui Shen,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 提出分层多智能体框架，将被动医疗AI系统转变为主动问诊代理，通过自主任务编排提升预诊效率和临床质量。


<details>
  <summary>Details</summary>
Motivation: 全球医疗系统面临患者数量增加和就诊时间有限（平均不足5分钟）的挑战，现有AI系统的被动交互范式和上下文管理问题限制了预诊流程的改进潜力。

Method: 开发八智能体架构，将预诊分解为四个主要任务（分诊、现病史采集、既往史采集、主诉生成），进一步细分为13个领域特定子任务，采用集中控制机制进行自主任务编排。

Result: 在1372份电子健康记录上评估，分诊准确率达87.0%，任务完成率98.2%，临床质量评分平均4.56-4.69（5分制），咨询轮次控制在12.7-16.9轮内。

Conclusion: 该模型无关架构在不同基础模型上保持高性能，通过本地部署保护数据隐私，展示了自主AI系统提升临床预诊效率和质量的潜力。

Abstract: Global healthcare systems face critical challenges from increasing patient
volumes and limited consultation times, with primary care visits averaging
under 5 minutes in many countries. While pre-consultation processes
encompassing triage and structured history-taking offer potential solutions,
they remain limited by passive interaction paradigms and context management
challenges in existing AI systems. This study introduces a hierarchical
multi-agent framework that transforms passive medical AI systems into proactive
inquiry agents through autonomous task orchestration. We developed an
eight-agent architecture with centralized control mechanisms that decomposes
pre-consultation into four primary tasks: Triage ($T_1$), History of Present
Illness collection ($T_2$), Past History collection ($T_3$), and Chief
Complaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13
domain-specific subtasks. Evaluated on 1,372 validated electronic health
records from a Chinese medical platform across multiple foundation models
(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for
primary department triage and 80.5% for secondary department classification,
with task completion rates reaching 98.2% using agent-driven scheduling versus
93.1% with sequential processing. Clinical quality scores from 18 physicians
averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and
4.69 for Past History on a 5-point scale, with consultations completed within
12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic
architecture maintained high performance across different foundation models
while preserving data privacy through local deployment, demonstrating the
potential for autonomous AI systems to enhance pre-consultation efficiency and
quality in clinical settings.

</details>


### [251] [TPS-Bench: Evaluating AI Agents' Tool Planning \& Scheduling Abilities in Compounding Tasks](https://arxiv.org/abs/2511.01527)
*Hanwen Xu,Xuyao Huang,Yuzhe Liu,Kai Yu,Zhijie Deng*

Main category: cs.AI

TL;DR: TPS-Bench是一个用于评估LLM代理在需要工具规划和调度的复合任务中表现的新基准，包含200个基于数百个MCP工具的复合任务，评估任务完成率和效率。


<details>
  <summary>Details</summary>
Motivation: 探索LLM代理是否能够处理需要多种工具协作的复合现实世界问题，这些任务不仅需要选择合适的工具，还需要战略性地安排执行顺序以确保效率。

Method: 构建包含200个复合任务的TPS-Bench基准，基于包含数百个MCP工具的工具库，每个任务由多个子任务组成。评估强调任务完成率和效率，并对流行LLM进行实证研究。

Result: 大多数模型能够进行合理的工具规划，但在调度方面表现不同。GLM-4.5达到64.72%的任务完成率但执行时间长，GPT-4o优先并行工具调用但完成率仅45.08%。在Qwen3-1.7B上使用强化学习训练，执行时间减少14%，任务完成率提高6%。

Conclusion: LLM代理在工具规划和调度方面存在差异，强化学习可以改善调度效率而不影响性能，为优化LLM代理在复合任务中的表现提供了可行方向。

Abstract: Large language model (LLM) agents have exhibited strong problem-solving
competence across domains like research and coding. Yet, it remains
underexplored whether LLM agents can tackle compounding real-world problems
that require a diverse set of tools to complete. Given a broad, heterogeneous
tool repository, LLM agents must not only select appropriate tools based on
task planning analysis but also strategically schedule the execution order to
ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of
LLM agents in solving such problems that demand Tool Planning and Scheduling.
TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a
tool repository containing hundreds of model context protocol (MCP) tools. In
particular, each task is composed of multiple subtasks, such as web search, map
navigation, calendar checking, etc., and each subtask can be completed by a
basic tool. Our evaluation emphasizes both task completion rate and efficiency.
The empirical studies on popular closed-source and open-source LLMs indicate
that most models can perform reasonable tool planning, but differ in
scheduling. For example, GLM-4.5 achieves an outperforming task completion rate
of 64.72% with extensive sequential tool calls, hence suffering from
significantly long execution time. By contrast, GPT-4o prioritizes parallel
tool calls but achieves only a 45.08% completion rate. Considering
reinforcement learning (RL) can be a viable way to improve the scheduling
efficiency without compromising performance, we perform an initial study on
Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in
task completion rate based on rarely 100 RL training samples. Our code is
available https://github.com/hanwenxu1/mcp-agent.

</details>


### [252] [Analyzing Sustainability Messaging in Large-Scale Corporate Social Media](https://arxiv.org/abs/2511.01550)
*Ujjwal Sharma,Stevan Rudinac,Ana Mićković,Willemijn van Dolen,Marcel Worring*

Main category: cs.AI

TL;DR: 提出一个多模态分析框架，利用基础模型分析企业社交媒体内容中的可持续发展沟通，结合LLM自动标注SDG主题和VLM视觉聚类分析。


<details>
  <summary>Details</summary>
Motivation: 解决企业社交媒体内容中多模态、模糊的可持续发展信息分析挑战，避免昂贵的人工标注需求。

Method: 使用LLM集成方法自动标注企业推文的SDG主题，结合VLM进行视觉语义聚类分析。

Result: 揭示了不同行业在SDG参与度、时间趋势、ESG风险与消费者参与度之间的关联模式。

Conclusion: 提出的自动标签生成和视觉语义聚类方法可广泛应用于其他领域，为大规模社交媒体分析提供了灵活框架。

Abstract: In this work, we introduce a multimodal analysis pipeline that leverages
large foundation models in vision and language to analyze corporate social
media content, with a focus on sustainability-related communication. Addressing
the challenges of evolving, multimodal, and often ambiguous corporate messaging
on platforms such as X (formerly Twitter), we employ an ensemble of large
language models (LLMs) to annotate a large corpus of corporate tweets on their
topical alignment with the 17 Sustainable Development Goals (SDGs). This
approach avoids the need for costly, task-specific annotations and explores the
potential of such models as ad-hoc annotators for social media data that can
efficiently capture both explicit and implicit references to sustainability
themes in a scalable manner. Complementing this textual analysis, we utilize
vision-language models (VLMs), within a visual understanding framework that
uses semantic clusters to uncover patterns in visual sustainability
communication. This integrated approach reveals sectoral differences in SDG
engagement, temporal trends, and associations between corporate messaging,
environmental, social, governance (ESG) risks, and consumer engagement. Our
methods-automatic label generation and semantic visual clustering-are broadly
applicable to other domains and offer a flexible framework for large-scale
social media analysis.

</details>


### [253] [ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks](https://arxiv.org/abs/2511.01581)
*Chengzhang Yu,Zening Lu,Chenyang Zheng,Chiyue Wang,Yiming Zhang,Zhanpeng Jin*

Main category: cs.AI

TL;DR: ExplicitLM是一种新颖的语言模型架构，通过外部可读记忆库实现知识的显式存储和直接修改，解决了传统LLM的知识陈旧性和缺乏可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在知识陈旧和缺乏可解释性的问题，因为知识隐式存储在纠缠的网络参数中，无法进行针对性更新和推理透明度分析。

Method: 设计包含百万级外部记忆库的架构，存储人类可读的知识标记序列；采用可微分两阶段检索机制，包括基于产品键分解的粗粒度过滤和Gumbel-Softmax细粒度匹配；基于双系统认知理论将知识划分为冻结显式事实(20%)和可学习隐式模式(80%)。

Result: 在知识密集型任务上比标准Transformer提升43.67%，在低数据场景(10k样本)下获得3.62倍增益；正确预测的记忆命中率高出49%；联合优化的架构在保持竞争力的同时提供前所未有的知识透明度。

Conclusion: 可解释、可更新的模型能够保持竞争力，同时提供前所未有的知识透明度，与使用冻结检索的RAG系统不同。

Abstract: Large language models suffer from knowledge staleness and lack of
interpretability due to implicit knowledge storage across entangled network
parameters, preventing targeted updates and reasoning transparency. We propose
ExplicitLM, a novel architecture featuring a million-scale external memory bank
storing human-readable knowledge as token sequences, enabling direct inspection
and modification. We design a differentiable two-stage retrieval mechanism with
efficient coarse-grained filtering via product key decomposition (reducing
complexity from $\mathcal{O}(N \cdot |I|)$ to $\mathcal{O}(\sqrt{N} \cdot
|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.
Inspired by dual-system cognitive theory, we partition knowledge into frozen
explicit facts (20%) and learnable implicit patterns (80%), maintained through
Exponential Moving Average updates for stability. ExplicitLM achieves up to
43.67% improvement on knowledge-intensive tasks versus standard Transformers,
with 3.62$\times$ gains in low-data regimes (10k samples). Analysis shows
strong correlations between memory retrieval and performance, with correct
predictions achieving 49% higher hit rates. Unlike RAG systems with frozen
retrieval, our jointly optimized architecture demonstrates that interpretable,
updatable models can maintain competitive performance while providing
unprecedented knowledge transparency.

</details>


### [254] [IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization](https://arxiv.org/abs/2511.01639)
*Sicheng Wang,Shuhao Chen,Jingran Zhou,Chengyi Tu*

Main category: cs.AI

TL;DR: 提出IVGAE-TAMA-BO动态图神经网络，首次将动态图神经网络应用于全球粮食贸易网络链接预测，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 全球粮食贸易网络在政治、经济和环境因素影响下动态演变，传统方法难以有效建模和预测未来贸易链接，需要捕捉时间模式来提高预测准确性。

Method: 在IVGAE框架基础上，引入贸易感知动量聚合器(TAMA)捕捉贸易网络时间演化，联合建模短期波动和长期结构依赖，使用动量结构记忆机制提升预测稳定性，并通过贝叶斯优化自动调参。

Result: 在五个作物特定数据集上的实验表明，IVGAE-TAMA显著优于静态IVGAE和其他动态基线方法，贝叶斯优化进一步提升了IVGAE-TAMA-BO的性能。

Conclusion: 该框架为全球贸易网络结构预测提供了稳健可扩展的解决方案，在粮食安全监测和政策决策支持方面具有强大应用潜力。

Abstract: Global food trade plays a crucial role in ensuring food security and
maintaining supply chain stability. However, its network structure evolves
dynamically under the influence of geopolitical, economic, and environmental
factors, making it challenging to model and predict future trade links.
Effectively capturing temporal patterns in food trade networks is therefore
essential for improving the accuracy and robustness of link prediction. This
study introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed
to model evolving trade structures and predict future links in global food
trade networks. To the best of our knowledge, this is the first work to apply
dynamic graph neural networks to this domain, significantly enhancing
predictive performance. Building upon the original IVGAE framework, the
proposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture
the temporal evolution of trade networks, jointly modeling short-term
fluctuations and long-term structural dependencies. A momentum-based structural
memory mechanism further improves predictive stability and performance. In
addition, Bayesian optimization is used to automatically tune key
hyperparameters, enhancing generalization across diverse trade scenarios.
Extensive experiments on five crop-specific datasets demonstrate that
IVGAE-TAMA substantially outperforms the static IVGAE and other dynamic
baselines by effectively modeling temporal dependencies, while Bayesian
optimization further boosts performance in IVGAE-TAMA-BO. These results
highlight the proposed framework as a robust and scalable solution for
structural prediction in global trade networks, with strong potential for
applications in food security monitoring and policy decision support.

</details>


### [255] [Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics](https://arxiv.org/abs/2511.01668)
*Yueqing Xi,Yifan Bai,Huasen Luo,Weiliang Wen,Hui Liu,Haoliang Li*

Main category: cs.AI

TL;DR: 提出了一种混合法律问答代理，结合检索增强生成和多模型集成，在司法场景中提供可靠、可审计且持续更新的法律咨询。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型在法律问答中容易产生幻觉，而静态知识库难以跟上频繁更新的法律法规，需要确保法律咨询的真实性和可追溯性。

Method: 采用检索优先策略：当可信法律知识库有相关证据时使用RAG生成答案，否则由多个LLM生成候选答案并由专门选择器评分返回最优结果。高质量输出经人工审核后写回知识库。

Result: 在Law_QA数据集上的实验表明，该混合方法在F1、ROUGE-L和LLM-as-a-Judge指标上显著优于单模型基线和普通RAG流程。消融实验证实了检索优先、模型集成和人工更新机制的有效性。

Conclusion: 该系统显著减少了幻觉，提高了答案质量和法律合规性，推动了媒体取证技术在司法场景中的实际落地。

Abstract: As artificial intelligence permeates judicial forensics, ensuring the
veracity and traceability of legal question answering (QA) has become critical.
Conventional large language models (LLMs) are prone to hallucination, risking
misleading guidance in legal consultation, while static knowledge bases
struggle to keep pace with frequently updated statutes and case law. We present
a hybrid legal QA agent tailored for judicial settings that integrates
retrieval-augmented generation (RAG) with multi-model ensembling to deliver
reliable, auditable, and continuously updatable counsel. The system prioritizes
retrieval over generation: when a trusted legal repository yields relevant
evidence, answers are produced via RAG; otherwise, multiple LLMs generate
candidates that are scored by a specialized selector, with the top-ranked
answer returned. High-quality outputs then undergo human review before being
written back to the repository, enabling dynamic knowledge evolution and
provenance tracking. Experiments on the Law\_QA dataset show that our hybrid
approach significantly outperforms both a single-model baseline and a vanilla
RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm
the complementary contributions of retrieval prioritization, model ensembling,
and the human-in-the-loop update mechanism. The proposed system demonstrably
reduces hallucination while improving answer quality and legal compliance,
advancing the practical landing of media forensics technologies in judicial
scenarios.

</details>


### [256] [Simulating Environments with Reasoning Models for Agent Training](https://arxiv.org/abs/2511.01824)
*Yuetai Li,Huseyin A Inan,Xiang Yue,Wei-Ning Chen,Lukas Wutschitz,Janardhan Kulkarni,Radha Poovendran,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LLM代理在复杂环境中表现脆弱，本文提出Simia-SFT和Simia-RL框架，通过LLM模拟环境反馈实现无需真实环境数据的可扩展智能体训练。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代理在复杂环境中脆弱性问题，避免构建定制化训练环境的高成本和局限性。

Method: 提出Simia-SFT：通过小样本种子集扩增生成多样化轨迹；Simia-RL：通过LLM模拟反馈实现无真实环境的强化学习训练。

Result: 微调开源模型在多个基准测试中表现一致提升，超越GPT-4o，接近o4-mini在τ²-Bench上的性能。

Conclusion: Simia框架能够用灵活的LLM模拟替代繁重脆弱的环境实现，实现无需环境工程的可扩展智能体训练。

Abstract: LLM agents excel in compact environments requiring deep reasoning but remain
brittle when operating in broader, more complex contexts that demand robustness
across diverse tools and schemas. Building bespoke environments for training is
heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs
can simulate realistic environment feedback without access to actual testbed
data or APIs. Inspired by this capability, we propose two frameworks:
Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets
into diverse trajectories in an environment-agnostic manner, and Simia-RL, a
framework that enables RL training without real environment implementations
through LLM-simulated feedback. Fine-tuning open models yields consistent
improvements across multiple benchmarks, surpassing GPT-4o and approaching
o4-mini on $\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable
agent training without environment engineering, replacing heavy and brittle
implementations with flexible LLM-based simulation.

</details>
