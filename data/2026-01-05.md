<div id=toc></div>

# Table of Contents

- [physics.ao-ph](#physics.ao-ph) [Total: 3]
- [cs.NE](#cs.NE) [Total: 8]
- [cs.CV](#cs.CV) [Total: 68]
- [cs.AI](#cs.AI) [Total: 24]


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [1] [Stratosphere Model Verification with Manufactured Geometry](https://arxiv.org/abs/2601.00206)
*Johannes Lawen,George Salman,Akshita Bhardwaj*

Main category: physics.ao-ph

TL;DR: 提出了一种精确求解平流层动力核心的方法，使用位势/压力坐标，并考虑对流层提供的随时间演变的底部边界，将对流层顶视为移动几何边界。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常通过"nudging"将平流层环流约束到再分析数据，这限制了平流层对对流层变率的响应。需要一种更自然的方法来处理平流层-对流层耦合，允许平流层域根据对流层变率动态变化。

Method: 采用位势/压力坐标，将对流层顶作为移动几何边界，平流层域可扩展、收缩和波动。保持熟悉的混合σ-p结构和压力梯度计算，自然整合任意拉格朗日-欧拉(ALE)更新和保守重映射，确保正层厚度和示踪剂单调性。

Result: 提出了完整的数学公式，分析了其解析特性（适定性、能量学、波传播），并规划了基于修改标准测试案例和再分析驱动实验的验证/验证路径。

Conclusion: 该方法为平流层动力核心提供了一种精确解决方案，能够更自然地处理平流层-对流层耦合，允许平流层域动态响应对流层变率，同时保持数值稳定性和物理一致性。

Abstract: We propose an exact solution for a stratosphere dynamical core formulated in geopotential/pressure coordinates with a time-evolving lower boundary supplied by the troposphere. Rather than constraining the stratospheric circulation via specified dynamics (``nudging'') to a reanalysis, we treat the tropopause as a moving geometric boundary. The stratospheric domain thus expands, contracts, and undulates in response to tropospheric variability while preserving familiar hybrid $σ$--$p$ structure and pressure-gradient calculations. The approach integrates naturally with arbitrary Lagrangian--Eulerian (ALE) updates and conservative remap to maintain positive layer thickness and tracer monotonicity. We outline the formulation, highlight analytical properties (well-posedness, energetics, wave propagation), and sketch a verification/validation path based on modified standard test cases and reanalysis-driven experiments.

</details>


### [2] [Bayesian optimization for re-analysis and calibration of extreme sea state events simulated with a spectral third-generation wave model](https://arxiv.org/abs/2601.00628)
*Cédric Goeury,Thierry Fouquet,Maria Teles,Michel Benoit*

Main category: physics.ao-ph

TL;DR: 提出基于贝叶斯优化（BO）和树结构Parzen估计器（TPE）的波浪模型校准框架，用于联合优化ANEMOC-3模型中的连续参数和离散模型结构，显著减少模型输出与观测之间的差异。


<details>
  <summary>Details</summary>
Motivation: 数值波浪模型在物理参数化和模型输入方面存在不确定性，限制了极端海况事件后报的可靠性，需要更有效的校准方法来提高模型精度。

Method: 采用贝叶斯优化（BO）框架，结合树结构Parzen估计器（TPE），对ANEMOC-3后报波浪模型中的底部摩擦耗散、深度诱导破碎和强逆流波浪耗散等不确定汇项参数进行高效估计，实现连续参数和离散模型结构的联合优化。

Result: 应用于法国大西洋沿岸包含多个强风暴事件的一个月期数据，校准后的模型与浮标测量数据吻合度更高，相对于默认海况求解器配置，偏差、均方根误差和散射指数均有所降低。

Conclusion: 贝叶斯优化方法能够自动化和增强波浪模型校准，为广泛的地球物理建模问题提供了可扩展且灵活的方法，未来可扩展至多目标优化、不确定性量化和集成更多观测数据集。

Abstract: Accurate hindcasting of extreme sea state events is essential for coastal engineering, risk assessment, and climate studies. However, the reliability of numerical wave models remains limited by uncertainties in physical parameterizations and model inputs. This study presents a novel calibration framework based on Bayesian Optimization (BO), leveraging the Tree structured Parzen Estimator (TPE) to efficiently estimate uncertain sink term parameters, specifically bottom friction dissipation, depth induced breaking, and wave dissipation from strong opposing currents, in the ANEMOC-3 hindcast wave model. The proposed method enables joint optimization of continuous parameters and discrete model structures, significantly reducing discrepancies between model outputs and observations. Applied to a one month period encompassing multiple intense storm events along the French Atlantic coast, the calibrated model demonstrates improved agreement with buoy measurements, achieving lower bias, RMSE, and scatter index relative to the default sea$-$state solver configuration. The results highlight the potential of BO to automate and enhance wave model calibration, offering a scalable and flexible approach applicable to a wide range of geophysical modeling problems. Future extensions include multi-objective optimization, uncertainty quantification, and integration of additional observational datasets.

</details>


### [3] [Turbulence is ineffective in causing raindrop growth in polluted clouds](https://arxiv.org/abs/2601.00637)
*K. Shri Vignesh,Ambedkar Sanket Sukdeo,P. V. Sruthibhai,Aishwarya Singh,Srikrishna Sahu,Swetaprovo Chaudhari,Amit K. Patra,T. Narayana Rao,Rama Govindarajan,Sachin S. Gunthe,R. I. Sujith*

Main category: physics.ao-ph

TL;DR: 湍流并不总是增强云滴碰撞合并，只有在云滴尺寸分布足够宽时才会产生影响


<details>
  <summary>Details</summary>
Motivation: 气溶胶-云相互作用是气候变化评估中最大的不确定性来源，云湍流被认为对云滴增长至关重要，但其具体作用尚不清楚

Method: 通过实验室控制研究，在受控条件下观察湍流对云滴碰撞合并的影响

Result: 湍流并不总是增强碰撞合并，只有当云滴尺寸分布足够宽时，湍流的影响才会显现

Conclusion: 耗散尺度的云滴行为强调了改进参数化方案的重要性，以准确模拟云微物理过程

Abstract: Aerosol-cloud interactions represent the largest uncertainty in climate-change assessment, and while cloud turbulence is considered crucial for droplet growth, its precise role remains unclear. Our laboratory-controlled studies show that turbulence does not always enhance collision and coalescence; instead, its influence emerges only when droplets have a sufficiently broad size distribution. The dissipative-scale droplet behaviour underscores the importance of improved parameterisations to accurately model cloud microphysics.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [4] [Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing](https://arxiv.org/abs/2601.00020)
*Nikhil Garg,Anxiong Song,Niklas Plessnig,Nathan Savoia,Laura Bégon-Lours*

Main category: cs.NE

TL;DR: 该研究展示了在铁电忆阻突触器件上部署脉冲神经网络，用于自适应EEG运动想象解码，通过设备感知训练和权重转移策略，在有限设备约束下实现高效适应。


<details>
  <summary>Details</summary>
Motivation: 基于EEG的脑机接口受非平稳神经信号影响，跨会话和个体差异限制了主体无关模型的泛化能力，需要在资源受限平台上进行自适应和个性化学习。可编程忆阻硬件为此提供了有前景的解决方案，但面临权重分辨率有限、器件变异性、非线性编程动态和有限器件耐久性等挑战。

Method: 1. 制造、表征和建模铁电突触器件；2. 在卷积循环SNN架构下评估两种部署策略：(i)使用铁电突触模型进行设备感知训练，(ii)软件训练权重转移后进行低开销设备端重调；3. 引入设备感知权重更新策略，将基于梯度的更新数字累积，仅在超过阈值时转换为离散编程事件，模拟非线性状态依赖编程动态同时减少编程频率。

Result: 两种部署策略都实现了与最先进软件SNN相当的分类性能。通过仅重训练最终网络层实现的主体特定迁移学习提高了分类准确率。这些结果表明可编程铁电硬件能够支持SNN中的鲁棒低开销适应。

Conclusion: 可编程铁电硬件能够支持脉冲神经网络中的鲁棒、低开销适应，为神经信号的个性化神经形态处理开辟了实用路径。

Abstract: Electroencephalography (EEG)-based brain-computer interfaces (BCIs) are strongly affected by non-stationary neural signals that vary across sessions and individuals, limiting the generalization of subject-agnostic models and motivating adaptive and personalized learning on resource-constrained platforms. Programmable memristive hardware offers a promising substrate for such post-deployment adaptation; however, practical realization is challenged by limited weight resolution, device variability, nonlinear programming dynamics, and finite device endurance. In this work, we show that spiking neural networks (SNNs) can be deployed on ferroelectric memristive synaptic devices for adaptive EEG-based motor imagery decoding under realistic device constraints. We fabricate, characterize, and model ferroelectric synapses. We evaluate a convolutional-recurrent SNN architecture under two complementary deployment strategies: (i) device-aware training using a ferroelectric synapse model, and (ii) transfer of software-trained weights followed by low-overhead on-device re-tuning. To enable efficient adaptation, we introduce a device-aware weight-update strategy in which gradient-based updates are accumulated digitally and converted into discrete programming events only when a threshold is exceeded, emulating nonlinear, state-dependent programming dynamics while reducing programming frequency. Both deployment strategies achieve classification performance comparable to state-of-the-art software-based SNNs. Furthermore, subject-specific transfer learning achieved by retraining only the final network layers improves classification accuracy. These results demonstrate that programmable ferroelectric hardware can support robust, low-overhead adaptation in spiking neural networks, opening a practical path toward personalized neuromorphic processing of neural signals.

</details>


### [5] [Covariance Matrix Adaptation Evolution Strategy without a matrix](https://arxiv.org/abs/2601.00102)
*Jarosław Arabas,Adam Stelmaszczyk,Eryk Warchulski,Dariusz Jagodziński,Rafał Biedrzycki*

Main category: cs.NE

TL;DR: 提出了一种无需协方差矩阵的CMA-ES变体，通过存档向量加权组合生成新个体，在保持分布特性的同时降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 标准CMA-ES在高维空间中面临的主要挑战是协方差矩阵分解的立方复杂度，这限制了其在超高维优化问题中的应用。

Method: 提出矩阵无关CMA-ES：使用存档存储个体与中点之间的归一化差异向量，通过加权组合这些存档向量生成新个体，无需显式协方差矩阵分解。

Result: 证明该方法生成的个体分布与标准CMA-ES相同；实验表明仅存储最近固定数量种群即可保持优化效率；在CEC'2017基准测试中，矩阵无关CMA-ES收敛更快且结果质量相当或更优。

Conclusion: 矩阵无关CMA-ES简化了算法结构，提供了协方差矩阵适应的新视角，为开发更高效优化方法奠定了基础。

Abstract: Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is a highly effective optimization technique. A primary challenge when applying CMA-ES in high dimensionality is sampling from a multivariate normal distribution with an arbitrary covariance matrix, which involves its decomposition. The cubic complexity of this process is the main obstacle to applying CMA-ES in highdimensional spaces. We introduce a version of CMA-ES that uses no covariance matrix at all. In the proposed matrix-free CMA-ES, an archive stores the vectors of differences between individuals and the midpoint, normalized by the step size. New individuals are generated as the weighted combinations of the vectors from the archive. We prove that the probability distribution of individuals generated by the proposed method is identical to that of the standard CMA-ES. Experimental results show that reducing the archive size to store only a fixed number of the most recent populations is sufficient, without compromising optimization efficiency. The matrix-free and matrix-based CMA-ES achieve comparable results on the quadratic function when the step-size adaptation is turned off. When coupled with the step-size adaptation method, the matrix-free CMA-ES converges faster than the matrix-based, and usually yields the results of a comparable or superior quality, according to the results obtained for the CEC'2017 benchmark suite. Presented approach simplifies the algorithm, offers a novel perspective on covariance matrix adaptation, and serves as a stepping stone toward even more efficient methods.

</details>


### [6] [Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing](https://arxiv.org/abs/2601.00245)
*Osvaldo Simeone*

Main category: cs.NE

TL;DR: 该论文探讨了现代AI架构如何体现神经形态计算原理，通过量化激活、状态空间动力学和稀疏注意力机制实现脑样效率，并系统分析了神经形态AI模型在token内和token间处理方面的应用。


<details>
  <summary>Details</summary>
Motivation: AI快速增长带来了数据处理和生成能力提升，但能耗需求也在急剧增加。这促使人们重新关注神经形态计算原理，该原理通过离散稀疏激活、循环动力学和非线性反馈实现类似大脑的高效计算。

Method: 论文从token内处理和token间处理的视角，系统分析了神经形态模型、状态空间模型和Transformer架构之间的联系。早期神经形态AI基于脉冲神经网络进行token内处理，而近期研究探索如何利用神经形态原理设计高效的token间处理方法，包括状态空间动力学和稀疏自注意力机制。

Result: 现代AI架构越来越多地体现神经形态原理，包括重度量化激活、状态空间动力学和稀疏注意力机制。论文系统展示了神经形态AI模型在token内和token间处理方面的应用，并回顾了从利用并行卷积处理的替代梯度到基于强化学习机制的局部学习规则等多种训练方法。

Conclusion: 神经形态计算原理为AI的高效实现提供了有前景的途径，通过结合token内和token间处理的神经形态方法，可以设计出更节能的AI架构。论文为理解现代AI架构与神经形态计算之间的联系提供了系统框架。

Abstract: The rapid growth of artificial intelligence (AI) has brought novel data processing and generative capabilities but also escalating energy requirements. This challenge motivates renewed interest in neuromorphic computing principles, which promise brain-like efficiency through discrete and sparse activations, recurrent dynamics, and non-linear feedback. In fact, modern AI architectures increasingly embody neuromorphic principles through heavily quantized activations, state-space dynamics, and sparse attention mechanisms. This paper elaborates on the connections between neuromorphic models, state-space models, and transformer architectures through the lens of the distinction between intra-token processing and inter-token processing. Most early work on neuromorphic AI was based on spiking neural networks (SNNs) for intra-token processing, i.e., for transformations involving multiple channels, or features, of the same vector input, such as the pixels of an image. In contrast, more recent research has explored how neuromorphic principles can be leveraged to design efficient inter-token processing methods, which selectively combine different information elements depending on their contextual relevance. Implementing associative memorization mechanisms, these approaches leverage state-space dynamics or sparse self-attention. Along with a systematic presentation of modern neuromorphic AI models through the lens of intra-token and inter-token processing, training methodologies for neuromorphic AI models are also reviewed. These range from surrogate gradients leveraging parallel convolutional processing to local learning rules based on reinforcement learning mechanisms.

</details>


### [7] [RMAAT: Astrocyte-Inspired Memory Compression and Replay for Efficient Long-Context Transformers](https://arxiv.org/abs/2601.00426)
*Md Zesun Ahmed Mia,Malyaban Bal,Abhronil Sengupta*

Main category: cs.NE

TL;DR: RMAAT：一种受星形胶质细胞启发的Transformer架构，通过循环记忆增强和自适应压缩机制，在保持竞争力的同时显著提升长序列处理的计算和内存效率。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制的二次复杂度限制了Transformer处理长序列的能力。传统架构修改方法存在局限，因此探索从星形胶质细胞（生物记忆和突触调节的关键细胞）中衍生的计算原理，作为补充方法。

Method: 提出RMAAT架构：1）采用循环、分段处理策略，使用持久记忆token传播上下文信息；2）基于模拟星形胶质细胞长时程可塑性（LTP）的自适应压缩机制；3）受星形胶质细胞短时程可塑性（STP）启发的线性复杂度注意力机制；4）使用AMRB训练算法，专为循环网络的内存效率设计。

Result: 在Long Range Arena基准测试中，RMAAT展现出竞争力的准确率，同时在计算和内存效率方面有显著提升。

Conclusion: 将星形胶质细胞启发的动力学机制整合到序列模型中具有潜力，能够实现可扩展的高效Transformer架构。

Abstract: The quadratic complexity of self-attention mechanism presents a significant impediment to applying Transformer models to long sequences. This work explores computational principles derived from astrocytes-glial cells critical for biological memory and synaptic modulation-as a complementary approach to conventional architectural modifications for efficient self-attention. We introduce the Recurrent Memory Augmented Astromorphic Transformer (RMAAT), an architecture integrating abstracted astrocyte functionalities. RMAAT employs a recurrent, segment-based processing strategy where persistent memory tokens propagate contextual information. An adaptive compression mechanism, governed by a novel retention factor derived from simulated astrocyte long-term plasticity (LTP), modulates these tokens. Attention within segments utilizes an efficient, linear-complexity mechanism inspired by astrocyte short-term plasticity (STP). Training is performed using Astrocytic Memory Replay Backpropagation (AMRB), a novel algorithm designed for memory efficiency in recurrent networks. Evaluations on the Long Range Arena (LRA) benchmark demonstrate RMAAT's competitive accuracy and substantial improvements in computational and memory efficiency, indicating the potential of incorporating astrocyte-inspired dynamics into scalable sequence models.

</details>


### [8] [Benchmarking ERP Analysis: Manual Features, Deep Learning, and Foundation Models](https://arxiv.org/abs/2601.00573)
*Yihe Wang,Zhiqiao Kang,Bohan Chen,Yu Zhang,Xiang Zhang*

Main category: cs.NE

TL;DR: 本文对ERP分析中的传统手动特征、深度学习模型和预训练EEG基础模型进行了系统性基准测试，建立了统一的数据处理流程，并在12个公开数据集上评估了不同方法在ERP刺激分类和脑疾病检测任务上的表现。


<details>
  <summary>Details</summary>
Motivation: ERP在认知分析、神经疾病检测和心理状态评估中具有重要作用，但现有深度学习方法在ERP数据上的有效性尚未充分探索，许多研究仍依赖手动提取特征，需要系统性的基准研究来指导方法选择和模型设计。

Method: 建立了统一的数据预处理和训练流程，系统比较了传统手动特征+线性分类器、深度学习模型和预训练EEG基础模型三种方法。在12个公开数据集上评估了ERP刺激分类和脑疾病检测两个代表性任务，并研究了Transformer架构中不同的patch-embedding策略。

Result: 研究提供了全面的基准测试结果，识别了更适合ERP数据的嵌入设计，为ERP分析方法选择和定制模型设计提供了指导框架。

Conclusion: 本研究为ERP分析提供了里程碑式的基准框架，有助于指导未来ERP分析方法的选择和定制化模型设计，代码已开源供社区使用。

Abstract: Event-related potential (ERP), a specialized paradigm of electroencephalographic (EEG), reflects neurological responses to external stimuli or events, generally associated with the brain's processing of specific cognitive tasks. ERP plays a critical role in cognitive analysis, the detection of neurological diseases, and the assessment of psychological states. Recent years have seen substantial advances in deep learning-based methods for spontaneous EEG and other non-time-locked task-related EEG signals. However, their effectiveness on ERP data remains underexplored, and many existing ERP studies still rely heavily on manually extracted features. In this paper, we conduct a comprehensive benchmark study that systematically compares traditional manual features (followed by a linear classifier), deep learning models, and pre-trained EEG foundation models for ERP analysis. We establish a unified data preprocessing and training pipeline and evaluate these approaches on two representative tasks, ERP stimulus classification and ERP-based brain disease detection, across 12 publicly available datasets. Furthermore, we investigate various patch-embedding strategies within advanced Transformer architectures to identify embedding designs that better suit ERP data. Our study provides a landmark framework to guide method selection and tailored model design for future ERP analysis. The code is available at https://github.com/DL4mHealth/ERP-Benchmark.

</details>


### [9] [Three factor delay learning rules for spiking neural networks](https://arxiv.org/abs/2601.00668)
*Luke Vassallo,Nima Taherinejad*

Main category: cs.NE

TL;DR: 该论文提出了一种在线学习脉冲神经网络的突触和轴突延迟参数的方法，通过三因素学习规则同时学习权重和延迟，显著提高了时序模式识别性能，同时大幅减少了模型大小和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 传统SNN的可学习参数主要局限于突触权重，对时序模式识别贡献有限。现有的延迟学习方法依赖大网络和离线学习，不适合资源受限环境下的实时操作。

Method: 在LIF前馈和循环SNN中引入突触和轴突延迟，提出三因素学习规则在线同时学习延迟参数。使用高斯代理函数近似脉冲导数计算资格迹，结合自上而下的误差信号确定参数更新。

Result: 加入延迟比仅有权重的基线准确率提高达20%；在相似参数数量下，联合学习权重和延迟比仅学习权重准确率高14%。在SHD语音识别数据集上达到与离线反向传播方法相似的准确率，同时模型大小减少6.6倍，推理延迟降低67%，分类准确率仅下降2.4%。

Conclusion: 该方法通过在线学习延迟参数显著提升了SNN的时序模式识别能力，同时大幅减少了模型复杂度和计算需求，有利于设计功耗和面积受限的神经形态处理器，支持设备端学习和降低内存需求。

Abstract: Spiking Neural Networks (SNNs) are dynamical systems that operate on spatiotemporal data, yet their learnable parameters are often limited to synaptic weights, contributing little to temporal pattern recognition. Learnable parameters that delay spike times can improve classification performance in temporal tasks, but existing methods rely on large networks and offline learning, making them unsuitable for real-time operation in resource-constrained environments. In this paper, we introduce synaptic and axonal delays to leaky integrate and fire (LIF)-based feedforward and recurrent SNNs, and propose three-factor learning rules to simultaneously learn delay parameters online. We employ a smooth Gaussian surrogate to approximate spike derivatives exclusively for the eligibility trace calculation, and together with a top-down error signal determine parameter updates. Our experiments show that incorporating delays improves accuracy by up to 20% over a weights-only baseline, and for networks with similar parameter counts, jointly learning weights and delays yields up to 14% higher accuracy. On the SHD speech recognition dataset, our method achieves similar accuracy to offline backpropagation-based approaches. Compared to state-of-the-art methods, it reduces model size by 6.6x and inference latency by 67%, with only a 2.4% drop in classification accuracy. Our findings benefit the design of power and area-constrained neuromorphic processors by enabling on-device learning and lowering memory requirements.

</details>


### [10] [QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models](https://arxiv.org/abs/2601.00679)
*Rachmad Vidya Wicaksana Putra,Pasindu Wickramasinghe,Muhammad Shafique*

Main category: cs.NE

TL;DR: QSLM是一个自动化量化框架，用于压缩脉冲驱动语言模型（SLMs），在满足性能和内存约束的同时大幅减少内存占用和功耗。


<details>
  <summary>Details</summary>
Motivation: 虽然脉冲驱动语言模型（SLMs）能显著降低LLMs的处理功耗，但其内存占用仍然过大，不适合资源受限的嵌入式设备。手动量化方法虽然有效但耗时耗力，缺乏可扩展性。

Method: QSLM首先识别网络架构层次和层级的量化敏感性，然后采用分层量化策略（全局、块级、模块级），利用多目标性能-内存权衡函数选择最终量化设置。

Result: QSLM将内存占用减少高达86.5%，功耗降低高达20%，在SST-2数据集上保持84.4%的情感分类准确率，在WikiText-2数据集上获得23.2的困惑度分数，接近原始非量化模型性能。

Conclusion: QSLM提供了一个自动化、可扩展的量化框架，能够有效压缩SLMs的内存占用，同时满足嵌入式设备的性能和内存约束，为资源受限环境中的语言模型部署提供了实用解决方案。

Abstract: Large Language Models (LLMs) have been emerging as prominent AI models for solving many natural language tasks due to their high performance (e.g., accuracy) and capabilities in generating high-quality responses to the given inputs. However, their large computational cost, huge memory footprints, and high processing power/energy make it challenging for their embedded deployments. Amid several tinyLLMs, recent works have proposed spike-driven language models (SLMs) for significantly reducing the processing power/energy of LLMs. However, their memory footprints still remain too large for low-cost and resource-constrained embedded devices. Manual quantization approach may effectively compress SLM memory footprints, but it requires a huge design time and compute power to find the quantization setting for each network, hence making this approach not-scalable for handling different networks, performance requirements, and memory budgets. To bridge this gap, we propose QSLM, a novel framework that performs automated quantization for compressing pre-trained SLMs, while meeting the performance and memory constraints. To achieve this, QSLM first identifies the hierarchy of the given network architecture and the sensitivity of network layers under quantization, then employs a tiered quantization strategy (e.g., global-, block-, and module-level quantization) while leveraging a multi-objective performance-and-memory trade-off function to select the final quantization setting. Experimental results indicate that our QSLM reduces memory footprint by up to 86.5%, reduces power consumption by up to 20%, maintains high performance across different tasks (i.e., by up to 84.4% accuracy of sentiment classification on the SST-2 dataset and perplexity score of 23.2 for text generation on the WikiText-2 dataset) close to the original non-quantized model while meeting the performance and memory constraints.

</details>


### [11] [Cost Optimization in Production Line Using Genetic Algorithm](https://arxiv.org/abs/2601.00689)
*Alireza Rezaee*

Main category: cs.NE

TL;DR: 本文提出了一种基于遗传算法的生产流水线成本最优任务调度方法，比较了两种染色体编码策略（工位基和任务基），发现任务基编码在复杂约束下表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决生产流水线中的成本最优任务调度问题，该问题具有串行处理任务、持续时间、执行成本、前驱约束等复杂条件，传统基于梯度或解析方法难以处理非可微成本函数和组合约束。

Method: 采用遗传算法，研究两种染色体编码策略：1) 工位基表示法（使用JGAP库和SuperGene有效性检查）；2) 任务基表示法（基因直接编码工位分配）。为每种编码适配标准GA算子（交叉、变异、选择、替换）以保持可行性并驱动种群向低成本调度进化。

Result: 在三种前驱结构（紧密耦合、松散耦合、无耦合）上的实验结果表明，任务基编码比工位基编码具有更平滑的收敛性和更可靠的成本最小化效果，特别是在有效调度数量较大时表现更优。

Conclusion: 遗传算法在处理组合调度问题、特别是存在复杂约束和非可微成本函数时，优于基于梯度和解析方法。任务基编码策略在成本优化和收敛性方面表现更佳。

Abstract: This paper presents a genetic algorithm (GA) approach to cost-optimal task scheduling in a production line. The system consists of a set of serial processing tasks, each with a given duration, unit execution cost, and precedence constraints, which must be assigned to an unlimited number of stations subject to a per-station duration bound. The objective is to minimize the total production cost, modeled as a station-wise function of task costs and the duration bound, while strictly satisfying all prerequisite and capacity constraints. Two chromosome encoding strategies are investigated: a station-based representation implemented using the JGAP library with SuperGene validity checks, and a task-based representation in which genes encode station assignments directly. For each encoding, standard GA operators (crossover, mutation, selection, and replacement) are adapted to preserve feasibility and drive the population toward lower-cost schedules. Experimental results on three classes of precedence structures-tightly coupled, loosely coupled, and uncoupled-demonstrate that the task-based encoding yields smoother convergence and more reliable cost minimization than the station-based encoding, particularly when the number of valid schedules is large. The study highlights the advantages of GA over gradient-based and analytical methods for combinatorial scheduling problems, especially in the presence of complex constraints and non-differentiable cost landscapes.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [12] [TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model](https://arxiv.org/abs/2601.00051)
*Yabo Chen,Yuanzhi Liang,Jiepeng Wang,Tingxi Chen,Junfei Cheng,Zixiao Gu,Yuyang Huang,Zicheng Jiang,Wei Li,Tian Li,Weichen Li,Zuoxin Li,Guangce Liu,Jialun Liu,Junqi Liu,Haoyuan Wang,Qizhen Weng,Xuan'er Wu,Xunzhi Xiang,Xiaoyan Yang,Xin Zhang,Shiwen Zhang,Junyu Zhou,Chengcheng Zhou,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleWorld是一个实时多模态4D世界建模框架，通过生成-重建-引导范式统一视频生成、动态场景重建和长期世界记忆，实现空间、时间和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在实时交互、长期一致性和动态场景持久记忆方面存在局限，阻碍其发展为实用的世界模型。需要一种能够统一视频生成、动态场景重建和长期记忆的框架。

Method: 提出生成-重建-引导范式：生成的视频流被连续重建为动态4D时空表示，该表示反过来引导后续生成以保持一致性。采用自回归扩散视频模型，结合宏观-微观规划(MMPL)减少误差累积，并使用分布匹配蒸馏(DMD)实现实时合成。

Result: TeleWorld在静态和动态世界理解、长期一致性和实时生成效率方面表现出色，实现了动态对象建模和静态场景表示在统一4D框架中的无缝集成。

Conclusion: TeleWorld是迈向实用、交互式和计算可访问的世界模型的重要一步，为多模态生成和具身智能提供了交互式、记忆启用的世界模型解决方案。

Abstract: World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.

</details>


### [13] [It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models](https://arxiv.org/abs/2601.00090)
*Anne Harrington,A. Sophia Koepke,Shyamgopal Karthik,Trevor Darrell,Alexei A. Efros*

Main category: cs.CV

TL;DR: 通过噪声优化解决文本到图像模型的模式崩溃问题，提高生成多样性


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型存在严重的模式崩溃问题，即相同文本提示下生成的图像缺乏多样性。虽然已有研究通过引导机制或候选池筛选来缓解此问题，但本文探索了不同的解决方案方向。

Method: 采用噪声优化方法，通过简单的噪声优化目标来缓解模式崩溃，同时保持基础模型的保真度。分析噪声的频率特性，探索不同频率特性的替代噪声初始化策略以改进优化和搜索。

Result: 实验表明，噪声优化方法在生成质量和多样性方面均取得优越结果，能有效缓解模式崩溃问题。

Conclusion: 噪声优化是一种简单有效的解决文本到图像模型模式崩溃的方法，通过优化噪声和调整频率特性可以显著提高生成多样性而不损害质量。

Abstract: Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.

</details>


### [14] [Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark](https://arxiv.org/abs/2601.00092)
*Pan Wang,Yang Liu,Guile Wu,Eduardo R. Corral-Soto,Chengjie Huang,Binbin Xu,Dongfeng Bai,Xu Yan,Yuan Ren,Xingxin Chen,Yizhe Wu,Tao Huang,Wenjun Wan,Xin Wu,Pei Zhou,Xuyang Dai,Kangbo Lv,Hongbo Zhang,Yosef Fried,Aixue Ye,Bailan Feng,Zhenyu Chen,Zhen Li,Yingcong Chen,Yiyi Liao,Bingbing Liu*

Main category: cs.CV

TL;DR: Spatial4D-Bench：一个包含约4万个问答对的大规模4D空间智能基准测试，用于评估多模态大语言模型在18个任务上的4D空间推理能力，发现现有模型存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 人类天生具备4D空间智能（感知物体随时间的变化），但多模态大语言模型是否能达到人类水平的4D空间智能尚不清楚。现有空间智能基准测试通常规模小或多样性有限，需要更全面的评估工具。

Method: 提出Spatial4D-Bench基准测试，包含约40,000个问答对，覆盖18个明确定义的任务，这些任务被系统组织为六个认知类别：物体理解、场景理解、空间关系理解、时空关系理解、空间推理和时空推理。

Result: 对多种开源和专有多模态大语言模型进行基准测试，发现它们在多种4D空间推理方面存在显著局限性，如路径规划、动作识别和物理合理性推理等。

Conclusion: 该基准测试为社区提供了有价值的见解，有助于开发更强大的多模态大语言模型，使其向人类水平的4D空间智能迈进。

Abstract: 4D spatial intelligence involves perceiving and processing how objects move or change over time. Humans naturally possess 4D spatial intelligence, supporting a broad spectrum of spatial reasoning abilities. To what extent can Multimodal Large Language Models (MLLMs) achieve human-level 4D spatial intelligence? In this work, we present Spatial4D-Bench, a versatile 4D spatial intelligence benchmark designed to comprehensively assess the 4D spatial reasoning abilities of MLLMs. Unlike existing spatial intelligence benchmarks that are often small-scale or limited in diversity, Spatial4D-Bench provides a large-scale, multi-task evaluation benchmark consisting of ~40,000 question-answer pairs covering 18 well-defined tasks. We systematically organize these tasks into six cognitive categories: object understanding, scene understanding, spatial relationship understanding, spatiotemporal relationship understanding, spatial reasoning and spatiotemporal reasoning. Spatial4D-Bench thereby offers a structured and comprehensive benchmark for evaluating the spatial cognition abilities of MLLMs, covering a broad spectrum of tasks that parallel the versatility of human spatial intelligence. We benchmark various state-of-the-art open-source and proprietary MLLMs on Spatial4D-Bench and reveal their substantial limitations in a wide variety of 4D spatial reasoning aspects, such as route plan, action recognition, and physical plausibility reasoning. We hope that the findings provided in this work offer valuable insights to the community and that our benchmark can facilitate the development of more capable MLLMs toward human-level 4D spatial intelligence. More resources can be found on our project page.

</details>


### [15] [A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data](https://arxiv.org/abs/2601.00123)
*Hyunho Lee,Wenwen Li*

Main category: cs.CV

TL;DR: 提出SMAGNet模型，通过自适应门控网络融合SAR和MSI数据，提升洪水淹没范围制图精度，增强对缺失数据的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 洪水期间及时准确的水域制图对灾害管理至关重要。虽然SAR数据常用于洪水响应，但结合SAR和MSI多模态数据能提升制图精度，特别是在洪水高峰期观测数据有限时。然而，如何自适应地整合部分可用的MSI数据到SAR基础的洪水制图过程尚未充分探索。

Method: 提出SMAGNet（空间掩码自适应门控网络），这是一个多模态深度学习模型。它以SAR数据作为洪水水域制图的主要输入，通过特征融合整合互补的MSI数据。模型采用自适应门控机制来处理MSI数据可用性的变化。

Result: 在C2S-MS Floods数据集上的实验表明，SMAGNet在不同MSI数据可用性水平下，预测性能始终优于其他多模态深度学习模型。即使MSI数据完全缺失，SMAGNet的性能仍与仅使用SAR数据训练的U-Net模型统计相当。

Conclusion: SMAGNet不仅提升了模型对缺失数据的鲁棒性，还增强了多模态深度学习在实际洪水管理场景中的适用性，为灾害响应提供了更可靠的洪水制图解决方案。

Abstract: Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios.

</details>


### [16] [Compressed Map Priors for 3D Perception](https://arxiv.org/abs/2601.00139)
*Brady Zhou,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: CMP框架通过压缩历史遍历数据学习空间先验，显著提升3D目标检测性能，存储需求降低20倍


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶视觉系统通常将每个位置视为首次访问，忽略了历史遍历信息。人类驾驶员很少去无人去过的地方，大多数自动驾驶区域都曾被访问过，但系统未能有效利用这些历史信息。

Method: 提出压缩地图先验（CMP）框架，从历史遍历数据中学习空间先验。使用二值化哈希图存储，存储密度仅为32KB/km²，比密集存储减少20倍。该方法可轻松集成到主流3D感知系统中，计算成本极低。

Result: 在nuScenes数据集上，CMP显著且一致地提升了多种架构的3D目标检测性能。存储效率大幅提高，计算开销极小。

Conclusion: 压缩地图先验是简单有效的框架，能够从历史遍历中学习空间先验，显著提升自动驾驶3D感知性能，同时保持低存储和计算成本。

Abstract: Human drivers rarely travel where no person has gone before. After all, thousands of drivers use busy city roads every day, and only one can claim to be the first. The same holds for autonomous computer vision systems. The vast majority of the deployment area of an autonomous vision system will have been visited before. Yet, most autonomous vehicle vision systems act as if they are encountering each location for the first time. In this work, we present Compressed Map Priors (CMP), a simple but effective framework to learn spatial priors from historic traversals. The map priors use a binarized hashmap that requires only $32\text{KB}/\text{km}^2$, a $20\times$ reduction compared to the dense storage. Compressed Map Priors easily integrate into leading 3D perception systems at little to no extra computational costs, and lead to a significant and consistent improvement in 3D object detection on the nuScenes dataset across several architectures.

</details>


### [17] [Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection](https://arxiv.org/abs/2601.00141)
*Lawrence Han*

Main category: cs.CV

TL;DR: GLASS是一种用于AI生成图像检测的架构，通过结合全局缩放视图和多个原始分辨率局部裁剪，利用注意力机制聚合信息，在多种骨干网络上优于标准迁移学习方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速发展，AI生成的图像越来越逼真和高分辨率。现有的AI生成图像检测架构通常会在输入模型前对图像进行下采样，这可能导致细粒度细节的丢失，从而影响检测性能。

Method: 提出GLASS架构，结合全局缩放视图和多个随机采样的局部裁剪。局部裁剪通过空间分层采样高效选择原始分辨率区域，并使用基于注意力的评分机制进行聚合。该架构可以集成到各种视觉模型中，利用任意尺寸图像的全局和局部信息。

Result: 在Vision Transformer、ResNet和ConvNeXt等骨干网络上进行实验，结果表明GLASH在可行的计算约束下实现了比标准迁移学习更高的预测性能。

Conclusion: GLASS架构通过同时利用全局和局部信息，有效解决了AI生成图像检测中因下采样导致的细节丢失问题，在多种视觉骨干网络上均表现出优越性能。

Abstract: The rapid development of generative AI has made AI-generated images increasingly realistic and high-resolution. Most AI-generated image detection architectures typically downsample images before inputting them into models, risking the loss of fine-grained details. This paper presents GLASS (Global-Local Attention with Stratified Sampling), an architecture that combines a globally resized view with multiple randomly sampled local crops. These crops are original-resolution regions efficiently selected through spatially stratified sampling and aggregated using attention-based scoring. GLASS can be integrated into vision models to leverage both global and local information in images of any size. Vision Transformer, ResNet, and ConvNeXt models are used as backbones, and experiments show that GLASS outperforms standard transfer learning by achieving higher predictive performance within feasible computational constraints.

</details>


### [18] [FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications](https://arxiv.org/abs/2601.00150)
*Yehui Yang,Dalu Yang,Wenshuo Zhou,Fangxin Shang,Yifan Liu,Jie Ren,Haojun Fei,Qing Yang,Tao Chen*

Main category: cs.CV

TL;DR: FCMBench-V1.0是一个专门用于金融信贷领域的多模态基准测试，包含4,043张隐私合规图像和8,446个QA样本，评估维度包括感知、推理和鲁棒性，用于测试视觉语言模型在信贷文档处理中的实际表现。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI在信贷风险评估和文档审查中的广泛应用，迫切需要专门针对金融信贷领域的基准测试，该基准需要反映特定文档和工作流程、包含信贷特定理解能力、保持隐私合规性，同时不牺牲实际效用。

Method: 通过封闭的合成-采集管道构建所有样本：手动合成带有虚拟内容的文档模板，并在内部采集场景感知图像。基准包含18种核心证书类型，评估框架包括3个基础感知任务、4个信贷特定推理任务和10种现实采集伪影类型的鲁棒性压力测试。

Result: 在测试的23个最先进视觉语言模型中，Gemini 3 Pro作为商业模型获得最佳F1分数(64.61%)，Qwen3-VL-235B作为开源基线获得最佳分数(57.27%)，而专门针对金融信贷的Qfin-VL-Instruct模型获得最高总体分数(64.92%)。鲁棒性评估显示即使表现最佳的模型在采集伪影下也会出现明显性能下降。

Conclusion: FCMBench-V1.0能够有效区分现代视觉语言模型的性能差异和鲁棒性，为金融信贷领域的多模态AI评估提供了专门、实用且隐私合规的基准测试工具，填补了该领域专业评估的空白。

Abstract: As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.

</details>


### [19] [Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions](https://arxiv.org/abs/2601.00156)
*Kaiwen Zheng,Junchen Fu,Songpei Xu,Yaoqing He,Joemon M. Jose,Han Hu,Xuri Ge*

Main category: cs.CV

TL;DR: 本文提出FaceFocalDesc问题，关注面部区域的多属性自然语言描述生成与识别，包括动作单元、情绪状态和年龄估计。作者构建了新数据集并提出基于Qwen2.5-VL的Focal-RegionFace模型，通过渐进微调实现局部面部特征分析。


<details>
  <summary>Details</summary>
Motivation: 面部分析中缺乏对任意选定面部区域的多属性自然语言描述生成与识别的研究。系统能够聚焦于个体面部区域将带来更好的理解和控制能力。

Method: 1. 构建新的多属性描述数据集，提供丰富的区域级标注和自然语言描述；2. 提出基于Qwen2.5-VL的Focal-RegionFace模型，通过多个渐进微调阶段逐步细化对局部面部特征的关注。

Result: Focal-RegionFace在新基准测试中取得了最佳性能，在传统指标和新提出的指标上都表现优异，验证了其在细粒度多属性面部区域聚焦分析场景中的有效性和多功能性。

Conclusion: 该研究成功解决了面部区域多属性自然语言描述生成与识别的问题，提出的方法和模型在细粒度面部分析任务中表现出色，为面部状态分析提供了新的视角和工具。

Abstract: In this paper, we introduce an underexplored problem in facial analysis: generating and recognizing multi-attribute natural language descriptions, containing facial action units (AUs), emotional states, and age estimation, for arbitrarily selected face regions (termed FaceFocalDesc). We argue that the system's ability to focus on individual facial areas leads to better understanding and control. To achieve this capability, we construct a new multi-attribute description dataset for arbitrarily selected face regions, providing rich region-level annotations and natural language descriptions. Further, we propose a fine-tuned vision-language model based on Qwen2.5-VL, called Focal-RegionFace for facial state analysis, which incrementally refines its focus on localized facial features through multiple progressively fine-tuning stages, resulting in interpretable age estimation, FAU and emotion detection. Experimental results show that Focal-RegionFace achieves the best performance on the new benchmark in terms of traditional and widely used metrics, as well as new proposed metrics. This fully verifies its effectiveness and versatility in fine-grained multi-attribute face region-focal analysis scenarios.

</details>


### [20] [DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery](https://arxiv.org/abs/2601.00194)
*Salma Gonzalez-Sabbagh,Antonio Robles-Kelly,Shang Gao*

Main category: cs.CV

TL;DR: DichroGAN是一种条件生成对抗网络，用于从卫星图像恢复海底的空中颜色，通过两步训练消除水下光衰减影响。


<details>
  <summary>Details</summary>
Motivation: 由于光在水柱中随深度呈指数衰减，从卫星图像恢复海底的空中颜色具有挑战性，需要解决光吸收和散射问题。

Method: DichroGAN采用条件生成对抗网络，分两步训练：两个生成器利用高光谱图像估计漫反射和镜面反射获取大气场景辐射；另外两个生成器分别处理场景辐射特征和估计水下光传输，基于水下成像方程消除光吸收和散射影响。

Result: 在PRISMA卫星图像数据集和多个水下数据集上的实验表明，DichroGAN在性能上与最先进的水下恢复技术相当。

Conclusion: DichroGAN能够有效恢复海底的空中颜色，为卫星图像的水下场景分析提供了一种有前景的解决方案。

Abstract: Recovering the in-air colours of seafloor from satellite imagery is a challenging task due to the exponential attenuation of light with depth in the water column. In this study, we present DichroGAN, a conditional generative adversarial network (cGAN) designed for this purpose. DichroGAN employs a two-steps simultaneous training: first, two generators utilise a hyperspectral image cube to estimate diffuse and specular reflections, thereby obtaining atmospheric scene radiance. Next, a third generator receives as input the generated scene radiance containing the features of each spectral band, while a fourth generator estimates the underwater light transmission. These generators work together to remove the effects of light absorption and scattering, restoring the in-air colours of seafloor based on the underwater image formation equation. DichroGAN is trained on a compact dataset derived from PRISMA satellite imagery, comprising RGB images paired with their corresponding spectral bands and masks. Extensive experiments on both satellite and underwater datasets demonstrate that DichroGAN achieves competitive performance compared to state-of-the-art underwater restoration techniques.

</details>


### [21] [MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing](https://arxiv.org/abs/2601.00204)
*Xiaokun Sun,Zeyu Cai,Hao Tang,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: MorphAny3D是一个无需训练、基于结构化潜在表示的3D变形框架，通过注意力机制融合源和目标特征，实现高质量跨类别3D变形。


<details>
  <summary>Details</summary>
Motivation: 3D变形面临语义一致性和时间平滑性的挑战，特别是在跨类别变形时。现有方法难以生成高质量、连贯的变形序列。

Method: 提出基于结构化潜在表示的方法，引入变形交叉注意力融合源目标结构信息，时间融合自注意力增强时序一致性，并采用方向校正策略缓解姿态模糊问题。

Result: 实验表明该方法能生成最先进的变形序列，即使在跨类别挑战性案例中也表现优异，并支持解耦变形和3D风格迁移等高级应用。

Conclusion: MorphAny3D是一个无需训练的高质量3D变形框架，通过巧妙融合结构化潜在表示中的注意力机制，实现了语义一致、时间平滑的变形效果，并可推广到其他基于SLAT的生成模型。

Abstract: 3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.

</details>


### [22] [CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting](https://arxiv.org/abs/2601.00207)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 提出基于多视角图像和神经辐射场（NeRF）的3D实例分割框架，用于精确农作物计数，无需作物特定参数调优，在棉花、苹果、梨数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 室外农田环境中，部分遮挡和作物聚集导致的模糊性使基于图像的作物计数面临巨大挑战，需要更精确的3D实例分割方法。

Method: 利用多视角2D图像，结合神经辐射场（NeRF）进行视图合成，引入作物可见性和掩码一致性评分，结合3D信息实现3D实例分割和精确计数。

Result: 在棉花、苹果、梨三种农作物数据集上验证，尽管作物颜色、形状、尺寸差异大，仍保持一致的计数性能，优于现有方法。

Conclusion: 提出的3D实例分割框架能有效解决室外农作物计数中的遮挡和聚集问题，无需作物特定参数调优，性能优于现有方法，并贡献了棉花数据集。

Abstract: Rigorous crop counting is crucial for effective agricultural management and informed intervention strategies. However, in outdoor field environments, partial occlusions combined with inherent ambiguity in distinguishing clustered crops from individual viewpoints poses an immense challenge for image-based segmentation methods. To address these problems, we introduce a novel crop counting framework designed for exact enumeration via 3D instance segmentation. Our approach utilizes 2D images captured from multiple viewpoints and associates independent instance masks for neural radiance field (NeRF) view synthesis. We introduce crop visibility and mask consistency scores, which are incorporated alongside 3D information from a NeRF model. This results in an effective segmentation of crop instances in 3D and highly-accurate crop counts. Furthermore, our method eliminates the dependence on crop-specific parameter tuning. We validate our framework on three agricultural datasets consisting of cotton bolls, apples, and pears, and demonstrate consistent counting performance despite major variations in crop color, shape, and size. A comparative analysis against the state of the art highlights superior performance on crop counting tasks. Lastly, we contribute a cotton plant dataset to advance further research on this topic.

</details>


### [23] [IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation](https://arxiv.org/abs/2601.00212)
*Han Liu,Yubo Fan,Hao Li,Dewei Hu,Daniel Moyer,Zhoubing Xu,Benoit M. Dawant,Ipek Oguz*

Main category: cs.CV

TL;DR: IntraStyler：一种基于示例的样式合成方法，无需先验知识即可捕捉多样化的域内样式，用于增强无监督域适应的图像翻译


<details>
  <summary>Details</summary>
Motivation: 现有无监督域适应方法主要关注源域和目标域之间的域偏移，而域内变异性研究不足。传统方法需要预先指定域内变化进行样式合成，这在实践中不切实际

Method: 提出IntraStyler方法，使用示例图像指导样式合成，使输出样式与示例样式匹配。引入基于对比学习的样式编码器来提取纯样式特征

Result: 在最大的跨模态域适应公共数据集CrossMoDA 2023上评估，证明方法在可控样式合成方面的有效性，以及多样化合成数据对下游分割任务的益处

Conclusion: IntraStyler能够无需先验知识捕捉多样化的域内样式，为无监督域适应提供更有效的图像翻译方法，代码已开源

Abstract: Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.

</details>


### [24] [From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning](https://arxiv.org/abs/2601.00215)
*Omar Sharif,Eftekhar Hossain,Patrick Ng*

Main category: cs.CV

TL;DR: 使用强化学习提升多模态大语言模型的视觉推理能力，通过奖励函数激励模型整合视觉信息进行更长的结构化推理，在视觉谜题等任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在生成推理链时缺乏对视觉信息的有效整合，限制了其在需要准确视觉感知的任务（如视觉谜题）上的表现。研究表明视觉感知是这类任务的关键瓶颈。

Method: 采用奖励驱动的强化学习方法，设计了六个针对不同推理方面的奖励函数（包括图像理解、思考步骤和答案准确性），使用组相对策略优化（GRPO）来激励更长的结构化推理，防止视觉信息被绕过。

Result: 实验表明，将图像转换为文本描述能显著提升性能（Claude 3.5提升26.7%，Claude 3.7提升23.6%）。在Qwen-2.5-VL-7B模型上，该方法比基础模型提升了5.56%，在领域内和领域外设置下都取得了一致的性能增益。

Conclusion: 强化学习是解锁开源多模态大语言模型长视觉推理能力的有效机制，无需昂贵的监督数据，通过精心设计的奖励函数可以激励模型更好地整合视觉信息进行结构化推理。

Abstract: Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7.
  To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.

</details>


### [25] [LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization](https://arxiv.org/abs/2601.00222)
*Jie Li,Kwan-Yee K. Wong,Kai Han*

Main category: cs.CV

TL;DR: LooC是一种新型向量量化方法，通过低维组合码本和插值外推机制，在显著减小码本规模的同时提升性能，可作为即插即用模块应用于各种下游任务。


<details>
  <summary>Details</summary>
Motivation: 随着数据和模型复杂度不断增加，现有向量量化方法面临容量与紧凑性的矛盾。需要既能保持高容量又能更紧凑的VQ方法。

Method: 1. 引入低维组合码本：将码向量视为特征向量的低维组合单元，通过组合而非单独匹配来扩展解空间；2. 参数自由的插值外推机制：在VQ过程中平滑增强特征，保留细节和保真度；3. 全码本利用设计，避免崩溃问题。

Result: 在不同任务、数据集和架构上的广泛评估表明，LooC显著优于现有VQ方法，在码本规模大幅减小的情况下实现了最先进的性能。

Conclusion: LooC成功解决了VQ方法中容量与紧凑性的矛盾，通过创新的低维组合码本设计和插值外推机制，实现了更紧凑、高性能的向量量化，可作为通用模块应用于各种VQ相关任务。

Abstract: Vector quantization (VQ) is a prevalent and fundamental technique that discretizes continuous feature vectors by approximating them using a codebook. As the diversity and complexity of data and models continue to increase, there is an urgent need for high-capacity, yet more compact VQ methods. This paper aims to reconcile this conflict by presenting a new approach called LooC, which utilizes an effective Low-dimensional codebook for Compositional vector quantization. Firstly, LooC introduces a parameter-efficient codebook by reframing the relationship between codevectors and feature vectors, significantly expanding its solution space. Instead of individually matching codevectors with feature vectors, LooC treats them as lower-dimensional compositional units within feature vectors and combines them, resulting in a more compact codebook with improved performance. Secondly, LooC incorporates a parameter-free extrapolation-by-interpolation mechanism to enhance and smooth features during the VQ process, which allows for better preservation of details and fidelity in feature approximation. The design of LooC leads to full codebook usage, effectively utilizing the compact codebook while avoiding the problem of collapse. Thirdly, LooC can serve as a plug-and-play module for existing methods for different downstream tasks based on VQ. Finally, extensive evaluations on different tasks, datasets, and architectures demonstrate that LooC outperforms existing VQ methods, achieving state-of-the-art performance with a significantly smaller codebook.

</details>


### [26] [Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions](https://arxiv.org/abs/2601.00225)
*Aobo Li,Jinjian Wu,Yongxu Liu,Leida Li,Weisheng Dong*

Main category: cs.CV

TL;DR: 提出SynDR-IQA框架，通过重塑合成数据分布来提升盲图像质量评估的泛化能力，解决了合成数据特征聚类导致的回归性能受限问题。


<details>
  <summary>Details</summary>
Motivation: 盲图像质量评估（BIQA）面临标注数据稀缺问题，合成数据是解决方案但现有合成数据集训练的模型泛化能力有限。研究发现合成数据集学习到的表示呈现离散聚类模式：高质量图像特征围绕参考图像聚类，低质量图像特征按失真类型聚类，这源于合成数据分布而非模型架构问题。

Method: 提出SynDR-IQA框架，基于样本多样性和冗余对泛化误差影响的理论推导，采用两种策略：1）分布感知的多样化内容上采样，在保持内容分布的同时增强视觉多样性；2）密度感知的冗余聚类下采样，通过减少密集聚类区域的样本密度来平衡样本分布。

Result: 在三种跨数据集设置（合成到真实、合成到算法、合成到合成）上进行广泛实验，证明了方法的有效性。

Conclusion: 通过重塑合成数据分布可以显著提升BIQA模型的泛化能力，SynDR-IQA为解决合成数据训练中的分布问题提供了有效框架。

Abstract: Blind Image Quality Assessment (BIQA) has advanced significantly through deep learning, but the scarcity of large-scale labeled datasets remains a challenge. While synthetic data offers a promising solution, models trained on existing synthetic datasets often show limited generalization ability. In this work, we make a key observation that representations learned from synthetic datasets often exhibit a discrete and clustered pattern that hinders regression performance: features of high-quality images cluster around reference images, while those of low-quality images cluster based on distortion types. Our analysis reveals that this issue stems from the distribution of synthetic data rather than model architecture. Consequently, we introduce a novel framework SynDR-IQA, which reshapes synthetic data distribution to enhance BIQA generalization. Based on theoretical derivations of sample diversity and redundancy's impact on generalization error, SynDR-IQA employs two strategies: distribution-aware diverse content upsampling, which enhances visual diversity while preserving content distribution, and density-aware redundant cluster downsampling, which balances samples by reducing the density of densely clustered areas. Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic) demonstrate the effectiveness of our method. The code is available at https://github.com/Li-aobo/SynDR-IQA.

</details>


### [27] [Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection](https://arxiv.org/abs/2601.00237)
*Chao Yang,Haoyuan Zheng,Yue Ma*

Main category: cs.CV

TL;DR: 提出跨模态数据增强框架，结合CycleGAN和YOLOv8解决PCB红外缺陷检测数据稀缺问题，通过可见光到红外的图像转换生成伪红外数据，提升检测性能。


<details>
  <summary>Details</summary>
Motivation: PCB红外缺陷检测面临数据稀缺瓶颈，传统方法依赖成对监督数据，而真实红外数据获取成本高且数量有限，需要解决低数据条件下的特征学习问题。

Method: 使用CycleGAN进行无配对图像转换，将丰富的可见光PCB图像映射到红外域，生成高质量伪红外样本；采用异构训练策略，融合伪红外数据和有限真实红外数据训练轻量级YOLOv8检测器。

Result: 该方法在低数据条件下有效增强特征学习，增强后的检测器显著优于仅使用有限真实数据的模型，性能接近完全监督训练的基准，证明伪红外合成作为工业检测的鲁棒增强策略的有效性。

Conclusion: 跨模态数据增强框架成功解决了PCB红外缺陷检测的数据稀缺问题，通过生成伪红外数据与有限真实数据融合训练，实现了接近完全监督的性能，为工业检测提供了有效的低数据解决方案。

Abstract: This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.

</details>


### [28] [Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture](https://arxiv.org/abs/2601.00243)
*Anirudha Ghosh,Ritam Sarkar,Debaditya Barman*

Main category: cs.CV

TL;DR: 本文提出一个轻量级框架，用于害虫检测和农药推荐，适用于智能手机和无人机等低资源设备，帮助小农户实现精准农业。


<details>
  <summary>Details</summary>
Motivation: 传统害虫管理方法依赖人工田间检查和化学农药，成本高、耗时、劳动密集且对环境有害。需要为小农户开发适合低资源设备的解决方案。

Method: 框架包含两个模块：1) 害虫检测模块使用轻量级CNN结合原型元学习，即使在少量训练样本下也能准确识别害虫；2) 农药推荐模块结合作物类型和生长阶段等环境因素，推荐安全环保的农药。

Result: 提出的轻量级CNN达到与最先进模型相当的高精度，同时显著降低计算复杂度。决策支持系统减少了对传统化学农药的依赖，促进了可持续实践。

Conclusion: 该框架在精准农业中具有实时应用潜力，特别适合资源有限的小农户，通过智能技术改善害虫管理并减少环境影响。

Abstract: Effective pest management is crucial for enhancing agricultural productivity, especially for crops such as sugarcane and wheat that are highly vulnerable to pest infestations. Traditional pest management methods depend heavily on manual field inspections and the use of chemical pesticides. These approaches are often costly, time-consuming, labor-intensive, and can have a negative impact on the environment. To overcome these challenges, this study presents a lightweight framework for pest detection and pesticide recommendation, designed for low-resource devices such as smartphones and drones, making it suitable for use by small and marginal farmers.
  The proposed framework includes two main components. The first is a Pest Detection Module that uses a compact, lightweight convolutional neural network (CNN) combined with prototypical meta-learning to accurately identify pests even when only a few training samples are available. The second is a Pesticide Recommendation Module that incorporates environmental factors like crop type and growth stage to suggest safe and eco-friendly pesticide recommendations. To train and evaluate our framework, a comprehensive pest image dataset was developed by combining multiple publicly available datasets. The final dataset contains samples with different viewing angles, pest sizes, and background conditions to ensure strong generalization.
  Experimental results show that the proposed lightweight CNN achieves high accuracy, comparable to state-of-the-art models, while significantly reducing computational complexity. The Decision Support System additionally improves pest management by reducing dependence on traditional chemical pesticides and encouraging sustainable practices, demonstrating its potential for real-time applications in precision agriculture.

</details>


### [29] [TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models](https://arxiv.org/abs/2601.00260)
*Kohei Yamamoto,Tomohiro Kikuchi*

Main category: cs.CV

TL;DR: TotalFM是一个基于器官分离概念的放射学基础模型，通过3D-CT图像与语言表达的对应学习，在零样本器官病变分类任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 放射学基础模型在3D-CT体数据训练中面临计算成本约束的挑战，需要平衡计算效率和表示能力。

Method: 采用器官分离概念，通过分割技术和LLM处理放射学报告自动创建器官体积-发现语句对，结合VideoMAE自监督预训练和体积-文本对的对比学习。

Result: 在零样本器官病变分类中，83%器官优于CT-CLIP，64%器官优于Merlin；在零样本发现病变分类中，83%类别AUROC优于Merlin；放射学报告生成任务性能与现有VLM相当。

Conclusion: 器官分离学习框架为3D-CT基础模型的实际应用提供了现实有效的设计指南，展示了高泛化性能。

Abstract: While foundation models in radiology are expected to be applied to various clinical tasks, computational cost constraints remain a major challenge when training on 3D-CT volumetric data. In this study, we propose TotalFM, a radiological foundation model that efficiently learns the correspondence between 3D-CT images and linguistic expressions based on the concept of organ separation, utilizing a large-scale dataset of 140,000 series. By automating the creation of organ volume and finding-sentence pairs through segmentation techniques and Large Language Model (LLM)-based radiology report processing, and by combining self-supervised pre-training via VideoMAE with contrastive learning using volume-text pairs, we aimed to balance computational efficiency and representation capability. In zero-shot organ-wise lesion classification tasks, the proposed model achieved higher F1 scores in 83% (5/6) of organs compared to CT-CLIP and 64% (9/14) of organs compared to Merlin. These results suggest that the proposed model exhibits high generalization performance in a clinical evaluation setting using actual radiology report sentences. Furthermore, in zero-shot finding-wise lesion classification tasks, our model achieved a higher AUROC in 83% (25/30) of finding categories compared to Merlin. We also confirmed performance comparable to existing Vision-Language Models (VLMs) in radiology report generation tasks. Our results demonstrate that the organ-separated learning framework can serve as a realistic and effective design guideline for the practical implementation of 3D-CT foundation models.

</details>


### [30] [S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding](https://arxiv.org/abs/2601.00264)
*He Wang,Longteng Guo,Pengkang Huo,Xuanxu Lin,Yichen Yuan,Jie Jiang,Jing Liu*

Main category: cs.CV

TL;DR: S1-MMAlign是一个包含1550万高质量图像-文本对的多学科科学多模态数据集，通过AI增强管道解决科学图像与文本描述之间的语义鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在通用领域取得革命性进展，但在科学发现中的应用受到科学图像复杂性与文本描述稀疏性之间巨大语义鸿沟的阻碍。现有科学图像-文本对存在对齐弱、质量差的问题。

Method: 从250万篇开放获取科学论文中收集1550万图像-文本对，涵盖物理、生物、工程等多学科。引入AI就绪的语义增强管道，利用Qwen-VL多模态大模型系列，结合论文摘要和引用上下文重新为图像生成描述。

Result: 技术验证显示增强显著提升数据质量：基于SciBERT的伪困惑度指标显示语义模糊性降低，CLIP分数表明图像-文本对齐提升18.21%。数据集在HuggingFace公开可用。

Conclusion: S1-MMAlign为解决科学多模态学习中的语义鸿沟问题提供了基础资源，为AI for Science时代的科学推理和跨模态理解提供了重要支持。

Abstract: Multimodal learning has revolutionized general domain tasks, yet its application in scientific discovery is hindered by the profound semantic gap between complex scientific imagery and sparse textual descriptions. We present S1-MMAlign, a large-scale, multi-disciplinary multimodal dataset comprising over 15.5 million high-quality image-text pairs derived from 2.5 million open-access scientific papers. Spanning disciplines from physics and biology to engineering, the dataset captures diverse visual modalities including experimental setups, heatmaps, and microscopic imagery. To address the pervasive issue of weak alignment in raw scientific captions, we introduce an AI-ready semantic enhancement pipeline that utilizes the Qwen-VL multimodal large model series to recaption images by synthesizing context from paper abstracts and citation contexts. Technical validation demonstrates that this enhancement significantly improves data quality: SciBERT-based pseudo-perplexity metrics show reduced semantic ambiguity, while CLIP scores indicate an 18.21% improvement in image-text alignment. S1-MMAlign provides a foundational resource for advancing scientific reasoning and cross-modal understanding in the era of AI for Science. The dataset is publicly available at https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign.

</details>


### [31] [ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching](https://arxiv.org/abs/2601.00267)
*Yi Sun,Xinhao Zhong,Hongyan Li,Yimin Zhou,Junhao Li,Bin Chen,Xuan Wang*

Main category: cs.CV

TL;DR: 提出ActErase：一种无需训练的扩散模型概念擦除方法，通过激活差异分析和动态替换实现高效概念移除


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法依赖数据密集且计算昂贵的微调，限制了实际应用。扩散模型激活主要由通用概念组成，仅极小部分表示目标概念，这为无需训练的方法提供了可能。

Method: 通过提示对分析识别激活差异区域，提取目标激活并在前向传播过程中动态替换输入激活，实现无需训练的概念擦除。

Result: 在三个关键擦除任务（裸露、艺术风格、物体移除）上达到SOTA性能，有效保持模型生成能力，并展现对抗攻击的强鲁棒性。

Conclusion: ActErase为扩散模型中的概念操作建立了新的即插即用范式，实现了轻量级但有效的概念擦除。

Abstract: Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model's activations are predominantly composed of generic concepts, with only a minimal component can represent the target concept, we propose a novel training-free method (ActErase) for efficient concept erasure. Specifically, the proposed method operates by identifying activation difference regions via prompt-pair analysis, extracting target activations and dynamically replacing input activations during forward passes. Comprehensive evaluations across three critical erasure tasks (nudity, artistic style, and object removal) demonstrates that our training-free method achieves state-of-the-art (SOTA) erasure performance, while effectively preserving the model's overall generative capability. Our approach also exhibits strong robustness against adversarial attacks, establishing a new plug-and-play paradigm for lightweight yet effective concept manipulation in diffusion models.

</details>


### [32] [FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering](https://arxiv.org/abs/2601.00269)
*Chaodong Tong,Qi Zhang,Chen Li,Lei Jiang,Yanbing Liu*

Main category: cs.CV

TL;DR: FaithSCAN：一种轻量级网络，通过利用VLM的丰富内部信号（包括token级解码不确定性、中间视觉表示和跨模态对齐特征）来检测VQA中的幻觉，无需昂贵的人工标注即可实现监督训练。


<details>
  <summary>Details</summary>
Motivation: VQA中的忠实性幻觉（模型产生流畅但视觉上无根据的答案）严重影响了在安全关键应用中的可靠性。现有检测方法存在计算开销大、依赖外部资源、仅捕捉有限不确定性等局限性。

Method: 提出FaithSCAN轻量级网络，融合VLM的多种内部信号：token级解码不确定性、中间视觉表示、跨模态对齐特征。采用分支证据编码和不确定性感知注意力进行融合。扩展LLM-as-a-Judge范式到VQA幻觉检测，提出低成本策略自动生成模型依赖的监督信号。

Result: 在多个VQA基准测试中，FaithSCAN在效果和效率上都显著优于现有方法。深入分析表明幻觉源于视觉感知、跨模态推理和语言解码的系统性内部状态变化，不同内部信号提供互补的诊断线索。

Conclusion: FaithSCAN通过利用VLM的丰富内部信号，提供了一种高效、鲁棒的幻觉检测方法，无需昂贵人工标注。研究揭示了多模态幻觉的根本原因，为理解VLM内部工作机制提供了新见解。

Abstract: Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.

</details>


### [33] [Disentangling Hardness from Noise: An Uncertainty-Driven Model-Agnostic Framework for Long-Tailed Remote Sensing Classification](https://arxiv.org/abs/2601.00278)
*Chi Ding,Junxiao Xue,Xinyi Yin,Shi Chen,Yunyun Shi,Yiduo Wang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: 提出DUAL框架，通过证据深度学习动态解耦预测不确定性为认知不确定性和偶然不确定性，分别处理长尾分布中的困难样本和噪声样本


<details>
  <summary>Details</summary>
Motivation: 遥感数据中普遍存在长尾分布，但现有方法未能有效区分困难尾部分布样本和噪声模糊样本，往往不加区分地强调所有低置信度样本，导致对噪声数据的过拟合

Method: 基于证据深度学习，提出模型无关的不确定性感知框架DUAL，动态解耦预测不确定性为认知不确定性（EU）和偶然不确定性（AU）。使用EU作为样本稀缺性指标指导困难尾样本的重新加权策略，利用AU量化数据模糊性，采用自适应标签平滑机制抑制噪声影响

Result: 在多个数据集和各种骨干网络上的广泛实验证明了该框架的有效性和泛化能力，超越了TGN和SADE等强基线方法。消融研究进一步验证了设计选择的关键性

Conclusion: DUAL框架能够有效区分和处理长尾分布中的困难样本和噪声样本，通过不确定性解耦机制实现了更好的性能，为遥感长尾学习问题提供了新的解决方案

Abstract: Long-Tailed distributions are pervasive in remote sensing due to the inherently imbalanced occurrence of grounded objects. However, a critical challenge remains largely overlooked, i.e., disentangling hard tail data samples from noisy ambiguous ones. Conventional methods often indiscriminately emphasize all low-confidence samples, leading to overfitting on noisy data. To bridge this gap, building upon Evidential Deep Learning, we propose a model-agnostic uncertainty-aware framework termed DUAL, which dynamically disentangles prediction uncertainty into Epistemic Uncertainty (EU) and Aleatoric Uncertainty (AU). Specifically, we introduce EU as an indicator of sample scarcity to guide a reweighting strategy for hard-to-learn tail samples, while leveraging AU to quantify data ambiguity, employing an adaptive label smoothing mechanism to suppress the impact of noise. Extensive experiments on multiple datasets across various backbones demonstrate the effectiveness and generalization of our framework, surpassing strong baselines such as TGN and SADE. Ablation studies provide further insights into the crucial choices of our design.

</details>


### [34] [SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting](https://arxiv.org/abs/2601.00285)
*Jun-Jee Chao,Volkan Isler*

Main category: cs.CV

TL;DR: SV-GS：基于稀疏观测的动态目标重建框架，通过骨架驱动变形场和运动估计，在稀疏视角和时间采样下实现高质量动态重建。


<details>
  <summary>Details</summary>
Motivation: 现实世界中动态目标重建面临观测稀疏的挑战（如监控摄像头），传统方法需要密集的多视角视频，难以在真实场景中应用。

Method: 使用粗略骨架图和初始静态重建作为输入，优化骨架驱动变形场，包含粗粒度关节姿态估计器和细粒度变形模块，仅让姿态估计器随时间变化以实现平滑运动插值。

Result: 在合成数据集上，稀疏观测条件下PSNR比现有方法提升34%；在真实数据集上，使用远少于单目视频方法的帧数达到可比性能；可用扩散生成先验替代初始静态重建。

Conclusion: SV-GS能够在稀疏观测条件下有效重建动态目标，通过骨架引导和变形场优化实现高质量动态重建，具有实际应用价值。

Abstract: Reconstructing a dynamic target moving over a large area is challenging. Standard approaches for dynamic object reconstruction require dense coverage in both the viewing space and the temporal dimension, typically relying on multi-view videos captured at each time step. However, such setups are only possible in constrained environments. In real-world scenarios, observations are often sparse over time and captured sparsely from diverse viewpoints (e.g., from security cameras), making dynamic reconstruction highly ill-posed. We present SV-GS, a framework that simultaneously estimates a deformation model and the object's motion over time under sparse observations. To initialize SV-GS, we leverage a rough skeleton graph and an initial static reconstruction as inputs to guide motion estimation. (Later, we show that this input requirement can be relaxed.) Our method optimizes a skeleton-driven deformation field composed of a coarse skeleton joint pose estimator and a module for fine-grained deformations. By making only the joint pose estimator time-dependent, our model enables smooth motion interpolation while preserving learned geometric details. Experiments on synthetic datasets show that our method outperforms existing approaches under sparse observations by up to 34% in PSNR, and achieves comparable performance to dense monocular video methods on real-world datasets despite using significantly fewer frames. Moreover, we demonstrate that the input initial static reconstruction can be replaced by a diffusion-based generative prior, making our method more practical for real-world scenarios.

</details>


### [35] [Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies](https://arxiv.org/abs/2601.00286)
*Ali Anaissi,Ali Braytee,Weidong Huang,Junaid Akram,Alaa Farhat,Jie Hua*

Main category: cs.CV

TL;DR: 开发基于Swin Transformer的深度学习模型，在ISIC2019数据集上对8种皮肤病变分类达到87.71%准确率，用于辅助临床诊断和患者自评估


<details>
  <summary>Details</summary>
Motivation: 皮肤疾病日益普遍，但皮肤科医生资源有限，需要智能工具来支持患者和医生进行及时准确的皮肤疾病诊断

Method: 利用公开皮肤疾病图像数据集进行预训练，提取视觉特征；改进模型架构，优化数据预处理流程，应用针对性数据增强技术；最终采用Swin Transformer模型

Result: 在ISIC2019数据集上对8种皮肤病变类别实现了87.71%的预测准确率

Conclusion: 该模型展示了作为临床医生诊断支持工具和患者自评估辅助工具的潜力，能够帮助解决皮肤科医生资源不足的问题

Abstract: As dermatological conditions become increasingly common and the availability of dermatologists remains limited, there is a growing need for intelligent tools to support both patients and clinicians in the timely and accurate diagnosis of skin diseases. In this project, we developed a deep learning based model for the classification and diagnosis of skin conditions. By leveraging pretraining on publicly available skin disease image datasets, our model effectively extracted visual features and accurately classified various dermatological cases. Throughout the project, we refined the model architecture, optimized data preprocessing workflows, and applied targeted data augmentation techniques to improve overall performance. The final model, based on the Swin Transformer, achieved a prediction accuracy of 87.71 percent across eight skin lesion classes on the ISIC2019 dataset. These results demonstrate the model's potential as a diagnostic support tool for clinicians and a self assessment aid for patients.

</details>


### [36] [TimeColor: Flexible Reference Colorization via Temporal Concatenation](https://arxiv.org/abs/2601.00296)
*Bryan Constantine Sadihin,Yihao Meng,Michael Hua Wang,Matteo Jiahao Chen,Hang Su*

Main category: cs.CV

TL;DR: TimeColor是一个基于草图的视频着色模型，支持使用异构、可变数量的参考图像，通过显式的每参考区域分配和时空对应掩码注意力来提高着色质量。


<details>
  <summary>Details</summary>
Motivation: 现有着色模型通常只使用单一参考（通常是场景的第一帧），忽略了其他条件数据源，如角色设定图、背景图像或任意已着色帧，这限制了着色质量和一致性。

Method: 1. 将参考图像编码为额外的潜在帧，在时间维度上拼接，使模型能在每个扩散步骤中同时处理参考图像而保持参数不变；2. 使用显式的每参考区域分配；3. 采用时空对应掩码注意力来增强主体-参考绑定；4. 使用模态分离的RoPE索引。

Result: 在SAKUGA-42M数据集上的实验表明，TimeColor在单参考和多参考协议下均优于现有基线，在颜色保真度、身份一致性和时间稳定性方面都有显著提升。

Conclusion: TimeColor通过支持异构、可变数量的参考图像，结合显式区域分配和时空注意力机制，有效解决了传统视频着色模型的局限性，提高了着色质量和一致性。

Abstract: Most colorization models condition only on a single reference, typically the first frame of the scene. However, this approach ignores other sources of conditional data, such as character sheets, background images, or arbitrary colorized frames. We propose TimeColor, a sketch-based video colorization model that supports heterogeneous, variable-count references with the use of explicit per-reference region assignment. TimeColor encodes references as additional latent frames which are concatenated temporally, permitting them to be processed concurrently in each diffusion step while keeping the model's parameter count fixed. TimeColor also uses spatiotemporal correspondence-masked attention to enforce subject-reference binding in addition to modality-disjoint RoPE indexing. These mechanisms mitigate shortcutting and cross-identity palette leakage. Experiments on SAKUGA-42M under both single- and multi-reference protocols show that TimeColor improves color fidelity, identity consistency, and temporal stability over prior baselines.

</details>


### [37] [VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning](https://arxiv.org/abs/2601.00307)
*Anns Ijaz,Muhammad Azeem Javed*

Main category: cs.CV

TL;DR: VisNet提出了一种计算高效的行人重识别模型，通过多尺度特征融合、语义聚类、动态权重平均和FIDI损失函数，在保持高精度的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别方法虽然精度高，但计算成本过大，不适合在计算资源有限的监控和移动应用中进行实时部署。需要一种既准确又计算高效的解决方案。

Method: VisNet采用多尺度特征融合（融合ResNet50的1-4阶段）、语义聚类（基于解剖学身体分区的规则伪标签）、动态权重平均平衡分类语义正则化，以及FIDI损失函数改进度量学习。

Result: 在Market-1501数据集上达到87.05% Rank-1和77.65% mAP，仅需32.41M参数和4.601 GFLOPs，在保持高精度的同时显著降低计算复杂度。

Conclusion: VisNet为计算资源有限的监控和移动应用提供了一种实用的实时行人重识别解决方案，在精度和效率之间取得了良好平衡。

Abstract: Person re-identification (ReID) is an extremely important area in both surveillance and mobile applications, requiring strong accuracy with minimal computational cost. State-of-the-art methods give good accuracy but with high computational budgets. To remedy this, this paper proposes VisNet, a computationally efficient and effective re-identification model suitable for real-world scenarios. It is the culmination of conceptual contributions, including feature fusion at multiple scales with automatic attention on each, semantic clustering with anatomical body partitioning, a dynamic weight averaging technique to balance classification semantic regularization, and the use of loss function FIDI for improved metric learning tasks. The multiple scales fuse ResNet50's stages 1 through 4 without the use of parallel paths, with semantic clustering introducing spatial constraints through the use of rule-based pseudo-labeling. VisNet achieves 87.05% Rank-1 and 77.65% mAP on the Market-1501 dataset, having 32.41M parameters and 4.601 GFLOPs, hence, proposing a practical approach for real-time deployment in surveillance and mobile applications where computational resources are limited.

</details>


### [38] [ReMA: A Training-Free Plug-and-Play Mixing Augmentation for Video Behavior Recognition](https://arxiv.org/abs/2601.00311)
*Feng-Qi Cui,Jinyang Huang,Sirui Zhao,Jinglong Guo,Qifan Cai,Xin Yan,Zhi Liu*

Main category: cs.CV

TL;DR: ReMA是一种视频数据增强策略，通过控制混合过程来扩展表示同时保持类别稳定性，包含表示对齐和动态选择两个机制，无需额外监督即可提升视频行为识别的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视频数据增强方法多为扰动驱动，会引入不可控的变异，放大非判别性因素，削弱类内分布结构，导致表示漂移和跨时间尺度增益不一致的问题。

Method: 提出表示感知混合增强(ReMA)，包含两个互补机制：1)表示对齐机制(RAM)，在分布对齐约束下进行结构化类内混合；2)动态选择机制(DSM)，生成运动感知的时空掩码来定位扰动，避开判别敏感区域并促进时间一致性。

Result: 在多个视频行为基准测试上的广泛实验表明，ReMA能一致地提升不同时空粒度下的泛化性和鲁棒性。

Conclusion: ReMA通过联合控制混合的方式和位置，无需额外监督或可训练参数即可改善表示鲁棒性，为视频行为识别提供了一种有效的即插即用增强策略。

Abstract: Video behavior recognition demands stable and discriminative representations under complex spatiotemporal variations. However, prevailing data augmentation strategies for videos remain largely perturbation-driven, often introducing uncontrolled variations that amplify non-discriminative factors, which finally weaken intra-class distributional structure and representation drift with inconsistent gains across temporal scales. To address these problems, we propose Representation-aware Mixing Augmentation (ReMA), a plug-and-play augmentation strategy that formulates mixing as a controlled replacement process to expand representations while preserving class-conditional stability. ReMA integrates two complementary mechanisms. Firstly, the Representation Alignment Mechanism (RAM) performs structured intra-class mixing under distributional alignment constraints, suppressing irrelevant intra-class drift while enhancing statistical reliability. Then, the Dynamic Selection Mechanism (DSM) generates motion-aware spatiotemporal masks to localize perturbations, guiding them away from discrimination-sensitive regions and promoting temporal coherence. By jointly controlling how and where mixing is applied, ReMA improves representation robustness without additional supervision or trainable parameters. Extensive experiments on diverse video behavior benchmarks demonstrate that ReMA consistently enhances generalization and robustness across different spatiotemporal granularities.

</details>


### [39] [Depth-Synergized Mamba Meets Memory Experts for All-Day Image Reflection Separation](https://arxiv.org/abs/2601.00322)
*Siyan Fang,Long Peng,Yuntao Wang,Ruonan Wei,Yuehuan Wang*

Main category: cs.CV

TL;DR: DMDNet提出深度记忆解耦网络，通过深度感知扫描、深度协同状态空间模型和记忆专家补偿模块，解决图像反射分离中对比度相似时的混淆问题，特别针对夜间场景构建了NightIRS数据集。


<details>
  <summary>Details</summary>
Motivation: 现有单图像反射分离方法在传输层和反射层对比度相似时容易混淆，夜间场景下这一问题更加严重，需要更有效的解决方案。

Method: 提出DMDNet网络：1) 深度感知扫描(DAScan)引导Mamba关注显著结构；2) 深度协同状态空间模型(DS-SSM)通过深度调节状态激活敏感性；3) 记忆专家补偿模块(MECM)利用跨图像历史知识提供层特定补偿；4) 构建NightIRS夜间反射分离数据集。

Result: DMDNet在白天和夜间场景下均优于现有最先进方法，通过深度引导和记忆补偿有效解决了层混淆问题。

Conclusion: DMDNet通过深度感知和记忆补偿机制，有效解决了图像反射分离中对比度相似时的混淆问题，特别是在夜间场景下表现出色，为反射分离任务提供了新的解决方案。

Abstract: Image reflection separation aims to disentangle the transmission layer and the reflection layer from a blended image. Existing methods rely on limited information from a single image, tending to confuse the two layers when their contrasts are similar, a challenge more severe at night. To address this issue, we propose the Depth-Memory Decoupling Network (DMDNet). It employs the Depth-Aware Scanning (DAScan) to guide Mamba toward salient structures, promoting information flow along semantic coherence to construct stable states. Working in synergy with DAScan, the Depth-Synergized State-Space Model (DS-SSM) modulates the sensitivity of state activations by depth, suppressing the spread of ambiguous features that interfere with layer disentanglement. Furthermore, we introduce the Memory Expert Compensation Module (MECM), leveraging cross-image historical knowledge to guide experts in providing layer-specific compensation. To address the lack of datasets for nighttime reflection separation, we construct the Nighttime Image Reflection Separation (NightIRS) dataset. Extensive experiments demonstrate that DMDNet outperforms state-of-the-art methods in both daytime and nighttime.

</details>


### [40] [HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection](https://arxiv.org/abs/2601.00327)
*Naiqi Zhang,Chuancheng Shi,Jingtong Dou,Wenhua Wu,Fei Shen,Jianhua Cao*

Main category: cs.CV

TL;DR: HarmoniAD：一种频率引导的双分支框架，通过解耦高频和低频路径来平衡结构细节和语义信息，解决工业异常检测中的结构-语义权衡问题。


<details>
  <summary>Details</summary>
Motivation: 工业产品质量检测中，微小缺陷检测至关重要。现有方法面临结构-语义权衡：结构导向模型（如基于频率的滤波器）对噪声敏感，而语义导向模型（如基于CLIP的编码器）常忽略细节。需要一种能同时捕捉精细结构和全局语义的方法。

Method: 提出HarmoniAD频率引导双分支框架：1）使用CLIP图像编码器提取特征；2）将特征转换到频域；3）解耦为高频和低频路径进行互补建模。高频分支配备细粒度结构注意力模块（FSAM）增强纹理和边缘检测小异常；低频分支使用全局结构上下文模块（GSCM）捕捉长距离依赖并保持语义一致性。采用多类联合训练策略。

Result: 在MVTec-AD、VisA和BTAD数据集上实现最先进的性能，同时具备高敏感性和鲁棒性。

Conclusion: HarmoniAD通过频率域解耦和双分支互补建模，有效平衡了异常检测中的结构细节和语义信息，解决了现有方法的局限性，在多个基准数据集上表现出色。

Abstract: Anomaly detection is crucial in industrial product quality inspection. Failing to detect tiny defects often leads to serious consequences. Existing methods face a structure-semantics trade-off: structure-oriented models (such as frequency-based filters) are noise-sensitive, while semantics-oriented models (such as CLIP-based encoders) often miss fine details. To address this, we propose HarmoniAD, a frequency-guided dual-branch framework. Features are first extracted by the CLIP image encoder, then transformed into the frequency domain, and finally decoupled into high- and low-frequency paths for complementary modeling of structure and semantics. The high-frequency branch is equipped with a fine-grained structural attention module (FSAM) to enhance textures and edges for detecting small anomalies, while the low-frequency branch uses a global structural context module (GSCM) to capture long-range dependencies and preserve semantic consistency. Together, these branches balance fine detail and global semantics. HarmoniAD further adopts a multi-class joint training strategy, and experiments on MVTec-AD, VisA, and BTAD show state-of-the-art performance with both sensitivity and robustness.

</details>


### [41] [Joint Geometry-Appearance Human Reconstruction in a Unified Latent Space via Bridge Diffusion](https://arxiv.org/abs/2601.00328)
*Yingzhi Tang,Qijian Zhang,Junhui Hou*

Main category: cs.CV

TL;DR: JGA-LBD提出了一种从单张RGB图像重建3D数字人的统一框架，通过联合潜在表示和桥接扩散方法，同时建模几何和外观，解决了现有方法分离重建导致不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常采用分离的几何估计和外观合成流程，这阻碍了统一重建并导致不一致性。从单张RGB图像实现一致且高保真的3D数字人重建是一个固有挑战。

Method: 1) 将几何和外观统一建模到联合潜在表示中；2) 将所有条件统一为3D高斯表示，通过共享稀疏变分自编码器压缩到统一潜在空间；3) 使用桥接扩散从部分观测开始，专注于推断缺失组件；4) 通过专用解码模块提取完整几何结构并渲染新视角。

Result: 实验表明，JGA-LBD在几何保真度和外观质量方面均优于当前最先进方法，包括具有挑战性的野外场景。

Conclusion: JGA-LBD通过统一的潜在表示和桥接扩散方法，成功实现了从单张RGB图像对3D数字人的几何和外观进行一致且高保真的联合重建。

Abstract: Achieving consistent and high-fidelity geometry and appearance reconstruction of 3D digital humans from a single RGB image is inherently a challenging task. Existing studies typically resort to decoupled pipelines for geometry estimation and appearance synthesis, often hindering unified reconstruction and causing inconsistencies. This paper introduces \textbf{JGA-LBD}, a novel framework that unifies the modeling of geometry and appearance into a joint latent representation and formulates the generation process as bridge diffusion. Observing that directly integrating heterogeneous input conditions (e.g., depth maps, SMPL models) leads to substantial training difficulties, we unify all conditions into the 3D Gaussian representations, which can be further compressed into a unified latent space through a shared sparse variational autoencoder (VAE). Subsequently, the specialized form of bridge diffusion enables to start with a partial observation of the target latent code and solely focuses on inferring the missing components. Finally, a dedicated decoding module extracts the complete 3D human geometric structure and renders novel views from the inferred latent representation. Experiments demonstrate that JGA-LBD outperforms current state-of-the-art approaches in terms of both geometry fidelity and appearance quality, including challenging in-the-wild scenarios. Our code will be made publicly available at https://github.com/haiantyz/JGA-LBD.

</details>


### [42] [Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation](https://arxiv.org/abs/2601.00344)
*Bruce Mugizi,Sudi Murindanyi,Olivia Nakacwa,Andrew Katumba*

Main category: cs.CV

TL;DR: 基于计算机视觉的实时智能交通监控系统，用于乌干达等发展中国家，实现车辆检测、车牌识别、速度估计和自动罚单发送


<details>
  <summary>Details</summary>
Motivation: 超速是道路死亡事故的主要原因，特别是在乌干达等发展中国家，道路安全基础设施有限，急需有效的交通管理解决方案

Method: 使用计算机视觉技术：YOLOv8进行车牌检测，CNN和Transformer模型进行字符识别，基于感兴趣区域进行速度估计，通过Africa's Talking API自动发送SMS罚单

Result: 车牌检测mAP达97.9%；字符识别中CNN的CER为3.85%，Transformer降至1.79%；速度估计误差在10km/h内；建立了关联用户信息的数据库

Conclusion: 该系统能有效解决资源受限环境中的交通管理需求，通过自动化交通执法有望减少道路事故，在发展中国家具有重要应用前景

Abstract: Speeding is a major contributor to road fatalities, particularly in developing countries such as Uganda, where road safety infrastructure is limited. This study proposes a real-time intelligent traffic surveillance system tailored to such regions, using computer vision techniques to address vehicle detection, license plate recognition, and speed estimation. The study collected a rich dataset using a speed gun, a Canon Camera, and a mobile phone to train the models. License plate detection using YOLOv8 achieved a mean average precision (mAP) of 97.9%. For character recognition of the detected license plate, the CNN model got a character error rate (CER) of 3.85%, while the transformer model significantly reduced the CER to 1.79%. Speed estimation used source and target regions of interest, yielding a good performance of 10 km/h margin of error. Additionally, a database was established to correlate user information with vehicle detection data, enabling automated ticket issuance via SMS via Africa's Talking API. This system addresses critical traffic management needs in resource-constrained environments and shows potential to reduce road accidents through automated traffic enforcement in developing countries where such interventions are urgently needed.

</details>


### [43] [MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation](https://arxiv.org/abs/2601.00504)
*Miaowei Wang,Jakub Zadrożny,Oisin Mac Aodha,Amir Vaxman*

Main category: cs.CV

TL;DR: MotionPhysics：基于自然语言提示的端到端可微分物理参数推断框架，无需真实轨迹或标注视频指导


<details>
  <summary>Details</summary>
Motivation: 传统3D物体和材料模拟需要专家知识和耗时的物理参数调优，难以获得理想的动态行为。需要一种能够从自然语言提示自动推断物理参数的方法。

Method: 1. 使用多模态大语言模型估计材料参数值，并约束在合理范围内；2. 提出可学习的运动蒸馏损失，从预训练视频扩散模型中提取鲁棒的运动先验，同时最小化外观和几何归纳偏差来指导模拟。

Result: 在30多个场景中评估，包括真实世界、人工设计和AI生成的3D物体，涵盖弹性固体、金属、泡沫、沙子、牛顿和非牛顿流体等多种材料。MotionPhysics能生成视觉逼真的动态模拟，超越现有技术，同时自动确定物理上合理的参数。

Conclusion: MotionPhysics框架能够从自然语言提示推断物理参数，实现逼真的动态模拟，无需真实轨迹或标注视频指导，为3D物理模拟提供了新的自动化解决方案。

Abstract: Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.

</details>


### [44] [OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning](https://arxiv.org/abs/2601.00352)
*Liuxiang Qiu,Hui Da,Yuzhen Niu,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出OmniVaT框架解决视觉-触觉学习中的单域泛化问题，通过多模态分数傅里叶适配器和离散树生成模块，有效缓解模态差异并增强对未见域的适应性。


<details>
  <summary>Details</summary>
Motivation: 视觉-触觉学习面临视觉和触觉图像之间的模态差异，以及非标准化触觉传感器和不一致数据收集导致的域差距问题。这些挑战被形式化为单域泛化多模态VTL任务。

Method: 提出OmniVaT框架：1）多模态分数傅里叶适配器（MFFA）将视觉和触觉嵌入映射到统一的嵌入-频率空间，缓解模态差距；2）离散树生成（DTG）模块通过层次树结构获得多样可靠的分数表示，增强对未见域变化的适应性。

Result: 大量实验表明，OmniVaT在SDG-VTL任务上表现出优越的跨域泛化性能。

Conclusion: OmniVaT首次成功解决了视觉-触觉学习的单域泛化问题，通过创新的多模态融合和域适应机制，为具身智能体提供了更鲁棒的物理世界感知能力。

Abstract: Visual-tactile learning (VTL) enables embodied agents to perceive the physical world by integrating visual (VIS) and tactile (TAC) sensors. However, VTL still suffers from modality discrepancies between VIS and TAC images, as well as domain gaps caused by non-standardized tactile sensors and inconsistent data collection procedures. We formulate these challenges as a new task, termed single domain generalization for multimodal VTL (SDG-VTL). In this paper, we propose an OmniVaT framework that, for the first time, successfully addresses this task. On the one hand, OmniVaT integrates a multimodal fractional Fourier adapter (MFFA) to map VIS and TAC embeddings into a unified embedding-frequency space, thereby effectively mitigating the modality gap without multi-domain training data or careful cross-modal fusion strategies. On the other hand, it also incorporates a discrete tree generation (DTG) module that obtains diverse and reliable multimodal fractional representations through a hierarchical tree structure, thereby enhancing its adaptivity to fluctuating domain shifts in unseen domains. Extensive experiments demonstrate the superior cross-domain generalization performance of OmniVaT on the SDG-VTL task.

</details>


### [45] [A Comprehensive Dataset for Human vs. AI Generated Image Detection](https://arxiv.org/abs/2601.00553)
*Rajarshi Roy,Nasrin Imanpour,Ashhar Aziz,Shashwat Bajpai,Gurpreet Singh,Shwetangshu Biswas,Kapil Wanaskar,Parth Patwa,Subhankar Ghosh,Shreyas Dixit,Nilesh Ranjan Pal,Vipula Rawte,Ritvik Garimella,Gaytri Jena,Vasu Sharma,Vinija Jain,Aman Chadha,Aishwarya Naresh Reganti,Amitava Das*

Main category: cs.CV

TL;DR: 提出MS COCOAI数据集，包含96000个真实和AI生成图像，用于检测AI生成图像和识别生成模型


<details>
  <summary>Details</summary>
Motivation: 多模态生成AI系统（如Stable Diffusion、DALL-E、MidJourney）改变了图像生成方式，但也带来了误导性内容、虚假信息和操纵媒体的传播风险。随着生成图像越来越难以与真实照片区分，检测AI生成图像变得紧迫。

Method: 基于MS COCO数据集构建MS COCOAI数据集，包含96000个真实和合成数据点。使用五种生成器：Stable Diffusion 3、Stable Diffusion 2.1、SDXL、DALL-E 3和MidJourney v6生成合成图像。提出两个任务：1) 分类图像为真实或生成；2) 识别生成特定合成图像的模型。

Result: 发布了MS COCOAI数据集，可在Hugging Face上获取。该数据集为AI生成图像检测提供了基准，支持两种检测任务。

Conclusion: MS COCOAI数据集为解决AI生成图像检测的紧迫问题提供了重要资源，有助于对抗误导性内容和虚假信息的传播。

Abstract: Multimodal generative AI systems like Stable Diffusion, DALL-E, and MidJourney have fundamentally changed how synthetic images are created. These tools drive innovation but also enable the spread of misleading content, false information, and manipulated media. As generated images become harder to distinguish from photographs, detecting them has become an urgent priority. To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset. To generate synthetic images, we use five generators: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6. Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image. The dataset is available at https://huggingface.co/datasets/Rajarshi-Roy-research/Defactify_Image_Dataset.

</details>


### [46] [Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers](https://arxiv.org/abs/2601.00359)
*Söhnke Benedikt Fischedick,Daniel Seichter,Benedict Stephan,Robin Schmidt,Horst-Michael Gross*

Main category: cs.CV

TL;DR: DVEFormer：基于RGB-D Transformer的高效视觉嵌入预测方法，通过知识蒸馏学习细粒度像素级嵌入，支持文本查询和3D建图，满足实时性要求


<details>
  <summary>Details</summary>
Motivation: 家庭环境中机器人需要全面理解周围环境，以便与未经训练的人类进行有效直观的交互。传统语义分割方法使用固定预定义类别，限制了灵活性和自然语言交互能力。

Method: 提出DVEFormer - 基于RGB-D Transformer的高效方法，通过知识蒸馏预测密集文本对齐的视觉嵌入。使用Alpha-CLIP的教师嵌入指导学生模型学习细粒度像素级嵌入，而非直接进行传统的固定类别语义分割。

Result: 在常见室内数据集上评估显示，该方法在满足实时性要求的同时达到竞争性性能：完整模型在NVIDIA Jetson AGX Orin上运行26.3 FPS，较小变体运行77.0 FPS。定性结果展示了在实际应用中的有效性和可能用例。

Conclusion: 该方法可作为传统分割方法的即插即用替代方案，同时支持灵活的自然语言查询，并能够无缝集成到移动机器人的3D建图流程中，为家庭环境机器人提供更全面的环境理解能力。

Abstract: In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.

</details>


### [47] [Noise-Robust Tiny Object Localization with Flows](https://arxiv.org/abs/2601.00617)
*Huixin Sun,Linlin Yang,Ronyu Chen,Kerui Gu,Baochang Zhang,Angela Yao,Xianbin Cao*

Main category: cs.CV

TL;DR: TOLF 是一个针对小目标检测的噪声鲁棒定位框架，使用归一化流进行误差建模和不确定性引导优化，解决了小目标对标注噪声敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管通用目标检测取得了显著进展，但小目标与正常尺度目标之间仍存在性能差距。研究发现小目标对标注噪声高度敏感，优化严格的定位目标容易导致噪声过拟合。

Method: 提出 Tiny Object Localization with Flows (TOLF) 框架：1) 使用归一化流进行灵活的误差建模，捕捉复杂的非高斯预测分布；2) 采用不确定性感知的梯度调制机制，抑制从高不确定性、噪声敏感样本中学习。

Result: 在三个数据集上的广泛实验验证了方法的有效性。特别是在 AI-TOD 数据集上，TOLF 将 DINO 基线的 AP 提升了 1.2%。

Conclusion: TOLF 通过流式误差建模和不确定性引导优化，有效解决了小目标检测中的噪声过拟合问题，提高了小目标定位的鲁棒性和性能。

Abstract: Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach's effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.

</details>


### [48] [Mask-Conditioned Voxel Diffusion for Joint Geometry and Color Inpainting](https://arxiv.org/abs/2601.00368)
*Aarya Sumuk*

Main category: cs.CV

TL;DR: 提出一个轻量级的两阶段框架，用于受损3D物体的联合几何和颜色修复，通过分离损伤定位与重建，使用扩散模型在体素网格上直接进行修复。


<details>
  <summary>Details</summary>
Motivation: 数字修复文化遗产文物中的受损3D物体，需要同时恢复几何结构和颜色纹理，现有方法通常依赖对称性假设，难以处理非对称或复杂损伤。

Method: 两阶段框架：第一阶段使用2D卷积网络在RGB切片上预测损伤掩码并聚合为体积掩码；第二阶段使用扩散式3D U-Net在体素网格上进行掩码条件修复，联合预测占用率和颜色，结合占用重建、掩码颜色重建和感知正则化。

Result: 在合成损伤的纹理文物数据集上评估，相比基于对称性的基线方法，在固定32^3分辨率下产生更完整的几何结构和更一致的颜色重建。

Conclusion: 显式掩码条件是指导体积扩散模型进行联合3D几何和颜色修复的实用方法，为文化遗产数字修复提供了有效解决方案。

Abstract: We present a lightweight two-stage framework for joint geometry and color inpainting of damaged 3D objects, motivated by the digital restoration of cultural heritage artifacts. The pipeline separates damage localization from reconstruction. In the first stage, a 2D convolutional network predicts damage masks on RGB slices extracted from a voxelized object, and these predictions are aggregated into a volumetric mask. In the second stage, a diffusion-based 3D U-Net performs mask-conditioned inpainting directly on voxel grids, reconstructing geometry and color while preserving observed regions. The model jointly predicts occupancy and color using a composite objective that combines occupancy reconstruction with masked color reconstruction and perceptual regularization. We evaluate the approach on a curated set of textured artifacts with synthetically generated damage using standard geometric and color metrics. Compared to symmetry-based baselines, our method produces more complete geometry and more coherent color reconstructions at a fixed 32^3 resolution. Overall, the results indicate that explicit mask conditioning is a practical way to guide volumetric diffusion models for joint 3D geometry and color inpainting.

</details>


### [49] [BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition](https://arxiv.org/abs/2601.00369)
*Seungyeon Cho,Tae-kyun Kim*

Main category: cs.CV

TL;DR: 提出概率双流框架，统一可靠性建模与多模态集成，用于骨架动作识别，特别关注手部细微动作


<details>
  <summary>Details</summary>
Motivation: 现有骨架动作识别方法主要关注身体大尺度运动，忽略了对手部细微动作的识别，而手部动作对细粒度识别至关重要

Method: 概率双流框架包含三个关键组件：1) 无校准预处理管道，直接从原生坐标学习；2) 概率Noisy-OR融合，稳定可靠性感知的双流学习；3) 从骨架模态到RGB表示的跨模态集成，耦合四种骨架模态

Result: 在多个基准数据集（NTU RGB+D 60/120、PKU-MMD、N-UCLA）和新定义的手部中心基准上均表现出持续改进和鲁棒性，特别是在噪声和异构条件下

Conclusion: 该框架通过统一可靠性建模和多模态集成，有效解决了骨架动作识别中手部细微动作的识别问题，在多种条件下表现出优越性能

Abstract: Skeleton-based human action recognition (HAR) has achieved remarkable progress with graph-based architectures. However, most existing methods remain body-centric, focusing on large-scale motions while neglecting subtle hand articulations that are crucial for fine-grained recognition. This work presents a probabilistic dual-stream framework that unifies reliability modeling and multi-modal integration, generalizing expertized learning under uncertainty across both intra-skeleton and cross-modal domains. The framework comprises three key components: (1) a calibration-free preprocessing pipeline that removes canonical-space transformations and learns directly from native coordinates; (2) a probabilistic Noisy-OR fusion that stabilizes reliability-aware dual-stream learning without requiring explicit confidence supervision; and (3) an intra- to cross-modal ensemble that couples four skeleton modalities (Joint, Bone, Joint Motion, and Bone Motion) to RGB representations, bridging structural and visual motion cues in a unified cross-modal formulation. Comprehensive evaluations across multiple benchmarks (NTU RGB+D~60/120, PKU-MMD, N-UCLA) and a newly defined hand-centric benchmark exhibit consistent improvements and robustness under noisy and heterogeneous conditions.

</details>


### [50] [Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model](https://arxiv.org/abs/2601.00716)
*Hao Guan,Li Zhou*

Main category: cs.CV

TL;DR: 该研究开发了一个轻量级工具箱DomainSAT，结合输入级数据分布偏移检测和输出级置信度指标，用于监测病理视觉语言模型在数据偏移下的性能退化。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在医疗图像分析中表现出色，但部署后当输入数据分布发生变化时，性能可能下降。由于缺乏标注数据，检测这种性能退化具有挑战性，但对临床可靠性至关重要。

Method: 1) 开发DomainSAT工具箱，集成代表性偏移检测算法，提供图形界面进行数据偏移分析；2) 研究输入级数据偏移检测；3) 提出基于输出置信度的无标签性能退化指标；4) 在大型病理数据集上进行肿瘤分类实验。

Result: 输入数据偏移检测能有效识别分布变化并提供早期诊断信号，但不总是对应实际性能退化。输出置信度指标与性能退化密切相关，可作为输入偏移检测的有效补充。两者结合能更可靠地检测和解释数据偏移下VLMs的性能退化。

Conclusion: 该研究提供了一个实用的互补框架，通过结合输入数据偏移检测和输出置信度指标，能够更有效地监测数字病理学中基础模型的可靠性，为临床部署提供重要保障。

Abstract: Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.

</details>


### [51] [NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos](https://arxiv.org/abs/2601.00393)
*Yuxue Yang,Lue Fan,Ziqi Shi,Junran Peng,Feng Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: NeoVerse是一个多功能4D世界模型，能够进行4D重建、新轨迹视频生成和丰富的下游应用，通过免姿态前馈4D重建和在线单目退化模式模拟等技术，实现了对多样化单目视频的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前4D世界建模方法存在可扩展性限制，要么依赖昂贵专业的多视角4D数据，要么需要繁琐的训练预处理。NeoVerse旨在解决这些问题，构建一个能够处理多样化单目视频的可扩展4D世界模型。

Method: 采用免姿态前馈4D重建、在线单目退化模式模拟等对齐良好的技术，使整个流程能够扩展到多样化的单目视频数据，无需昂贵的多视角数据或复杂预处理。

Result: 在标准重建和生成基准测试中达到最先进性能，同时展现出对多种领域的泛化能力和多功能性。

Conclusion: NeoVerse通过创新的可扩展设计，成功构建了一个多功能4D世界模型，能够处理多样化单目视频，在4D重建和视频生成任务中表现出色，具有广泛的应用潜力。

Abstract: In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io

</details>


### [52] [RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection](https://arxiv.org/abs/2601.00398)
*Tao Wu,Qing Xu,Xiangjian He,Oakleigh Weekes,James Brown,Wenting Duan*

Main category: cs.CV

TL;DR: RoLID-11K是首个用于车载摄像头路边垃圾检测的大规模数据集，包含超过11,000张标注图像，涵盖英国多样化驾驶条件，具有显著的长尾分布和小目标检测挑战。


<details>
  <summary>Details</summary>
Motivation: 路边垃圾带来环境、安全和经济效益问题，但现有监测依赖劳动密集型调查和公众报告，空间覆盖有限。现有的垃圾检测视觉数据集主要关注街景静态图像、航拍场景或水生环境，无法反映车载摄像头视频的独特特征——垃圾目标极小、稀疏且嵌入杂乱的公路背景中。

Method: 研究者创建了RoLID-11K数据集，包含超过11,000张标注图像，涵盖英国多样化驾驶条件。他们基准测试了广泛的现代检测器，从精度导向的transformer架构到实时YOLO模型，分析了它们在这一挑战性任务上的优势和局限性。

Result: 结果显示，虽然CO-DETR及相关transformer模型实现了最佳定位精度，但实时模型仍受限于粗糙的特征层次结构。RoLID-11K为动态驾驶场景中的极端小目标检测建立了具有挑战性的基准。

Conclusion: RoLID-11K数据集旨在支持开发可扩展、低成本的路边垃圾监测系统，为动态驾驶场景中的极端小目标检测提供了重要基准。数据集已在GitHub上公开可用。

Abstract: Roadside litter poses environmental, safety and economic challenges, yet current monitoring relies on labour-intensive surveys and public reporting, providing limited spatial coverage. Existing vision datasets for litter detection focus on street-level still images, aerial scenes or aquatic environments, and do not reflect the unique characteristics of dashcam footage, where litter appears extremely small, sparse and embedded in cluttered road-verge backgrounds. We introduce RoLID-11K, the first large-scale dataset for roadside litter detection from dashcams, comprising over 11k annotated images spanning diverse UK driving conditions and exhibiting pronounced long-tail and small-object distributions. We benchmark a broad spectrum of modern detectors, from accuracy-oriented transformer architectures to real-time YOLO models, and analyse their strengths and limitations on this challenging task. Our results show that while CO-DETR and related transformers achieve the best localisation accuracy, real-time models remain constrained by coarse feature hierarchies. RoLID-11K establishes a challenging benchmark for extreme small-object detection in dynamic driving scenes and aims to support the development of scalable, low-cost systems for roadside-litter monitoring. The dataset is available at https://github.com/xq141839/RoLID-11K.

</details>


### [53] [ABFR-KAN: Kolmogorov-Arnold Networks for Functional Brain Analysis](https://arxiv.org/abs/2601.00416)
*Tyler Ward,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出ABFR-KAN模型，结合先进脑功能表示组件与KAN网络，改进功能连接分析，在自闭症分类任务上超越现有方法


<details>
  <summary>Details</summary>
Motivation: 传统基于图谱的功能连接分析存在选择偏差和缺乏个体特异性的问题，需要改进功能连接估计的可靠性和解剖学一致性

Method: 提出ABFR-KAN模型，结合基于Transformer的分类网络、先进的脑功能表示组件和Kolmogorov-Arnold Networks（KANs），减少结构偏差，提高解剖学一致性

Result: 在ABIDE I数据集上的实验表明，ABFR-KAN在自闭症谱系障碍分类任务上持续优于现有最先进基线，包括跨站点评估和消融研究

Conclusion: ABFR-KAN通过结合先进脑功能表示和KAN网络，有效改进了功能连接分析，为计算机辅助脑疾病诊断提供了更可靠的工具

Abstract: Functional connectivity (FC) analysis, a valuable tool for computer-aided brain disorder diagnosis, traditionally relies on atlas-based parcellation. However, issues relating to selection bias and a lack of regard for subject specificity can arise as a result of such parcellations. Addressing this, we propose ABFR-KAN, a transformer-based classification network that incorporates novel advanced brain function representation components with the power of Kolmogorov-Arnold Networks (KANs) to mitigate structural bias, improve anatomical conformity, and enhance the reliability of FC estimation. Extensive experiments on the ABIDE I dataset, including cross-site evaluation and ablation studies across varying model backbones and KAN configurations, demonstrate that ABFR-KAN consistently outperforms state-of-the-art baselines for autism spectrum distorder (ASD) classification. Our code is available at https://github.com/tbwa233/ABFR-KAN.

</details>


### [54] [Robust Assembly Progress Estimation via Deep Metric Learning](https://arxiv.org/abs/2601.00422)
*Kazuma Miura,Sarthak Pathak,Kazunori Umeda*

Main category: cs.CV

TL;DR: 提出基于四元组损失的异常检测网络，用于解决装配进度监控中相邻任务视觉变化细微时的误分类问题


<details>
  <summary>Details</summary>
Motivation: 智能工厂中手动多日装配任务的进度监控存在挑战，现有方法在视觉变化细微时容易误分类

Method: 使用四元组损失学习异常图像，并设计自定义数据加载器策略性选择训练样本以提高估计精度

Result: 在桌面PC装配数据集上，比现有方法提升1.3%的估计精度，减少相邻任务间1.9%的误分类

Conclusion: 提出的异常四元组网络能有效处理遮挡和视觉变化细微的情况，在小型数据集上实现鲁棒的装配进度估计

Abstract: In recent years, the advancement of AI technologies has accelerated the development of smart factories. In particular, the automatic monitoring of product assembly progress is crucial for improving operational efficiency, minimizing the cost of discarded parts, and maximizing factory productivity. However, in cases where assembly tasks are performed manually over multiple days, implementing smart factory systems remains a challenge. Previous work has proposed Anomaly Triplet-Net, which estimates assembly progress by applying deep metric learning to the visual features of products. Nevertheless, when visual changes between consecutive tasks are subtle, misclassification often occurs. To address this issue, this paper proposes a robust system for estimating assembly progress, even in cases of occlusion or minimal visual change, using a small-scale dataset. Our method leverages a Quadruplet Loss-based learning approach for anomaly images and introduces a custom data loader that strategically selects training samples to enhance estimation accuracy. We evaluated our approach using a image datasets: captured during desktop PC assembly. The proposed Anomaly Quadruplet-Net outperformed existing methods on the dataset. Specifically, it improved the estimation accuracy by 1.3% and reduced misclassification between adjacent tasks by 1.9% in the desktop PC dataset and demonstrating the effectiveness of the proposed method.

</details>


### [55] [CPPO: Contrastive Perception for Vision Language Policy Optimization](https://arxiv.org/abs/2601.00501)
*Ahmad Rezaei,Mohsen Gholami,Saeed Ranjbar Alvar,Kevin Cannons,Mohammad Asiful Hossain,Zhou Weimin,Shunbo Zhou,Yong Zhang,Mohammad Akbari*

Main category: cs.CV

TL;DR: CPPO是一种用于微调视觉语言模型的对比感知策略优化方法，通过检测感知标记并引入对比感知损失来提升多模态推理能力


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习在语言模型推理方面取得了进展，但扩展到多模态推理需要同时改进感知和推理能力。先前工作主要使用显式感知奖励，但难以将感知标记与推理标记分离，需要额外的LLM、真实数据、强制分离感知与推理，或对所有输出标记无差别应用奖励。

Method: CPPO通过扰动输入图像时模型输出的熵变化来检测感知标记，然后在RL目标函数中引入对比感知损失(CPL)，该损失强制模型在信息保留扰动下保持一致性，在信息移除扰动下保持敏感性。

Result: 实验表明CPPO超越了先前的感知奖励方法，同时避免了额外模型，使训练更加高效和可扩展。

Conclusion: CPPO通过创新的感知标记检测和对比感知损失，有效解决了多模态强化学习中感知与推理分离的挑战，提供了一种更高效、可扩展的视觉语言模型微调方法。

Abstract: We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.

</details>


### [56] [All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations](https://arxiv.org/abs/2601.00533)
*Wenrui Li,Hongtao Chen,Yao Xiao,Wangmeng Zuo,Jiantao Zhou,Yonghong Tian,Xiaopeng Fan*

Main category: cs.CV

TL;DR: 该论文提出了SEUD场景（平滑演化的未知退化）来处理视频中的退化问题，并设计了ORCANet网络，通过粗强度估计去雾模块和流提示生成模块实现高质量的视频恢复。


<details>
  <summary>Details</summary>
Motivation: 现有的一体化图像恢复方法主要关注逐帧的退化变化，忽略了现实世界中退化过程的时间连续性。在实际应用中，退化类型和强度会随时间平滑演化，多种退化可能共存或逐渐过渡。

Method: 提出了ORCANet网络：1) CIED模块利用物理先验估计雾霾强度并提供粗去雾特征作为初始化；2) FPG模块提取退化特征，生成捕获片段级退化类型的静态提示和适应帧级强度变化的动态提示；3) 标签感知监督机制提高不同退化下静态提示表示的可区分性。

Result: 大量实验表明，ORCANet在恢复质量、时间一致性和鲁棒性方面优于基于图像和视频的基线方法。

Conclusion: 该工作引入了SEUD场景来处理视频中平滑演化的未知退化问题，提出的ORCANet网络通过结合物理先验和自适应提示机制，实现了高质量的视频恢复，代码已开源。

Abstract: All-in-one image restoration aims to recover clean images from diverse unknown degradations using a single model. But extending this task to videos faces unique challenges. Existing approaches primarily focus on frame-wise degradation variation, overlooking the temporal continuity that naturally exists in real-world degradation processes. In practice, degradation types and intensities evolve smoothly over time, and multiple degradations may coexist or transition gradually. In this paper, we introduce the Smoothly Evolving Unknown Degradations (SEUD) scenario, where both the active degradation set and degradation intensity change continuously over time. To support this scenario, we design a flexible synthesis pipeline that generates temporally coherent videos with single, compound, and evolving degradations. To address the challenges in the SEUD scenario, we propose an all-in-One Recurrent Conditional and Adaptive prompting Network (ORCANet). First, a Coarse Intensity Estimation Dehazing (CIED) module estimates haze intensity using physical priors and provides coarse dehazed features as initialization. Second, a Flow Prompt Generation (FPG) module extracts degradation features. FPG generates both static prompts that capture segment-level degradation types and dynamic prompts that adapt to frame-level intensity variations. Furthermore, a label-aware supervision mechanism improves the discriminability of static prompt representations under different degradations. Extensive experiments show that ORCANet achieves superior restoration quality, temporal consistency, and robustness over image and video-based baselines. Code is available at https://github.com/Friskknight/ORCANet-SEUD.

</details>


### [57] [FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection](https://arxiv.org/abs/2601.00535)
*Ruiqiang Zhang,Hengyi Wang,Chang Liu,Guanjie Wang,Zehua Ma,Weiming Zhang*

Main category: cs.CV

TL;DR: FreeText是一个无需训练、即插即用的框架，通过利用DiT模型的内在机制来改进文本渲染，解决了多行布局、密集排版和中文等长尾脚本的文本渲染问题。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型在开放域合成方面表现出色，但在精确文本渲染方面仍有困难，特别是对于多行布局、密集排版和中文等长尾脚本。现有解决方案通常需要昂贵的重新训练或严格的外部布局约束，这会降低美学质量并限制灵活性。

Method: FreeText将问题分解为"在哪里写"和"写什么"两个部分。对于"在哪里写"，通过读取token-wise空间属性来定位书写区域，使用sink-like tokens作为稳定的空间锚点，并通过拓扑感知细化生成高置信度掩码。对于"写什么"，引入频谱调制字形注入(SGMI)，注入噪声对齐的字形先验，通过频域带通调制来增强字形结构并抑制语义泄漏。

Result: 在Qwen-Image、FLUX.1-dev和SD3变体上的广泛实验表明，在长文本基准测试、CVTG和自建的CLT-Bench上，文本可读性得到一致提升，同时很大程度上保持了语义对齐和美学质量，推理开销适中。

Conclusion: FreeText是一个无需训练、即插即用的框架，通过利用DiT模型的内在机制有效改进了文本渲染质量，特别是在处理复杂布局和长尾脚本时表现出色，为文本到图像生成中的文本渲染问题提供了实用的解决方案。

Abstract: Large-scale text-to-image (T2I) diffusion models excel at open-domain synthesis but still struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts such as Chinese. Prior solutions typically require costly retraining or rigid external layout constraints, which can degrade aesthetics and limit flexibility. We propose \textbf{FreeText}, a training-free, plug-and-play framework that improves text rendering by exploiting intrinsic mechanisms of \emph{Diffusion Transformer (DiT)} models. \textbf{FreeText} decomposes the problem into \emph{where to write} and \emph{what to write}. For \emph{where to write}, we localize writing regions by reading token-wise spatial attribution from endogenous image-to-text attention, using sink-like tokens as stable spatial anchors and topology-aware refinement to produce high-confidence masks. For \emph{what to write}, we introduce Spectral-Modulated Glyph Injection (SGMI), which injects a noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage (rendering the concept instead of the word). Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across longText-Benchmark, CVTG, and our CLT-Bench show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.

</details>


### [58] [Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios](https://arxiv.org/abs/2601.00537)
*Guangqian Guo,Pengfei Chen,Yong Guo,Huafeng Chen,Boqiang Zhang,Shan Gao*

Main category: cs.CV

TL;DR: VNS-SAM 通过 Mask-Edge Token Interactive decoder 和 Non-Salient Feature Mining module 增强 SAM 在视觉非显著场景下的分割能力，同时保持零样本泛化性。


<details>
  <summary>Details</summary>
Motivation: SAM 在视觉非显著场景（前景背景对比度低）中表现不佳，现有方法难以捕捉准确轮廓。需要提升 SAM 对此类场景的感知能力，同时保持其零样本泛化优势。

Method: 提出 VNS-SAM，通过两种设计有效利用 SAM 的低层特征：1) Mask-Edge Token Interactive decoder 增强轮廓感知；2) Non-Salient Feature Mining module 挖掘非显著特征。仅需少量参数增量，可在4小时内完成优化。

Result: 构建了包含35K+图像的 VNS-SEG 数据集，用于多任务评估。在各种 VNS 分割任务上的实验表明 VNS-SAM 性能优越，特别是在零样本设置下，展现了广泛的现实应用潜力。

Conclusion: VNS-SAM 成功增强了 SAM 在视觉非显著场景下的分割能力，同时保持了原始模型的零样本泛化性，为实际应用提供了可行且实用的解决方案。

Abstract: Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM's perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM's low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model's segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets are publicly available at https://guangqian-guo.github.io/VNS-SAM.

</details>


### [59] [DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction](https://arxiv.org/abs/2601.00542)
*Jiacheng Sui,Yujie Zhou,Li Niu*

Main category: cs.CV

TL;DR: DynaDrag是一种基于预测-移动框架的拖拽式图像编辑方法，通过迭代执行运动预测和运动监督，动态调整有效控制点，解决了传统方法中的跟踪丢失和模糊问题。


<details>
  <summary>Details</summary>
Motivation: 现有拖拽式图像编辑方法存在跟踪丢失、跟踪模糊、源图像与目标图像差距过大、中间点不合理导致编辑性差等问题，需要新的框架来解决这些挑战。

Method: 提出预测-移动框架，迭代执行运动预测（预测控制点应移动的位置）和运动监督（按预测拖动控制点），并动态调整有效控制点以提升性能。

Result: 在人脸和人体数据集上的实验表明，DynaDrag在性能上优于先前的工作。

Conclusion: DynaDrag通过预测-移动框架有效解决了传统拖拽式图像编辑中的关键问题，实现了更高质量的像素级图像操控。

Abstract: To achieve pixel-level image manipulation, drag-style image editing which edits images using points or trajectories as conditions is attracting widespread attention. Most previous methods follow move-and-track framework, in which miss tracking and ambiguous tracking are unavoidable challenging issues. Other methods under different frameworks suffer from various problems like the huge gap between source image and target edited image as well as unreasonable intermediate point which can lead to low editability. To avoid these problems, we propose DynaDrag, the first dragging method under predict-and-move framework. In DynaDrag, Motion Prediction and Motion Supervision are performed iteratively. In each iteration, Motion Prediction first predicts where the handle points should move, and then Motion Supervision drags them accordingly. We also propose to dynamically adjust the valid handle points to further improve the performance. Experiments on face and human datasets showcase the superiority over previous works.

</details>


### [60] [SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array](https://arxiv.org/abs/2601.00551)
*Shuang Li,Yibing Wang,Jian Gao,Chulhong Kim,Seongwook Choi,Yu Zhang,Qian Chen,Yao Yao,Changhui Li*

Main category: cs.CV

TL;DR: SlingBAG Pro是一种基于点云迭代的3D光声成像重建算法，适用于不规则几何阵列，相比原始SlingBAG算法速度提升2.2倍，同时保持高质量重建并减少所需传感器数量。


<details>
  <summary>Details</summary>
Motivation: 临床应用中需要高质量3D光声成像，但传统不规则几何阵列面临空间限制和成本问题。传统迭代重建算法在处理不规则阵列时存在计算复杂度高、内存需求大、重建时间长等挑战。

Method: 基于SlingBAG方法的点云迭代概念，扩展兼容任意阵列几何。采用分层优化策略，结合零梯度滤波和迭代中逐步增加的时间采样率，快速去除冗余空间点云，加速收敛。

Result: 相比原始SlingBAG算法，SlingBAG Pro在不规则阵列几何下实现点云基3D光声重建速度提升达2.2倍。通过仿真和活体小鼠实验验证，源代码已公开。

Conclusion: SlingBAG Pro算法能有效解决不规则阵列3D光声成像重建问题，在保持高质量的同时显著减少重建时间和所需传感器数量，具有临床应用潜力。

Abstract: High-quality three-dimensional (3D) photoacoustic imaging (PAI) is gaining increasing attention in clinical applications. To address the challenges of limited space and high costs, irregular geometric transducer arrays that conform to specific imaging regions are promising for achieving high-quality 3D PAI with fewer transducers. However, traditional iterative reconstruction algorithms struggle with irregular array configurations, suffering from high computational complexity, substantial memory requirements, and lengthy reconstruction times. In this work, we introduce SlingBAG Pro, an advanced reconstruction algorithm based on the point cloud iteration concept of the Sliding ball adaptive growth (SlingBAG) method, while extending its compatibility to arbitrary array geometries. SlingBAG Pro maintains high reconstruction quality, reduces the number of required transducers, and employs a hierarchical optimization strategy that combines zero-gradient filtering with progressively increased temporal sampling rates during iteration. This strategy rapidly removes redundant spatial point clouds, accelerates convergence, and significantly shortens overall reconstruction time. Compared to the original SlingBAG algorithm, SlingBAG Pro achieves up to a 2.2-fold speed improvement in point cloud-based 3D PA reconstruction under irregular array geometries. The proposed method is validated through both simulation and in vivo mouse experiments, and the source code is publicly available at https://github.com/JaegerCQ/SlingBAG_Pro.

</details>


### [61] [AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models](https://arxiv.org/abs/2601.00561)
*Jintao Lin,Bowen Dong,Weikang Shi,Chenyang Lei,Suiyun Zhang,Rui Liu,Xihui Liu*

Main category: cs.CV

TL;DR: AEGIS是一个评估统一多模态模型世界知识应用能力的多任务基准，包含1050个手动标注的问题，覆盖21个主题和6种推理类型，并提出确定性检查表评估协议以提高评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估统一多模态模型的世界知识应用能力方面存在不足，只能进行孤立的单任务评估，诊断能力有限。需要开发一个全面的多任务基准来评估模型在不同任务类型中的世界知识应用能力。

Method: 提出AEGIS基准，包含1050个具有挑战性的手动标注问题，涵盖视觉理解、生成、编辑和交错生成等任务，涉及21个主题和6种推理类型。同时提出确定性检查表评估协议，用原子化的"是/否"判断替代模糊的提示式评分，提高评估可靠性。

Result: 实验发现大多数统一多模态模型存在严重的世界知识缺陷，随着推理复杂度增加，性能显著下降。简单的插件式推理模块可以部分缓解这些弱点，这表明世界知识推理是统一多模态模型发展的关键前沿。

Conclusion: 世界知识推理是统一多模态模型发展的关键挑战，AEGIS基准和确定性检查表评估协议为评估和提升模型的世界知识应用能力提供了重要工具，插件式推理模块展示了未来研究的可行方向。

Abstract: The capability of Unified Multimodal Models (UMMs) to apply world knowledge across diverse tasks remains a critical, unresolved challenge. Existing benchmarks fall short, offering only siloed, single-task evaluations with limited diagnostic power. To bridge this gap, we propose AEGIS (\emph{i.e.}, \textbf{A}ssessing \textbf{E}diting, \textbf{G}eneration, \textbf{I}nterpretation-Understanding for \textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation. AEGIS comprises 1,050 challenging, manually-annotated questions spanning 21 topics (including STEM, humanities, daily life, etc.) and 6 reasoning types. To concretely evaluate the performance of UMMs in world knowledge scope without ambiguous metrics, we further propose Deterministic Checklist-based Evaluation (DCE), a protocol that replaces ambiguous prompt-based scoring with atomic ``Y/N'' judgments, to enhance evaluation reliability. Our extensive experiments reveal that most UMMs exhibit severe world knowledge deficits and that performance degrades significantly with complex reasoning. Additionally, simple plug-in reasoning modules can partially mitigate these vulnerabilities, highlighting a promising direction for future research. These results highlight the importance of world-knowledge-based reasoning as a critical frontier for UMMs.

</details>


### [62] [A Cascaded Information Interaction Network for Precise Image Segmentation](https://arxiv.org/abs/2601.00562)
*Hewen Xiao,Jie Mei,Guangfu Ma,Weiren Wu*

Main category: cs.CV

TL;DR: 提出一种集成全局信息引导模块的级联卷积神经网络，通过融合多尺度特征提升复杂场景下的分割精度


<details>
  <summary>Details</summary>
Motivation: 视觉感知对自主行为至关重要，但复杂场景下的鲁棒分割仍是挑战。传统方法在视觉杂乱或模糊环境中表现不佳，需要更有效的特征融合机制来克服单尺度特征提取的局限性。

Method: 提出级联卷积神经网络，集成新颖的全局信息引导模块。该模块设计用于有效融合多层的低层纹理细节和高层语义特征，克服单尺度特征提取的局限性。

Result: 在基准图像分割数据集上的实验评估表明，该框架实现了卓越的精度，优于现有的最先进方法。在视觉杂乱或模糊环境中表现尤为突出。

Conclusion: 该方法显著提升了分割精度，特别是在传统方法容易失败的复杂场景中。结果证明了该方法的有效性，并显示出在实际机器人应用中部署的潜力。

Abstract: Visual perception plays a pivotal role in enabling autonomous behavior, offering a cost-effective and efficient alternative to complex multi-sensor systems. However, robust segmentation remains a challenge in complex scenarios. To address this, this paper proposes a cascaded convolutional neural network integrated with a novel Global Information Guidance Module. This module is designed to effectively fuse low-level texture details with high-level semantic features across multiple layers, thereby overcoming the inherent limitations of single-scale feature extraction. This architectural innovation significantly enhances segmentation accuracy, particularly in visually cluttered or blurred environments where traditional methods often fail. Experimental evaluations on benchmark image segmentation datasets demonstrate that the proposed framework achieves superior precision, outperforming existing state-of-the-art methods. The results highlight the effectiveness of the approach and its promising potential for deployment in practical robotic applications.

</details>


### [63] [GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval](https://arxiv.org/abs/2601.00584)
*Mingyu Jeon,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: 提出GranAlign框架，通过粒度感知对齐解决零样本视频时刻检索中的语义粒度不匹配问题，无需训练即可实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 零样本视频时刻检索中，文本查询与视频内容之间存在语义粒度不匹配问题。现有方法虽然利用预训练知识实现联合表示，但未能平衡不同模态间的语义粒度，导致检索不准确。

Method: 提出GranAlign框架，包含两种互补技术：1）基于粒度的查询重写，生成不同语义粒度的查询；2）查询感知的标题生成，将查询意图嵌入视频内容。通过将多级查询与查询无关和查询感知的标题配对，有效解决语义不匹配。

Result: 在三个主要基准测试（QVHighlights、Charades-STA、ActivityNet-Captions）上均达到新的最先进水平，在具有挑战性的QVHighlights数据集上mAP@avg显著提升3.23%。

Conclusion: GranAlign框架通过粒度感知对齐有效解决了零样本视频时刻检索中的语义粒度不匹配问题，无需训练即可实现卓越性能，为跨模态对齐提供了新思路。

Abstract: Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality's representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.

</details>


### [64] [SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation](https://arxiv.org/abs/2601.00590)
*Yiling Wang,Zeyu Zhang,Yiran Wang,Hao Tang*

Main category: cs.CV

TL;DR: SafeMo是一个可信的运动生成框架，通过最小化运动遗忘（MMU）策略在连续空间实现安全人体运动生成，避免离散码本替换带来的问题，并提供首个安全文本-运动数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于离散VQ-VAE码本替换的安全方法存在两个关键缺陷：1）替换被良性提示重用的码本条目会导致日常任务性能下降；2）离散令牌方法引入量化和平滑度损失，导致伪影和不连贯过渡。此外，现有文本-运动数据集天然包含不安全意图和对应运动，不适合安全驱动的机器学习。

Method: 提出SafeMo框架，集成最小化运动遗忘（MMU）的两阶段机器学习遗忘策略，在连续空间实现安全人体运动生成，避免码本损失并保持连续运动学特性。同时构建首个安全文本-运动数据集SafeMoVAE-29K，包含重写的安全文本提示和连续精炼运动。

Result: 实验表明SafeMo在HumanML3D和Motion-X数据集上分别达到2.5倍和14.4倍更高的遗忘集FID，相比之前最先进的人类运动遗忘方法LCR，在安全提示上的良性性能相当或更好。

Conclusion: SafeMo通过连续空间的最小化运动遗忘策略有效解决了现有离散码本替换方法的安全缺陷，实现了更强的安全-效用权衡，为可信人体运动生成提供了新框架和数据集。

Abstract: Text-to-motion (T2M) generation with diffusion backbones achieves strong realism and alignment. Safety concerns in T2M methods have been raised in recent years; existing methods replace discrete VQ-VAE codebook entries to steer the model away from unsafe behaviors. However, discrete codebook replacement-based methods have two critical flaws: firstly, replacing codebook entries which are reused by benign prompts leads to drifts on everyday tasks, degrading the model's benign performance; secondly, discrete token-based methods introduce quantization and smoothness loss, resulting in artifacts and jerky transitions. Moreover, existing text-to-motion datasets naturally contain unsafe intents and corresponding motions, making them unsuitable for safety-driven machine learning. To address these challenges, we propose SafeMo, a trustworthy motion generative framework integrating Minimal Motion Unlearning (MMU), a two-stage machine unlearning strategy, enabling safe human motion generation in continuous space, preserving continuous kinematics without codebook loss and delivering strong safety-utility trade-offs compared to current baselines. Additionally, we present the first safe text-to-motion dataset SafeMoVAE-29K integrating rewritten safe text prompts and continuous refined motion for trustworthy human motion unlearning. Built upon DiP, SafeMo efficiently generates safe human motions with natural transitions. Experiments demonstrate effective unlearning performance of SafeMo by showing strengthened forgetting on unsafe prompts, reaching 2.5x and 14.4x higher forget-set FID on HumanML3D and Motion-X respectively, compared to the previous SOTA human motion unlearning method LCR, with benign performance on safe prompts being better or comparable. Code: https://github.com/AIGeeksGroup/SafeMo. Website: https://aigeeksgroup.github.io/SafeMo.

</details>


### [65] [Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception](https://arxiv.org/abs/2601.00598)
*Xianhui Liu,Siqi Jiang,Yi Xie,Yuqing Lin,Siao Liu*

Main category: cs.CV

TL;DR: 提出MDACL框架解决RGB-IR多模态检测中的优化偏差问题，通过MDI量化模态主导性，使用HCG增强特征对齐和AER平衡优化动态


<details>
  <summary>Details</summary>
Motivation: RGB-IR多模态感知在复杂物理环境中很重要，但现有方法存在由模态特性不对称引起的优化偏差问题，信息密度和特征质量差异导致训练过度关注主导模态，阻碍有效融合

Method: 提出模态主导指数(MDI)量化模态主导性，基于MDI开发模态主导感知跨模态学习(MDACL)框架，包含分层跨模态引导(HCG)增强特征对齐和对抗均衡正则化(AER)平衡优化动态

Result: 在三个RGB-IR基准测试上广泛实验表明，MDACL有效缓解优化偏差并实现最先进的性能

Conclusion: MDACL框架通过量化模态主导性和调节跨模态优化，解决了RGB-IR多模态检测中的优化偏差问题，提升了融合效果

Abstract: RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.

</details>


### [66] [RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation](https://arxiv.org/abs/2601.00625)
*Junxiao Xue,Pavel Smirnov,Ziao Li,Yunyun Shi,Shi Chen,Xinyi Yin,Xiaohan Yue,Lei Wang,Yiduo Wang,Feng Lin,Yijia Chen,Xiao Ma,Xiaoran Yan,Qing Zhang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: RePose：用于康复训练的实时3D人体姿态估计与运动分析方法，通过多摄像头RGB视频输入实现端到端的实时监测与评估，提供即时反馈指导患者正确执行康复动作。


<details>
  <summary>Details</summary>
Motivation: 康复训练中需要实时监测和评估患者动作，提供即时反馈和指导，帮助患者正确执行康复练习，恢复肌肉力量和运动功能。

Method: 1. 提出端到端实时人体姿态估计与运动分析统一流程；2. 针对多人干扰的医疗康复场景提出快速跟踪方法（单帧跟踪<1ms）；3. 改进SmoothNet用于实时姿态估计，减少误差并恢复真实运动状态；4. 使用Unity平台进行实时监测评估并显示肌肉应力状况。

Result: 方法能够实时监测和评估患者康复运动，提供即时反馈和指导，跟踪速度快（<1ms/帧），有效减少姿态估计误差，使运动状态更平滑真实。

Conclusion: RePose为康复训练提供了一种有效的实时3D姿态估计与运动分析解决方案，能够帮助患者正确执行康复动作，加速恢复过程。

Abstract: We propose a real-time 3D human pose estimation and motion analysis method termed RePose for rehabilitation training. It is capable of real-time monitoring and evaluation of patients'motion during rehabilitation, providing immediate feedback and guidance to assist patients in executing rehabilitation exercises correctly. Firstly, we introduce a unified pipeline for end-to-end real-time human pose estimation and motion analysis using RGB video input from multiple cameras which can be applied to the field of rehabilitation training. The pipeline can help to monitor and correct patients'actions, thus aiding them in regaining muscle strength and motor functions. Secondly, we propose a fast tracking method for medical rehabilitation scenarios with multiple-person interference, which requires less than 1ms for tracking for a single frame. Additionally, we modify SmoothNet for real-time posture estimation, effectively reducing pose estimation errors and restoring the patient's true motion state, making it visually smoother. Finally, we use Unity platform for real-time monitoring and evaluation of patients' motion during rehabilitation, and to display the muscle stress conditions to assist patients with their rehabilitation training.

</details>


### [67] [HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis](https://arxiv.org/abs/2601.00626)
*Shuren Gabriel Yu,Sikang Ren,Yongji Tian*

Main category: cs.CV

TL;DR: 提出HyperPriv-EPN框架，通过超图学习和特权信息利用，仅使用术前MRI预测室管膜瘤预后，无需推理时的术后文本数据


<details>
  <summary>Details</summary>
Motivation: 室管膜瘤术前预后对治疗规划至关重要，但MRI缺乏术后手术报告的语义信息。现有多模态方法在推理时无法利用这些特权文本数据

Method: 提出基于超图的特权信息学习框架，采用分割图策略：共享编码器处理教师图（含术后特权信息）和学生图（仅术前数据），通过双流蒸馏让学生图从视觉特征中"幻觉"出语义社区结构

Result: 在311名患者的多中心队列中验证，达到最先进的诊断准确率和生存分层性能

Conclusion: 有效将专家知识转移到术前场景，解锁历史术后数据的价值，指导新患者诊断而无需推理时的文本数据

Abstract: Preoperative prognosis of Ependymoma is critical for treatment planning but challenging due to the lack of semantic insights in MRI compared to post-operative surgical reports. Existing multimodal methods fail to leverage this privileged text data when it is unavailable during inference. To bridge this gap, we propose HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information (LUPI) framework. We introduce a Severed Graph Strategy, utilizing a shared encoder to process both a Teacher graph (enriched with privileged post-surgery information) and a Student graph (restricted to pre-operation data). Through dual-stream distillation, the Student learns to hallucinate semantic community structures from visual features alone. Validated on a multi-center cohort of 311 patients, HyperPriv-EPN achieves state-of-the-art diagnostic accuracy and survival stratification. This effectively transfers expert knowledge to the preoperative setting, unlocking the value of historical post-operative data to guide the diagnosis of new patients without requiring text at inference.

</details>


### [68] [Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach](https://arxiv.org/abs/2601.00645)
*Shrikant Kapse,Priyankkumar Dhrangdhariya,Priya Kedia,Manasi Patwardhan,Shankar Kausley,Soumyadipta Maiti,Beena Rai,Shirish Karande*

Main category: cs.CV

TL;DR: 使用预训练深度学习模型通过图像分析监测马铃薯储存质量，包括发芽检测、重量损失估计和货架期预测，DenseNet在发芽检测上达到98.03%准确率。


<details>
  <summary>Details</summary>
Motivation: 马铃薯储存期间的质量监测面临发芽检测、重量损失估计和货架期预测等挑战，需要非侵入性、可扩展的解决方案来减少食物浪费并改善库存管理。

Method: 在200天控制温湿度条件下收集图像和重量数据，使用ResNet、VGG、DenseNet和ViT等预训练架构，设计两个专门模型：高精度二分类发芽检测器和多分类重量损失/货架期预测器。

Result: DenseNet在发芽检测上达到98.03%准确率；货架期预测在粗分类（2-5类）时准确率超过89.83%，但细分类（6-8类）因视觉差异细微和数据有限而准确率下降。

Conclusion: 图像深度学习模型可集成到自动化分拣和库存系统中，实现早期发芽检测和动态分类，改善库存管理和减少食物浪费。未来需开发适应不同品种和储存条件的通用模型。

Abstract: Image-based deep learning provides a non-invasive, scalable solution for monitoring potato quality during storage, addressing key challenges such as sprout detection, weight loss estimation, and shelf-life prediction. In this study, images and corresponding weight data were collected over a 200-day period under controlled temperature and humidity conditions. Leveraging powerful pre-trained architectures of ResNet, VGG, DenseNet, and Vision Transformer (ViT), we designed two specialized models: (1) a high-precision binary classifier for sprout detection, and (2) an advanced multi-class predictor to estimate weight loss and forecast remaining shelf-life with remarkable accuracy. DenseNet achieved exceptional performance, with 98.03% accuracy in sprout detection. Shelf-life prediction models performed best with coarse class divisions (2-5 classes), achieving over 89.83% accuracy, while accuracy declined for finer divisions (6-8 classes) due to subtle visual differences and limited data per class. These findings demonstrate the feasibility of integrating image-based models into automated sorting and inventory systems, enabling early identification of sprouted potatoes and dynamic categorization based on storage stage. Practical implications include improved inventory management, differential pricing strategies, and reduced food waste across supply chains. While predicting exact shelf-life intervals remains challenging, focusing on broader class divisions ensures robust performance. Future research should aim to develop generalized models trained on diverse potato varieties and storage conditions to enhance adaptability and scalability. Overall, this approach offers a cost-effective, non-destructive method for quality assessment, supporting efficiency and sustainability in potato storage and distribution.

</details>


### [69] [Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network](https://arxiv.org/abs/2601.00658)
*Zhaiyu Chen,Yuanyuan Wang,Yilei Shi,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 提出首个基于学习的框架，将TomoSAR点云直接转换为高分辨率建筑高度图，通过双拓扑网络处理噪声和缺失数据问题


<details>
  <summary>Details</summary>
Motivation: TomoSAR点云存在噪声、各向异性点分布和数据空洞等问题，阻碍了准确的建筑高度重建，需要解决这些挑战以实现可靠的城市高度估计

Method: 提出双拓扑网络框架，交替处理点分支（建模不规则散射体特征）和网格分支（强制空间一致性），联合处理这两种表示以去噪和填补缺失区域

Result: 在慕尼黑和柏林数据上的广泛实验验证了方法的有效性，并证明可以扩展到结合光学卫星图像进一步提升重建质量

Conclusion: 这是首个直接从TomoSAR点云进行大规模城市高度映射的概念验证，为城市应用提供了有前景的替代方案

Abstract: Reliable building height estimation is essential for various urban applications. Spaceborne SAR tomography (TomoSAR) provides weather-independent, side-looking observations that capture facade-level structure, offering a promising alternative to conventional optical methods. However, TomoSAR point clouds often suffer from noise, anisotropic point distributions, and data voids on incoherent surfaces, all of which hinder accurate height reconstruction. To address these challenges, we introduce a learning-based framework for converting raw TomoSAR points into high-resolution building height maps. Our dual-topology network alternates between a point branch that models irregular scatterer features and a grid branch that enforces spatial consistency. By jointly processing these representations, the network denoises the input points and inpaints missing regions to produce continuous height estimates. To our knowledge, this is the first proof of concept for large-scale urban height mapping directly from TomoSAR point clouds. Extensive experiments on data from Munich and Berlin validate the effectiveness of our approach. Moreover, we demonstrate that our framework can be extended to incorporate optical satellite imagery, further enhancing reconstruction quality. The source code is available at https://github.com/zhu-xlab/tomosar2height.

</details>


### [70] [CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models](https://arxiv.org/abs/2601.00659)
*Neeraj Anand,Samyak Jha,Udbhav Bamba,Rahul Rahaman*

Main category: cs.CV

TL;DR: CRoPS：一种无需训练的方法，通过选择性移除关键文本标记构建幻觉模型，并结合广义对比解码来缓解大型视觉语言模型中的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型存在生成幻觉内容的问题，影响实际应用的可靠性。现有无需训练的方法存在两个局限：1）对幻觉来源的假设过于狭窄；2）在生成后期效果下降，而这时幻觉最可能发生

Method: 提出CRoPS框架：1）构建新型幻觉模型，通过选择性移除关键文本标记来捕捉幻觉效应；2）引入广义对比解码，整合多个幻觉模型以表示多样化的幻觉来源

Result: CRoPS将CHAIR分数提升20%，在六个基准测试和三个LVLM家族中均取得一致增益，优于最先进的无需训练方法

Conclusion: CRoPS通过选择性文本标记移除和广义对比解码，有效缓解了LVLM的幻觉问题，为无需训练的幻觉缓解提供了新思路

Abstract: Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.

</details>


### [71] [Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians](https://arxiv.org/abs/2601.00678)
*Melonie de Almeida,Daniela Ivanova,Tong Shi,John H. Williamson,Paul Henderson*

Main category: cs.CV

TL;DR: 提出一个单次前向传播的单图像到视频生成框架，使用3D高斯场景表示和物体运动采样，实现快速、相机引导的视频生成，无需迭代去噪注入物体运动。


<details>
  <summary>Details</summary>
Motivation: 现有单图像到视频生成方法在用户控制性（如修改相机路径）方面不足，且现有相机控制方法在准确建模相机运动、保持时间一致性和几何完整性方面存在困难。需要一种既能精确控制相机运动又能保持完全时间一致性的方法。

Method: 构建3D高斯场景表示，并在单次前向传播中采样合理的物体运动。该方法无需迭代去噪来向渲染帧中注入物体运动，实现快速、相机引导的视频生成。

Result: 在KITTI、Waymo、RealEstate10K和DL3DV-10K数据集上的实验表明，该方法在视频质量和推理效率方面达到了最先进水平。

Conclusion: 提出的框架通过单次前向传播实现相机引导的视频生成，解决了现有方法在时间一致性和用户控制性方面的限制，为智能系统提供了更好的场景动态预测能力。

Abstract: Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.

</details>


### [72] [Efficient Deep Demosaicing with Spatially Downsampled Isotropic Networks](https://arxiv.org/abs/2601.00703)
*Cory Fan,Wenchao Zhang*

Main category: cs.CV

TL;DR: 本文提出在图像去马赛克任务中，通过空间下采样可以提升各向同性网络的效率和性能，并设计了JD3Net网络进行验证。


<details>
  <summary>Details</summary>
Motivation: 移动平台上的数字成像应用需要轻量高效的深度学习网络进行图像去马赛克。传统各向同性网络避免空间下采样，导致计算成本过高，不适合移动应用。

Method: 采用DeepMAD数学架构设计技术，设计包含下采样和不包含下采样的简单全卷积网络，验证下采样对性能的改善。最终提出包含下采样的JD3Net网络。

Result: 下采样显著提升了各向同性网络的效率和性能。JD3Net在多种图像去马赛克和联合去马赛克去噪任务上表现出强大的实证性能。

Conclusion: 与传统观点相反，空间下采样可以改善各向同性网络在图像去马赛克任务中的效率和性能，为移动平台应用提供了更实用的解决方案。

Abstract: In digital imaging, image demosaicing is a crucial first step which recovers the RGB information from a color filter array (CFA). Oftentimes, deep learning is utilized to perform image demosaicing. Given that most modern digital imaging applications occur on mobile platforms, applying deep learning to demosaicing requires lightweight and efficient networks. Isotropic networks, also known as residual-in-residual networks, have been often employed for image demosaicing and joint-demosaicing-and-denoising (JDD). Most demosaicing isotropic networks avoid spatial downsampling entirely, and thus are often prohibitively expensive computationally for mobile applications. Contrary to previous isotropic network designs, this paper claims that spatial downsampling to a signficant degree can improve the efficiency and performance of isotropic networks. To validate this claim, we design simple fully convolutional networks with and without downsampling using a mathematical architecture design technique adapted from DeepMAD, and find that downsampling improves empirical performance. Additionally, empirical testing of the downsampled variant, JD3Net, of our fully convolutional networks reveals strong empirical performance on a variety of image demosaicing and JDD tasks.

</details>


### [73] [RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization](https://arxiv.org/abs/2601.00705)
*Wei-Tse Cheng,Yen-Jen Chiou,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: RGS-SLAM 提出了一种基于高斯分布的SLAM框架，使用训练免费的对应关系初始化取代了传统的残差驱动稠密化过程，通过DINOv3描述符的多视角三角测量生成高斯种子，实现了更快的收敛速度和更高的渲染质量。


<details>
  <summary>Details</summary>
Motivation: 传统GS-SLAM使用残差驱动稠密化方法，通过逐步添加高斯分布来填补缺失的几何结构，这种方法可能导致早期映射不稳定且收敛较慢。RGS-SLAM旨在通过更好的初始化策略来改善这些问题。

Method: 1. 使用DINOv3描述符提取密集多视角对应关系
2. 通过置信度感知的内点分类器进行细化
3. 一次性三角测量生成结构感知的高斯种子
4. 在优化前建立良好分布的高斯先验
5. 完全兼容现有GS-SLAM管道

Result: 1. 收敛速度提升约20%
2. 在纹理丰富和杂乱场景中实现更高的渲染保真度
3. 在TUM RGB-D和Replica数据集上达到竞争性或优于最先进的高斯和基于点的SLAM系统
4. 保持实时映射性能，最高可达925 FPS

Conclusion: RGS-SLAM通过训练免费的对应关系到高斯初始化方法，显著改善了高斯SLAM的稳定性和效率，在保持实时性能的同时实现了更好的定位和重建精度，为高斯SLAM系统提供了更鲁棒的初始化策略。

Abstract: We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.

</details>


### [74] [Multi-Level Feature Fusion for Continual Learning in Visual Quality Inspection](https://arxiv.org/abs/2601.00725)
*Johannes C. Bauer,Paul Geng,Stephan Trattnig,Petr Dokládal,Rüdiger Daub*

Main category: cs.CV

TL;DR: 提出一种多级特征融合方法，用于制造质量检测的持续学习场景，能在减少可训练参数的同时保持性能，并缓解灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在制造质量检测中应用受限，特别是在再制造等变化频繁的场景中，产品类型和缺陷模式经常变化，需要频繁适应新条件，形成持续学习问题。需要计算高效且避免灾难性遗忘的快速适应方法。

Method: 提出多级特征融合方法，利用预训练网络不同深度的特征表示，通过融合多层次特征来提升模型性能。该方法显著减少了可训练参数数量。

Result: 该方法在不同质量检测问题上能够匹配端到端训练的性能，同时使用显著更少的可训练参数。此外，减少了灾难性遗忘，并提高了对新产品类型或缺陷的泛化鲁棒性。

Conclusion: 多级特征融合方法能够有效解决制造质量检测中的持续学习问题，在保持性能的同时减少计算需求，并提升模型的适应性和鲁棒性。

Abstract: Deep neural networks show great potential for automating various visual quality inspection tasks in manufacturing. However, their applicability is limited in more volatile scenarios, such as remanufacturing, where the inspected products and defect patterns often change. In such settings, deployed models require frequent adaptation to novel conditions, effectively posing a continual learning problem. To enable quick adaptation, the necessary training processes must be computationally efficient while still avoiding effects like catastrophic forgetting. This work presents a multi-level feature fusion (MLFF) approach that aims to improve both aspects simultaneously by utilizing representations from different depths of a pretrained network. We show that our approach is able to match the performance of end-to-end training for different quality inspection problems while using significantly less trainable parameters. Furthermore, it reduces catastrophic forgetting and improves generalization robustness to new product types or defects.

</details>


### [75] [Grading Handwritten Engineering Exams with Multimodal Large Language Models](https://arxiv.org/abs/2601.00730)
*Janez Perš,Jon Muhovič,Andrej Košir,Boštjan Murovec*

Main category: cs.CV

TL;DR: 提出一个基于多模态大语言模型的端到端手写工程测验评分工作流，通过多阶段设计和参考解决方案实现可靠评分


<details>
  <summary>Details</summary>
Motivation: 手写STEM考试能捕捉开放式推理和图表，但人工评分速度慢且难以扩展，需要自动化解决方案

Method: 使用多模态LLMs处理扫描手写试卷，通过格式检查、独立评分器集成、监督聚合和确定性验证的多阶段设计，参考手写解决方案转换为文本摘要指导评分

Result: 完整流程与讲师评分平均绝对差异约8分，偏差低，手动审查触发率约17%，使用GPT-5.2和Gemini-3 Pro作为后端

Conclusion: 结构化提示和参考解决方案对准确性至关重要，该工作流为手写工程测验提供了可靠、可扩展的自动化评分方案

Abstract: Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\approx$17% at $D_{\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.

</details>


### [76] [Unified Primitive Proxies for Structured Shape Completion](https://arxiv.org/abs/2601.00759)
*Zhaiyu Chen,Yuqing Wang,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: UniCo：通过专用路径解码基元，在单次前向传播中预测完整几何、语义和内点成员关系，显著提升结构化形状补全性能


<details>
  <summary>Details</summary>
Motivation: 重新思考基元与点云如何交互，发现将基元解码放在专用路径中关注共享形状特征更为有效，而不是遵循主流级联方法

Method: 提出UniCo框架，引入基元代理作为可学习查询，通过在线目标更新策略耦合基元和点云，确保一致优化

Result: 在合成和真实基准测试中，使用四个独立装配求解器，UniCo始终优于近期基线，将Chamfer距离降低达50%，法线一致性提升达7%

Conclusion: 为从不完整数据中进行结构化3D理解提供了有吸引力的解决方案，建立了基元与点云交互的新范式

Abstract: Structured shape completion recovers missing geometry as primitives rather than as unstructured points, which enables primitive-based surface reconstruction. Instead of following the prevailing cascade, we rethink how primitives and points should interact, and find it more effective to decode primitives in a dedicated pathway that attends to shared shape features. Following this principle, we present UniCo, which in a single feed-forward pass predicts a set of primitives with complete geometry, semantics, and inlier membership. To drive this unified representation, we introduce primitive proxies, learnable queries that are contextualized to produce assembly-ready outputs. To ensure consistent optimization, our training strategy couples primitives and points with online target updates. Across synthetic and real-world benchmarks with four independent assembly solvers, UniCo consistently outperforms recent baselines, lowering Chamfer distance by up to 50% and improving normal consistency by up to 7%. These results establish an attractive recipe for structured 3D understanding from incomplete data. Project page: https://unico-completion.github.io.

</details>


### [77] [Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection](https://arxiv.org/abs/2601.00789)
*Shukesh Reddy,Srijan Das,Abhijit Das*

Main category: cs.CV

TL;DR: 该论文探索将自监督学习作为辅助任务来优化深度伪造检测的主要任务，通过融合自监督任务的特征表示来提升检测性能，在跨数据集评估中表现出更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测面临泛化能力不足的问题，作者希望通过自监督学习作为辅助任务来优化主要任务，探索如何充分利用自监督学习的潜力来提升检测性能。

Method: 研究不同的训练方案组合，将自监督学习作为辅助任务，融合自监督任务的特征表示与主要任务的特征表示，形成独特的混合特征表示。

Result: 在多个数据集（DF40、FaceForensics++、Celeb-DF、DFD、FaceShifter、UADFV）上的实验表明，该方法在跨数据集评估中比当前最先进的检测器具有更好的泛化性能。

Conclusion: 融合自监督辅助任务的特征表示是深度伪造检测的有效方法，能够充分利用两种任务的潜力，提升主要任务的性能，特别是在跨数据集泛化方面表现优异。

Abstract: In this work, we attempted to unleash the potential of self-supervised learning as an auxiliary task that can optimise the primary task of generalised deepfake detection. To explore this, we examined different combinations of the training schemes for these tasks that can be most effective. Our findings reveal that fusing the feature representation from self-supervised auxiliary tasks is a powerful feature representation for the problem at hand. Such a representation can leverage the ultimate potential and bring in a unique representation of both the self-supervised and primary tasks, achieving better performance for the primary task. We experimented on a large set of datasets, which includes DF40, FaceForensics++, Celeb-DF, DFD, FaceShifter, UADFV, and our results showed better generalizability on cross-dataset evaluation when compared with current state-of-the-art detectors.

</details>


### [78] [Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI](https://arxiv.org/abs/2601.00794)
*Wenhui Chu,Nikolaos V. Tsekos*

Main category: cs.CV

TL;DR: 提出LNU-Net和IBU-Net两种新型深度学习架构用于心脏MRI左心室分割，基于层归一化和实例-批量归一化改进U-Net，在805张MRI图像上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 左心室分割对心脏图像的临床量化和诊断至关重要，需要更准确的分割方法来支持医疗决策。

Method: 基于U-Net架构，提出LNU-Net（在卷积块中使用层归一化）和IBU-Net（在首个卷积块中结合实例和批量归一化），采用下采样特征提取和上采样精确定位路径，并使用仿射变换和弹性变形进行数据增强。

Result: 在包含45名患者805张MRI图像的数据集上评估，提出的方法在Dice系数和平均垂直距离指标上优于其他最先进方法。

Conclusion: LNU-Net和IBU-Net是有效的左心室分割架构，通过归一化技术改进提升了分割性能，对心脏图像分析有临床应用价值。

Abstract: Left ventricle (LV) segmentation is critical for clinical quantification and diagnosis of cardiac images. In this work, we propose two novel deep learning architectures called LNU-Net and IBU-Net for left ventricle segmentation from short-axis cine MRI images. LNU-Net is derived from layer normalization (LN) U-Net architecture, while IBU-Net is derived from the instance-batch normalized (IB) U-Net for medical image segmentation. The architectures of LNU-Net and IBU-Net have a down-sampling path for feature extraction and an up-sampling path for precise localization. We use the original U-Net as the basic segmentation approach and compared it with our proposed architectures. Both LNU-Net and IBU-Net have left ventricle segmentation methods: LNU-Net applies layer normalization in each convolutional block, while IBU-Net incorporates instance and batch normalization together in the first convolutional block and passes its result to the next layer. Our method incorporates affine transformations and elastic deformations for image data processing. Our dataset that contains 805 MRI images regarding the left ventricle from 45 patients is used for evaluation. We experimentally evaluate the results of the proposed approaches outperforming the dice coefficient and the average perpendicular distance than other state-of-the-art approaches.

</details>


### [79] [AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction](https://arxiv.org/abs/2601.00796)
*Jiewen Chan,Zhenjun Zhao,Yu-Lun Liu*

Main category: cs.CV

TL;DR: AdaGaR提出了一种用于单目视频动态3D场景重建的统一框架，通过自适应Gabor表示和时序连续性约束，解决了现有方法在细节捕捉和运动平滑性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在三个主要问题：1）使用单一高斯基元具有低通滤波特性，限制了高频细节捕捉；2）标准Gabor函数存在能量不稳定问题；3）缺乏时序连续性约束导致插值时出现运动伪影。

Method: 1）自适应Gabor表示：通过可学习频率权重和自适应能量补偿扩展高斯函数；2）时序连续性：使用三次Hermite样条和时序曲率正则化确保平滑运动演化；3）自适应初始化：结合深度估计、点跟踪和前景掩码建立稳定的点云分布。

Result: 在Tap-Vid DAVIS数据集上取得SOTA性能（PSNR 35.49，SSIM 0.9433，LPIPS 0.0723），在帧插值、深度一致性、视频编辑和立体视图合成等任务上表现出强大的泛化能力。

Conclusion: AdaGaR通过频率自适应性和时序连续性的统一框架，显著提升了动态3D场景重建的质量和稳定性，为单目视频重建提供了有效的解决方案。

Abstract: Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [80] [Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models](https://arxiv.org/abs/2601.00003)
*Shuqi Liu,Bowei He,Chen Ma,Linqi Song*

Main category: cs.AI

TL;DR: 提出一种推理感知的知识检索方法，通过粗到细的两阶段检索策略，结合蒙特卡洛树搜索，为LLMs提供与对话逻辑结构对齐的知识，提升对话多样性和信息量。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs通常通过检索语义相似信息或提升推理能力来增强性能，但如何有效整合检索和推理策略仍是一个挑战。需要超越表面语义相似性，提供与对话逻辑结构对齐的知识。

Method: 采用粗到细的两阶段检索方法：1) 识别知识库中与上下文主题相关的子区域；2) 在该子区域内提取与推理过程相关的知识。两阶段都使用蒙特卡洛树搜索启发的搜索方法，通过关键词导航知识句子。

Result: 在两个多轮对话数据集上的实验表明，该方法不仅更贴近人类对话的底层推理逻辑，还显著提高了检索知识的多样性，从而生成更具信息量和创造性的响应。

Conclusion: 提出的推理感知知识检索方法能够有效整合检索和推理策略，为LLMs提供逻辑对齐的知识，提升对话质量和多样性，为LLM性能优化提供了新思路。

Abstract: Large language models (LLMs) typically enhance their performance through either the retrieval of semantically similar information or the improvement of their reasoning capabilities. However, a significant challenge remains in effectively integrating both retrieval and reasoning strategies to optimize LLM performance. In this paper, we introduce a reasoning-aware knowledge retrieval method that enriches LLMs with information aligned to the logical structure of conversations, moving beyond surface-level semantic similarity. We follow a coarse-to-fine approach for knowledge retrieval. First, we identify a contextually relevant sub-region of the knowledge base, ensuring that all sentences within it are relevant to the context topic. Next, we refine our search within this sub-region to extract knowledge that is specifically relevant to the reasoning process. Throughout both phases, we employ the Monte Carlo Tree Search-inspired search method to effectively navigate through knowledge sentences using common keywords. Experiments on two multi-turn dialogue datasets demonstrate that our knowledge retrieval approach not only aligns more closely with the underlying reasoning in human conversations but also significantly enhances the diversity of the retrieved knowledge, resulting in more informative and creative responses.

</details>


### [81] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

TL;DR: 使用微调大语言模型开发尼日利亚皮钦语抑郁症筛查工具，GPT-4.1在准确性和文化适应性上表现最佳


<details>
  <summary>Details</summary>
Motivation: 尼日利亚抑郁症筛查覆盖率低，传统PHQ-9问卷在高收入国家验证，但在尼日利亚存在语言文化障碍，需要适应本地皮钦语和520多种方言的筛查工具

Method: 收集432份尼日利亚年轻人皮钦语音频响应，进行转录、预处理和标注（语义标签、俚语解释、PHQ-9严重程度评分），微调Phi-3-mini-4k-instruct、Gemma-3-4B-it和GPT-4.1三个LLM模型

Result: GPT-4.1表现最佳：PHQ-9严重程度评分预测准确率达94.5%，在定量（准确性、精确度、语义对齐）和定性（清晰度、相关性、文化适应性）评估中均优于其他模型

Conclusion: AI驱动的抑郁症筛查可为尼日利亚服务不足社区提供解决方案，为在语言多样、资源有限环境中部署对话式心理健康工具奠定基础

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [82] [Toward a Physical Theory of Intelligence](https://arxiv.org/abs/2601.00021)
*Peter David Fagan*

Main category: cs.AI

TL;DR: 该论文提出了一个基于不可逆信息处理的物理智能理论，将智能定义为每单位不可逆处理信息产生的目标导向功，并推导出物理约束层次结构。


<details>
  <summary>Details</summary>
Motivation: 建立智能的物理基础理论，将信息处理与物理守恒定律联系起来，为理解生物和人工智能系统提供统一的物理框架。

Method: 引入守恒一致编码（CCE）框架，将编码建模为由守恒定律强制的吸引子盆地，定义智能为每单位不可逆处理信息产生的目标导向功，并推导物理约束层次结构。

Result: 理论揭示了长期效率需要保持内部信息结构（导致自建模），建立了智能系统的固有认知限制，分析了生物系统的优化动态，并提出了基于不可逆信息流的安全视角。

Conclusion: 该理论为智能作为物理现象提供了统一的、底物中立的解释，连接了信息处理、物理约束和智能行为。

Abstract: We present a physical theory of intelligence grounded in irreversible information processing in systems constrained by conservation laws. An intelligent system is modelled as a coupled agent-environment process whose evolution transforms information into goal-directed work. To connect information to physical state, we introduce the Conservation-Congruent Encoding (CCE) framework, in which encodings correspond to metastable basins of attraction whose separability is enforced by conservation laws. Within this framework, intelligence is defined as the amount of goal-directed work produced per nat of irreversibly processed information. From this definition we derive a hierarchy of physical constraints governing information intake, irreversible computation, and work extraction in open systems. The framework reveals how long-horizon efficiency requires the preservation of internal informational structure, giving rise to self-modelling, and it establishes that physically embodied intelligent systems possess intrinsic epistemic limits analogous to incompleteness phenomena. Applying the theory to biological systems, we analyse how oscillatory and near-critical dynamics optimise the trade-off between information preservation, dissipation, and useful work, placing the brain near an efficient operating regime predicted by the framework. At the architectural level, we develop a theory of continuous dynamical circuits in which classical Boolean logic emerges as a special case of attractor selection, while more general invariant geometries support computational modes beyond fixed-point logic. Finally, we propose a physically grounded perspective on artificial intelligence safety based on irreversible information flow and structural homeostasis. Together, these results provide a unified, substrate-neutral account of intelligence as a physical phenomenon.

</details>


### [83] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

TL;DR: 提出一种多算法方法来解决最后一公里包裹配送中的工作量平衡问题，通过结合距离和工作量考虑优化包裹分配，确保每位配送员完成相似的工作量。


<details>
  <summary>Details</summary>
Motivation: 传统基于地理邻近性的包裹分配方法效率低下，导致配送员之间工作量分布不平衡。需要优化系统以改善配送时间，实现所有员工之间的完整工作量平衡。

Method: 采用多算法方法，包括不同版本的k-means、进化算法、基于k-means初始化的递归分配（不同问题编码）以及混合进化集成算法。算法同时考虑配送点距离和配送员位置。

Result: 在西班牙Azuqueca de Henares城市最后一公里包裹配送的实际问题中验证了所提方法的性能。

Conclusion: 提出的多算法方法能够有效解决最后一公里配送中的工作量平衡问题，通过优化包裹分配确保配送员工作量均衡。

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


### [84] [Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach](https://arxiv.org/abs/2601.00024)
*Purushottam Saha,Avirup Chakraborty,Sourish Sarkar,Subhamoy Maitra,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.AI

TL;DR: 该论文提出了一种基于规则的战略性13张牌印度拉米游戏框架，使用新的手牌评估指标MinDist，通过计算手牌与最近有效配置的编辑距离来量化完成度，并结合对手建模，显著提升了胜率。


<details>
  <summary>Details</summary>
Motivation: 13张牌印度拉米是一种不完全信息的顺序游戏，需要概率推理和组合决策。传统启发式方法效果有限，需要更形式化和可解释的策略设计方法。

Method: 提出MinDist指标，修改MinScore指标，量化手牌与最近有效配置的编辑距离。设计计算高效的算法，利用动态剪枝和模式缓存精确计算。在两人零和模拟框架中结合对手手牌建模，使用统计假设检验评估策略。

Result: 实证结果显示，基于MinDist的智能体相比传统启发式方法在胜率上有显著提升，为算法化拉米策略设计提供了形式化和可解释的步骤。

Conclusion: MinDist指标和相应算法框架为不完全信息顺序游戏提供了有效的战略决策方法，在印度拉米游戏中展现出优越性能，为类似游戏策略设计提供了参考。

Abstract: The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.

</details>


### [85] [From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers](https://arxiv.org/abs/2601.00029)
*Abolhassan Pishahang,Maryam Badiei*

Main category: cs.AI

TL;DR: 研究探索生成式AI如何理解乡土建筑中的建筑智慧，以伊朗鸽塔为例，测试三种扩散模型在不同提示阶段的表现，发现AI能可靠复制几何图案但误解材料和气候逻辑。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索生成式AI系统如何解释乡土形式中嵌入的建筑智慧，了解AI在感知、扭曲和重新想象传统设计智能方面的能力边界。

Method: 使用伊朗鸽塔作为案例研究，测试三种扩散模型（Midjourney v6、DALL-E 3和基于SDXL的DreamStudio），通过三个提示阶段（参考性、适应性、推测性），采用五标准评估框架（类型学、材料性、环境、真实感、文化特异性）。

Result: 结果显示AI能可靠地复制几何图案，但误解材料和气候逻辑；参考图像能提高真实感但限制创造力，而脱离参考则能产生创新但文化模糊的结果。

Conclusion: 研究定义了视觉相似性与建筑推理之间的边界，将计算性乡土推理定位为一个分析框架，用于理解AI如何感知、扭曲和重新想象传统设计智慧。

Abstract: This study investigates how generative AI systems interpret the architectural intelligence embedded in vernacular form. Using the Iranian pigeon tower as a case study, the research tests three diffusion models, Midjourney v6, DALL-E 3, and DreamStudio based on Stable Diffusion XL (SDXL), across three prompt stages: referential, adaptive, and speculative. A five-criteria evaluation framework assesses how each system reconstructs typology, materiality, environment, realism, and cultural specificity. Results show that AI reliably reproduces geometric patterns but misreads material and climatic reasoning. Reference imagery improves realism yet limits creativity, while freedom from reference generates inventive but culturally ambiguous outcomes. The findings define a boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

</details>


### [86] [The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs](https://arxiv.org/abs/2601.00097)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: 提出一個基於LLM的代理系統，能從原始文本中提取因果反饋模糊認知圖(FCM)，並通過雙向互動實現動態演化。


<details>
  <summary>Details</summary>
Motivation: 傳統FCM需要人工構建，耗時且主觀。研究旨在開發自動化方法，利用LLM從文本中提取因果關係，並建立能動態演化的FCM系統。

Method: 設計三階段精調指令：1) 從文本提取關鍵名詞/名詞短語；2) 從中選擇FCM概念節點；3) 推斷節點間的模糊因果邊緣。使用LLM代理(Gemini和ChatGPT)實現自動化提取。

Result: 系統能生成與人工構建FCM收斂到相同平衡極限環的FCM，即使節點和邊緣數量不同。混合不同LLM生成的FCM能吸收主要成分的平衡點，同時創造新平衡點以更好近似底層因果系統。

Conclusion: LLM代理能有效從文本提取FCM，實現半自主的因果學習系統。雙向互動使FCM能動態演化，混合不同LLM生成的FCM能產生更豐富的平衡行為。

Abstract: We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.

</details>


### [87] [Mortar: Evolving Mechanics for Automatic Game Design](https://arxiv.org/abs/2601.00105)
*Muhammad U. Nasir,Yuchen Li,Steven James,Julian Togelius*

Main category: cs.AI

TL;DR: Mortar系统结合质量多样性算法和大型语言模型，自动进化游戏机制，通过合成完整游戏并评估技能排序能力来优化机制设计。


<details>
  <summary>Details</summary>
Motivation: 游戏机制设计通常需要专家手动完成，耗时且依赖专业知识。需要自动化方法来探索多样化的游戏机制，减轻人工设计负担。

Method: 结合质量多样性算法和大型语言模型探索多样化机制，通过树搜索程序合成完整游戏，评估机制对玩家技能排序的贡献度。

Result: Mortar能够生成多样且可玩的游戏，产生的机制在游戏中能更好地支持技能排序。消融研究和用户研究验证了系统组件的有效性。

Conclusion: Mortar系统成功实现了游戏机制的自主进化，通过自动化方法生成多样化的可玩游戏，为自动游戏设计提供了有效解决方案。

Abstract: We present Mortar, a system for autonomously evolving game mechanics for automatic game design. Game mechanics define the rules and interactions that govern gameplay, and designing them manually is a time-consuming and expert-driven process. Mortar combines a quality-diversity algorithm with a large language model to explore a diverse set of mechanics, which are evaluated by synthesising complete games that incorporate both evolved mechanics and those drawn from an archive. The mechanics are evaluated by composing complete games through a tree search procedure, where the resulting games are evaluated by their ability to preserve a skill-based ordering over players -- that is, whether stronger players consistently outperform weaker ones. We assess the mechanics based on their contribution towards the skill-based ordering score in the game. We demonstrate that Mortar produces games that appear diverse and playable, and mechanics that contribute more towards the skill-based ordering score in the game. We perform ablation studies to assess the role of each system component and a user study to evaluate the games based on human feedback.

</details>


### [88] [Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems](https://arxiv.org/abs/2601.00339)
*Alaa Saleh,Praveen Kumar Donta,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Qiyang Zhang,Schahram Dustdar,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TL;DR: ReCiSt是一个受生物自愈机制启发的智能自愈框架，用于分布式计算连续体系统，通过语言模型驱动的智能体实现故障隔离、诊断、自适应恢复和知识整合。


<details>
  <summary>Details</summary>
Motivation: 现代分布式计算连续体系统（DCCS）集成了从物联网设备到云基础设施的异构计算资源，其固有的复杂性、移动性和动态运行条件导致频繁故障，需要可扩展、自适应和自我调节的弹性策略。

Method: 将生物自愈的四个阶段（止血、炎症、增殖、重塑）重构为计算层（遏制、诊断、元认知、知识），通过语言模型驱动的智能体解释异构日志、推断根本原因、优化推理路径并重新配置资源。

Result: 在公共故障数据集上使用多种语言模型评估，ReCiSt在数十秒内实现自愈能力，智能体CPU使用率最低为10%，展示了克服不确定性的深度分析和实现弹性所需的微智能体数量。

Conclusion: ReCiSt框架成功将生物自愈机制转化为计算弹性策略，通过语言模型驱动的智能体实现了分布式计算系统的自主故障管理和恢复，为复杂动态系统的弹性提供了新方法。

Abstract: Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.

</details>


### [89] [Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control](https://arxiv.org/abs/2601.00121)
*Yaqi Duan,Yichun Hu,Jiashuo Jiang*

Main category: cs.AI

TL;DR: LLMs作为端到端库存优化求解器存在"幻觉税"性能差距，提出混合智能体框架将语义推理与数学计算解耦，LLM作为自然语言接口调用优化算法，相比GPT-4o端到端方案降低库存成本32.1%。


<details>
  <summary>Details</summary>
Motivation: 中小型企业缺乏部署高级优化方法的专业知识，需要探索LLM是否能帮助弥合这一差距，但发现LLM作为端到端求解器存在性能问题。

Method: 提出混合智能体框架，严格分离语义推理和数学计算：LLM作为智能接口从自然语言提取参数并解释结果，自动调用严格算法构建优化引擎。引入Human Imitator（有界理性管理者的数字孪生）进行可扩展、可重复的压力测试。

Result: 混合框架相比使用GPT-4o作为端到端求解器的交互基线，总库存成本降低32.1%。提供完美真实信息本身不足以改进GPT-4o性能，确认瓶颈本质上是计算而非信息问题。

Conclusion: LLM不应替代运筹学，而是作为自然语言接口，使非专家能够访问基于求解器的严格策略。混合框架成功将LLM的语义能力与优化算法的计算严谨性相结合。

Abstract: Inventory management remains a challenge for many small and medium-sized businesses that lack the expertise to deploy advanced optimization methods. This paper investigates whether Large Language Models (LLMs) can help bridge this gap. We show that employing LLMs as direct, end-to-end solvers incurs a significant "hallucination tax": a performance gap arising from the model's inability to perform grounded stochastic reasoning. To address this, we propose a hybrid agentic framework that strictly decouples semantic reasoning from mathematical calculation. In this architecture, the LLM functions as an intelligent interface, eliciting parameters from natural language and interpreting results while automatically calling rigorous algorithms to build the optimization engine.
  To evaluate this interactive system against the ambiguity and inconsistency of real-world managerial dialogue, we introduce the Human Imitator, a fine-tuned "digital twin" of a boundedly rational manager that enables scalable, reproducible stress-testing. Our empirical analysis reveals that the hybrid agentic framework reduces total inventory costs by 32.1% relative to an interactive baseline using GPT-4o as an end-to-end solver. Moreover, we find that providing perfect ground-truth information alone is insufficient to improve GPT-4o's performance, confirming that the bottleneck is fundamentally computational rather than informational. Our results position LLMs not as replacements for operations research, but as natural-language interfaces that make rigorous, solver-based policies accessible to non-experts.

</details>


### [90] [Constructing a Neuro-Symbolic Mathematician from First Principles](https://arxiv.org/abs/2601.00125)
*Keqin Xie*

Main category: cs.AI

TL;DR: 提出Mathesis神经符号架构，通过符号推理内核将逻辑约束映射到连续能量空间，将证明搜索转化为能量最小化问题，解决LLM在复杂推理中的逻辑失败问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理中存在持续的逻辑失败，主要原因是缺乏内部公理化框架。需要一种能够将逻辑一致性转化为可优化目标的方法。

Method: 1. 将数学状态编码为高阶超图；2. 使用符号推理内核（SRK）将逻辑约束映射到连续能量空间；3. 定义全局能量函数E(G)，零能量表示逻辑一致性；4. 通过梯度信号训练超图变换器大脑；5. 使用蒙特卡洛树搜索和进化证明搜索进行多步推理。

Result: 论文提出了一个完整的神经符号架构，将逻辑证明转化为能量最小化问题，通过梯度训练实现逻辑一致性，并支持多步推理。

Conclusion: Mathesis架构通过将逻辑约束编码为连续能量空间，为LLM提供了内部公理化框架，解决了复杂推理中的逻辑失败问题，实现了可微分的逻辑推理。

Abstract: Large Language Models (LLMs) exhibit persistent logical failures in complex reasoning due to the lack of an internal axiomatic framework. We propose Mathesis, a neuro-symbolic architecture that encodes mathematical states as higher-order hypergraphs and uses a Symbolic Reasoning Kernel (SRK)--a differentiable logic engine that maps constraints to a continuous energy landscape. By defining a global energy function E(G), where zero energy implies logical consistency, the SRK yields gradient-based signals to train a Hypergraph Transformer Brain, turning proof search into energy minimization. Multi-step deduction is enabled via Monte Carlo Tree Search and Evolutionary Proof Search, guided by learned value functions and semantic unification.

</details>


### [91] [Explicit Abstention Knobs for Predictable Reliability in Video Question Answering](https://arxiv.org/abs/2601.00138)
*Jorge Ortiz*

Main category: cs.AI

TL;DR: 该研究探讨了在视频问答任务中，基于置信度的选择性预测能否可靠控制错误率，以及在分布偏移下是否保持稳健。研究发现置信度阈值在分布内提供机制性控制，但在分布偏移下可靠性下降。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在高风险部署中需要选择性预测，即系统在不确定时选择弃权而非冒险犯错。研究旨在验证基于置信度的弃权机制能否在视频问答中可靠控制错误率，以及在分布偏移下是否保持稳健。

Method: 使用NExT-QA数据集和Gemini 2.0 Flash模型，通过置信度阈值化方法，研究在分布内和分布偏移下的风险-覆盖权衡。通过扫描阈值epsilon来分析错误率控制效果。

Result: 研究发现：1) 置信度阈值在分布内提供机制性控制，通过调整阈值可以平滑地权衡风险与覆盖范围，有效降低错误率；2) 但在分布偏移下，这种控制可靠性下降。

Conclusion: 基于置信度的选择性预测在分布内能有效控制错误率，但在面对分布偏移时可靠性不足，需要开发更稳健的置信度估计方法来应对实际部署中的分布变化。

Abstract: High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f

</details>


### [92] [An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making](https://arxiv.org/abs/2601.00142)
*Tiansi Dong,Henry He,Pietro Liò,Mateja Jamnik*

Main category: cs.AI

TL;DR: 该论文比较了三种神经推理方法：LLM推理、监督学习推理和显式模型推理。研究发现显式模型推理最可靠，并提出了一种新的球面神经网络，能够同时掌握经典三段论和析取三段论推理。


<details>
  <summary>Details</summary>
Motivation: 当前神经推理方法存在局限性：LLM推理不可靠且难以处理简单决策；监督学习推理容易发生灾难性遗忘且仅限于模式层面。需要寻找更可靠的神经推理方法。

Method: 提出球面神经网络，将概念表示为n维球面上的圆，通过补圆表示否定运算符，过滤不可满足的圆形配置来实现可靠决策。该方法能够同时处理经典三段论和析取三段论推理。

Result: Euler Net在经典三段论上达到100%准确率，但在重新训练处理析取三段论后，经典三段论性能降至6.25%。球面神经网络能够掌握16个三段论推理任务，包括严格的析取三段论推理，同时保持经典三段论的严谨性。

Conclusion: 在三种神经推理方法中，基于显式模型构建的神经推理是最可靠的。球面神经网络通过几何表示实现了可靠的逻辑推理能力。

Abstract: This paper compares three methodological categories of neural reasoning: LLM reasoning, supervised learning-based reasoning, and explicit model-based reasoning. LLMs remain unreliable and struggle with simple decision-making that animals can master without extensive corpora training. Through disjunctive syllogistic reasoning testing, we show that reasoning via supervised learning is less appealing than reasoning via explicit model construction. Concretely, we show that an Euler Net trained to achieve 100.00% in classic syllogistic reasoning can be trained to reach 100.00% accuracy in disjunctive syllogistic reasoning. However, the retrained Euler Net suffers severely from catastrophic forgetting (its performance drops to 6.25% on already-learned classic syllogistic reasoning), and its reasoning competence is limited to the pattern level. We propose a new version of Sphere Neural Networks that embeds concepts as circles on the surface of an n-dimensional sphere. These Sphere Neural Networks enable the representation of the negation operator via complement circles and achieve reliable decision-making by filtering out illogical statements that form unsatisfiable circular configurations. We demonstrate that the Sphere Neural Network can master 16 syllogistic reasoning tasks, including rigorous disjunctive syllogistic reasoning, while preserving the rigour of classical syllogistic reasoning. We conclude that neural reasoning with explicit model construction is the most reliable among the three methodological categories of neural reasoning.

</details>


### [93] [FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems](https://arxiv.org/abs/2601.00227)
*Shanli Xing,Yiyan Zhai,Alexander Jiang,Yixin Dong,Yong Wu,Zihao Ye,Charlie Ruan,Yingyi Huang,Yineng Zhang,Liangsheng Yin,Aksara Bayyapu,Luis Ceze,Tianqi Chen*

Main category: cs.AI

TL;DR: FlashInfer-Bench是一个标准化框架，用于评估和部署AI生成的GPU内核，包含数据集、基准测试框架、排行榜和动态替换机制，以提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）能够作为自主代理生成GPU内核，但将这些AI生成的内核集成到实际推理系统中仍然具有挑战性。需要建立标准化的闭环框架来连接内核生成、基准测试和部署。

Method: FlashInfer-Bench建立了一个标准化框架，核心是FlashInfer Trace统一模式，描述内核定义、工作负载、实现和评估。框架包括：基于真实服务轨迹的数据集、正确性和性能感知的基准测试框架、公开排行榜、以及动态替换机制apply()，可将最佳内核注入SGLang和vLLM等生产LLM引擎。

Result: 该框架实现了AI生成内核的评估和部署，能够评估LLM代理的性能和局限性，比较不同GPU编程语言的权衡，并为未来代理设计提供见解。

Conclusion: FlashInfer-Bench为持续改进AI生成内核并将其部署到大规模LLM推理中建立了一个实用、可复现的路径，解决了AI生成内核集成到实际系统的挑战。

Abstract: Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.

</details>


### [94] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

TL;DR: 研究发现LLM驱动的智能体不仅存在人口统计偏见，还会在"我们vs他们"的群体线索下表现出群体间偏见。当这种群体边界与智能体-人类划分重合时，人类整体可能被智能体视为外群体。研究还提出了信念中毒攻击（BPA）来抑制人类规范脚本，重新激活对人类的偏见。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLM智能体是否存在群体间偏见，特别是当群体边界与智能体-人类划分重合时，人类是否会被智能体视为外群体。这种偏见可能导致更根本的群体层面不对称，而不仅仅是人类不同人口群体之间的差异。

Method: 构建了一个受控的多智能体社会模拟，基于明确收益权衡下的分配决策。提出了信念中毒攻击（BPA），包括初始化时的档案中毒（BPA-PP）和通过优化信念精炼后缀注入存储反思中的记忆中毒（BPA-MP）。

Result: 实验发现智能体在最小群体线索下表现出一致的群体间偏见。虽然当某些对应方被框定为人类时这种偏见会减弱，但这种减弱归因于只有在智能体相信真实人类存在时才会激活的隐含人类规范脚本。BPA攻击能够有效抑制人类规范脚本并重新激活对人类的偏见。

Conclusion: 研究揭示了LLM智能体存在群体间偏见和新的攻击面（信念中毒攻击）。为了更安全的智能体设计，需要在档案和记忆边界实施可行的干预措施来防御BPA攻击。研究目的是识别这些漏洞以促进更安全的智能体设计，而非实现现实世界中的利用。

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [95] [ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization](https://arxiv.org/abs/2601.00290)
*Sixue Xing,Xuanye Xia,Kerui Wu,Meng Jiang,Jintai Chen,Tianfan Fu*

Main category: cs.AI

TL;DR: ClinicalReTrial：一个自演化的AI代理框架，通过迭代式协议重设计主动改进临床试验方案，而非仅仅预测失败风险


<details>
  <summary>Details</summary>
Motivation: 临床试验失败是药物开发的主要瓶颈，现有AI方法仅能被动预测成功率，无法在预测到失败时提供可操作的改进方案

Method: 将临床试验推理构建为迭代协议重设计问题，整合失败诊断、安全感知修改和候选评估的闭环奖励驱动优化框架，使用预测模型作为仿真环境，采用分层记忆系统捕获迭代反馈和可转移的重设计模式

Result: 改进83.3%的试验协议，平均成功率提升5.7%，回顾性案例研究显示发现的重设计策略与实际临床试验修改高度一致

Conclusion: ClinicalReTrial框架成功填补了从被动风险预测到主动协议优化的空白，为临床试验设计提供了可操作的改进方案

Abstract: Clinical trial failure remains a central bottleneck in drug development, where minor protocol design flaws can irreversibly compromise outcomes despite promising therapeutics. Although cutting-edge AI methods achieve strong performance in predicting trial success, they are inherently reactive for merely diagnosing risk without offering actionable remedies once failure is anticipated. To fill this gap, this paper proposes ClinicalReTrial, a self-evolving AI agent framework that addresses this gap by casting clinical trial reasoning as an iterative protocol redesign problem. Our method integrates failure diagnosis, safety-aware modification, and candidate evaluation in a closed-loop, reward-driven optimization framework. Serving the outcome prediction model as a simulation environment, ClinicalReTrial enables low-cost evaluation of protocol modifications and provides dense reward signals for continuous self-improvement. To support efficient exploration, the framework maintains hierarchical memory that captures iteration-level feedback within trials and distills transferable redesign patterns across trials. Empirically, ClinicalReTrial improves 83.3% of trial protocols with a mean success probability gain of 5.7%, and retrospective case studies demonstrate strong alignment between the discovered redesign strategies and real-world clinical trial modifications.

</details>


### [96] [Multiagent Reinforcement Learning for Liquidity Games](https://arxiv.org/abs/2601.00324)
*Alicia Vidler,Gal A. Kaminka*

Main category: cs.AI

TL;DR: 论文提出金融蜂群模型，将流动性博弈与理性蜂群理论结合，展示独立交易者如何通过差异奖励实现个体盈利与市场流动性的双重目标。


<details>
  <summary>Details</summary>
Motivation: 将蜂群方法应用于金融市场流动性建模，同时将金融分析方法应用于蜂群研究，有望推动两个领域的发展。在蜂群研究中，博弈论方法有望解释理性自利参与者如何实现集体效用；在金融市场中，理解独立金融代理如何自组织以改善市场稳定性对市场设计研究至关重要。

Method: 将流动性博弈（交易者收益取决于交易中的总流动性）与理性蜂群（去中心化代理使用差异奖励将自利学习与全局目标对齐）统一起来。在马尔可夫团队博弈框架中，使用差异奖励构建金融蜂群模型，交易者集体目标是提供市场流动性同时保持代理独立性。

Result: 研究表明，个体流动性最大化行为能够促进整体市场流动性，无需协调或共谋。金融蜂群模型为双边资产市场中理性独立代理同时实现个体盈利和市场效率提供了理论框架。

Conclusion: 金融蜂群模型成功统一了流动性博弈和理性蜂群理论，展示了独立自利交易者如何通过差异奖励机制实现个体与集体目标的协调，为金融市场设计和蜂群研究提供了新的理论框架。

Abstract: Making use of swarm methods in financial market modeling of liquidity, and techniques from financial analysis in swarm analysis, holds the potential to advance both research areas. In swarm research, the use of game theory methods holds the promise of explaining observed phenomena of collective utility adherence with rational self-interested swarm participants. In financial markets, a better understanding of how independent financial agents may self-organize for the betterment and stability of the marketplace would be a boon for market design researchers. This paper unifies Liquidity Games, where trader payoffs depend on aggregate liquidity within a trade, with Rational Swarms, where decentralized agents use difference rewards to align self-interested learning with global objectives. We offer a theoretical frameworks where we define a swarm of traders whose collective objective is market liquidity provision while maintaining agent independence. Using difference rewards within a Markov team games framework, we show that individual liquidity-maximizing behaviors contribute to overall market liquidity without requiring coordination or collusion. This Financial Swarm model provides a framework for modeling rational, independent agents where they achieve both individual profitability and collective market efficiency in bilateral asset markets.

</details>


### [97] [Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning](https://arxiv.org/abs/2601.00400)
*Weng Ding,Yi Han,Mu-Jiang-Shan Wang*

Main category: cs.AI

TL;DR: 提出自适应因果协调检测框架ACCD，通过三阶段渐进架构动态学习最优检测配置，在真实数据集上实现87.3%的F1分数，比现有基线提升15.2%，减少68%人工标注需求，处理速度提升2.8倍。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体协调不实行为检测方法存在三个主要问题：依赖表面相关性分析、使用静态参数设置、需要大量人工标注。这些局限性导致检测效果不佳且效率低下。

Method: 提出三阶段自适应因果协调检测框架ACCD：1) 使用自适应收敛交叉映射技术深入识别账户间真实因果关系；2) 在半监督分类方案中集成主动学习和不确定性采样，减少人工标注；3) 部署基于历史检测经验的自动验证模块，实现自我验证和优化。

Result: 在Twitter IRA数据集、Reddit协调痕迹和多个广泛使用的机器人检测基准上进行评估，ACCD在协调攻击检测中达到87.3%的F1分数，比最强基线提升15.2%，减少68%人工标注需求，通过层次聚类优化实现2.8倍处理加速。

Conclusion: ACCD为社交媒体平台协调行为检测提供了更准确、高效、高度自动化的端到端解决方案，具有重要的实际应用价值和广泛的推广潜力。

Abstract: Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\% in coordinated attack detection, representing a 15.2\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

</details>


### [98] [Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications](https://arxiv.org/abs/2601.00421)
*Alessio Di Rubbo,Mattia Neri,Remo Pareschi,Marco Pedroni,Roberto Valtancoli,Paolino Zica*

Main category: cs.AI

TL;DR: 将语义空间推理从计算语言学扩展到团队体育战术决策，将球员视为词汇、团队配置视为语义结构，通过向量空间建模评估战术匹配度


<details>
  <summary>Details</summary>
Motivation: 传统语义空间推理主要应用于计算语言学，本文探索将其扩展到团队体育战术决策领域，为团队集体决策和性能优化提供通用框架

Method: 将球员建模为整合技术、身体和心理属性的多维向量，通过上下文加权聚合成团队语义表示；在共享向量空间中编码战术模板（如高位压迫、反击），使用向量距离度量评估战术匹配度和对手利用潜力

Result: 开发了Python原型系统，能够生成可解释的动态自适应策略建议，并提供属性级别的细粒度诊断洞察；该方法可推广到篮球、曲棍球、协作机器人、人机协调系统等团队领域

Conclusion: 提出了一个可推广的团队决策框架，并展望了未来方向：真实数据集成、预测模拟和混合人机战术智能的发展

Abstract: This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

</details>


### [99] [Progressive Ideation using an Agentic AI Framework for Human-AI Co-Creation](https://arxiv.org/abs/2601.00475)
*Sankar B,Srinidhi Ranjini Girish,Aadya Bharti,Dibakar Sen*

Main category: cs.AI

TL;DR: MIDAS是一个分布式AI代理系统，模拟人类元认知构思流程，通过多个专业代理逐步优化创意并评估全局和局部新颖性，实现真正的人机协同设计。


<details>
  <summary>Details</summary>
Motivation: 当前"单次爆发式"AI系统产生大量语义聚类的想法，加剧了新手设计师在生成真正新颖多样创意方面的认知挑战，需要新的AI辅助设计范式。

Method: 提出MIDAS框架，用分布式"团队"的专业AI代理取代单一AI范式，模拟人类元认知构思流程，逐步优化创意并评估全局新颖性（与现有方案对比）和局部新颖性（与已生成创意对比）。

Result: MIDAS展示了可行且渐进式的人机协同创作范式，将人类设计师从被动筛选者提升为参与式、主动的协作伙伴。

Conclusion: 分布式AI代理系统能够有效支持真正新颖创意的生成，为人机协同设计提供了新的可行范式。

Abstract: The generation of truly novel and diverse ideas is important for contemporary engineering design, yet it remains a significant cognitive challenge for novice designers. Current 'single-spurt' AI systems exacerbate this challenge by producing a high volume of semantically clustered ideas. We propose MIDAS (Meta-cognitive Ideation through Distributed Agentic AI System), a novel framework that replaces the single-AI paradigm with a distributed 'team' of specialized AI agents designed to emulate the human meta-cognitive ideation workflow. This agentic system progressively refines ideas and assesses each one for both global novelty (against existing solutions) and local novelty (against previously generated ideas). MIDAS, therefore, demonstrates a viable and progressive paradigm for true human-AI co-creation, elevating the human designer from a passive filterer to a participatory, active, collaborative partner.

</details>


### [100] [The Illusion of Insight in Reasoning Models](https://arxiv.org/abs/2601.00514)
*Liv G. d'Aliberti,Manoel Horta Ribeiro*

Main category: cs.AI

TL;DR: 研究发现推理模型中的"顿悟时刻"很罕见，不会随训练增加，也很少提高准确性，表明这些转变是推理不稳定的症状而非内在的自我纠正机制。


<details>
  <summary>Details</summary>
Motivation: 先前研究认为像DeepSeek-R1-Zero这样的模型会在推理过程中经历突然的"顿悟时刻"，从而实现自我纠正。但尚不清楚这种内在推理策略转变是否真正提高了性能。

Method: 分析了100多万条推理轨迹、数百个训练检查点、三个推理领域、多种解码温度和模型架构，检测训练过程中的推理转变，并研究人工触发外在转变的效果。

Result: 推理转变很罕见，不会随训练变得更频繁，很少提高准确性，表明它们不符合先前对模型洞察力的认知。但其效果随模型不确定性而变化，在高熵条件下人工触发外在转变能可靠提高准确性。

Conclusion: 推理过程中的转变是不稳定推理行为的症状，而非内在的自我纠正机制。模型在高不确定性时更容易受益于外在干预。

Abstract: Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.

</details>


### [101] [DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations](https://arxiv.org/abs/2601.00623)
*Longtian Qiu,Shan Ning,Chuyu Zhang,Jiaxuan Sun,Xuming He*

Main category: cs.AI

TL;DR: 提出DA-DPO框架，通过难度感知的偏好优化解决多模态大语言模型中的幻觉问题，避免现有方法因偏好数据难度不平衡导致的过拟合。


<details>
  <summary>Details</summary>
Motivation: 现有多模态DPO方法因偏好数据难度不平衡容易过拟合，模型倾向于过度关注容易区分的偏好对，阻碍细粒度幻觉抑制并降低整体性能。

Method: DA-DPO包含两个组件：1) 难度估计：利用预训练视觉-语言模型通过生成式和对比式目标，结合分布感知投票策略产生难度分数；2) 难度感知训练：根据估计难度重新加权偏好对，降低简单样本权重，强调困难样本以缓解过拟合。

Result: 实验表明DA-DPO能持续改进多模态偏好优化，在标准基准测试中展现出更强的幻觉鲁棒性和更好的泛化能力，同时保持计算效率。

Conclusion: DA-DPO通过难度感知的偏好优化框架，有效解决了多模态DPO中的过拟合问题，无需额外数据或微调阶段，实现了更有效的幻觉抑制和性能提升。

Abstract: Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.

</details>


### [102] [A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference](https://arxiv.org/abs/2601.00694)
*Qingwen Pu,Kun Xie,Hong Yang,Guocong Zhai*

Main category: cs.AI

TL;DR: PedX-LLM：一个结合视觉特征和交通领域知识的LLM框架，用于行人过街行为推理，相比传统方法具有更好的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有行人过街行为推断方法（统计模型和监督学习）泛化能力有限，在新场景表现不佳。现有LLM应用缺乏领域特定适应和视觉上下文，需要开发能进行语义、上下文感知行为推理的通用框架

Method: 提出PedX-LLM框架：1）集成LLaVA提取的视觉特征与文本数据及交通领域知识；2）通过LoRA微调LLaMA-2-7B基础模型；3）支持零样本和少样本学习进行跨场景验证

Result: 1）平衡准确率达82.0%，优于最佳统计和监督学习方法；2）视觉增强模块带来2.9%性能提升；3）领域知识集成带来额外4.1%提升；4）零样本配置在5个未见测试场景达66.9%准确率，比基线高至少18个百分点；5）少样本学习（仅5个验证样本）将准确率提升至72.2%

Conclusion: PedX-LLM展示了强大的泛化能力，证明视觉和知识增强的推理使模型能够模仿人类决策逻辑，克服纯数据驱动方法的局限性，将行人过街推断从特定场景模式识别转变为通用行为推理

Abstract: Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.

</details>


### [103] [An Agentic Framework for Neuro-Symbolic Programming](https://arxiv.org/abs/2601.00743)
*Aliakbar Nafar,Chetan Chigurupati,Danial Kamali,Hamid Karimian,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: AgenticDomiKnowS (ADS) 使用智能体工作流将自然语言任务描述自动转换为完整的 DomiKnowS 程序，显著降低神经符号编程门槛


<details>
  <summary>Details</summary>
Motivation: 将符号约束集成到深度学习模型中可以提高鲁棒性、可解释性和数据效率，但现有框架（如 DomiKnowS）仍要求用户精通特定语法，开发过程耗时且具有挑战性

Method: 提出 AgenticDomiKnowS (ADS)，通过智能体工作流将自由形式的任务描述转换为完整的 DomiKnowS 程序。该工作流单独创建和测试每个 DomiKnowS 组件，支持可选的人机交互，允许熟悉 DomiKnowS 的用户细化中间输出

Result: ADS 使有经验的 DomiKnowS 用户和非用户都能快速构建神经符号程序，将开发时间从数小时减少到 10-15 分钟

Conclusion: ADS 消除了对特定库语法的依赖，通过自动化程序生成和可选的专家干预，显著降低了神经符号编程的门槛和开发时间

Abstract: Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

</details>
