<div id=toc></div>

# Table of Contents

- [physics.ao-ph](#physics.ao-ph) [Total: 5]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.CV](#cs.CV) [Total: 58]
- [cs.AI](#cs.AI) [Total: 8]


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [1] [Geographic variability in reanalysis wind speed biases: A high-resolution bias correction approach for UK wind energy](https://arxiv.org/abs/2511.04781)
*Yan Wang,Simon C. Warder,Ellyess F. Benmoufok,Andrew Wynn,Oliver R. H. Buxton,Iain Staffell,Matthew D. Piggott*

Main category: physics.ao-ph

TL;DR: 提出了一种基于聚类的空间分辨偏差校正框架，用于纠正再分析数据集中的风速偏差，在英国319个风电场上的应用显示月均风电模拟误差减少32%以上。


<details>
  <summary>Details</summary>
Motivation: 再分析数据集在风资源评估中存在系统性风速偏差，特别是低空间分辨率难以准确捕捉局部变化，而现有行业实践要么不进行偏差校正，要么使用粗粒度的全国统一调整。

Method: 扩展并深入分析了一个基于聚类的空间分辨偏差校正框架，该方法能更好地考虑局部异质性，并应用于ERA5和MERRA-2两个再分析数据集进行对比评估。

Result: 该方法使月均风电模拟误差相比未校正的ERA5数据集减少超过32%，并揭示了ERA5风速误差在英国具有强烈空间变异性，苏格兰高地和威尔士山区存在最显著的低估。

Conclusion: 研究强调了在纠正再分析风速时明确考虑地理变异性的重要性，为高分辨率风能建模提供了区域特定偏差模式的新见解。

Abstract: Reanalysis datasets have become indispensable tools for wind resource
assessment and wind power simulation, offering long-term and spatially
continuous wind fields across large regions. However, they inherently contain
systematic wind speed biases arising from various factors, including simplified
physical parameterizations, observational uncertainties, and limited spatial
resolution. Among these, low spatial resolution poses a particular challenge
for capturing local variability accurately. Whereas prevailing industry
practice generally relies on either no bias correction or coarse, nationally
uniform adjustments, we extend and thoroughly analyse a recently proposed
spatially resolved, cluster-based bias correction framework. This approach is
designed to better account for local heterogeneity and is applied to 319 wind
farms across the United Kingdom to evaluate its effectiveness. Results show
that this method reduced monthly wind power simulation errors by more than 32%
compared to the uncorrected ERA5 reanalysis dataset. The method is further
applied to the MERRA-2 dataset for comparative evaluation, demonstrating its
effectiveness and robustness for different reanalysis products. In contrast to
prior studies, which rarely quantify the influence of topography on reanalysis
biases, this research presents a detailed spatial mapping of bias correction
factors across the UK. The analysis reveals that for wind energy applications,
ERA5 wind speed errors exhibit strong spatial variability, with the most
significant underestimations in the Scottish Highlands and mountainous areas of
Wales. These findings highlight the importance of explicitly accounting for
geographic variability when correcting reanalysis wind speeds, and provide new
insights into region-specific bias patterns relevant for high-resolution wind
energy modelling.

</details>


### [2] [Cracking the Code of Arctic Sea Ice: Why Models Fail to Predict Its Retreat?](https://arxiv.org/abs/2511.04961)
*Ruijian Gou,Gerrit Lohmann,Deliang Chen,Shiming Xu,Ruiqi Shu,Shaoqing Zhang,Lixin Wu*

Main category: physics.ao-ph

TL;DR: 当前气候模型因分辨率不足而低估了北极海冰消退速度，需要更高分辨率模型来准确预测海冰融化。


<details>
  <summary>Details</summary>
Motivation: 北极海冰因全球变暖快速消退，现有证据表明消退速度可能被低估，主要原因是当前气候模型分辨率不足。

Method: 通过分析涡旋浮冰相互作用、气候极端事件等小尺度过程在加速海冰融化中的作用。

Result: 阐明了这些小尺度动力学在加速海冰融化中的关键作用。

Conclusion: 强调需要更高分辨率的气候模型来改进北极海冰预测。

Abstract: Arctic sea ice is rapidly retreating due to global warming, and emerging
evidence suggests that the rate of decline may have been underestimated. A key
factor contributing to this underestimation is the coarse resolution of current
climate models, which fail to accurately represent eddy floe interactions,
climate extremes, and other critical small scale processes. Here, we elucidate
the roles of these dynamics in accelerating sea ice melt and emphasize the need
for higher resolution models to improve projections of Arctic sea ice.

</details>


### [3] [Improvement of a neural network convection scheme by including triggering and evaluation in present and future climates](https://arxiv.org/abs/2511.05074)
*Hugo Germain,Blanka Balogh,Olivier Geoffroy,David Saint-Martin*

Main category: physics.ao-ph

TL;DR: 改进ARPGEM全球大气模型中深度对流的神经网络参数化，通过引入触发机制检测对流活动，在现在和更暖气候下都表现出良好性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络参数化没有考虑对流的偶发性特征，需要开发能够检测对流是否活跃的触发机制来提高参数化性能。

Method: 开发包含触发机制的神经网络参数化，使用相对湿度而非比湿作为输入，并在现在和更暖气候数据下训练网络。

Result: 新参数化在现在气候下优于现有神经网络参数化，在线模拟无稳定性问题，在更暖气候下也表现良好，相对湿度输入改善了泛化能力。

Conclusion: 包含触发机制的神经网络参数化在现在和更暖气候下都能有效工作，使用相对湿度作为输入和用更暖气候数据训练可提高泛化性能。

Abstract: In this study, we improve a neural network (NN) parameterization of deep
convection in the global atmosphere model ARP-GEM. To take into account the
sporadic nature of convection, we develop a NN parameterization that includes a
triggering mechanism that can detect whether deep convection is active or not
within a grid-cell. This new data-driven parameterization outperforms the
existing NN parameterization in present climate when replacing the original
deep convection scheme of ARP-GEM. Online simulations with the NN
parameterization run without stability issues. Then, this NN parameterization
is evaluated online in a warmer climate. We confirm that using relative
humidity instead of the specific total humidity as input for the NN (trained
with present data) improves the performance and generalization in warmer
climate. Finally, we perform the training of the NN parameterization with data
from a warmer climate and this configuration get similar results when used in
simulations in present or warmer climates.

</details>


### [4] [Climate Downscaling of Tropical Cyclone Intensity using Deep Learning](https://arxiv.org/abs/2511.05392)
*Minh-Khanh Luong,Chanh Kieu*

Main category: physics.ao-ph

TL;DR: 使用深度学习模型从粗分辨率气候再分析数据中降尺度热带气旋强度和结构，性能优于传统涡旋检测方法，但受数据分辨率限制。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖动力或统计降尺度来增强热带气旋强度估计，本研究探索深度学习是否能为气候数据中的TC强度估计提供替代方法。

Method: 使用基于卷积神经网络(CNN)的深度学习架构，选择关键环境特征，从0.5度分辨率的气候再分析数据中降尺度TC强度和结构。

Result: DL模型在最大10米风速上的均方根误差为3-9 m/s，最小中心气压误差为10-20 hPa，性能优于直接涡旋检测方法，但误差范围较宽。

Conclusion: TC强度和结构不仅受内部动力学控制，还受局部环境影响，DL模型能学习并捕捉这些关系。但0.5度分辨率数据包含的TC信息有限，限制了模型性能。

Abstract: Traditional methods for enhancing tropical cyclone (TC) intensity from
climate model outputs or projections have primarily relied on either dynamical
or statistical downscaling. With recent advances in deep learning (DL)
techniques, a natural question is whether DL can provide an alternative
approach for improving TC intensity estimation from climate data. Using a
common DL architecture based on convolutional neural networks (CNN) and
selecting a set of key environmental features, we show that both TC intensity
and structure can be effectively downscaled from climate reanalysis data as
compared to common vortex detection methods, even when applied to
coarse-resolution (0.5-degree) data. Our results thus highlight that TC
intensity and structure are governed not only by its internal dynamics but also
by local environments during TC development, for which DL models can learn and
capture beyond the potential intensity framework. The performance of our DL
model depends on several factors such as data sampling strategy, season, or the
stage of TC development, with root-mean-square errors ranging from 3-9
ms$^{-1}$ for maximum 10 m wind and 10-20 hPa for minimum central pressure.
Although these errors are better than direct vortex detection methods, their
wide ranges also suggest that 0.5-degree resolution climate data may contain
limited TC information for DL models to learn from, regardless of model
optimizations or architectures. Possible improvements and challenges in
addressing the lack of fine-scale TC information in coarse resolution climate
reanalysis datasets will be discussed.

</details>


### [5] [A Satellite Remote Sensing and Doppler LiDAR-based Framework for Evaluating Mesoscale Flows Driven by Surface Heterogeneity](https://arxiv.org/abs/2511.05429)
*Tyler Waterman,Peter Germ,Marc Calaf,Eric Pardyjak,Nathaniel Chaney*

Main category: physics.ao-ph

TL;DR: 该研究结合卫星遥感和多普勒激光雷达观测评估地表异质性驱动的中尺度大气流动，提出分散动能(DKE)作为有效指标，并通过大涡模拟验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 地表异质性对中尺度大气流动有重要影响，但观测约束和建模限制阻碍了全面理解和模型参数化。

Method: 使用GOES地表温度场量化地表异质性，通过多普勒激光雷达网络计算分散动能(DKE)，并结合大涡模拟进行验证。

Result: DKE及其与平均动能的比值是地表异质性驱动流动的有效指标，与地表加热梯度的空间相关性、标准差和方向等指标相关。

Conclusion: 该方法提供了可扩展的观测基础方法，有助于理解陆气相互作用并改进气候和天气预报模型中的参数化方案。

Abstract: Surface heterogeneity, particularly complex patterns of surface heating,
significantly influences mesoscale atmospheric flows, yet observational
constraints and modeling limitations have hindered comprehensive understanding
and model parameterization. This study introduces a framework combining
satellite remote sensing and Doppler LiDAR to observationally evaluate
heterogeneity-driven mesoscale flows in the atmospheric boundary layer. We
quantify surface heterogeneity using metrics derived from GOES land surface
temperature fields, and assess atmospheric impact through the Dispersive
Kinetic Energy (DKE) calculated from a network of Doppler LiDAR profiles at the
Southern Great Plains (SGP) Atmospheric Radiation Measurement (ARM) site.
Results demonstrate that DKE and its ratio to the Mean Kinetic Energy (MKE)
serve as effective indicators of heterogeneity driven flows, including breezes
and circulations. The DKE and DKE ratio are correlated with metrics for surface
heterogeneity, including the spatial correlation lengthscale, the spatial
standard deviation, and the orientation of the surface heating gradient
relative to the wind. The correlation becomes stronger when other flows that
would affect DKE, including deep convection, low level jets, and storm fronts,
are accounted for. Large Eddy Simulations contextualize the findings and
validate the metric's behavior, showing general agreement with expectations
from prior literature. Simulations also illustrate the sensitivity to
configuration of LiDAR networks using virtual LiDAR sites, indicating that even
smaller networks can be used effectively. This approach offers a scalable,
observationally grounded method to explore heterogeneity-driven flows,
advancing understanding of land-atmosphere interactions as well as efforts to
parameterize these dynamics in climate and weather prediction models.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [6] [NeuroFlex: Column-Exact ANN-SNN Co-Execution Accelerator with Cost-Guided Scheduling](https://arxiv.org/abs/2511.05215)
*Varun Manjunath,Pranav Ramesh,Gopalakrishnan Srinivasan*

Main category: cs.NE

TL;DR: NeuroFlex是一种列级加速器，通过协同执行人工神经网络和脉冲神经网络来最小化稀疏边缘工作负载的能耗延迟积，同时保持竞争性精度。


<details>
  <summary>Details</summary>
Motivation: 针对稀疏边缘工作负载，现有单模式设计在能耗和延迟方面存在局限性，需要一种能够同时利用ANN和SNN优势的混合架构。

Method: 采用列级ANN-SNN混合执行，扩展整数精确的QCFS转换技术，统一INT8存储并实时生成脉冲，使用离线成本模型分配列到ANN或SNN核心，并通过确定性运行时跨处理元素打包工作。

Result: 相比随机映射，成本引导调度算法提升吞吐量16-19%；相比纯ANN基线，降低能耗延迟积57-67%；相比LoAS实现2.5倍加速，相比SparTen实现2.51倍能耗降低。

Conclusion: 细粒度和整数精确的混合设计在能耗和延迟方面优于单模式设计，且不牺牲精度。

Abstract: NeuroFlex is a column-level accelerator that co-executes artificial and
spiking neural networks to minimize energy-delay product on sparse edge
workloads with competitive accuracy. The design extends integer-exact QCFS
ANN-SNN conversion from layers to independent columns. It unifies INT8 storage
with on-the-fly spike generation using an offline cost model to assign columns
to ANN or SNN cores and pack work across processing elements with deterministic
runtime. Our cost-guided scheduling algorithm improves throughput by 16-19%
over random mapping and lowers EDP by 57-67% versus a strong ANN-only baseline
across VGG-16, ResNet-34, GoogLeNet, and BERT models. NeuroFlex also delivers
up to 2.5x speedup over LoAS and 2.51x energy reduction over SparTen. These
results indicate that fine-grained and integer-exact hybridization outperforms
single-mode designs on energy and latency without sacrificing accuracy.

</details>


### [7] [FPGA-Based Real-Time Waveform Classification](https://arxiv.org/abs/2511.05479)
*Alperen Aksoy,Ilja Bekman,Chimezie Eguzo,Christian Grewing,Andre Zambanini*

Main category: cs.NE

TL;DR: 该论文研究了基于查找表的神经网络用于SiPM信号波形分类，以辅助简单阈值触发在线提取粒子击中信息，减少数据传输量。


<details>
  <summary>Details</summary>
Motivation: 为了在早期阶段在线可靠地提取量热粒子击中信息并减少传输数据量，需要开发能够在FPGA上实现的自触发读出系统。

Method: 使用基于查找表的二进制多层神经网络，采用遗传算法进行训练，解决网络布局、占用空间、性能和训练方面的挑战。

Result: 这些神经网络结构能够实现与无死区时间在线处理兼容的推理延迟。

Conclusion: 基于查找表的二进制神经网络可以在FPGA上有效实现，满足SiPM信号波形分类的实时处理需求。

Abstract: For self-triggered readout of SiPM sum signals, a waveform classification can
aid a simple threshold trigger to reliably extract calorimetric particle hit
information online at an early stage and thus reduce the volume of transmitted
data. Typically, the ADC data acquisition is based on FPGAs for edge data
processing. In this study, we consider look-up-table-based neural-networks and
address challenges of binary multi-layer neural networks' layout, footprint,
performance and training. We show that these structures can be trained using a
genetic algorithm and achieve the inference latency compatible with dead-time
free processing online.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs](https://arxiv.org/abs/2511.04727)
*Ali Faraz,Akash,Shaharukh Khan,Raja Kolla,Akshat Patidar,Suranjan Goswami,Abhinav Ravi,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

TL;DR: IndicVisionBench是首个以印度次大陆为中心的大规模多模态基准测试，涵盖英语和10种印度语言，包含OCR、MMT和VQA三个任务，共约5K图像和37K+问答对，评估了8个模型在文化多样性环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大多数多模态评估基准都是西方中心的，缺乏对文化多样性和多语言环境的评估，需要填补这一空白。

Method: 构建包含英语和10种印度语言的基准测试，涵盖OCR、MMT和VQA三个多模态任务，6种问题类型，13个文化相关主题，共约5K图像和37K+问答对。

Result: 评估了8个模型（从闭源系统到开源模型），发现当前VLMs在文化多样性环境中存在显著的性能差距。

Conclusion: IndicVisionBench为多模态研究建立了可复现的评估框架，推动更具包容性的多模态研究发展。

Abstract: Vision-language models (VLMs) have demonstrated impressive generalization
across multimodal tasks, yet most evaluation benchmarks remain Western-centric,
leaving open questions about their performance in culturally diverse and
multilingual settings. To address this gap, we introduce IndicVisionBench, the
first large-scale benchmark centered on the Indian subcontinent. Covering
English and 10 Indian languages, our benchmark spans 3 multimodal tasks,
including Optical Character Recognition (OCR), Multimodal Machine Translation
(MMT), and Visual Question Answering (VQA), covering 6 kinds of question types.
Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across
13 culturally grounded topics. In addition, we release a paired parallel corpus
of annotations across 10 Indic languages, creating a unique resource for
analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum
of 8 models, from proprietary closed-source systems to open-weights medium and
large-scale models. Our experiments reveal substantial performance gaps,
underscoring the limitations of current VLMs in culturally diverse contexts. By
centering cultural diversity and multilinguality, IndicVisionBench establishes
a reproducible evaluation framework that paves the way for more inclusive
multimodal research.

</details>


### [9] [Knowledge-based anomaly detection for identifying network-induced shape artifacts](https://arxiv.org/abs/2511.04729)
*Rucha Deshpande,Tahsin Rahman,Miguel Lago,Adarsh Subbaswamy,Jana G. Delfino,Ghada Zamzmi,Elim Thompson,Aldo Badano,Seyed Kahaki*

Main category: cs.CV

TL;DR: 提出了一种基于知识的异常检测方法，用于检测合成医学图像中的网络诱导形状伪影，通过两阶段框架（特征提取器和隔离森林检测器）在乳腺X光合成数据上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 合成数据可解决机器学习模型训练中的数据稀缺问题，但未经质量评估的采用可能引入伪影、失真和不真实特征，损害模型性能和临床实用性。

Method: 两阶段框架：(i) 新颖的特征提取器，通过分析解剖边界角度梯度的每图像分布构建专门特征空间；(ii) 基于隔离森林的异常检测器。

Result: 在两个合成乳腺X光数据集上验证，AUC值分别为0.97和0.91；人机一致性研究中，最异常分区的平均同意率达66%和68%，Kendall-Tau相关性为0.45和0.43。

Conclusion: 该方法推动了合成数据的负责任使用，使开发者能够根据已知解剖约束评估合成图像，识别并解决特定问题以提高合成数据集整体质量。

Abstract: Synthetic data provides a promising approach to address data scarcity for
training machine learning models; however, adoption without proper quality
assessments may introduce artifacts, distortions, and unrealistic features that
compromise model performance and clinical utility. This work introduces a novel
knowledge-based anomaly detection method for detecting network-induced shape
artifacts in synthetic images. The introduced method utilizes a two-stage
framework comprising (i) a novel feature extractor that constructs a
specialized feature space by analyzing the per-image distribution of angle
gradients along anatomical boundaries, and (ii) an isolation forest-based
anomaly detector. We demonstrate the effectiveness of the method for
identifying network-induced shape artifacts in two synthetic mammography
datasets from models trained on CSAW-M and VinDr-Mammo patient datasets
respectively. Quantitative evaluation shows that the method successfully
concentrates artifacts in the most anomalous partition (1st percentile), with
AUC values of 0.97 (CSAW-syn) and 0.91 (VMLO-syn). In addition, a reader study
involving three imaging scientists confirmed that images identified by the
method as containing network-induced shape artifacts were also flagged by human
readers with mean agreement rates of 66% (CSAW-syn) and 68% (VMLO-syn) for the
most anomalous partition, approximately 1.5-2 times higher than the least
anomalous partition. Kendall-Tau correlations between algorithmic and human
rankings were 0.45 and 0.43 for the two datasets, indicating reasonable
agreement despite the challenging nature of subtle artifact detection. This
method is a step forward in the responsible use of synthetic data, as it allows
developers to evaluate synthetic images for known anatomic constraints and
pinpoint and address specific issues to improve the overall quality of a
synthetic dataset.

</details>


### [10] [CPO: Condition Preference Optimization for Controllable Image Generation](https://arxiv.org/abs/2511.04753)
*Zonglin Lyu,Ming Li,Xinxin Liu,Chen Chen*

Main category: cs.CV

TL;DR: 该论文提出了条件偏好优化(CPO)方法，通过在控制信号而非生成图像上进行偏好学习，解决了传统DPO方法中混淆变量的问题，显著提升了文本到图像生成的精确控制能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决ControlNet++在优化过程中只关注低噪声时间步而忽略高噪声时间步贡献的问题，以及DPO方法中难以确保图像对仅在可控性上存在差异而其他因素保持不变的挑战。

Method: 提出条件偏好优化(CPO)方法，构建获胜和失败的控制信号对(c^w和c^l)，直接在控制条件上进行偏好学习，避免了图像质量等混淆因素的影响。

Result: CPO在多个控制类型上显著优于最先进的ControlNet++：分割任务错误率降低超过10%，人体姿态任务降低70-80%，边缘和深度图任务一致降低2-5%。

Conclusion: CPO通过直接在控制信号上进行偏好学习，提供了低方差训练目标，在理论上比DPO具有更低的对比损失方差，在实际应用中取得了更优越的结果，同时减少了计算和存储需求。

Abstract: To enhance controllability in text-to-image generation, ControlNet introduces
image-based control signals, while ControlNet++ improves pixel-level cycle
consistency between generated images and the input control signal. To avoid the
prohibitive cost of back-propagating through the sampling process, ControlNet++
optimizes only low-noise timesteps (e.g., $t < 200$) using a single-step
approximation, which not only ignores the contribution of high-noise timesteps
but also introduces additional approximation errors. A straightforward
alternative for optimizing controllability across all timesteps is Direct
Preference Optimization (DPO), a fine-tuning method that increases model
preference for more controllable images ($I^{w}$) over less controllable ones
($I^{l}$). However, due to uncertainty in generative models, it is difficult to
ensure that win--lose image pairs differ only in controllability while keeping
other factors, such as image quality, fixed. To address this, we propose
performing preference learning over control conditions rather than generated
images. Specifically, we construct winning and losing control signals,
$\mathbf{c}^{w}$ and $\mathbf{c}^{l}$, and train the model to prefer
$\mathbf{c}^{w}$. This method, which we term \textit{Condition Preference
Optimization} (CPO), eliminates confounding factors and yields a low-variance
training objective. Our approach theoretically exhibits lower contrastive loss
variance than DPO and empirically achieves superior results. Moreover, CPO
requires less computation and storage for dataset curation. Extensive
experiments show that CPO significantly improves controllability over the
state-of-the-art ControlNet++ across multiple control types: over $10\%$ error
rate reduction in segmentation, $70$--$80\%$ in human pose, and consistent
$2$--$5\%$ reductions in edge and depth maps.

</details>


### [11] [DARN: Dynamic Adaptive Regularization Networks for Efficient and Robust Foundation Model Adaptation](https://arxiv.org/abs/2511.04766)
*Dhenenjay Yadav,Rohan Sawai*

Main category: cs.CV

TL;DR: DARN是一种动态自适应正则化网络，通过任务复杂度预测、自适应dropout调制和动态容量门控来解决卫星图像异质性问题，在GeoBench基准测试中达到86.66% mIoU的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用固定正则化策略，无法应对卫星图像的显著异质性，限制了基础模型在遥感分析中的适应效果。

Method: 提出DARN架构，包含三个核心创新：轻量级任务复杂度预测器(TCP)、自适应dropout调制(ADM)和动态容量门控(DCG)，理论证明其优化收敛性和自适应信息瓶颈机制。

Result: 在全微调模式下，DARN在GeoBench上达到86.66% mIoU（比之前SOTA提升5.56个百分点）；在高效适应模式下，在Sen1Floods11上达到90.5% mIoU，并在OOD泛化、鲁棒性和少数类性能方面表现优异。

Conclusion: DARN为关键地理空间应用提供了更智能、鲁棒和高效的基础模型利用方法。

Abstract: Foundation models (FMs) offer powerful representations for geospatial
analysis, but adapting them effectively remains challenging. Standard
adaptation methods, whether full fine-tuning or efficient frozen-backbone
approaches, typically employ decoders with fixed regularization strategies,
failing to account for the significant heterogeneity in satellite imagery. We
introduce Dynamic Adaptive Regularization Networks (DARN), a novel decoder
architecture designed to address this limitation. DARN integrates three key
innovations: (1) a lightweight Task Complexity Predictor (TCP) that estimates
per-sample difficulty, (2) Adaptive Dropout Modulation (ADM), dynamically
adjusting dropout rates (from 0.1 to 0.5) based on predicted complexity, and
(3) Dynamic Capacity Gating (DCG) that modulates channel activation. We provide
theoretical justifications linking DARN's optimization to stationary point
convergence and its mechanism to adaptive information bottlenecks. Empirically,
DARN demonstrates exceptional performance across both major adaptation
paradigms. In full fine-tuning (unfrozen backbone), DARN achieves a new
state-of-the-art on the multi-task GeoBench benchmark (86.66% mIoU, +5.56 pp
over prior SOTA). In efficient adaptation (frozen backbone), DARN achieves
SOTA-competitive accuracy (90.5% mIoU on Sen1Floods11) while delivering
substantial advantages crucial for real-world deployment: superior
out-of-distribution (OOD) generalization (+9.5 pp mIoU on AI4SmallFarms),
enhanced robustness (17% relative reduction in corruption error), and improved
performance on minority classes. DARN offers a more intelligent, robust, and
efficient approach to leveraging FMs in critical geospatial applications.

</details>


### [12] [Global 3D Reconstruction of Clouds & Tropical Cyclones](https://arxiv.org/abs/2511.04773)
*Shirin Ermis,Cesar Aybar,Lilli Freischem,Stella Girtsou,Kyriaki-Margarita Bintsi,Emiliano Diaz Salas-Porras,Michael Eisinger,William Jones,Anna Jungbluth,Benoit Tremblay*

Main category: cs.CV

TL;DR: 提出基于预训练-微调框架的新方法，利用多卫星全球观测数据将2D卫星图像转换为3D云图，首次实现全球瞬时3D云图重建，特别针对强热带气旋的3D结构进行准确重建。


<details>
  <summary>Details</summary>
Motivation: 热带气旋预报面临卫星观测有限和云属性解析困难的挑战，现有机器学习方法在热带气旋常见区域表现不佳且对强风暴验证不足。

Method: 采用预训练-微调流程，从多卫星全球覆盖数据中学习，将2D卫星图像转换为相关云属性的3D云图，并在定制热带气旋数据集上评估性能。

Result: 首次创建全球瞬时3D云图，准确重建强风暴的3D结构，不仅扩展了可用卫星观测，还能在观测完全缺失时提供估计。

Conclusion: 该方法对于增进对热带气旋增强机制的理解和改进预报至关重要。

Abstract: Accurate forecasting of tropical cyclones (TCs) remains challenging due to
limited satellite observations probing TC structure and difficulties in
resolving cloud properties involved in TC intensification. Recent research has
demonstrated the capabilities of machine learning methods for 3D cloud
reconstruction from satellite observations. However, existing approaches have
been restricted to regions where TCs are uncommon, and are poorly validated for
intense storms. We introduce a new framework, based on a
pre-training--fine-tuning pipeline, that learns from multiple satellites with
global coverage to translate 2D satellite imagery into 3D cloud maps of
relevant cloud properties. We apply our model to a custom-built TC dataset to
evaluate performance in the most challenging and relevant conditions. We show
that we can - for the first time - create global instantaneous 3D cloud maps
and accurately reconstruct the 3D structure of intense storms. Our model not
only extends available satellite observations but also provides estimates when
observations are missing entirely. This is crucial for advancing our
understanding of TC intensification and improving forecasts.

</details>


### [13] [EETnet: a CNN for Gaze Detection and Tracking for Smart-Eyewear](https://arxiv.org/abs/2511.04779)
*Andrea Aspesi,Andrea Simpsi,Aaron Tognoli,Simone Mentasti,Luca Merigo,Matteo Matteucci*

Main category: cs.CV

TL;DR: EETnet是一个专为基于事件的眼动追踪设计的卷积神经网络，能够在资源受限的微控制器上运行，并提出了分类和回归两种架构版本。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件相机的眼动追踪解决方案大多依赖强大的GPU，无法在嵌入式设备上部署，需要开发能在资源受限设备上运行的解决方案。

Method: 设计了EETnet卷积神经网络，使用纯事件数据进行眼动追踪；提出了训练、评估和量化网络的方法论；开发了两种架构：基于网格分类的模型和像素级回归模型。

Result: 成功开发了能够在微控制器上运行的基于事件相机的眼动追踪神经网络。

Conclusion: EETnet证明了在资源受限的嵌入式设备上实现高效、低功耗基于事件眼动追踪的可行性。

Abstract: Event-based cameras are becoming a popular solution for efficient, low-power
eye tracking. Due to the sparse and asynchronous nature of event data, they
require less processing power and offer latencies in the microsecond range.
However, many existing solutions are limited to validation on powerful GPUs,
with no deployment on real embedded devices. In this paper, we present EETnet,
a convolutional neural network designed for eye tracking using purely
event-based data, capable of running on microcontrollers with limited
resources. Additionally, we outline a methodology to train, evaluate, and
quantize the network using a public dataset. Finally, we propose two versions
of the architecture: a classification model that detects the pupil on a grid
superimposed on the original image, and a regression model that operates at the
pixel level.

</details>


### [14] [3D Gaussian Point Encoders](https://arxiv.org/abs/2511.04797)
*Jim James,Ben Wilson,Simon Lucey,James Hays*

Main category: cs.CV

TL;DR: 提出3D高斯点编码器，这是一种基于学习3D高斯混合的显式逐点嵌入方法，相比传统PointNet等隐式表示，在3D识别任务中具有更高的计算效率和参数效率。


<details>
  <summary>Details</summary>
Motivation: 当前3D识别任务广泛使用隐式表示（如PointNet），但存在计算效率问题。受3D重建领域从隐式表示（如NeRF）转向显式表示（如高斯泼溅）的启发，希望开发显式几何表示来提升3D识别的计算性能。

Method: 基于自然梯度和从PointNet蒸馏的优化技术，学习能够重建PointNet激活的高斯基。扩展3D高斯泼溅的滤波技术来加速编码器。

Result: 3D高斯点编码器比传统PointNet快2.7倍，内存减少46%，FLOPs减少88%。在Mamba3D中运行快1.27倍，内存和FLOPs分别减少42%和54%。轻量级设计可在仅CPU设备上实现高帧率。

Conclusion: 3D高斯点编码器为3D识别任务提供了一种高效的显式几何表示替代方案，显著提升了计算效率，同时保持了可比的准确度。

Abstract: In this work, we introduce the 3D Gaussian Point Encoder, an explicit
per-point embedding built on mixtures of learned 3D Gaussians. This explicit
geometric representation for 3D recognition tasks is a departure from widely
used implicit representations such as PointNet. However, it is difficult to
learn 3D Gaussian encoders in end-to-end fashion with standard optimizers. We
develop optimization techniques based on natural gradients and distillation
from PointNets to find a Gaussian Basis that can reconstruct PointNet
activations. The resulting 3D Gaussian Point Encoders are faster and more
parameter efficient than traditional PointNets. As in the 3D reconstruction
literature where there has been considerable interest in the move from implicit
(e.g., NeRF) to explicit (e.g., Gaussian Splatting) representations, we can
take advantage of computational geometry heuristics to accelerate 3D Gaussian
Point Encoders further. We extend filtering techniques from 3D Gaussian
Splatting to construct encoders that run 2.7 times faster as a comparable
accuracy PointNet while using 46% less memory and 88% fewer FLOPs. Furthermore,
we demonstrate the effectiveness of 3D Gaussian Point Encoders as a component
in Mamba3D, running 1.27 times faster and achieving a reduction in memory and
FLOPs by 42% and 54% respectively. 3D Gaussian Point Encoders are lightweight
enough to achieve high framerates on CPU-only devices.

</details>


### [15] [Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose](https://arxiv.org/abs/2511.04803)
*Shuo Zhao,Jianxu Chen*

Main category: cs.CV

TL;DR: 本研究分析了生物医学图像分割模型Cellpose的数据冗余和跨域遗忘问题，提出了数据集量化策略来构建紧凑训练子集，并探讨了选择性重放和训练域排序对缓解灾难性遗忘的作用。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学图像分割中训练数据冗余和跨域迁移导致的模型遗忘问题，提高训练效率和模型泛化能力。

Method: 使用数据集量化(DQ)策略构建紧凑训练子集，通过MAE嵌入和t-SNE进行潜在空间分析，进行跨域微调实验并采用选择性DQ重放策略。

Result: 实验显示仅需10%数据即可达到性能饱和，选择性重放5-10%源数据能有效恢复源域性能，训练域排序可改善泛化并减少遗忘。

Conclusion: 生物医学图像分割需要数据中心的训练设计，包括紧凑子集、保留感知学习策略和合理的域排序，以提高训练效率。

Abstract: Generalist biomedical image segmentation models such as Cellpose are
increasingly applied across diverse imaging modalities and cell types. However,
two critical challenges remain underexplored: (1) the extent of training data
redundancy and (2) the impact of cross domain transfer on model retention. In
this study, we conduct a systematic empirical analysis of these challenges
using Cellpose as a case study. First, to assess data redundancy, we propose a
simple dataset quantization (DQ) strategy for constructing compact yet diverse
training subsets. Experiments on the Cyto dataset show that image segmentation
performance saturates with only 10% of the data, revealing substantial
redundancy and potential for training with minimal annotations. Latent space
analysis using MAE embeddings and t-SNE confirms that DQ selected patches
capture greater feature diversity than random sampling. Second, to examine
catastrophic forgetting, we perform cross domain finetuning experiments and
observe significant degradation in source domain performance, particularly when
adapting from generalist to specialist domains. We demonstrate that selective
DQ based replay reintroducing just 5-10% of the source data effectively
restores source performance, while full replay can hinder target adaptation.
Additionally, we find that training domain sequencing improves generalization
and reduces forgetting in multi stage transfer. Our findings highlight the
importance of data centric design in biomedical image segmentation and suggest
that efficient training requires not only compact subsets but also retention
aware learning strategies and informed domain ordering. The code is available
at https://github.com/MMV-Lab/biomedseg-efficiency.

</details>


### [16] [An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention](https://arxiv.org/abs/2511.04811)
*Shuo Zhao,Yu Zhou,Jianxu Chen*

Main category: cs.CV

TL;DR: 提出了一种结合主动学习和伪标注的数据中心AI工作流，将大型基础模型与nnU-Net结合，显著减少医学图像分割中的人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中nnU-Net需要大量标注数据进行交叉验证的问题，以及大型基础模型在特定数据集上性能不足的局限性。

Method: 使用基础模型生成伪标注，用于nnU-Net自配置，然后通过主动学习选择代表性核心集进行少量人工标注，最后微调nnU-Net模型。

Result: 显著减少了人工标注需求，同时保持了有竞争力的分割性能。

Conclusion: 为生物医学研究人员提供了一种可访问的解决方案，能够在分割任务中应用最先进的AI技术，同时最小化人工干预。

Abstract: Biomedical image segmentation is critical for precise structure delineation
and downstream analysis. Traditional methods often struggle with noisy data,
while deep learning models such as U-Net have set new benchmarks in
segmentation performance. nnU-Net further automates model configuration, making
it adaptable across datasets without extensive tuning. However, it requires a
substantial amount of annotated data for cross-validation, posing a challenge
when only raw images but no labels are available. Large foundation models offer
zero-shot generalizability, but may underperform on specific datasets with
unique characteristics, limiting their direct use for analysis. This work
addresses these bottlenecks by proposing a data-centric AI workflow that
leverages active learning and pseudo-labeling to combine the strengths of
traditional neural networks and large foundation models while minimizing human
intervention. The pipeline starts by generating pseudo-labels from a foundation
model, which are then used for nnU-Net's self-configuration. Subsequently, a
representative core-set is selected for minimal manual annotation, enabling
effective fine-tuning of the nnU-Net model. This approach significantly reduces
the need for manual annotations while maintaining competitive performance,
providing an accessible solution for biomedical researchers to apply
state-of-the-art AI techniques in their segmentation tasks. The code is
available at https://github.com/MMV-Lab/AL_BioMed_img_seg.

</details>


### [17] [Geometry Denoising with Preferred Normal Vectors](https://arxiv.org/abs/2511.04848)
*Manuel Weiß,Lukas Baumgärtner,Roland Herzog,Stephan Schmidt*

Main category: cs.CV

TL;DR: 提出了一种基于表面法向量先验知识的几何去噪新方法，通过标签向量集进行自然分割，使用总变差正则化，采用分裂Bregman方法求解优化问题。


<details>
  <summary>Details</summary>
Motivation: 利用表面法向量的先验知识来改进几何去噪，通过标签向量集嵌入自然分割过程，提高去噪效果。

Method: 基于标签向量集的法向量相似性进行分割，使用总变差正则化，采用分裂Bregman（ADMM）方法求解优化问题，顶点更新基于二阶形状计算。

Result: 该方法能够有效进行几何去噪并同时实现基于法向量的表面分割。

Conclusion: 提出的新范式成功地将法向量先验知识融入几何去噪过程，通过分裂Bregman方法有效解决了优化问题。

Abstract: We introduce a new paradigm for geometry denoising using prior knowledge
about the surface normal vector. This prior knowledge comes in the form of a
set of preferred normal vectors, which we refer to as label vectors. A
segmentation problem is naturally embedded in the denoising process. The
segmentation is based on the similarity of the normal vector to the elements of
the set of label vectors. Regularization is achieved by a total variation term.
We formulate a split Bregman (ADMM) approach to solve the resulting
optimization problem. The vertex update step is based on second-order shape
calculus.

</details>


### [18] [Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction](https://arxiv.org/abs/2511.04864)
*Kyle Fogarty,Chenyue Cai,Jing Yang,Zhilin Guo,Cengiz Öztireli*

Main category: cs.CV

TL;DR: 提出了一种隐式自先验方法，直接从输入点云中提取形状特定的几何先验，通过交叉注意力机制捕获重复结构和长程相关性，结合RIMLS生成高质量表面。


<details>
  <summary>Details</summary>
Motivation: 从不规则点云恢复高质量表面是病态问题，需要强几何先验。传统方法依赖外部训练数据，本文旨在直接从输入点云本身学习形状特定的先验。

Method: 联合训练可学习嵌入字典与隐式距离场，通过交叉注意力机制捕获形状的重复结构和相关性。使用自监督点云重建损失优化，无需外部数据。结合RIMLS集成稠密点云和法向量。

Result: 实验表明该方法在生成高保真表面方面优于传统和基于学习的方法，具有更好的细节保持能力和对数据退化的鲁棒性。

Conclusion: 提出的隐式自先验方法能够有效从点云中学习形状特定的几何先验，结合RIMLS实现了高质量表面重建，无需外部训练数据。

Abstract: Recovering high-quality surfaces from irregular point cloud is ill-posed
unless strong geometric priors are available. We introduce an implicit
self-prior approach that distills a shape-specific prior directly from the
input point cloud itself and embeds it within an implicit neural
representation. This is achieved by jointly training a small dictionary of
learnable embeddings with an implicit distance field; at every query location,
the field attends to the dictionary via cross-attention, enabling the network
to capture and reuse repeating structures and long-range correlations inherent
to the shape. Optimized solely with self-supervised point cloud reconstruction
losses, our approach requires no external training data. To effectively
integrate this learned prior while preserving input fidelity, the trained field
is then sampled to extract densely distributed points and analytic normals via
automatic differentiation. We integrate the resulting dense point cloud and
corresponding normals into a robust implicit moving least squares (RIMLS)
formulation. We show this hybrid strategy preserves fine geometric details in
the input data, while leveraging the learned prior to regularize sparse
regions. Experiments show that our method outperforms both classical and
learning-based approaches in generating high-fidelity surfaces with superior
detail preservation and robustness to common data degradations.

</details>


### [19] [Clinical-ComBAT: a diffusion-weighted MRI harmonization method for clinical applications](https://arxiv.org/abs/2511.04871)
*Gabriel Girard,Manon Edde,Félix Dumais,Yoan David,Matthieu Dumont,Guillaume Theaud,Jean-Christophe Houde,Arnaud Boré,Maxime Descoteaux,Pierre-Marc Jodoin*

Main category: cs.CV

TL;DR: 提出了Clinical-ComBAT方法，用于解决DW-MRI数据在多站点采集时的扫描仪偏差问题，相比传统ComBAT方法更适合临床应用场景。


<details>
  <summary>Details</summary>
Motivation: DW-MRI数据在多站点采集时存在扫描仪特定的偏差，传统ComBAT方法依赖线性协变量关系、同质人群、固定站点数量等假设，限制了其在临床环境中的应用。

Method: 开发了Clinical-ComBAT方法，采用非线性多项式数据模型、基于规范站点的站点特定协调、适用于小队列的方差先验，并包含超参数调优和拟合优度指标。

Result: 在模拟和真实数据上验证了方法的有效性，显示扩散指标对齐得到改善，增强了规范建模的适用性。

Conclusion: Clinical-ComBAT为临床环境中的多站点DW-MRI数据协调提供了灵活有效的解决方案，克服了传统方法的局限性。

Abstract: Diffusion-weighted magnetic resonance imaging (DW-MRI) derived scalar maps
are effective for assessing neurodegenerative diseases and microstructural
properties of white matter in large number of brain conditions. However, DW-MRI
inherently limits the combination of data from multiple acquisition sites
without harmonization to mitigate scanner-specific biases. While the widely
used ComBAT method reduces site effects in research, its reliance on linear
covariate relationships, homogeneous populations, fixed site numbers, and well
populated sites constrains its clinical use. To overcome these limitations, we
propose Clinical-ComBAT, a method designed for real-world clinical scenarios.
Clinical-ComBAT harmonizes each site independently, enabling flexibility as new
data and clinics are introduced. It incorporates a non-linear polynomial data
model, site-specific harmonization referenced to a normative site, and variance
priors adaptable to small cohorts. It further includes hyperparameter tuning
and a goodness-of-fit metric for harmonization assessment. We demonstrate its
effectiveness on simulated and real data, showing improved alignment of
diffusion metrics and enhanced applicability for normative modeling.

</details>


### [20] [Validating Vision Transformers for Otoscopy: Performance and Data-Leakage Effects](https://arxiv.org/abs/2511.04872)
*James Ndubuisi,Fernando Auat,Marta Vallejo*

Main category: cs.CV

TL;DR: 本研究评估了视觉变换器模型在耳科疾病诊断中的效果，发现最初Swin变换器模型表现优异，但存在数据泄露问题。修正后性能显著下降，强调了医学机器学习研究中严格数据预处理的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于耳鼻喉科专家存在27%的误诊率，需要提高耳科疾病的诊断准确性，因此评估先进视觉变换器模型在耳科诊断中的潜力。

Method: 使用智利大学临床医院耳鼻喉科的真实耳镜视频数据集，基于拉普拉斯和香农熵阈值选择帧，去除空白帧，比较Swin v1、Swin v2变换器模型与ResNet模型的性能。

Result: 最初Swin v1和Swin v2分别达到100%和99.1%准确率，ResNet为99.5%。但发现数据泄露问题后，修正后的准确率分别为83%、83%和82%。

Conclusion: 视觉变换器模型在耳科诊断中具有潜力，但需要在先进模型架构和有效数据预处理之间找到最佳平衡，这对开发可靠的医学机器学习模型至关重要。

Abstract: This study evaluates the efficacy of vision transformer models, specifically
Swin transformers, in enhancing the diagnostic accuracy of ear diseases
compared to traditional convolutional neural networks. With a reported 27%
misdiagnosis rate among specialist otolaryngologists, improving diagnostic
accuracy is crucial. The research utilised a real-world dataset from the
Department of Otolaryngology at the Clinical Hospital of the Universidad de
Chile, comprising otoscopic videos of ear examinations depicting various middle
and external ear conditions. Frames were selected based on the Laplacian and
Shannon entropy thresholds, with blank frames removed. Initially, Swin v1 and
Swin v2 transformer models achieved accuracies of 100% and 99.1%, respectively,
marginally outperforming the ResNet model (99.5%). These results surpassed
metrics reported in related studies. However, the evaluation uncovered a
critical data leakage issue in the preprocessing step, affecting both this
study and related research using the same raw dataset. After mitigating the
data leakage, model performance decreased significantly. Corrected accuracies
were 83% for both Swin v1 and Swin v2, and 82% for the ResNet model. This
finding highlights the importance of rigorous data handling in machine learning
studies, especially in medical applications. The findings indicate that while
vision transformers show promise, it is essential to find an optimal balance
between the benefits of advanced model architectures and those derived from
effective data preprocessing. This balance is key to developing a reliable
machine learning model for diagnosing ear diseases.

</details>


### [21] [Beta Distribution Learning for Reliable Roadway Crash Risk Assessment](https://arxiv.org/abs/2511.04886)
*Ahmad Elallaf,Nathan Jacobs,Xinyue Ye,Mei Chen,Gongbo Liang*

Main category: cs.CV

TL;DR: 提出了一种基于卫星影像的地理空间深度学习框架，用于预测致命交通事故风险，通过估计完整的Beta概率分布提供不确定性感知预测，相比基线方法在召回率上提升17-23%。


<details>
  <summary>Details</summary>
Motivation: 传统交通安全研究通常孤立分析风险因素，忽视了建成环境中的空间复杂性和上下文交互。传统神经网络风险估计器仅生成点估计，缺乏模型不确定性信息，限制了在关键决策中的实用性。

Method: 利用卫星影像作为综合空间输入，开发地理空间深度学习框架，捕捉导致致命事故风险的细微空间模式和嵌入式环境风险因素。模型估计致命事故风险的完整Beta概率分布，而非单一确定性输出。

Result: 模型在召回率这一关键指标上比基线方法提升17-23%，同时提供更优的校准性能。仅通过卫星影像即可提供可靠且可解释的风险评估。

Conclusion: 该方法能够实现更安全的自动驾驶导航，并为城市规划者和政策制定者提供高度可扩展的工具，以公平且经济高效的方式提升道路安全。

Abstract: Roadway traffic accidents represent a global health crisis, responsible for
over a million deaths annually and costing many countries up to 3% of their
GDP. Traditional traffic safety studies often examine risk factors in
isolation, overlooking the spatial complexity and contextual interactions
inherent in the built environment. Furthermore, conventional Neural
Network-based risk estimators typically generate point estimates without
conveying model uncertainty, limiting their utility in critical
decision-making. To address these shortcomings, we introduce a novel geospatial
deep learning framework that leverages satellite imagery as a comprehensive
spatial input. This approach enables the model to capture the nuanced spatial
patterns and embedded environmental risk factors that contribute to fatal crash
risks. Rather than producing a single deterministic output, our model estimates
a full Beta probability distribution over fatal crash risk, yielding accurate
and uncertainty-aware predictions--a critical feature for trustworthy AI in
safety-critical applications. Our model outperforms baselines by achieving a
17-23% improvement in recall, a key metric for flagging potential dangers,
while delivering superior calibration. By providing reliable and interpretable
risk assessments from satellite imagery alone, our method enables safer
autonomous navigation and offers a highly scalable tool for urban planners and
policymakers to enhance roadway safety equitably and cost-effectively.

</details>


### [22] [Learning to Restore Multi-Degraded Images via Ingredient Decoupling and Task-Aware Path Adaptation](https://arxiv.org/abs/2511.04920)
*Hu Gao,Xiaoning Lei,Ying Zhang,Xichen Xu,Guannan Jiang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出IMDNet网络，通过解耦退化成分来指导路径选择，实现多退化图像恢复，在单退化任务上也保持竞争力


<details>
  <summary>Details</summary>
Motivation: 现实世界图像往往同时存在多种退化（如雨、噪声、雾霾），而现有方法大多只针对单一退化类型，限制了实际应用效果

Method: 设计退化成分解耦块(DIDBlock)分离退化成分统计特征，融合块(FBlock)整合多尺度退化信息，任务适应块(TABlock)动态激活功能分支选择最优恢复路径

Result: 在多退化恢复任务上表现出优越性能，同时在单退化任务上保持强大竞争力

Conclusion: IMDNet通过紧密集成的架构有效解决了多退化图像恢复问题，为实际应用提供了更实用的解决方案

Abstract: Image restoration (IR) aims to recover clean images from degraded
observations. Despite remarkable progress, most existing methods focus on a
single degradation type, whereas real-world images often suffer from multiple
coexisting degradations, such as rain, noise, and haze coexisting in a single
image, which limits their practical effectiveness. In this paper, we propose an
adaptive multi-degradation image restoration network that reconstructs images
by leveraging decoupled representations of degradation ingredients to guide
path selection. Specifically, we design a degradation ingredient decoupling
block (DIDBlock) in the encoder to separate degradation ingredients
statistically by integrating spatial and frequency domain information,
enhancing the recognition of multiple degradation types and making their
feature representations independent. In addition, we present fusion block
(FBlock) to integrate degradation information across all levels using learnable
matrices. In the decoder, we further introduce a task adaptation block
(TABlock) that dynamically activates or fuses functional branches based on the
multi-degradation representation, flexibly selecting optimal restoration paths
under diverse degradation conditions. The resulting tightly integrated
architecture, termed IMDNet, is extensively validated through experiments,
showing superior performance on multi-degradation restoration while maintaining
strong competitiveness on single-degradation tasks.

</details>


### [23] [A benchmark multimodal oro-dental dataset for large vision-language models](https://arxiv.org/abs/2511.04948)
*Haoxin Lv,Ijazul Haq,Jin Du,Jiaxin Ma,Binnian Zhu,Xiaobing Dang,Chaoan Liang,Ruxu Du,Yingjie Zhang,Muhammad Saqib*

Main category: cs.CV

TL;DR: 提出了一个包含8775次牙科检查的多模态数据集，包含50000张口内图像、8056张X光片和详细文本记录，用于训练AI模型进行口腔疾病分类和诊断报告生成。


<details>
  <summary>Details</summary>
Motivation: 口腔医疗AI的发展需要能够反映临床实践复杂性的大规模多模态数据集，但目前缺乏这样的公开资源。

Method: 收集了8年间的牙科检查数据，包含图像和文本记录，并使用Qwen-VL 3B和7B模型进行微调，评估其在口腔异常分类和诊断报告生成任务上的表现。

Result: 微调后的模型在口腔异常分类和诊断报告生成任务上显著优于基础模型和GPT-4o，验证了数据集的有效性。

Conclusion: 该数据集为AI驱动的口腔医疗解决方案提供了重要资源，能够有效推动口腔医疗AI的发展。

Abstract: The advancement of artificial intelligence in oral healthcare relies on the
availability of large-scale multimodal datasets that capture the complexity of
clinical practice. In this paper, we present a comprehensive multimodal
dataset, comprising 8775 dental checkups from 4800 patients collected over
eight years (2018-2025), with patients ranging from 10 to 90 years of age. The
dataset includes 50000 intraoral images, 8056 radiographs, and detailed textual
records, including diagnoses, treatment plans, and follow-up notes. The data
were collected under standard ethical guidelines and annotated for
benchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large
vision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks:
classification of six oro-dental anomalies and generation of complete
diagnostic reports from multimodal inputs. We compared the fine-tuned models
with their base counterparts and GPT-4o. The fine-tuned models achieved
substantial gains over these baselines, validating the dataset and underscoring
its effectiveness in advancing AI-driven oro-dental healthcare solutions. The
dataset is publicly available, providing an essential resource for future
research in AI dentistry.

</details>


### [24] [DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning](https://arxiv.org/abs/2511.04949)
*Tharindu Fernando,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: 提出了一种基于高维潜在空间表示和多智能体对抗强化学习(MAARL)的深度学习框架，用于开发鲁棒且自适应的水印方法，以解决深度伪造检测中的挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展导致深度伪造越来越逼真，现有被动检测器难以跟上发展，因为它们依赖特定伪造伪影，泛化能力有限。主动水印检测方法在平衡对良性失真的鲁棒性和对恶意篡改的敏感性方面存在困难。

Method: 开发了一种在潜在空间中操作的可学习水印嵌入器，捕获高级图像语义，同时提供对消息编码和提取的精确控制。采用MAARL范式，通过水印智能体与对抗攻击智能体模拟的良性/恶意图像操作动态课程进行交互，寻求鲁棒性和脆弱性的最佳平衡。

Result: 在CelebA和CelebA-HQ基准测试上的综合评估表明，该方法始终优于最先进的方法，在具有挑战性的操作场景下，在CelebA上提升超过4.5%，在CelebA-HQ上提升超过5.3%。

Conclusion: 该框架通过结合高维潜在空间表示和MAARL范式，成功开发了一种鲁棒且自适应的水印方法，有效解决了深度伪造检测中的关键挑战。

Abstract: Rapid advances in generative AI have led to increasingly realistic deepfakes,
posing growing challenges for law enforcement and public trust. Existing
passive deepfake detectors struggle to keep pace, largely due to their
dependence on specific forgery artifacts, which limits their ability to
generalize to new deepfake types. Proactive deepfake detection using watermarks
has emerged to address the challenge of identifying high-quality synthetic
media. However, these methods often struggle to balance robustness against
benign distortions with sensitivity to malicious tampering. This paper
introduces a novel deep learning framework that harnesses high-dimensional
latent space representations and the Multi-Agent Adversarial Reinforcement
Learning (MAARL) paradigm to develop a robust and adaptive watermarking
approach. Specifically, we develop a learnable watermark embedder that operates
in the latent space, capturing high-level image semantics, while offering
precise control over message encoding and extraction. The MAARL paradigm
empowers the learnable watermarking agent to pursue an optimal balance between
robustness and fragility by interacting with a dynamic curriculum of benign and
malicious image manipulations simulated by an adversarial attacker agent.
Comprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that
our method consistently outperforms state-of-the-art approaches, achieving
improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under
challenging manipulation scenarios.

</details>


### [25] [Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement](https://arxiv.org/abs/2511.04963)
*Xiongri Shen,Jiaqi Wang,Yi Zhong,Zhenxi Song,Leilei Zhao,Yichen Wei,Lingyan Liang,Shuqiang Wang,Baiying Lei,Demao Deng,Zhiguo Zhang*

Main category: cs.CV

TL;DR: 提出PDS方法解决fMRI和dMRI模态缺失问题，通过模式感知双模态3D扩散框架和微结构细化网络，在三个数据集上取得最先进结果。


<details>
  <summary>Details</summary>
Motivation: fMRI和dMRI在神经退行性疾病研究中至关重要，但模态缺失限制了其临床应用。现有GAN和扩散模型方法在fMRI-dMRI合成中存在BOLD信号与扩散加权信号差异大、疾病相关神经解剖模式整合不足的问题。

Method: PDS方法包含两个关键创新：(1) 模式感知双模态3D扩散框架进行跨模态学习；(2) 集成高效微结构细化的组织细化网络，保持结构保真度和精细细节。

Result: 在OASIS-3、ADNI和内部数据集上，fMRI合成PSNR/SSIM达29.83 dB/90.84%，dMRI合成达30.00 dB/77.55%，均优于基线方法。临床验证中，合成数据在NC vs MCI vs AD分类中达到67.92%/66.02%/64.15%准确率。

Conclusion: PDS方法有效解决了fMRI和dMRI模态合成中的挑战，在图像质量和临床诊断性能方面均表现出色，为神经退行性疾病研究提供了可靠工具。

Abstract: Magnetic resonance imaging (MRI), especially functional MRI (fMRI) and
diffusion MRI (dMRI), is essential for studying neurodegenerative diseases.
However, missing modalities pose a major barrier to their clinical use.
Although GAN- and diffusion model-based approaches have shown some promise in
modality completion, they remain limited in fMRI-dMRI synthesis due to (1)
significant BOLD vs. diffusion-weighted signal differences between fMRI and
dMRI in time/gradient axis, and (2) inadequate integration of disease-related
neuroanatomical patterns during generation. To address these challenges, we
propose PDS, introducing two key innovations: (1) a pattern-aware dual-modal 3D
diffusion framework for cross-modality learning, and (2) a tissue refinement
network integrated with a efficient microstructure refinement to maintain
structural fidelity and fine details. Evaluated on OASIS-3, ADNI, and in-house
datasets, our method achieves state-of-the-art results, with PSNR/SSIM scores
of 29.83 dB/90.84\% for fMRI synthesis (+1.54 dB/+4.12\% over baselines) and
30.00 dB/77.55\% for dMRI synthesis (+1.02 dB/+2.2\%). In clinical validation,
the synthesized data show strong diagnostic performance, achieving
67.92\%/66.02\%/64.15\% accuracy (NC vs. MCI vs. AD) in hybrid real-synthetic
experiments. Code is available in \href{https://github.com/SXR3015/PDS}{PDS
GitHub Repository}

</details>


### [26] [CLM: Removing the GPU Memory Barrier for 3D Gaussian Splatting](https://arxiv.org/abs/2511.04951)
*Hexu Zhao,Xiwen Min,Xiaoteng Liu,Moonjun Gong,Yiming Li,Ang Li,Saining Xie,Jinyang Li,Aurojit Panda*

Main category: cs.CV

TL;DR: CLM系统通过CPU内存卸载和智能加载策略，使3D高斯泼溅技术能够在单个消费级GPU上渲染大型场景，解决了内存不足问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术虽然渲染速度快、质量高，但在处理大型场景时GPU内存需求过大，超出大多数消费级GPU的容量限制。

Method: 采用CPU内存卸载策略，仅在需要时将高斯数据加载到GPU；利用3DGS内存访问模式进行流水线化，重叠GPU-CPU通信、GPU计算和CPU计算；通过访问模式分析减少通信量。

Result: 在单个RTX4090上成功渲染需要1亿个高斯的大型场景，并达到最先进的重建质量。

Conclusion: CLM系统有效解决了3DGS在大型场景中的内存限制问题，实现了在消费级硬件上的高效渲染。

Abstract: 3D Gaussian Splatting (3DGS) is an increasingly popular novel view synthesis
approach due to its fast rendering time, and high-quality output. However,
scaling 3DGS to large (or intricate) scenes is challenging due to its large
memory requirement, which exceed most GPU's memory capacity. In this paper, we
describe CLM, a system that allows 3DGS to render large scenes using a single
consumer-grade GPU, e.g., RTX4090. It does so by offloading Gaussians to CPU
memory, and loading them into GPU memory only when necessary. To reduce
performance and communication overheads, CLM uses a novel offloading strategy
that exploits observations about 3DGS's memory access pattern for pipelining,
and thus overlap GPU-to-CPU communication, GPU computation and CPU computation.
Furthermore, we also exploit observation about the access pattern to reduce
communication volume. Our evaluation shows that the resulting implementation
can render a large scene that requires 100 million Gaussians on a single
RTX4090 and achieve state-of-the-art reconstruction quality.

</details>


### [27] [Learning Fourier shapes to probe the geometric world of deep neural networks](https://arxiv.org/abs/2511.04970)
*Jian Wang,Yixing Yong,Haixia Bi,Lijun He,Fan Li*

Main category: cs.CV

TL;DR: 该论文提出了一种端到端的可微分框架，通过傅里叶级数参数化任意形状，使用基于环绕数的映射将其转换为DNN所需的像素网格，并利用信号能量约束来优化几何形状，从而探索深度神经网络的几何理解能力。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络研究主要关注纹理特征，而对其几何理解能力的探索不足。本文旨在研究几何形状如何作为有效的语义载体，以及如何利用几何形状作为可解释性工具和对抗性攻击手段。

Method: 开发了一个端到端的可微分框架，结合傅里叶级数参数化形状、基于环绕数的像素网格映射方法，以及信号能量约束来确保物理上合理的形状优化。

Result: 优化的几何形状能够作为强大的语义载体产生高置信度的分类结果；可作为高保真度的可解释性工具精确识别模型的显著区域；构成了一种新的、可泛化的对抗性范式，能够欺骗下游视觉任务。

Conclusion: 该工作为探索深度神经网络的几何世界提供了一个多功能框架，并为挑战和理解机器感知开辟了新前沿。

Abstract: While both shape and texture are fundamental to visual recognition, research
on deep neural networks (DNNs) has predominantly focused on the latter, leaving
their geometric understanding poorly probed. Here, we show: first, that
optimized shapes can act as potent semantic carriers, generating
high-confidence classifications from inputs defined purely by their geometry;
second, that they are high-fidelity interpretability tools that precisely
isolate a model's salient regions; and third, that they constitute a new,
generalizable adversarial paradigm capable of deceiving downstream visual
tasks. This is achieved through an end-to-end differentiable framework that
unifies a powerful Fourier series to parameterize arbitrary shapes, a winding
number-based mapping to translate them into the pixel grid required by DNNs,
and signal energy constraints that enhance optimization efficiency while
ensuring physically plausible shapes. Our work provides a versatile framework
for probing the geometric world of DNNs and opens new frontiers for challenging
and understanding machine perception.

</details>


### [28] [Dynamic Residual Encoding with Slide-Level Contrastive Learning for End-to-End Whole Slide Image Representation](https://arxiv.org/abs/2511.05034)
*Jing Jin,Xu Liu,Te Gao,Zhihong Shi,Yixiong Liang,Ruiqing Zheng,Hulin Kuang,Min Zeng,Shichao Kan*

Main category: cs.CV

TL;DR: 提出了一种用于全切片图像表示学习的动态残差编码与切片级对比学习方法，解决了GPU内存限制下无法同时处理所有图像块的问题。


<details>
  <summary>Details</summary>
Motivation: 全切片图像包含数万个图像块，由于GPU内存限制，无法在单个小批次中计算所有块的梯度，这给端到端的WSI表示学习带来了挑战。

Method: 使用内存库存储所有WSI中图像块的特征，在训练时随机采样部分块计算特征，并从内存库中检索同一WSI的额外特征，通过残差编码技术生成WSI表示，最后基于小批次内WSI的表示和病理报告计算切片级对比损失。

Result: 在癌症亚型分类、癌症识别和突变预测任务上的实验证明了DRE-SLCL方法的有效性。

Conclusion: 提出的DRE-SLCL方法能够有效解决GPU内存限制下的WSI表示学习问题，在多个癌症相关任务上表现出色。

Abstract: Whole Slide Image (WSI) representation is critical for cancer subtyping,
cancer recognition and mutation prediction.Training an end-to-end WSI
representation model poses significant challenges, as a standard gigapixel
slide can contain tens of thousands of image tiles, making it difficult to
compute gradients of all tiles in a single mini-batch due to current GPU
limitations. To address this challenge, we propose a method of dynamic residual
encoding with slide-level contrastive learning (DRE-SLCL) for end-to-end WSI
representation. Our approach utilizes a memory bank to store the features of
tiles across all WSIs in the dataset. During training, a mini-batch usually
contains multiple WSIs. For each WSI in the batch, a subset of tiles is
randomly sampled and their features are computed using a tile encoder. Then,
additional tile features from the same WSI are selected from the memory bank.
The representation of each individual WSI is generated using a residual
encoding technique that incorporates both the sampled features and those
retrieved from the memory bank. Finally, the slide-level contrastive loss is
computed based on the representations and histopathology reports ofthe WSIs
within the mini-batch. Experiments conducted over cancer subtyping, cancer
recognition, and mutation prediction tasks proved the effectiveness of the
proposed DRE-SLCL method.

</details>


### [29] [No Pose Estimation? No Problem: Pose-Agnostic and Instance-Aware Test-Time Adaptation for Monocular Depth Estimation](https://arxiv.org/abs/2511.05055)
*Mingyu Sung,Hyeonmin Choe,Il-Min Kim,Sangseok Yun,Jae Mo Kang*

Main category: cs.CV

TL;DR: 提出了一种名为PITTA的新型单目深度估计测试时自适应框架，通过姿态无关的自适应范式和实例感知图像掩码策略，在动态环境中显著提升深度估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有的单目深度估计测试时自适应方法在多样化和动态环境中效果不佳，需要解决姿态依赖和动态对象干扰的问题。

Method: PITTA框架包含两个关键创新：姿态无关的测试时自适应范式，无需相机姿态信息；实例感知图像掩码策略，通过预训练的泛化分割网络提取动态对象掩码并移除静态背景。

Result: 在DrivingStereo和Waymo数据集上的广泛实验表明，PITTA在多种环境条件下显著超越了现有最先进技术，在测试时自适应过程中取得了显著的性能提升。

Conclusion: PITTA框架通过姿态无关的自适应和实例感知掩码策略，有效解决了单目深度估计在动态环境中的测试时自适应挑战，展现了卓越的性能表现。

Abstract: Monocular depth estimation (MDE), inferring pixel-level depths in single RGB
images from a monocular camera, plays a crucial and pivotal role in a variety
of AI applications demanding a three-dimensional (3D) topographical scene. In
the real-world scenarios, MDE models often need to be deployed in environments
with different conditions from those for training. Test-time (domain)
adaptation (TTA) is one of the compelling and practical approaches to address
the issue. Although there have been notable advancements in TTA for MDE,
particularly in a self-supervised manner, existing methods are still
ineffective and problematic when applied to diverse and dynamic environments.
To break through this challenge, we propose a novel and high-performing TTA
framework for MDE, named PITTA. Our approach incorporates two key innovative
strategies: (i) pose-agnostic TTA paradigm for MDE and (ii) instance-aware
image masking. Specifically, PITTA enables highly effective TTA on a pretrained
MDE network in a pose-agnostic manner without resorting to any camera pose
information. Besides, our instance-aware masking strategy extracts
instance-wise masks for dynamic objects (e.g., vehicles, pedestrians, etc.)
from a segmentation mask produced by a pretrained panoptic segmentation
network, by removing static objects including background components. To further
boost performance, we also present a simple yet effective edge extraction
methodology for the input image (i.e., a single monocular image) and depth map.
Extensive experimental evaluations on DrivingStereo and Waymo datasets with
varying environmental conditions demonstrate that our proposed framework,
PITTA, surpasses the existing state-of-the-art techniques with remarkable
performance improvements in MDE during TTA.

</details>


### [30] [Challenges in 3D Data Synthesis for Training Neural Networks on Topological Features](https://arxiv.org/abs/2511.04972)
*Dylan Peek,Matthew P. Skerritt,Siddharth Pritam,Stephan Chalup*

Main category: cs.CV

TL;DR: 本文提出了一种使用排斥表面算法生成带标签3D数据集的新方法，用于训练拓扑数据分析中的神经网络估计器，解决了该领域缺乏专门标注数据的问题。


<details>
  <summary>Details</summary>
Motivation: 传统拓扑数据分析方法（如持久同调）计算成本高，而神经网络估计器能减少计算开销和推理时间，但缺乏专门用于监督学习的带标签3D数据集阻碍了这些方法的发展。

Method: 使用排斥表面算法系统生成带标签3D数据集，可控制拓扑不变量（如孔洞数量），并采用3D卷积transformer架构训练属数估计网络。

Result: 生成的数据集提供了具有拓扑标签的多样化几何形状，适合训练和基准测试神经网络估计器。观察到随着变形增加，准确性下降，表明几何复杂性在训练泛化估计器中起着重要作用。

Conclusion: 该数据集填补了拓扑数据分析中用于训练和评估模型的带标签3D数据集的空白，强调了拓扑复杂性和几何复杂性对模型泛化能力的影响。

Abstract: Topological Data Analysis (TDA) involves techniques of analyzing the
underlying structure and connectivity of data. However, traditional methods
like persistent homology can be computationally demanding, motivating the
development of neural network-based estimators capable of reducing
computational overhead and inference time. A key barrier to advancing these
methods is the lack of labeled 3D data with class distributions and diversity
tailored specifically for supervised learning in TDA tasks. To address this, we
introduce a novel approach for systematically generating labeled 3D datasets
using the Repulsive Surface algorithm, allowing control over topological
invariants, such as hole count. The resulting dataset offers varied geometry
with topological labeling, making it suitable for training and benchmarking
neural network estimators. This paper uses a synthetic 3D dataset to train a
genus estimator network, created using a 3D convolutional transformer
architecture. An observed decrease in accuracy as deformations increase
highlights the role of not just topological complexity, but also geometric
complexity, when training generalized estimators. This dataset fills a gap in
labeled 3D datasets and generation for training and evaluating models and
techniques for TDA.

</details>


### [31] [Deep learning models are vulnerable, but adversarial examples are even more vulnerable](https://arxiv.org/abs/2511.05073)
*Jun Li,Yanwei Xu,Keran Li,Xiaoli Zhang*

Main category: cs.CV

TL;DR: 研究发现对抗样本对遮挡高度敏感，提出SMCE指标量化遮挡下的置信度波动，并基于此开发SWM-AED检测方法，在CIFAR-10上取得良好检测效果。


<details>
  <summary>Details</summary>
Motivation: 理解对抗样本与干净样本的内在差异是提升DNN鲁棒性和检测能力的关键。研究发现图像对抗样本对遮挡特别敏感。

Method: 在CIFAR-10上使用9种典型攻击生成对抗样本，提出SMCE指标量化模型在遮挡下的置信度波动，并开发SWM-AED检测方法。

Result: 1800+测试图像显示对抗样本在遮挡下的置信度波动显著高于原始样本。SWM-AED在CIFAR-10上检测准确率大多超过62%，最高达96.5%。

Conclusion: 基于遮挡敏感性的检测方法能有效识别对抗样本，避免传统对抗训练的灾难性过拟合问题。

Abstract: Understanding intrinsic differences between adversarial examples and clean
samples is key to enhancing DNN robustness and detection against adversarial
attacks. This study first empirically finds that image-based adversarial
examples are notably sensitive to occlusion. Controlled experiments on CIFAR-10
used nine canonical attacks (e.g., FGSM, PGD) to generate adversarial examples,
paired with original samples for evaluation. We introduce Sliding Mask
Confidence Entropy (SMCE) to quantify model confidence fluctuation under
occlusion. Using 1800+ test images, SMCE calculations supported by Mask Entropy
Field Maps and statistical distributions show adversarial examples have
significantly higher confidence volatility under occlusion than originals.
Based on this, we propose Sliding Window Mask-based Adversarial Example
Detection (SWM-AED), which avoids catastrophic overfitting of conventional
adversarial training. Evaluations across classifiers and attacks on CIFAR-10
demonstrate robust performance, with accuracy over 62% in most cases and up to
96.5%.

</details>


### [32] [GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder](https://arxiv.org/abs/2511.04977)
*Heng Er Metilda Chee,Jiayin Wang,Zhiqiang Guo,Weizhi Ma,Min Zhang*

Main category: cs.CV

TL;DR: 提出了首个贴纸语义相似度基准Triple-S和通用贴纸编码器GSE，解决了贴纸语义理解难题，在未见贴纸上表现优异，支持下游任务如情感分类和检索。


<details>
  <summary>Details</summary>
Motivation: 贴纸已成为流行的视觉交流形式，但由于其内容高度多样化和符号化，理解其语义关系仍然具有挑战性。

Method: 正式定义贴纸语义相似度任务，构建包含905个人工标注正负贴纸对的Triple-S基准，并提出轻量级通用贴纸编码器GSE，利用Triple-S和额外数据集学习鲁棒的贴纸嵌入。

Result: GSE在未见贴纸上表现优异，在情感分类和贴纸检索等下游任务中取得强劲结果。现有预训练视觉和多模态模型难以捕捉贴纸的细微语义。

Conclusion: 通过发布Triple-S基准和GSE模型，为贴纸理解、检索和多模态内容生成提供了标准化评估工具和鲁棒嵌入表示，推动未来研究发展。

Abstract: Stickers have become a popular form of visual communication, yet
understanding their semantic relationships remains challenging due to their
highly diverse and symbolic content. In this work, we formally {define the
Sticker Semantic Similarity task} and introduce {Triple-S}, the first benchmark
for this task, consisting of 905 human-annotated positive and negative sticker
pairs. Through extensive evaluation, we show that existing pretrained vision
and multimodal models struggle to capture nuanced sticker semantics. To address
this, we propose the {General Sticker Encoder (GSE)}, a lightweight and
versatile model that learns robust sticker embeddings using both Triple-S and
additional datasets. GSE achieves superior performance on unseen stickers, and
demonstrates strong results on downstream tasks such as emotion classification
and sticker-to-sticker retrieval. By releasing both Triple-S and GSE, we
provide standardized evaluation tools and robust embeddings, enabling future
research in sticker understanding, retrieval, and multimodal content
generation. The Triple-S benchmark and GSE have been publicly released and are
available here.

</details>


### [33] [From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation Model Bridging Global and Cellular Representations in Biomarker Detection](https://arxiv.org/abs/2511.05150)
*Jingsong Liu,Han Li,Nassir Navab,Peter J. Schüffler*

Main category: cs.CV

TL;DR: JWTH是一种病理学基础模型，通过细胞中心的后调优和注意力池化融合局部和全局token，在生物标志物检测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大多数病理学基础模型依赖全局patch级嵌入，忽视了细胞级形态学特征，限制了AI生物标志物检测的准确性和可解释性。

Method: 结合大规模自监督预训练与细胞中心的后调优，使用注意力池化融合局部和全局token。

Result: 在涉及4个生物标志物和8个队列的四个任务中，JWTH比现有PFMs实现了最高8.3%的平衡准确率提升，平均提升1.2%。

Conclusion: JWTH推进了数字病理学中可解释和稳健的AI生物标志物检测能力。

Abstract: AI-based biomarkers can infer molecular features directly from hematoxylin &
eosin (H&E) slides, yet most pathology foundation models (PFMs) rely on global
patch-level embeddings and overlook cell-level morphology. We present a PFM
model, JWTH (Joint-Weighted Token Hierarchy), which integrates large-scale
self-supervised pretraining with cell-centric post-tuning and attention pooling
to fuse local and global tokens. Across four tasks involving four biomarkers
and eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and 1.2%
average improvement over prior PFMs, advancing interpretable and robust
AI-based biomarker detection in digital pathology.

</details>


### [34] [Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings](https://arxiv.org/abs/2511.05017)
*Aakriti Agrawal,Gouthaman KV,Rohith Aralikatti,Gauri Jagatap,Jiaxin Yuan,Vijay Kamarshi,Andrea Fanelli,Furong Huang*

Main category: cs.CV

TL;DR: 本文发现主流LVLM架构存在语言模态偏见，并提出通过平均池化视觉特征来优化文本嵌入的方法，有效改善了视觉定位并显著减少了幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 识别主流LVLM架构中存在的语言模态偏见问题，这种偏见主要源于将视觉嵌入简单附加到输入文本序列的常见做法，导致模型对视觉信息的利用不足。

Method: 提出一种简单有效的方法，通过集成平均池化的视觉特征来优化文本嵌入，从而更好地融合视觉信息。

Result: 该方法在已有基准测试中显著改善了视觉定位能力，并大幅减少了幻觉现象。

Conclusion: 虽然平均池化提供了一种简单、稳健且高效的视觉信息融合方式，但更复杂的融合方法可能进一步改善视觉定位和跨模态对齐，这将是未来工作的方向。

Abstract: In this work, we identify an inherent bias in prevailing LVLM architectures
toward the language modality, largely resulting from the common practice of
simply appending visual embeddings to the input text sequence. To address this,
we propose a simple yet effective method that refines textual embeddings by
integrating average-pooled visual features. Our approach demonstrably improves
visual grounding and significantly reduces hallucinations on established
benchmarks. While average pooling offers a straightforward, robust, and
efficient means of incorporating visual information, we believe that more
sophisticated fusion methods could further enhance visual grounding and
cross-modal alignment. Given that the primary focus of this work is to
highlight the modality imbalance and its impact on hallucinations -- and to
show that refining textual embeddings with visual information mitigates this
issue -- we leave exploration of advanced fusion strategies for future work.

</details>


### [35] [4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos](https://arxiv.org/abs/2511.05229)
*Mengqi Guo,Bo Xu,Yanyan Li,Gim Hee Lee*

Main category: cs.CV

TL;DR: 4D3R是一个无需相机位姿的动态神经渲染框架，通过两阶段方法解耦静态和动态组件，在动态场景的新视角合成方面取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决从单目视频进行动态场景新视角合成的挑战，现有方法如NeRF和3DGS在动态内容和未知相机位姿方面存在局限。

Method: 两阶段方法：首先利用3D基础模型进行初始位姿和几何估计，然后通过运动感知细化。关键技术包括运动感知束调整模块(MA-BA)和运动感知高斯溅射表示(MA-GS)。

Result: 在真实世界动态数据集上，相比最先进方法获得高达1.8dB PSNR提升，在具有大动态物体的挑战性场景中表现优异，同时计算需求比先前动态场景表示减少5倍。

Conclusion: 4D3R框架在动态场景渲染方面实现了显著的性能提升和计算效率改进，为无需相机位姿的动态神经渲染提供了有效解决方案。

Abstract: Novel view synthesis from monocular videos of dynamic scenes with unknown
camera poses remains a fundamental challenge in computer vision and graphics.
While recent advances in 3D representations such as Neural Radiance Fields
(NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static
scenes, they struggle with dynamic content and typically rely on pre-computed
camera poses. We present 4D3R, a pose-free dynamic neural rendering framework
that decouples static and dynamic components through a two-stage approach. Our
method first leverages 3D foundational models for initial pose and geometry
estimation, followed by motion-aware refinement. 4D3R introduces two key
technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that
combines transformer-based learned priors with SAM2 for robust dynamic object
segmentation, enabling more accurate camera pose refinement; and (2) an
efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses
control points with a deformation field MLP and linear blend skinning to model
dynamic motion, significantly reducing computational cost while maintaining
high-quality reconstruction. Extensive experiments on real-world dynamic
datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement
over state-of-the-art methods, particularly in challenging scenarios with large
dynamic objects, while reducing computational requirements by 5x compared to
previous dynamic scene representations.

</details>


### [36] [Accurate online action and gesture recognition system using detectors and Deep SPD Siamese Networks](https://arxiv.org/abs/2511.05250)
*Mohamed Sanim Akremi,Rim Slama,Hedi Tabia*

Main category: cs.CV

TL;DR: 提出了一种基于骨骼序列流的在线连续动作识别系统，包含检测器和分类器两个组件，使用SPD矩阵表示和孪生网络，能够在未分割序列中实时预测动作时间区间并识别动作类型。


<details>
  <summary>Details</summary>
Motivation: 现有的骨骼动作识别方法主要关注分段识别，不适合在线场景。在线连续动作识别在现实应用中更具实用性，需要能够实时处理骨骼序列流的方法。

Method: 系统由检测器和分类器组成：检测器使用SPD矩阵表示骨骼数据，通过孪生网络学习语义相似性来预测动作时间区间；分类器识别每个预测区间内的动作类型。检测器能够连续识别运动状态。

Result: 在手势和身体动作识别基准测试上进行了广泛实验，在大多数情况下优于现有最先进方法的性能。

Conclusion: 提出的在线识别系统能够有效处理骨骼序列流，实现连续动作识别，在准确率上表现出色，适用于实际应用场景。

Abstract: Online continuous motion recognition is a hot topic of research since it is
more practical in real life application cases. Recently, Skeleton-based
approaches have become increasingly popular, demonstrating the power of using
such 3D temporal data. However, most of these works have focused on
segment-based recognition and are not suitable for the online scenarios. In
this paper, we propose an online recognition system for skeleton sequence
streaming composed from two main components: a detector and a classifier, which
use a Semi-Positive Definite (SPD) matrix representation and a Siamese network.
The powerful statistical representations for the skeletal data given by the SPD
matrices and the learning of their semantic similarity by the Siamese network
enable the detector to predict time intervals of the motions throughout an
unsegmented sequence. In addition, they ensure the classifier capability to
recognize the motion in each predicted interval. The proposed detector is
flexible and able to identify the kinetic state continuously. We conduct
extensive experiments on both hand gesture and body action recognition
benchmarks to prove the accuracy of our online recognition system which in most
cases outperforms state-of-the-art performances.

</details>


### [37] [Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance](https://arxiv.org/abs/2511.05038)
*Zhengxuan Li,Qinhui Yang,Yiyu Zhuang,Chuan Guo,Xinxin Zuo,Xiaoxiao Long,Yao Yao,Xun Cao,Qiu Shen,Hao Zhu*

Main category: cs.CV

TL;DR: Pressure2Motion是一种从地面压力序列和文本提示生成人体运动的新算法，无需摄像头或穿戴设备，适用于隐私保护、低光照和低成本场景。


<details>
  <summary>Details</summary>
Motivation: 解决传统运动捕捉需要专门设备的问题，为隐私保护、低光照和低成本场景提供解决方案。由于压力信号到全身运动的不确定性，该任务具有严重的不适定性。

Method: 使用生成模型，以压力特征为输入，文本提示为高级约束。包含双级特征提取器准确解释压力数据，以及分层扩散模型识别大尺度运动轨迹和细微姿势调整。

Result: 实验表明该方法能生成高保真、物理上合理的运动，为该任务建立了新的最先进水平。

Conclusion: Pressure2Motion是利用压力数据和语言先验进行运动生成的先驱工作，建立的MPL基准是该任务的第一个基准。代码和基准将在发表后公开。

Abstract: We present Pressure2Motion, a novel motion capture algorithm that synthesizes
human motion from a ground pressure sequence and text prompt. It eliminates the
need for specialized lighting setups, cameras, or wearable devices, making it
suitable for privacy-preserving, low-light, and low-cost motion capture
scenarios. Such a task is severely ill-posed due to the indeterminate nature of
the pressure signals to full-body motion. To address this issue, we introduce
Pressure2Motion, a generative model that leverages pressure features as input
and utilizes a text prompt as a high-level guiding constraint. Specifically,
our model utilizes a dual-level feature extractor that accurately interprets
pressure data, followed by a hierarchical diffusion model that discerns
broad-scale movement trajectories and subtle posture adjustments. Both the
physical cues gained from the pressure sequence and the semantic guidance
derived from descriptive texts are leveraged to guide the motion generation
with precision. To the best of our knowledge, Pressure2Motion is a pioneering
work in leveraging both pressure data and linguistic priors for motion
generation, and the established MPL benchmark is the first benchmark for this
task. Experiments show our method generates high-fidelity, physically plausible
motions, establishing a new state-of-the-art for this task. The codes and
benchmarks will be publicly released upon publication.

</details>


### [38] [OregairuChar: A Benchmark Dataset for Character Appearance Frequency Analysis in My Teen Romantic Comedy SNAFU](https://arxiv.org/abs/2511.05263)
*Qi Sun,Dingju Zhou,Lina Zhang*

Main category: cs.CV

TL;DR: 提出了OregairuChar数据集，用于分析动漫《我的青春恋爱物语果然有问题》中角色出现频率，包含1600帧手动标注图像，涵盖11个主要角色，用于研究叙事结构和角色重要性。


<details>
  <summary>Details</summary>
Motivation: 角色出现频率分析对于理解动漫叙事结构、角色重要性和故事进展至关重要，但目前缺乏专门针对动漫的此类数据集。

Method: 创建包含1600帧手动选择图像的数据集，标注2860个边界框覆盖11个主要角色，并基于多个目标检测模型进行基准测试，用于细粒度的逐集角色出现分析。

Result: 数据集捕捉了遮挡、姿态变化和角色间相似性等视觉挑战，通过模型预测揭示了角色重要性随时间演变的模式。

Conclusion: OregairuChar数据集为计算叙事动力学和角色中心化故事讲述研究提供了宝贵资源，特别适用于风格化媒体分析。

Abstract: The analysis of character appearance frequency is essential for understanding
narrative structure, character prominence, and story progression in anime. In
this work, we introduce OregairuChar, a benchmark dataset designed for
appearance frequency analysis in the anime series My Teen Romantic Comedy
SNAFU. The dataset comprises 1600 manually selected frames from the third
season, annotated with 2860 bounding boxes across 11 main characters.
OregairuChar captures diverse visual challenges, including occlusion, pose
variation, and inter-character similarity, providing a realistic basis for
appearance-based studies. To enable quantitative research, we benchmark several
object detection models on the dataset and leverage their predictions for
fine-grained, episode-level analysis of character presence over time. This
approach reveals patterns of character prominence and their evolution within
the narrative. By emphasizing appearance frequency, OregairuChar serves as a
valuable resource for exploring computational narrative dynamics and
character-centric storytelling in stylized media.

</details>


### [39] [Medical Referring Image Segmentation via Next-Token Mask Prediction](https://arxiv.org/abs/2511.05044)
*Xinyu Chen,Yiran Wang,Gaoyang Pang,Jiafu Hao,Chentao Yue,Luping Zhou,Yonghui Li*

Main category: cs.CV

TL;DR: NTP-MRISeg将医学指代图像分割重新定义为基于多模态序列的自回归下一个token预测任务，通过统一架构简化设计，无需复杂多模态融合或外部分割模型，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学指代图像分割方法通常涉及复杂的多模态融合或多阶段解码器设计，作者希望提出更简化的统一框架。

Method: 将MRIS重新定义为多模态序列的自回归下一个token预测任务；提出三种策略：Next-k Token Prediction减少累积误差、Token-level Contrastive Learning增强边界敏感性和缓解长尾分布、Hard Error Token优化策略强调困难token。

Result: 在QaTa-COV19和MosMedData+数据集上的广泛实验表明，NTP-MRISeg实现了新的最先进性能。

Conclusion: NTP-MRISeg为传统MRIS流程提供了一个简化且有效的替代方案，支持端到端训练并利用预训练多模态模型的tokenizer增强泛化能力。

Abstract: Medical Referring Image Segmentation (MRIS) involves segmenting target
regions in medical images based on natural language descriptions. While
achieving promising results, recent approaches usually involve complex design
of multimodal fusion or multi-stage decoders. In this work, we propose
NTP-MRISeg, a novel framework that reformulates MRIS as an autoregressive
next-token prediction task over a unified multimodal sequence of tokenized
image, text, and mask representations. This formulation streamlines model
design by eliminating the need for modality-specific fusion and external
segmentation models, supports a unified architecture for end-to-end training.
It also enables the use of pretrained tokenizers from emerging large-scale
multimodal models, enhancing generalization and adaptability. More importantly,
to address challenges under this formulation-such as exposure bias, long-tail
token distributions, and fine-grained lesion edges-we propose three novel
strategies: (1) a Next-k Token Prediction (NkTP) scheme to reduce cumulative
prediction errors, (2) Token-level Contrastive Learning (TCL) to enhance
boundary sensitivity and mitigate long-tail distribution effects, and (3) a
memory-based Hard Error Token (HET) optimization strategy that emphasizes
difficult tokens during training. Extensive experiments on the QaTa-COV19 and
MosMedData+ datasets demonstrate that NTP-MRISeg achieves new state-of-the-art
performance, offering a streamlined and effective alternative to traditional
MRIS pipelines.

</details>


### [40] [DeepEyesV2: Toward Agentic Multimodal Model](https://arxiv.org/abs/2511.05271)
*Jack Hong,Chenxiao Zhao,ChengLin Zhu,Weiheng Lu,Guohai Xu,Xing Yu*

Main category: cs.CV

TL;DR: DeepEyesV2是一个代理式多模态模型，通过两阶段训练（冷启动+强化学习）实现稳健的工具调用能力，在真实世界理解、数学推理和搜索任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 构建能够主动调用外部工具（如代码执行环境、网络搜索）并将其整合到推理过程中的代理式多模态模型。

Method: 采用两阶段训练流程：冷启动阶段建立工具使用模式，强化学习阶段进一步优化工具调用。构建多样化训练数据集，并引入RealX-Bench评估基准。

Result: DeepEyesV2在RealX-Bench和其他代表性基准测试中表现优异，能够根据任务自适应调用工具（感知任务使用图像操作，推理任务使用数值计算）。

Conclusion: 两阶段训练方法有效解决了单纯强化学习无法诱导稳健工具使用行为的问题，为开发代理式多模态模型提供了指导。

Abstract: Agentic multimodal models should not only comprehend text and images, but
also actively invoke external tools, such as code execution environments and
web search, and integrate these operations into reasoning. In this work, we
introduce DeepEyesV2 and explore how to build an agentic multimodal model from
the perspectives of data construction, training methods, and model evaluation.
We observe that direct reinforcement learning alone fails to induce robust
tool-use behavior. This phenomenon motivates a two-stage training pipeline: a
cold-start stage to establish tool-use patterns, and reinforcement learning
stage to further refine tool invocation. We curate a diverse, moderately
challenging training dataset, specifically including examples where tool use is
beneficial. We further introduce RealX-Bench, a comprehensive benchmark
designed to evaluate real-world multimodal reasoning, which inherently requires
the integration of multiple capabilities, including perception, search, and
reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative
benchmarks, demonstrating its effectiveness across real-world understanding,
mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2
exhibits task-adaptive tool invocation, tending to use image operations for
perception tasks and numerical computations for reasoning tasks. Reinforcement
learning further enables complex tool combinations and allows model to
selectively invoke tools based on context. We hope our study can provide
guidance for community in developing agentic multimodal models.

</details>


### [41] [LiveStar: Live Streaming Assistant for Real-World Online Video Understanding](https://arxiv.org/abs/2511.05299)
*Zhenyu Yang,Kairui Zhang,Yuhang Hu,Bing Wang,Shengsheng Qian,Bin Wen,Fan Yang,Tingting Gao,Weiming Dong,Changsheng Xu*

Main category: cs.CV

TL;DR: LiveStar是一个开创性的直播流助手，通过自适应流式解码实现始终在线的主动响应，解决了现有在线视频大语言模型在实时响应和叙事连贯性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的在线视频大语言模型通常难以同时处理连续帧输入和确定最佳响应时机，往往在实时响应性和叙事连贯性之间做出妥协。

Method: LiveStar包含：(1)支持可变长度视频流增量视频-语言对齐的训练策略；(2)通过单次前向传播验证确定最佳主动响应时机的响应-静默解码框架；(3)通过峰值-末端内存压缩和流式键值缓存实现内存感知加速。

Result: 在三个基准测试上的广泛实验表明，LiveStar实现了最先进的性能，在语义正确性上平均提升19.5%，时序差异减少18.1%，同时在所有五个OmniStar任务上FPS提升12.0%。

Conclusion: LiveStar通过创新的自适应流式解码方法，显著提升了在线视频理解模型的实时响应能力和性能表现。

Abstract: Despite significant progress in Video Large Language Models (Video-LLMs) for
offline video understanding, existing online Video-LLMs typically struggle to
simultaneously process continuous frame-by-frame inputs and determine optimal
response timing, often compromising real-time responsiveness and narrative
coherence. To address these limitations, we introduce LiveStar, a pioneering
live streaming assistant that achieves always-on proactive responses through
adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a
training strategy enabling incremental video-language alignment for
variable-length video streams, preserving temporal consistency across
dynamically evolving frame sequences; (2) a response-silence decoding framework
that determines optimal proactive response timing via a single forward pass
verification; (3) memory-aware acceleration via peak-end memory compression for
online inference on 10+ minute videos, combined with streaming key-value cache
to achieve 1.53x faster inference. We also construct an OmniStar dataset, a
comprehensive dataset for training and benchmarking that encompasses 15 diverse
real-world scenarios and 5 evaluation tasks for online video understanding.
Extensive experiments across three benchmarks demonstrate LiveStar's
state-of-the-art performance, achieving an average 19.5% improvement in
semantic correctness with 18.1% reduced timing difference compared to existing
online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.
Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.

</details>


### [42] [Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach](https://arxiv.org/abs/2511.05057)
*Yuanxiang Huangfu,Chaochao Wang,Weilei Wang*

Main category: cs.CV

TL;DR: Role-SynthCLIP是一个通过多视角角色扮演提示来生成语义多样化的图像-文本对的数据合成框架，显著提升了CLIP模型的训练效果，仅用100万对数据就在MS COCO上达到64.1%的Recall@1，超越了现有使用500万对数据的最佳基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据生成方法主要关注增加数据量，但这往往导致语义多样性有限和标题冗余或浅显的问题。需要一种能够生成语义丰富、细粒度对齐的图像-文本对的方法。

Method: 提出Role-SynthCLIP框架，利用多视角角色扮演提示（如组合分析师、图像上下文解释器等）指导多模态大语言模型从不同视角生成语义多样化的标题，增强合成对的语义多样性和细粒度图像-文本对齐。

Result: 仅使用100万对Role-SynthCLIP数据训练的CLIP-B/16模型在MS COCO验证集上达到64.1%的Recall@1，比现有最佳合成数据基线（使用500万对数据）高出2.8个百分点。

Conclusion: Role-SynthCLIP通过多视角角色提示有效提升了合成数据的语义多样性，在保持数据量不变的情况下显著提高了CLIP模型的性能，证明了语义质量比数据量更重要的观点。

Abstract: The effectiveness of Contrastive Language-Image Pre-training (CLIP) models
critically depends on the semantic diversity and quality of their training
data. However, while existing synthetic data generation methods primarily focus
on increasing data volume, such emphasis often leads to limited semantic
diversity and redundant or shallow captions. To address this limitation, we
propose Role-SynthCLIP, a novel data synthesis framework that leverages
multi-perspective role-playing prompts (e.g., a compositional analyst, an
interpreter of image context) to guide Multimodal Large Language Models (MLLMs)
in generating semantically diverse captions from distinct viewpoints. This
mechanism enhances the semantic diversity and fine-grained image-text alignment
of synthetic pairs, thereby improving caption expressiveness and accuracy while
keeping the total number of image-text pairs unchanged. Experimental results
demonstrate the effectiveness and efficiency of our method. A CLIP-B/16 model
trained on only 1 million Role-SynthCLIP pairs achieves a Recall@1 of 64.1% on
the MS COCO validation set, surpassing the best existing synthetic data
baseline (trained on 5M pairs) by 2.8 percentage points. The code and trained
models are released at https://github.com/huangfu170/Role-SynthCLIP.

</details>


### [43] [Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation](https://arxiv.org/abs/2511.05308)
*Matteo Bastico,David Ryckelynck,Laurent Corté,Yannick Tillier,Etienne Decencière*

Main category: cs.CV

TL;DR: 本文揭示了现有点云生成评估指标（特别是基于Chamfer距离的指标）的不足，提出了改进的评估方法（DCD和SNC），并开发了新的Diffusion Point Transformer模型，在ShapeNet数据集上达到了最先进的生成质量。


<details>
  <summary>Details</summary>
Motivation: 随着3D点云技术的广泛应用，需要更可靠的生成模型和评估指标。现有基于Chamfer距离的评估指标存在缺陷，无法准确捕捉几何保真度和局部形状一致性。

Method: 1) 引入样本对齐和Density-Aware Chamfer Distance (DCD)改进评估指标；2) 提出Surface Normal Concordance (SNC)新指标，通过比较估计点法向来近似表面相似性；3) 基于transformer的点云分析模型，开发Diffusion Point Transformer生成架构。

Result: 在ShapeNet数据集上的广泛实验表明，提出的模型在生成点云质量方面优于先前解决方案，达到了新的最先进水平。

Conclusion: 通过改进评估指标和开发新的生成架构，本文为点云生成提供了更可靠的评估方法和更高质量的生成模型。

Abstract: As 3D point clouds become a cornerstone of modern technology, the need for
sophisticated generative models and reliable evaluation metrics has grown
exponentially. In this work, we first expose that some commonly used metrics
for evaluating generated point clouds, particularly those based on Chamfer
Distance (CD), lack robustness against defects and fail to capture geometric
fidelity and local shape consistency when used as quality indicators. We
further show that introducing samples alignment prior to distance calculation
and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet
essential steps to ensure the consistency and robustness of point cloud
generative model evaluation metrics. While existing metrics primarily focus on
directly comparing 3D Euclidean coordinates, we present a novel metric, named
Surface Normal Concordance (SNC), which approximates surface similarity by
comparing estimated point normals. This new metric, when combined with
traditional ones, provides a more comprehensive evaluation of the quality of
generated samples. Finally, leveraging recent advancements in transformer-based
models for point cloud analysis, such as serialized patch attention , we
propose a new architecture for generating high-fidelity 3D structures, the
Diffusion Point Transformer. We perform extensive experiments and comparisons
on the ShapeNet dataset, showing that our model outperforms previous solutions,
particularly in terms of quality of generated point clouds, achieving new
state-of-the-art. Code available at
https://github.com/matteo-bastico/DiffusionPointTransformer.

</details>


### [44] [SurgiATM: A Physics-Guided Plug-and-Play Model for Deep Learning-Based Smoke Removal in Laparoscopic Surgery](https://arxiv.org/abs/2511.05059)
*Mingyu Sheng,Jianan Fan,Dongnan Liu,Guoyan Zheng,Ron Kikinis,Weidong Cai*

Main category: cs.CV

TL;DR: 提出SurgiATM手术烟雾去除方法，通过统计建模将物理大气模型与深度学习结合，作为轻量级即插即用模块提升现有去烟方法的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜手术中组织烧灼产生的烟雾会显著降低内窥镜图像质量，增加手术错误风险，影响临床决策和计算机视觉分析，因此需要有效去除手术烟雾。

Method: SurgiATM统计性地桥接基于物理的大气模型和数据驱动的深度学习模型，结合前者的泛化能力和后者的高精度。作为轻量级即插即用模块，仅引入两个超参数且无额外可训练权重。

Result: 在三个公共手术数据集和十种去烟方法上的实验表明，集成SurgiATM能普遍降低现有模型的恢复误差，相对提升泛化能力，且不增加可训练层或权重。

Conclusion: 该方法具有便利性、低成本、有效性和泛化性优势，能够更好地满足临床需求。

Abstract: During laparoscopic surgery, smoke generated by tissue cauterization can
significantly degrade the visual quality of endoscopic frames, increasing the
risk of surgical errors and hindering both clinical decision-making and
computer-assisted visual analysis. Consequently, removing surgical smoke is
critical to ensuring patient safety and maintaining operative efficiency. In
this study, we propose the Surgical Atmospheric Model (SurgiATM) for surgical
smoke removal. SurgiATM statistically bridges a physics-based atmospheric model
and data-driven deep learning models, combining the superior generalizability
of the former with the high accuracy of the latter. Furthermore, SurgiATM is
designed as a lightweight, plug-and-play module that can be seamlessly
integrated into diverse surgical desmoking architectures to enhance their
accuracy and stability, better meeting clinical requirements. It introduces
only two hyperparameters and no additional trainable weights, preserving the
original network architecture with minimal computational and modification
overhead. We conduct extensive experiments on three public surgical datasets
with ten desmoking methods, involving multiple network architectures and
covering diverse procedures, including cholecystectomy, partial nephrectomy,
and diaphragm dissection. The results demonstrate that incorporating SurgiATM
commonly reduces the restoration errors of existing models and relatively
enhances their generalizability, without adding any trainable layers or
weights. This highlights the convenience, low cost, effectiveness, and
generalizability of the proposed method. The code for SurgiATM is released at
https://github.com/MingyuShengSMY/SurgiATM.

</details>


### [45] [AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly](https://arxiv.org/abs/2511.05394)
*Alexander Htet Kyaw,Haotian Ma,Sasa Zivkovic,Jenny Sabin*

Main category: cs.CV

TL;DR: AI辅助的AR装配工作流，使用深度学习目标识别技术识别装配组件并显示分步指导，通过边界框标注组件位置和安装位置，消除手动搜索和分类需求。


<details>
  <summary>Details</summary>
Motivation: 解决传统装配过程中需要手动搜索、分类和标记组件的问题，通过AI和AR技术提高装配效率和准确性。

Method: 使用深度学习目标识别技术识别不同装配组件，结合AR显示边界框和安装位置指导，实现实时组件定位与装配指令连接。

Result: 通过LEGO雕塑装配案例验证了该方法的可行性，系统能够准确识别组件并显示装配指导。

Conclusion: 基于目标识别的AR辅助装配系统能够有效提升装配过程的效率和用户体验，具有实际应用价值。

Abstract: We present an AI-assisted Augmented Reality assembly workflow that uses deep
learning-based object recognition to identify different assembly components and
display step-by-step instructions. For each assembly step, the system displays
a bounding box around the corresponding components in the physical space, and
where the component should be placed. By connecting assembly instructions with
the real-time location of relevant components, the system eliminates the need
for manual searching, sorting, or labeling of different components before each
assembly. To demonstrate the feasibility of using object recognition for
AR-assisted assembly, we highlight a case study involving the assembly of LEGO
sculptures.

</details>


### [46] [Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments](https://arxiv.org/abs/2511.05404)
*Laura Alejandra Encinar Gonzalez,John Folkesson,Rudolph Triebel,Riccardo Giubilato*

Main category: cs.CV

TL;DR: MPRF是一个多模态闭环检测管道，结合视觉和LiDAR模态，在非结构化环境中实现鲁棒的闭环检测和6-DoF位姿估计。


<details>
  <summary>Details</summary>
Motivation: 在GNSS拒止环境（如行星探测）中，视觉位置识别因混叠和弱纹理而失败，LiDAR方法则受稀疏性和模糊性影响，需要更鲁棒的闭环检测方法。

Method: 使用基于transformer的基础模型，结合两阶段视觉检索策略（DINOv2特征+SALAD聚合）进行候选筛选，以及SONATA LiDAR描述子进行几何验证，实现显式6-DoF位姿估计。

Result: 在S3LI数据集和S3LI Vulcano数据集上的实验表明，MPRF在精度上优于最先进的检索方法，并在低纹理区域增强了位姿估计的鲁棒性。

Conclusion: MPRF在准确性、效率和可靠性之间实现了良好平衡，展示了基础模型在统一位置识别和位姿估计方面的潜力。

Abstract: Robust loop closure detection is a critical component of Simultaneous
Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as
in the context of planetary exploration. In these settings, visual place
recognition often fails due to aliasing and weak textures, while LiDAR-based
methods suffer from sparsity and ambiguity. This paper presents MPRF, a
multimodal pipeline that leverages transformer-based foundation models for both
vision and LiDAR modalities to achieve robust loop closure in severely
unstructured environments. Unlike prior work limited to retrieval, MPRF
integrates a two-stage visual retrieval strategy with explicit 6-DoF pose
estimation, combining DINOv2 features with SALAD aggregation for efficient
candidate screening and SONATA-based LiDAR descriptors for geometric
verification. Experiments on the S3LI dataset and S3LI Vulcano dataset show
that MPRF outperforms state-of-the-art retrieval methods in precision while
enhancing pose estimation robustness in low-texture regions. By providing
interpretable correspondences suitable for SLAM back-ends, MPRF achieves a
favorable trade-off between accuracy, efficiency, and reliability,
demonstrating the potential of foundation models to unify place recognition and
pose estimation. Code and models will be released at github.com/DLR-RM/MPRF.

</details>


### [47] [A Dual-stage Prompt-driven Privacy-preserving Paradigm for Person Re-Identification](https://arxiv.org/abs/2511.05092)
*Ruolin Li,Min Liu,Yuan Bian,Zhaoyang Li,Yuzhen Li,Xueping Wang,Yaonan Wang*

Main category: cs.CV

TL;DR: 提出DPPP双阶段提示驱动隐私保护范式，首阶段生成多样化虚拟数据集GenePerson，次阶段通过提示驱动解耦机制学习域不变特征，在行人重识别任务中实现最优泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有虚拟数据集构建复杂、域泛化能力差的问题，在保护数据隐私的同时提升模型在真实场景中的泛化能力。

Method: 1. 首阶段：使用多维属性提示驱动扩散模型生成多样化虚拟数据集GenePerson；2. 次阶段：通过提示驱动解耦机制，利用对比学习和文本反演网络分离风格与内容特征，学习域不变内容特征。

Result: 在GenePerson数据集上训练的模型实现了最优泛化性能，超越了在真实和虚拟Re-ID数据集上训练的模型。

Conclusion: DPPP范式有效解决了虚拟数据集构建和域泛化问题，为隐私保护下的行人重识别提供了可行方案。

Abstract: With growing concerns over data privacy, researchers have started using
virtual data as an alternative to sensitive real-world images for training
person re-identification (Re-ID) models. However, existing virtual datasets
produced by game engines still face challenges such as complex construction and
poor domain generalization, making them difficult to apply in real scenarios.
To address these challenges, we propose a Dual-stage Prompt-driven
Privacy-preserving Paradigm (DPPP). In the first stage, we generate rich
prompts incorporating multi-dimensional attributes such as pedestrian
appearance, illumination, and viewpoint that drive the diffusion model to
synthesize diverse data end-to-end, building a large-scale virtual dataset
named GenePerson with 130,519 images of 6,641 identities. In the second stage,
we propose a Prompt-driven Disentanglement Mechanism (PDM) to learn
domain-invariant generalization features. With the aid of contrastive learning,
we employ two textual inversion networks to map images into pseudo-words
representing style and content, respectively, thereby constructing
style-disentangled content prompts to guide the model in learning
domain-invariant content features at the image level. Experiments demonstrate
that models trained on GenePerson with PDM achieve state-of-the-art
generalization performance, surpassing those on popular real and virtual Re-ID
datasets.

</details>


### [48] [TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning](https://arxiv.org/abs/2511.05489)
*Junwen Pan,Qizhe Zhang,Rui Zhang,Ming Lu,Xin Wan,Yuan Zhang,Chang Liu,Qi She*

Main category: cs.CV

TL;DR: TimeSearch-R将时间搜索重新定义为交错文本-视频思考过程，通过强化学习将视频片段搜索整合到推理过程中，并提出GRPO-CSV方法解决强化学习训练中的问题，在多个基准测试中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有时间搜索方法通常依赖手工设计的搜索过程，缺乏端到端优化来学习最优搜索策略，导致搜索效率受限。

Method: 提出TimeSearch-R框架，将时间搜索重新定义为交错文本-视频思考过程；引入GRPO-CSV（带完整性自验证的组相对策略优化）方法，使用相同策略模型验证搜索帧的充分性；构建专门数据集用于SFT冷启动和RL训练。

Result: 在Haystack-LVBench、Haystack-Ego4D等时间搜索基准以及VideoMME、MLVU等长视频理解基准上取得显著改进；在LongVideoBench上比基础模型Qwen2.5-VL提升4.1%，比先进视频推理模型Video-R1提升2.0%。

Conclusion: TimeSearch-R通过将时间搜索重新定义为交错推理过程并引入完整性自验证机制，有效提升了长视频理解中的时间搜索能力，在多个基准测试中建立了新的最先进水平。

Abstract: Temporal search aims to identify a minimal set of relevant frames from tens
of thousands based on a given query, serving as a foundation for accurate
long-form video understanding. Existing works attempt to progressively narrow
the search space. However, these approaches typically rely on a hand-crafted
search process, lacking end-to-end optimization for learning optimal search
strategies. In this paper, we propose TimeSearch-R, which reformulates temporal
search as interleaved text-video thinking, seamlessly integrating searching
video clips into the reasoning process through reinforcement learning (RL).
However, applying RL training methods, such as Group Relative Policy
Optimization (GRPO), to video reasoning can result in unsupervised intermediate
search decisions. This leads to insufficient exploration of the video content
and inconsistent logical reasoning. To address these issues, we introduce GRPO
with Completeness Self-Verification (GRPO-CSV), which gathers searched video
frames from the interleaved reasoning process and utilizes the same policy
model to verify the adequacy of searched frames, thereby improving the
completeness of video reasoning. Additionally, we construct datasets
specifically designed for the SFT cold-start and RL training of GRPO-CSV,
filtering out samples with weak temporal dependencies to enhance task
difficulty and improve temporal search capabilities. Extensive experiments
demonstrate that TimeSearch-R achieves significant improvements on temporal
search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as
long-form video understanding benchmarks like VideoMME and MLVU. Notably,
TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1%
improvement over the base model Qwen2.5-VL and 2.0% over the advanced video
reasoning model Video-R1. Our code is available at
https://github.com/Time-Search/TimeSearch-R.

</details>


### [49] [Real-World Adverse Weather Image Restoration via Dual-Level Reinforcement Learning with High-Quality Cold Start](https://arxiv.org/abs/2511.05095)
*Fuyang Liu,Jiaqi Xu,Xiaowei Hu*

Main category: cs.CV

TL;DR: 提出了一个双层次强化学习框架，通过物理驱动的高保真数据集HFLS-Weather进行冷启动训练，在局部层次通过扰动驱动的图像质量优化精炼天气特定恢复模型，在全局层次通过元控制器动态编排模型选择和执行顺序，实现恶劣天气条件下的持续适应。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气严重损害真实世界视觉感知，而现有基于合成数据训练的视觉模型难以泛化到复杂的退化情况。

Method: 构建HFLS-Weather物理驱动高保真数据集，设计双层次强化学习框架：局部层次通过扰动驱动图像质量优化精炼天气特定恢复模型，全局层次通过元控制器动态编排模型选择和执行顺序。

Result: 该框架能够持续适应真实世界条件，在广泛的恶劣天气场景中达到最先进的性能。

Conclusion: 提出的双层次强化学习框架有效解决了恶劣天气下视觉感知的泛化问题，实现了在复杂退化条件下的持续适应和优异性能。

Abstract: Adverse weather severely impairs real-world visual perception, while existing
vision models trained on synthetic data with fixed parameters struggle to
generalize to complex degradations. To address this, we first construct
HFLS-Weather, a physics-driven, high-fidelity dataset that simulates diverse
weather phenomena, and then design a dual-level reinforcement learning
framework initialized with HFLS-Weather for cold-start training. Within this
framework, at the local level, weather-specific restoration models are refined
through perturbation-driven image quality optimization, enabling reward-based
learning without paired supervision; at the global level, a meta-controller
dynamically orchestrates model selection and execution order according to scene
degradation. This framework enables continuous adaptation to real-world
conditions and achieves state-of-the-art performance across a wide range of
adverse weather scenarios. Code is available at
https://github.com/xxclfy/AgentRL-Real-Weather

</details>


### [50] [Early Alzheimer's Disease Detection from Retinal OCT Images: A UK Biobank Study](https://arxiv.org/abs/2511.05106)
*Yasemin Turkan,F. Boray Tek,M. Serdar Nazlı,Öykü Eren*

Main category: cs.CV

TL;DR: 本研究首次应用深度学习直接对原始OCT B扫描图像进行阿尔茨海默病早期检测，使用ResNet-34模型在4年队列中达到0.62 AUC，虽低于临床应用阈值，但为OCT-based AD预测提供了基线。


<details>
  <summary>Details</summary>
Motivation: 视网膜层厚度变化与神经退行性疾病相关，但以往研究主要关注分割层厚度测量。本研究探索直接对OCT B扫描图像进行分类以实现AD早期检测，因为成像比临床诊断早数年。

Method: 微调并评估多个预训练模型（包括ImageNet网络和OCT专用RETFound transformer），使用UK Biobank队列中年龄、性别和成像实例匹配的受试者级交叉验证数据集。应用标准和OCT专用增强技术以减少过拟合，并使用年加权损失函数优先考虑成像后4年内诊断的病例。

Result: ResNet-34产生最稳定结果，在4年队列中AUC为0.62。可解释性分析确认AD组和对照组在中央黄斑亚区存在局部结构差异。

Conclusion: 研究为基于OCT的AD预测提供了基线，突出了在AD诊断前数年检测细微视网膜生物标志物的挑战，并指出需要更大数据集和多模态方法。

Abstract: Alterations in retinal layer thickness, measurable using Optical Coherence
Tomography (OCT), have been associated with neurodegenerative diseases such as
Alzheimer's disease (AD). While previous studies have mainly focused on
segmented layer thickness measurements, this study explored the direct
classification of OCT B-scan images for the early detection of AD. To our
knowledge, this is the first application of deep learning to raw OCT B-scans
for AD prediction in the literature. Unlike conventional medical image
classification tasks, early detection is more challenging than diagnosis
because imaging precedes clinical diagnosis by several years. We fine-tuned and
evaluated multiple pretrained models, including ImageNet-based networks and the
OCT-specific RETFound transformer, using subject-level cross-validation
datasets matched for age, sex, and imaging instances from the UK Biobank
cohort. To reduce overfitting in this small, high-dimensional dataset, both
standard and OCT-specific augmentation techniques were applied, along with a
year-weighted loss function that prioritized cases diagnosed within four years
of imaging. ResNet-34 produced the most stable results, achieving an AUC of
0.62 in the 4-year cohort. Although below the threshold for clinical
application, our explainability analyses confirmed localized structural
differences in the central macular subfield between the AD and control groups.
These findings provide a baseline for OCT-based AD prediction, highlight the
challenges of detecting subtle retinal biomarkers years before AD diagnosis,
and point to the need for larger datasets and multimodal approaches.

</details>


### [51] [SnowyLane: Robust Lane Detection on Snow-covered Rural Roads Using Infrastructural Elements](https://arxiv.org/abs/2511.05108)
*Jörg Gamerdinger,Benedict Wetzel,Patrick Schulz,Sven Teufel,Oliver Bringmann*

Main category: cs.CV

TL;DR: 提出一种在积雪环境中通过检测路边标杆来间接识别车道的实时方法，解决了传统车道线被积雪遮挡的问题，并发布了包含8万帧的SnowyLane合成数据集。


<details>
  <summary>Details</summary>
Motivation: 在积雪环境中，传统车道线经常被遮挡或缺失，导致自动驾驶车辆难以进行可靠的车道检测，这成为冬季自动驾驶的主要挑战。

Method: 通过检测路边垂直标杆作为间接车道指示器，使用参数化贝塞尔曲线模型拟合平滑车道轨迹，利用空间一致性和道路几何特性。

Result: 与最先进的车道检测系统相比，该方法在恶劣天气下表现出显著改进的鲁棒性，特别是在大雪遮挡情况下。

Conclusion: 这项工作为冬季场景下的可靠车道检测奠定了坚实基础，并为全天候自动驾驶研究贡献了宝贵资源。

Abstract: Lane detection for autonomous driving in snow-covered environments remains a
major challenge due to the frequent absence or occlusion of lane markings. In
this paper, we present a novel, robust and realtime capable approach that
bypasses the reliance on traditional lane markings by detecting roadside
features,specifically vertical roadside posts called delineators, as indirect
lane indicators. Our method first perceives these posts, then fits a smooth
lane trajectory using a parameterized Bezier curve model, leveraging spatial
consistency and road geometry. To support training and evaluation in these
challenging scenarios, we introduce SnowyLane, a new synthetic dataset
containing 80,000 annotated frames capture winter driving conditions, with
varying snow coverage, and lighting conditions. Compared to state-of-the-art
lane detection systems, our approach demonstrates significantly improved
robustness in adverse weather, particularly in cases with heavy snow occlusion.
This work establishes a strong foundation for reliable lane detection in winter
scenarios and contributes a valuable resource for future research in
all-weather autonomous driving. The dataset is available at
https://ekut-es.github.io/snowy-lane

</details>


### [52] [Splatography: Sparse multi-view dynamic Gaussian Splatting for filmmaking challenges](https://arxiv.org/abs/2511.05152)
*Adrian Azzarelli,Nantheera Anantrasirichai,David R Bull*

Main category: cs.CV

TL;DR: 提出了一种将可变形高斯泼溅分解为前景和背景组件的方法，用于从稀疏多视角视频中进行动态3D重建，在模型大小减半的情况下实现更好的重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决电影制作中因预算限制导致的稀疏相机配置问题，现有方法在捕捉复杂动态特征时存在限制。

Method: 使用t=0帧的稀疏掩码将规范高斯和变形场分解为前景和背景组件，分别用不同损失函数训练，前景学习颜色、位置和旋转变化，背景仅学习位置变化。

Result: 在3D和2.5D娱乐数据集上实现SotA结果，PSNR提高达3分，模型大小减半，并能生成包含透明和动态纹理的分割动态重建。

Conclusion: 该方法在稀疏相机配置下实现了高质量的动态3D重建，无需密集掩码监督即可生成分割的动态重建结果。

Abstract: Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D
reconstruction from dense multi-view video (MVV) by learning to deform a
canonical GS representation. However, in filmmaking, tight budgets can result
in sparse camera configurations, which limits state-of-the-art (SotA) methods
when capturing complex dynamic features. To address this issue, we introduce an
approach that splits the canonical Gaussians and deformation field into
foreground and background components using a sparse set of masks for frames at
t=0. Each representation is separately trained on different loss functions
during canonical pre-training. Then, during dynamic training, different
parameters are modeled for each deformation field following common filmmaking
practices. The foreground stage contains diverse dynamic features so changes in
color, position and rotation are learned. While, the background containing
film-crew and equipment, is typically dimmer and less dynamic so only changes
in point position are learned. Experiments on 3-D and 2.5-D entertainment
datasets show that our method produces SotA qualitative and quantitative
results; up to 3 PSNR higher with half the model size on 3-D scenes. Unlike the
SotA and without the need for dense mask supervision, our method also produces
segmented dynamic reconstructions including transparent and dynamic textures.
Code and video comparisons are available online:
https://interims-git.github.io/

</details>


### [53] [Another BRIXEL in the Wall: Towards Cheaper Dense Features](https://arxiv.org/abs/2511.05168)
*Alexander Lappe,Martin A. Giese*

Main category: cs.CV

TL;DR: BRIXEL是一种简单的知识蒸馏方法，通过让学生模型学习在更高分辨率下重现自身的特征图，解决了DINOv3模型在高分辨率下计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: Vision foundation models如DINOv3在高分辨率下能产生细粒度密集特征图，但计算成本高昂，需要高分辨率输入和大量计算资源。

Method: 提出BRIXEL知识蒸馏方法，让学生模型学习在更高分辨率下重现自身的特征图，从而降低计算复杂度。

Result: BRIXEL在固定分辨率下大幅超越基线DINOv3模型，能以较低计算成本产生与教师模型相似的特征图。

Conclusion: BRIXEL通过简单有效的知识蒸馏方法，在保持性能的同时显著降低了vision foundation models的计算成本。

Abstract: Vision foundation models achieve strong performance on both global and
locally dense downstream tasks. Pretrained on large images, the recent DINOv3
model family is able to produce very fine-grained dense feature maps, enabling
state-of-the-art performance. However, computing these feature maps requires
the input image to be available at very high resolution, as well as large
amounts of compute due to the squared complexity of the transformer
architecture. To address these issues, we propose BRIXEL, a simple knowledge
distillation approach that has the student learn to reproduce its own feature
maps at higher resolution. Despite its simplicity, BRIXEL outperforms the
baseline DINOv3 models by large margins on downstream tasks when the resolution
is kept fixed. Moreover, it is able to produce feature maps that are very
similar to those of the teacher at a fraction of the computational cost. Code
and model weights are available at https://github.com/alexanderlappe/BRIXEL.

</details>


### [54] [MUSE: Multi-Scale Dense Self-Distillation for Nucleus Detection and Classification](https://arxiv.org/abs/2511.05170)
*Zijiang Yang,Hanqing Chao,Bokai Zhao,Yelin Yang,Yunshuo Zhang,Dongmei Fu,Junping Zhang,Le Lu,Ke Yan,Dakai Jin,Minfeng Xu,Yun Bian,Hui Jiang*

Main category: cs.CV

TL;DR: 提出MUSE方法，一种针对病理学中细胞核检测和分类的自监督学习方法，通过多尺度密集自蒸馏机制解决现有方法依赖人工标注和无法充分利用无标签数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有细胞核检测分类方法严重依赖人工标注，难以充分利用大规模无标签数据学习判别性细胞核表示。

Method: 提出NuLo机制实现基于预测细胞核位置的灵活局部自蒸馏，设计编码器-解码器架构和大视野半监督微调策略。

Result: 在三个基准测试上的实验表明，MUSE不仅超越最先进的监督基线，还优于通用病理学基础模型。

Conclusion: MUSE有效解决了病理学细胞核检测分类的核心挑战，为利用无标签数据提供了有效解决方案。

Abstract: Nucleus detection and classification (NDC) in histopathology analysis is a
fundamental task that underpins a wide range of high-level pathology
applications. However, existing methods heavily rely on labor-intensive
nucleus-level annotations and struggle to fully exploit large-scale unlabeled
data for learning discriminative nucleus representations. In this work, we
propose MUSE (MUlti-scale denSE self-distillation), a novel self-supervised
learning method tailored for NDC. At its core is NuLo (Nucleus-based Local
self-distillation), a coordinate-guided mechanism that enables flexible local
self-distillation based on predicted nucleus positions. By removing the need
for strict spatial alignment between augmented views, NuLo allows critical
cross-scale alignment, thus unlocking the capacity of models for fine-grained
nucleus-level representation. To support MUSE, we design a simple yet effective
encoder-decoder architecture and a large field-of-view semi-supervised
fine-tuning strategy that together maximize the value of unlabeled pathology
images. Extensive experiments on three widely used benchmarks demonstrate that
MUSE effectively addresses the core challenges of histopathological NDC. The
resulting models not only surpass state-of-the-art supervised baselines but
also outperform generic pathology foundation models.

</details>


### [55] [Walk the Lines 2: Contour Tracking for Detailed Segmentation](https://arxiv.org/abs/2511.05210)
*André Peter Kelm,Max Braeschke,Emre Gülsoylu,Simone Frintrop*

Main category: cs.CV

TL;DR: WtL2是一种改进的轮廓跟踪算法，专门用于红外和RGB图像中船舶及其他物体的精细分割，通过轮廓跟踪替代传统非极大值抑制，生成1像素宽的闭合轮廓。


<details>
  <summary>Details</summary>
Motivation: 扩展原始WtL算法，使其不仅适用于彩色图像中的船舶分割，还能处理红外图像中的船舶分割，并扩展到RGB图像中的多种物体分割。

Method: 通过轮廓跟踪算法替代标准NMS，将物体轮廓细化为1像素宽的闭合形状，形成可分割的前景-背景区域；针对红外图像适配物体轮廓检测器输入。

Result: 在实现闭合物体轮廓方面优于最新一代基于轮廓的方法，提供高IoU峰值和精细细节，适用于需要详细分割或高质量样本的专业应用。

Conclusion: WtL2是一个有前景的方法，特别适用于需要精细分割的专业应用场景，有望推动图像分割在多个细分领域的进展。

Abstract: This paper presents Walk the Lines 2 (WtL2), a unique contour tracking
algorithm specifically adapted for detailed segmentation of infrared (IR) ships
and various objects in RGB.1 This extends the original Walk the Lines (WtL)
[12], which focused solely on detailed ship segmentation in color. These
innovative WtLs can replace the standard non-maximum suppression (NMS) by using
contour tracking to refine the object contour until a 1-pixel-wide closed shape
can be binarized, forming a segmentable area in foreground-background
scenarios. WtL2 broadens the application range of WtL beyond its original
scope, adapting to IR and expanding to diverse objects within the RGB context.
To achieve IR segmentation, we adapt its input, the object contour detector, to
IR ships. In addition, the algorithm is enhanced to process a wide range of RGB
objects, outperforming the latest generation of contour-based methods when
achieving a closed object contour, offering high peak Intersection over Union
(IoU) with impressive details. This positions WtL2 as a compelling method for
specialized applications that require detailed segmentation or high-quality
samples, potentially accelerating progress in several niche areas of image
segmentation.

</details>


### [56] [FreeControl: Efficient, Training-Free Structural Control via One-Step Attention Extraction](https://arxiv.org/abs/2511.05219)
*Jiang Lin,Xinyu Chen,Song Wu,Zhiqiu Zhang,Jizhi Zhang,Ye Wang,Qiang Tang,Qian Wang,Jian Yang,Zili Yi*

Main category: cs.CV

TL;DR: FreeControl是一个无需训练的扩散模型控制框架，通过单步注意力提取和潜在条件解耦实现高效的结构语义控制，支持组合控制且兼容现代扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型控制方法如ControlNet依赖手工条件图和重训练，灵活性差；基于反转的方法对齐效果好但推理成本高。需要一种既高效又灵活的控制方法。

Method: FreeControl从单个最优关键时间步提取注意力并复用；引入潜在条件解耦(LCD)分离关键时间步和噪声潜在；支持通过多源参考图像进行组合控制。

Result: FreeControl实现了结构和语义对齐的视觉连贯生成，直接从原始图像生成，支持直观的场景布局设计，与现代扩散模型兼容且仅增加约5%的计算成本。

Conclusion: FreeControl为测试时控制提供了新范式，无需反转或重训练即可实现高效、灵活的结构语义控制，支持组合设计并保持高质量生成。

Abstract: Controlling the spatial and semantic structure of diffusion-generated images
remains a challenge. Existing methods like ControlNet rely on handcrafted
condition maps and retraining, limiting flexibility and generalization.
Inversion-based approaches offer stronger alignment but incur high inference
cost due to dual-path denoising. We present FreeControl, a training-free
framework for semantic structural control in diffusion models. Unlike prior
methods that extract attention across multiple timesteps, FreeControl performs
one-step attention extraction from a single, optimally chosen key timestep and
reuses it throughout denoising. This enables efficient structural guidance
without inversion or retraining. To further improve quality and stability, we
introduce Latent-Condition Decoupling (LCD): a principled separation of the key
timestep and the noised latent used in attention extraction. LCD provides finer
control over attention quality and eliminates structural artifacts. FreeControl
also supports compositional control via reference images assembled from
multiple sources - enabling intuitive scene layout design and stronger prompt
alignment. FreeControl introduces a new paradigm for test-time control,
enabling structurally and semantically aligned, visually coherent generation
directly from raw images, with the flexibility for intuitive compositional
design and compatibility with modern diffusion models at approximately 5
percent additional cost.

</details>


### [57] [ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining](https://arxiv.org/abs/2511.05245)
*Xincheng Yao,Yan Luo,Zefeng Qian,Chongyang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种专门为工业异常检测设计的预训练表示学习框架，通过角度和范数导向的对比损失来最大化正常和异常特征之间的差异，并在大规模异常检测数据集上进行预训练。


<details>
  <summary>Details</summary>
Motivation: 当前主流的异常检测方法依赖于ImageNet预训练特征，但ImageNet预训练与异常检测目标不匹配，且自然图像与工业图像存在分布偏移，导致预训练特征在AD任务中表现次优。

Method: 提出角度和范数导向的对比损失来同时最大化正常和异常特征之间的角度大小和范数差异；在大规模AD数据集RealIAD上进行预训练；基于类别泛化表示（残差特征）学习预训练AD表示。

Result: 在五个AD数据集和五个骨干网络上进行的广泛实验表明，使用本文预训练特征的嵌入式AD方法始终表现出优越性能。

Conclusion: 本文提出的专门针对异常检测任务的预训练表示学习框架能够产生鲁棒且具有区分性的特征，显著提升了工业异常检测的性能。

Abstract: The current mainstream and state-of-the-art anomaly detection (AD) methods
are substantially established on pretrained feature networks yielded by
ImageNet pretraining. However, regardless of supervised or self-supervised
pretraining, the pretraining process on ImageNet does not match the goal of
anomaly detection (i.e., pretraining in natural images doesn't aim to
distinguish between normal and abnormal). Moreover, natural images and
industrial image data in AD scenarios typically have the distribution shift.
The two issues can cause ImageNet-pretrained features to be suboptimal for AD
tasks. To further promote the development of the AD field, pretrained
representations specially for AD tasks are eager and very valuable. To this
end, we propose a novel AD representation learning framework specially designed
for learning robust and discriminative pretrained representations for
industrial anomaly detection. Specifically, closely surrounding the goal of
anomaly detection (i.e., focus on discrepancies between normals and anomalies),
we propose angle- and norm-oriented contrastive losses to maximize the angle
size and norm difference between normal and abnormal features simultaneously.
To avoid the distribution shift from natural images to AD images, our
pretraining is performed on a large-scale AD dataset, RealIAD. To further
alleviate the potential shift between pretraining data and downstream AD
datasets, we learn the pretrained AD representations based on the
class-generalizable representation, residual features. For evaluation, based on
five embedding-based AD methods, we simply replace their original features with
our pretrained representations. Extensive experiments on five AD datasets and
five backbones consistently show the superiority of our pretrained features.
The code is available at https://github.com/xcyao00/ADPretrain.

</details>


### [58] [Automatic segmentation of colorectal liver metastases for ultrasound-based navigated resection](https://arxiv.org/abs/2511.05253)
*Tiziano Natali,Karin A. Olthof,Niels F. M. Kok,Koert F. D. Kuhlmann,Theo J. M. Ruers,Matteo Fusaglia*

Main category: cs.CV

TL;DR: 开发了一种基于3D U-Net的自动分割方法，用于结直肠肝转移瘤(CRLM)的术中超声(iUS)图像分割，通过裁剪体积训练显著提升性能，实现近实时分割结果。


<details>
  <summary>Details</summary>
Motivation: 术中准确划定结直肠肝转移瘤边界对实现阴性切缘至关重要，但术中超声因对比度低、噪声大和操作者依赖性而具有挑战性，自动分割可提高超声导航工作流程的精度和效率。

Method: 使用85例CRLM患者的追踪3D iUS体积数据，通过nnU-Net框架实现3D U-Net，比较了完整体积和裁剪肿瘤区域两种训练变体，使用Dice系数、Hausdorff距离和相对体积差评估分割精度，并集成到3D Slicer中实时使用。

Result: 裁剪体积模型在所有指标上显著优于完整体积模型(AUC-ROC = 0.898 vs 0.718)，中位DSC = 0.74，召回率 = 0.79，Hausdorff距离 = 17.1 mm，与半自动分割相当但速度快约4倍(~1分钟)，前瞻性测试证实了稳健一致的性能。

Conclusion: 使用裁剪3D U-Net的CRLM术中超声自动3D分割提供可靠、近实时的结果，操作者输入最少，为肝脏手术实现高效、无需配准的超声导航，接近专家级精度同时显著减少人工工作量和手术时间。

Abstract: Introduction: Accurate intraoperative delineation of colorectal liver
metastases (CRLM) is crucial for achieving negative resection margins but
remains challenging using intraoperative ultrasound (iUS) due to low contrast,
noise, and operator dependency. Automated segmentation could enhance precision
and efficiency in ultrasound-based navigation workflows.
  Methods: Eighty-five tracked 3D iUS volumes from 85 CRLM patients were used
to train and evaluate a 3D U-Net implemented via the nnU-Net framework. Two
variants were compared: one trained on full iUS volumes and another on cropped
regions around tumors. Segmentation accuracy was assessed using Dice Similarity
Coefficient (DSC), Hausdorff Distance (HDist.), and Relative Volume Difference
(RVD) on retrospective and prospective datasets. The workflow was integrated
into 3D Slicer for real-time intraoperative use.
  Results: The cropped-volume model significantly outperformed the full-volume
model across all metrics (AUC-ROC = 0.898 vs 0.718). It achieved median DSC =
0.74, recall = 0.79, and HDist. = 17.1 mm comparable to semi-automatic
segmentation but with ~4x faster execution (~ 1 min). Prospective
intraoperative testing confirmed robust and consistent performance, with
clinically acceptable accuracy for real-time surgical guidance.
  Conclusion: Automatic 3D segmentation of CRLM in iUS using a cropped 3D U-Net
provides reliable, near real-time results with minimal operator input. The
method enables efficient, registration-free ultrasound-based navigation for
hepatic surgery, approaching expert-level accuracy while substantially reducing
manual workload and procedure time.

</details>


### [59] [What's on Your Plate? Inferring Chinese Cuisine Intake from Wearable IMUs](https://arxiv.org/abs/2511.05292)
*Jiaxi Yin,Pengcheng Wang,Han Ding,Fei Wang*

Main category: cs.CV

TL;DR: CuisineSense是一个基于可穿戴设备的系统，通过智能手表的手部运动和智能眼镜的头部动态来分类中餐食物类型，解决了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统自我报告方法存在回忆偏差，基于摄像头的方法涉及隐私问题，现有可穿戴方法主要针对有限食物类型，无法应对中餐的多样性。

Method: 采用两阶段检测流程：第一阶段通过区分特征性时间模式识别进食状态；第二阶段基于进食期间捕获的动作进行细粒度食物类型识别。

Result: 构建了包含27.5小时IMU记录的数据集，涵盖11个食物类别和10名参与者，实验显示在进食状态检测和食物分类方面均达到高准确率。

Conclusion: CuisineSense为无干扰、基于可穿戴设备的饮食监测提供了实用解决方案，系统代码已公开。

Abstract: Accurate food intake detection is vital for dietary monitoring and chronic
disease prevention. Traditional self-report methods are prone to recall bias,
while camera-based approaches raise concerns about privacy. Furthermore,
existing wearable-based methods primarily focus on a limited number of food
types, such as hamburgers and pizza, failing to address the vast diversity of
Chinese cuisine. To bridge this gap, we propose CuisineSense, a system that
classifies Chinese food types by integrating hand motion cues from a smartwatch
with head dynamics from smart glasses. To filter out irrelevant daily
activities, we design a two-stage detection pipeline. The first stage
identifies eating states by distinguishing characteristic temporal patterns
from non-eating behaviors. The second stage then conducts fine-grained food
type recognition based on the motions captured during food intake. To evaluate
CuisineSense, we construct a dataset comprising 27.5 hours of IMU recordings
across 11 food categories and 10 participants. Experiments demonstrate that
CuisineSense achieves high accuracy in both eating state detection and food
classification, offering a practical solution for unobtrusive, wearable-based
dietary monitoring.The system code is publicly available at
https://github.com/joeeeeyin/CuisineSense.git.

</details>


### [60] [Cross-domain EEG-based Emotion Recognition with Contrastive Learning](https://arxiv.org/abs/2511.05293)
*Rui Yan,Yibo Li,Han Ding,Fei Wang*

Main category: cs.CV

TL;DR: EmotionCLIP将EEG情绪识别重新定义为CLIP框架下的EEG-文本匹配任务，使用SST-LegoViT骨干网络提取空间、频谱和时间特征，在SEED数据集上取得了优异的跨被试和跨时间识别准确率。


<details>
  <summary>Details</summary>
Motivation: 基于脑电图的情绪识别在特征利用和跨域泛化方面面临挑战，需要开发更有效的特征提取和泛化方法。

Method: 提出EmotionCLIP框架，将情绪识别重构为EEG-文本匹配任务，使用专门设计的SST-LegoViT骨干网络结合多尺度卷积和Transformer模块来捕捉空间、频谱和时间特征。

Result: 在SEED和SEED-IV数据集上，跨被试准确率分别达到88.69%和73.50%，跨时间准确率分别达到88.46%和77.54%，优于现有模型。

Conclusion: 多模态对比学习对于鲁棒的EEG情绪识别是有效的，EmotionCLIP框架在跨域泛化方面表现出色。

Abstract: Electroencephalogram (EEG)-based emotion recognition is vital for affective
computing but faces challenges in feature utilization and cross-domain
generalization. This work introduces EmotionCLIP, which reformulates
recognition as an EEG-text matching task within the CLIP framework. A tailored
backbone, SST-LegoViT, captures spatial, spectral, and temporal features using
multi-scale convolution and Transformer modules. Experiments on SEED and
SEED-IV datasets show superior cross-subject accuracies of 88.69% and 73.50%,
and cross-time accuracies of 88.46% and 77.54%, outperforming existing models.
Results demonstrate the effectiveness of multimodal contrastive learning for
robust EEG emotion recognition.

</details>


### [61] [$\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language Models](https://arxiv.org/abs/2511.05319)
*Huanqi Wu,Huangbiao Xu,Runfeng Xie,Jiaxin Cai,Kaixin Zhang,Xiao Ke*

Main category: cs.CV

TL;DR: 提出了一种基于大语言模型的语义隐写方法S²LM，能够将句子级别的语义信息嵌入到图像中，突破了传统比特级隐写的限制。


<details>
  <summary>Details</summary>
Motivation: 在AIGC时代，隐写术的容量需求日益增长，但现有方法难以嵌入语义丰富的句子级信息。

Method: 设计了S²LM模型，利用大语言模型在整个隐写过程中嵌入高层次的文本信息，建立了IVT基准数据集进行评估。

Result: 定量和定性实验表明，该方法有效解锁了大语言模型的新语义隐写能力。

Conclusion: 该方法为语义隐写开辟了新途径，能够将句子甚至段落级别的语义信息嵌入图像中。

Abstract: Although steganography has made significant advancements in recent years, it
still struggles to embed semantically rich, sentence-level information into
carriers. However, in the era of AIGC, the capacity of steganography is more
critical than ever. In this work, we present Sentence-to-Image Steganography,
an instance of Semantic Steganography, a novel task that enables the hiding of
arbitrary sentence-level messages within a cover image. Furthermore, we
establish a benchmark named Invisible Text (IVT), comprising a diverse set of
sentence-level texts as secret messages for evaluation. Finally, we present
$\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large
language models (LLMs) to embed high-level textual information, such as
sentences or even paragraphs, into images. Unlike traditional bit-level
counterparts, $\mathrm{S^2LM}$ enables the integration of semantically rich
content through a newly designed pipeline in which the LLM is involved
throughout the entire process. Both quantitative and qualitative experiments
demonstrate that our method effectively unlocks new semantic steganographic
capabilities for LLMs. The source code will be released soon.

</details>


### [62] [Canonical Space Representation for 4D Panoptic Segmentation of Articulated Objects](https://arxiv.org/abs/2511.05356)
*Manuel Gomes,Bogdan Raducanu,Miguel Oliveira*

Main category: cs.CV

TL;DR: 提出了Artic4D数据集和CanonSeg4D框架，用于4D铰接物体的全景分割，通过将物体部分映射到规范空间来提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了铰接物体的时间动态特性，且缺乏4D时间数据的探索和基准数据集。

Method: 提出CanonSeg4D框架，估计每帧偏移将物体部分映射到学习的规范空间，实现跨帧一致对齐。

Result: 在Artic4D数据集上的实验表明，CanonSeg4D在复杂场景下的全景分割精度优于现有方法。

Conclusion: 时间建模和规范对齐在动态物体理解中有效，为4D铰接物体感知的未来发展铺平道路。

Abstract: Articulated object perception presents significant challenges in computer
vision, particularly because most existing methods ignore temporal dynamics
despite the inherently dynamic nature of such objects. The use of 4D temporal
data has not been thoroughly explored in articulated object perception and
remains unexamined for panoptic segmentation. The lack of a benchmark dataset
further hurt this field. To this end, we introduce Artic4D as a new dataset
derived from PartNet Mobility and augmented with synthetic sensor data,
featuring 4D panoptic annotations and articulation parameters. Building on this
dataset, we propose CanonSeg4D, a novel 4D panoptic segmentation framework.
This approach explicitly estimates per-frame offsets mapping observed object
parts to a learned canonical space, thereby enhancing part-level segmentation.
The framework employs this canonical representation to achieve consistent
alignment of object parts across sequential frames. Comprehensive experiments
on Artic4D demonstrate that the proposed CanonSeg4D outperforms state of the
art approaches in panoptic segmentation accuracy in more complex scenarios.
These findings highlight the effectiveness of temporal modeling and canonical
alignment in dynamic object understanding, and pave the way for future advances
in 4D articulated object perception.

</details>


### [63] [Dense Motion Captioning](https://arxiv.org/abs/2511.05369)
*Shiyao Xu,Benedetta Liberatori,Gül Varol,Paolo Rota*

Main category: cs.CV

TL;DR: 提出了密集运动描述任务和CompMo数据集，开发了DEMO模型用于3D人体运动的时序定位和描述生成


<details>
  <summary>Details</summary>
Motivation: 当前3D人体运动与语言集成研究主要集中在文本到运动生成，而运动理解任务相对被忽视，现有数据集缺乏详细时序标注且多为短序列

Method: 构建CompMo大规模数据集（6万条运动序列），开发DEMO模型集成大语言模型和简单运动适配器，生成密集时序锚定描述

Result: DEMO在CompMo数据集和适应基准上显著优于现有方法，为3D运动理解和描述研究建立了坚实基础

Conclusion: 该工作填补了3D运动理解领域的空白，通过新任务、数据集和模型推动了该领域的发展

Abstract: Recent advances in 3D human motion and language integration have primarily
focused on text-to-motion generation, leaving the task of motion understanding
relatively unexplored. We introduce Dense Motion Captioning, a novel task that
aims to temporally localize and caption actions within 3D human motion
sequences. Current datasets fall short in providing detailed temporal
annotations and predominantly consist of short sequences featuring few actions.
To overcome these limitations, we present the Complex Motion Dataset (CompMo),
the first large-scale dataset featuring richly annotated, complex motion
sequences with precise temporal boundaries. Built through a carefully designed
data generation pipeline, CompMo includes 60,000 motion sequences, each
composed of multiple actions ranging from at least two to ten, accurately
annotated with their temporal extents. We further present DEMO, a model that
integrates a large language model with a simple motion adapter, trained to
generate dense, temporally grounded captions. Our experiments show that DEMO
substantially outperforms existing methods on CompMo as well as on adapted
benchmarks, establishing a robust baseline for future research in 3D motion
understanding and captioning.

</details>


### [64] [PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization](https://arxiv.org/abs/2511.05393)
*Zehui Feng,Tian Qiu,Tong Wu,Junxuan Li,Huayuan Xu,Ting Han*

Main category: cs.CV

TL;DR: PreResQ-R1是一个基于偏好-响应解耦强化学习的视觉质量评估框架，统一了绝对分数回归和相对排序一致性，通过双分支奖励设计实现细粒度、稳定且可解释的思维链推理。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在视觉质量评估中主要依赖监督微调或仅排序目标，导致推理浅层、分数校准差和跨域泛化能力有限。

Method: 提出偏好-响应解耦强化学习框架，采用双分支奖励公式分别建模样本内响应一致性和样本间偏好对齐，通过组相对策略优化进行优化。针对视频质量评估设计了全局-时间和局部-空间数据流策略。

Result: 仅用6K图像和28K视频进行强化微调，在10个IQA和5个VQA基准测试中达到最先进水平，IQA任务中SRCC和PLCC指标分别提升5.30%和2.15%。

Conclusion: PreResQ-R1不仅实现了定量性能提升，还产生了与人类对齐的推理轨迹，揭示了质量判断背后的感知线索。

Abstract: Visual Quality Assessment (QA) seeks to predict human perceptual judgments of
visual fidelity. While recent multimodal large language models (MLLMs) show
promise in reasoning about image and video quality, existing approaches mainly
rely on supervised fine-tuning or rank-only objectives, resulting in shallow
reasoning, poor score calibration, and limited cross-domain generalization. We
propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning
framework that unifies absolute score regression and relative ranking
consistency within a single reasoning-driven optimization scheme. Unlike prior
QA methods, PreResQ-R1 introduces a dual-branch reward formulation that
separately models intra-sample response coherence and inter-sample preference
alignment, optimized via Group Relative Policy Optimization (GRPO). This design
encourages fine-grained, stable, and interpretable chain-of-thought reasoning
about perceptual quality. To extend beyond static imagery, we further design a
global-temporal and local-spatial data flow strategy for Video Quality
Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and
28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5
VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30%
and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it
produces human-aligned reasoning traces that reveal the perceptual cues
underlying quality judgments. Code and model are available.

</details>


### [65] [PALM: A Dataset and Baseline for Learning Multi-subject Hand Prior](https://arxiv.org/abs/2511.05403)
*Zicong Fan,Edoardo Remelli,David Dimond,Fadime Sener,Liuhao Ge,Bugra Tekin,Cem Keskin,Shreyas Hampali*

Main category: cs.CV

TL;DR: 提出了PALM大规模手部数据集，包含263名受试者的13K高质量手部扫描和90K多视角图像，并开发了PALM-Net基线模型，通过基于物理的逆向渲染实现单图像手部虚拟形象个性化。


<details>
  <summary>Details</summary>
Motivation: 从图像创建高质量个性化手部虚拟形象具有挑战性，主要由于复杂的几何形状、外观和关节结构，特别是在无约束光照和有限视角下。现有数据集缺乏准确的3D几何、高分辨率多视角图像和多样化的受试者群体。

Method: 构建PALM大规模数据集，包含13K手部扫描和90K多视角图像，覆盖不同肤色、年龄和几何形状。开发PALM-Net基线模型，通过基于物理的逆向渲染学习手部几何和材质属性的多受试者先验。

Result: PALM数据集提供了丰富的真实世界手部数据资源，PALM-Net能够实现逼真、可重新照明的单图像手部虚拟形象个性化。

Conclusion: PALM的规模和多样性使其成为手部建模及相关研究的宝贵资源，解决了现有数据集的局限性，推动了高质量手部虚拟形象创建技术的发展。

Abstract: The ability to grasp objects, signal with gestures, and share emotion through
touch all stem from the unique capabilities of human hands. Yet creating
high-quality personalized hand avatars from images remains challenging due to
complex geometry, appearance, and articulation, particularly under
unconstrained lighting and limited views. Progress has also been limited by the
lack of datasets that jointly provide accurate 3D geometry, high-resolution
multiview imagery, and a diverse population of subjects. To address this, we
present PALM, a large-scale dataset comprising 13k high-quality hand scans from
263 subjects and 90k multi-view images, capturing rich variation in skin tone,
age, and geometry. To show its utility, we present a baseline PALM-Net, a
multi-subject prior over hand geometry and material properties learned via
physically based inverse rendering, enabling realistic, relightable
single-image hand avatar personalization. PALM's scale and diversity make it a
valuable real-world resource for hand modeling and related research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [66] [A hybrid solution approach for the Integrated Healthcare Timetabling Competition 2024](https://arxiv.org/abs/2511.04685)
*Daniela Guericke,Rolf van der Hulst,Asal Karimpour,Ieke Schrader,Matthias Walter*

Main category: cs.AI

TL;DR: 本文介绍了团队在2024年综合医疗排班竞赛中获得第三名的算法、实现和结果，采用混合整数规划、约束规划和模拟退火的三阶段分解方法。


<details>
  <summary>Details</summary>
Motivation: 解决医疗排班优化问题，通过多方法融合提高排班效率和质量。

Method: 结合混合整数规划、约束规划和模拟退火的三阶段分解方法，将问题分解为子问题求解。

Result: 在竞赛中获得第三名，并首次提供了基准实例的最优解下界。

Conclusion: 方法有效但仍有改进空间，指出了未来可优化的开放性问题。

Abstract: We report about the algorithm, implementation and results submitted to the
Integrated Healthcare Timetabling Competition 2024 by Team Twente, which scored
third in the competition. Our approach combines mixed-integer programming,
constraint programming and simulated annealing in a 3-phase solution approach
based on decomposition into subproblems. Next to describing our approach and
describing our design decisions, we share our insights and, for the first time,
lower bounds on the optimal solution values for the benchmark instances. We
finally highlight open problems for which we think that addressing them could
improve our approach even further.

</details>


### [67] [Epistemic Reject Option Prediction](https://arxiv.org/abs/2511.04855)
*Vojtech Franc,Jakub Paplham*

Main category: cs.AI

TL;DR: 本文提出了一种基于认知不确定性的拒绝选项预测器，当训练数据不足导致认知不确定性高时，模型会选择弃权，以最小化预期遗憾。


<details>
  <summary>Details</summary>
Motivation: 传统拒绝选项方法仅关注随机不确定性，假设训练数据充足使得认知不确定性可忽略。但在实际应用中，数据有限使得这一假设不成立，需要同时考虑认知不确定性。

Method: 基于贝叶斯学习，重新定义最优预测器为最小化预期遗憾的模型。当给定输入的遗憾超过指定拒绝成本时，模型选择弃权。

Result: 提出了第一个原则性框架，能够学习识别训练数据不足以做出可靠决策的输入。

Conclusion: 该框架为高风险应用中的不确定性量化提供了更全面的解决方案，特别适用于数据有限的实际场景。

Abstract: In high-stakes applications, predictive models must not only produce accurate
predictions but also quantify and communicate their uncertainty. Reject-option
prediction addresses this by allowing the model to abstain when prediction
uncertainty is high. Traditional reject-option approaches focus solely on
aleatoric uncertainty, an assumption valid only when large training data makes
the epistemic uncertainty negligible. However, in many practical scenarios,
limited data makes this assumption unrealistic. This paper introduces the
epistemic reject-option predictor, which abstains in regions of high epistemic
uncertainty caused by insufficient data. Building on Bayesian learning, we
redefine the optimal predictor as the one that minimizes expected regret -- the
performance gap between the learned model and the Bayes-optimal predictor with
full knowledge of the data distribution. The model abstains when the regret for
a given input exceeds a specified rejection cost. To our knowledge, this is the
first principled framework that enables learning predictors capable of
identifying inputs for which the training data is insufficient to make reliable
decisions.

</details>


### [68] [DMA: Online RAG Alignment with Human Feedback](https://arxiv.org/abs/2511.04880)
*Yu Bai,Yukai Miao,Dawei Wang,Li Chen,Fei Long,Rundi Zhai,Dan Li,Yanyu Ren,Tianfeng Liu,Hongtao Xie,Ce Yang,Xuhui Cai*

Main category: cs.AI

TL;DR: DMA是一个在线学习框架，通过多粒度人类反馈来动态调整RAG系统的检索排序，在保持基础检索能力的同时显著提升人机交互效果。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统依赖静态检索，无法适应动态的用户意图和内容变化，需要能够实时学习人类反馈的适应性框架。

Method: DMA将文档级、列表级和响应级信号整合到统一学习流程中：监督训练点对和列表排序器，基于响应偏好的策略优化，以及将知识蒸馏到轻量级评分器中。

Result: 在线工业部署显示人类参与度显著提升，离线测试在对话式QA任务(TriviaQA, HotpotQA)上取得明显进步，同时保持基础检索竞争力。

Conclusion: DMA为RAG系统提供了一种原则性的反馈驱动实时适应方法，在不牺牲基线能力的前提下实现动态优化。

Abstract: Retrieval-augmented generation (RAG) systems often rely on static retrieval,
limiting adaptation to evolving intent and content drift. We introduce Dynamic
Memory Alignment (DMA), an online learning framework that systematically
incorporates multi-granularity human feedback to align ranking in interactive
settings. DMA organizes document-, list-, and response-level signals into a
coherent learning pipeline: supervised training for pointwise and listwise
rankers, policy optimization driven by response-level preferences, and
knowledge distillation into a lightweight scorer for low-latency serving.
Throughout this paper, memory refers to the model's working memory, which is
the entire context visible to the LLM for In-Context Learning.
  We adopt a dual-track evaluation protocol mirroring deployment: (i)
large-scale online A/B ablations to isolate the utility of each feedback
source, and (ii) few-shot offline tests on knowledge-intensive benchmarks.
Online, a multi-month industrial deployment further shows substantial
improvements in human engagement. Offline, DMA preserves competitive
foundational retrieval while yielding notable gains on conversational QA
(TriviaQA, HotpotQA). Taken together, these results position DMA as a
principled approach to feedback-driven, real-time adaptation in RAG without
sacrificing baseline capability.

</details>


### [69] [Real-Time Reasoning Agents in Evolving Environments](https://arxiv.org/abs/2511.04898)
*Yule Wen,Yixin Ye,Yanzhe Zhang,Diyi Yang,Hao Zhu*

Main category: cs.AI

TL;DR: 本文提出了实时推理的新问题框架，研究在动态环境中部署语言模型的两种范式：反应式代理和规划式代理，并提出了同时结合两种范式的AgileThinker方法，在任务难度和时间压力增加时表现更优。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的智能体需要同时做出逻辑性和及时性的判断，而现有语言模型推理方法未能充分考虑环境的动态特性。

Method: 构建Real-Time Reasoning Gym测试平台，研究反应式代理（有限推理计算）和规划式代理（扩展推理计算）两种范式，并提出同时结合两者的AgileThinker方法。

Result: 实验表明即使最先进的模型在两种范式中都难以同时满足逻辑性和及时性要求，而AgileThinker在任务难度和时间压力增加时持续优于单一范式代理。

Conclusion: 实时推理是开发实用智能体的关键测试平台，AgileThinker通过平衡推理深度和响应延迟，为时间约束AI系统研究提供了基础。

Abstract: Agents in the real world must make not only logical but also timely
judgments. This requires continuous awareness of the dynamic environment:
hazards emerge, opportunities arise, and other agents act, while the agent's
reasoning is still unfolding. Despite advances in language model reasoning,
existing approaches fail to account for this dynamic nature. We introduce
real-time reasoning as a new problem formulation for agents in evolving
environments and build Real-Time Reasoning Gym to demonstrate it. We study two
paradigms for deploying language models in agents: (1) reactive agents, which
employ language models with bounded reasoning computation for rapid responses,
and (2) planning agents, which allow extended reasoning computation for complex
problems. Our experiments show that even state-of-the-art models struggle with
making logical and timely judgments in either paradigm. To address this
limitation, we propose AgileThinker, which simultaneously engages both
reasoning paradigms. AgileThinker consistently outperforms agents engaging only
one reasoning paradigm as the task difficulty and time pressure rise,
effectively balancing reasoning depth and response latency. Our work
establishes real-time reasoning as a critical testbed for developing practical
agents and provides a foundation for research in temporally constrained AI
systems, highlighting a path toward real-time capable agents.

</details>


### [70] [ORCHID: Orchestrated Retrieval-Augmented Classification with Human-in-the-Loop Intelligent Decision-Making for High-Risk Property](https://arxiv.org/abs/2511.04956)
*Maria Mahbub,Vanessa Lama,Sanjay Das,Brian Starks,Christopher Polchek,Saffell Silvers,Lauren Deck,Prasanna Balaprakash,Tirthankar Ghosal*

Main category: cs.AI

TL;DR: ORCHID是一个模块化代理系统，用于美国能源部的高风险财产分类，结合检索增强生成和人工监督，提供基于政策的可审计输出。


<details>
  <summary>Details</summary>
Motivation: 传统专家工作流程耗时、易积压，难以跟上不断变化的监管边界，需要更高效透明的合规决策系统。

Method: 使用小型协作代理（检索、描述精炼、分类器、验证器、反馈记录器），通过代理间消息协调和模型上下文协议实现模型无关的本地操作。

Result: 在真实高风险财产案例的初步测试中，ORCHID相比非代理基线提高了准确性和可追溯性，同时将不确定项目转交给领域专家。

Conclusion: ORCHID展示了在敏感能源部合规工作流程中实现可信赖大语言模型辅助的实用路径。

Abstract: High-Risk Property (HRP) classification is critical at U.S. Department of
Energy (DOE) sites, where inventories include sensitive and often dual-use
equipment. Compliance must track evolving rules designated by various export
control policies to make transparent and auditable decisions. Traditional
expert-only workflows are time-consuming, backlog-prone, and struggle to keep
pace with shifting regulatory boundaries. We demo ORCHID, a modular agentic
system for HRP classification that pairs retrieval-augmented generation (RAG)
with human oversight to produce policy-based outputs that can be audited. Small
cooperating agents, retrieval, description refiner, classifier, validator, and
feedback logger, coordinate via agent-to-agent messaging and invoke tools
through the Model Context Protocol (MCP) for model-agnostic on-premise
operation. The interface follows an Item to Evidence to Decision loop with
step-by-step reasoning, on-policy citations, and append-only audit bundles
(run-cards, prompts, evidence). In preliminary tests on real HRP cases, ORCHID
improves accuracy and traceability over a non-agentic baseline while deferring
uncertain items to Subject Matter Experts (SMEs). The demonstration shows
single item submission, grounded citations, SME feedback capture, and
exportable audit artifacts, illustrating a practical path to trustworthy LLM
assistance in sensitive DOE compliance workflows.

</details>


### [71] [Autonomous generation of different courses of action in mechanized combat operations](https://arxiv.org/abs/2511.05182)
*Johan Schubert,Patrik Hansen,Pontus Hörling,Ronnie Johansson*

Main category: cs.AI

TL;DR: 提出一种支持军事地面作战决策的方法论，为机械化营生成和评估行动方案，通过系统生成数千个行动替代方案并评估其预期结果。


<details>
  <summary>Details</summary>
Motivation: 在军事地面作战执行阶段支持决策制定，特别关注己方行动，帮助指挥官在动态战场环境中做出更好的决策。

Method: 从初始行动集开始，基于预期结果评估，系统生成数千个行动替代方案，并评估识别具有更优结果的替代行动方案。考虑对手状态和行动、单位组成、兵力比率、攻防类型和预期推进率等因素。

Result: 生成和评估过程并行工作，产生多样化的替代行动方案，便于基于先前评估行动管理新方案生成。随着战斗展开和条件变化，在序贯决策框架内为决策者制定修订的行动方案。

Conclusion: 该方法论能够有效支持军事地面作战中的动态决策过程，通过系统化的方案生成和评估机制，为指挥官提供优化的行动建议。

Abstract: In this paper, we propose a methodology designed to support decision-making
during the execution phase of military ground combat operations, with a focus
on one's actions. This methodology generates and evaluates recommendations for
various courses of action for a mechanized battalion, commencing with an
initial set assessed by their anticipated outcomes. It systematically produces
thousands of individual action alternatives, followed by evaluations aimed at
identifying alternative courses of action with superior outcomes. These
alternatives are appraised in light of the opponent's status and actions,
considering unit composition, force ratios, types of offense and defense, and
anticipated advance rates. Field manuals evaluate battle outcomes and
advancement rates. The processes of generation and evaluation work
concurrently, yielding a variety of alternative courses of action. This
approach facilitates the management of new course generation based on
previously evaluated actions. As the combat unfolds and conditions evolve,
revised courses of action are formulated for the decision-maker within a
sequential decision-making framework.

</details>


### [72] [Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance](https://arxiv.org/abs/2511.05311)
*Valeriu Dimidov,Faisal Hawlader,Sasan Jafarnejad,Raphaël Frank*

Main category: cs.AI

TL;DR: 本文探讨了基于大语言模型（LLM）的智能体在汽车行业预测性维护（PdM）数据清洗管道中的应用潜力，特别是在处理维护日志中的六种噪声类型方面表现出有效性。


<details>
  <summary>Details</summary>
Motivation: 汽车行业预测性维护面临经济约束、数据集可用性有限和专业知识短缺等挑战，LLM的进展为克服这些障碍提供了机会，加速PdM从研究到工业实践的转型。

Method: 评估LLM智能体在维护日志清洗任务中的表现，重点关注六种不同类型的噪声处理，包括拼写错误、缺失字段、近重复条目和错误日期等。

Result: 研究发现LLM在处理通用清洗任务方面表现有效，为未来工业应用提供了有前景的基础。尽管领域特定错误仍具挑战性，但结果显示通过专门训练和增强智能体能力可以进一步改进。

Conclusion: LLM智能体在预测性维护数据清洗方面具有显著潜力，特别是在处理维护日志中的常见噪声类型时表现良好，为工业应用奠定了基础。

Abstract: Economic constraints, limited availability of datasets for reproducibility
and shortages of specialized expertise have long been recognized as key
challenges to the adoption and advancement of predictive maintenance (PdM) in
the automotive sector. Recent progress in large language models (LLMs) presents
an opportunity to overcome these barriers and speed up the transition of PdM
from research to industrial practice. Under these conditions, we explore the
potential of LLM-based agents to support PdM cleaning pipelines. Specifically,
we focus on maintenance logs, a critical data source for training
well-performing machine learning (ML) models, but one often affected by errors
such as typos, missing fields, near-duplicate entries, and incorrect dates. We
evaluate LLM agents on cleaning tasks involving six distinct types of noise.
Our findings show that LLMs are effective at handling generic cleaning tasks
and offer a promising foundation for future industrial applications. While
domain-specific errors remain challenging, these results highlight the
potential for further improvements through specialized training and enhanced
agentic capabilities.

</details>


### [73] [Reasoning Is All You Need for Urban Planning AI](https://arxiv.org/abs/2511.05375)
*Sijie Yang,Jiatong Li,Filip Biljecki*

Main category: cs.AI

TL;DR: 提出了一个基于推理能力的城市规划AI框架，通过多智能体协作整合感知、基础和推理三个认知层，实现价值导向、规则约束和可解释的规划决策。


<details>
  <summary>Details</summary>
Motivation: 传统AI在城市规划中主要进行数据分析预测，但缺乏透明推理和决策能力。需要开发能够考虑约束条件、利益相关者价值并进行透明推理的AI辅助决策系统。

Method: 构建了包含感知层、基础层和推理层三个认知层的框架，整合分析、生成、验证、评估、协作和决策六个逻辑组件，通过多智能体协作实现规划决策。

Result: 开发了能够系统探索解决方案空间、验证法规合规性、透明权衡利弊的AI代理框架，增强了人类规划师的决策能力。

Conclusion: 该框架展示了AI代理如何通过计算推理能力增强而非替代人类判断，为城市规划提供透明、可解释的决策支持。

Abstract: AI has proven highly successful at urban planning analysis -- learning
patterns from data to predict future conditions. The next frontier is
AI-assisted decision-making: agents that recommend sites, allocate resources,
and evaluate trade-offs while reasoning transparently about constraints and
stakeholder values. Recent breakthroughs in reasoning AI -- CoT prompting,
ReAct, and multi-agent collaboration frameworks -- now make this vision
achievable.
  This position paper presents the Agentic Urban Planning AI Framework for
reasoning-capable planning agents that integrates three cognitive layers
(Perception, Foundation, Reasoning) with six logic components (Analysis,
Generation, Verification, Evaluation, Collaboration, Decision) through a
multi-agents collaboration framework. We demonstrate why planning decisions
require explicit reasoning capabilities that are value-based (applying
normative principles), rule-grounded (guaranteeing constraint satisfaction),
and explainable (generating transparent justifications) -- requirements that
statistical learning alone cannot fulfill. We compare reasoning agents with
statistical learning, present a comprehensive architecture with benchmark
evaluation metrics, and outline critical research challenges. This framework
shows how AI agents can augment human planners by systematically exploring
solution spaces, verifying regulatory compliance, and deliberating over
trade-offs transparently -- not replacing human judgment but amplifying it with
computational reasoning capabilities.

</details>
