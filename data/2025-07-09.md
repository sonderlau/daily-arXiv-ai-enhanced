<div id=toc></div>

# Table of Contents

- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.CV](#cs.CV) [Total: 81]
- [cs.AI](#cs.AI) [Total: 41]


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [1] [Site-Specific Parameterization of Ocean Spectra for Power Estimates of Wave Energy Converters](https://arxiv.org/abs/2507.05440)
*Rafael Baez Ramirez,Ethan J. Sloan,Carlos Alejandro Michelén Ströfer*

Main category: physics.ao-ph

TL;DR: 研究比较了使用2参数和4参数波谱模型对波浪能转换器（WEC）年均功率预测的影响，发现4参数模型（尤其是基于机器学习的自编码器）显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统2参数波谱模型无法充分捕捉实际海洋环境的波谱形状变化，导致WEC功率预测不准确，因此需要更复杂的模型。

Method: 研究对比了传统2参数波谱与两种4参数波谱（文献中的模型和新型机器学习自编码器）在年均功率预测中的表现。

Result: 4参数模型（尤其是自编码器）预测误差约为1%，优于2参数模型（误差-8%和1%）。4参数模型在整个参数空间中表现更稳定。

Conclusion: 4参数波谱模型（特别是基于机器学习的自编码器）能显著提高WEC功率预测精度，建议采用更复杂的资源表征方法。

Abstract: Estimating the mean annual power of a wave energy converter (WEC) through the
method of bins relies on a parametric representation of all possible sea
states. In practice, two-parameter spectra based on significant wave height and
energy period are ubiquitous. Two-parameter spectra have been shown
insufficient in capturing the range of spectral shapes that can occur in an
actual ocean environment. Furthermore, through sensitivity analysis, these
two-parameters have been shown to be insufficient for predicting power
performance of WECs. Four parameter spectra, which expand the parameter space
to include two additional shape parameters have been shown sufficient in
capturing sea state variance, but their effect on mean power estimates has not
been presented. This work directly looks at the effects of incorporating
4-parameter spectra into annual power estimates compared to using the
traditional 2-parameter spectra. We use two different 4-parameter spectra: one
from the literature and a novel machine learning-based autoencoder, presented
here. Both are shown to improve the information retained when parameterizing
spectra. The site-specific autoencoder performs consistently the best across
two case studies of mean annual power prediction, achieving an error around 1%
in each instance. The 2-parameter spectra resulted in less consistent
predictive performance, with errors of -8% and 1% in the two case studies. For
the case study where all three models performed well, it is shown that the low
error in the 2-parameter model is attributable to a symmetrical distribution of
large errors whereas both 4-parameter spectra result in relatively low errors
throughout the parametric space. These results highlight the need for more
sophisticated resource characterization methods for estimating the power
performance of WECs and suggest site-specific machine learning-based spectra
are an adequate option.

</details>


### [2] [HRRRCast: a data-driven emulator for regional weather forecasting at convection allowing scales](https://arxiv.org/abs/2507.05658)
*Daniel Abdi,Isidora Jankov,Paul Madden,Vanderlei Vargas,Timothy A. Smith,Sergey Frolov,Montgomery Flora,Corey Potvin*

Main category: physics.ao-ph

TL;DR: HRRRCast是一种基于机器学习的高效天气预测模型，包括ResNet和GNN两种架构，优于传统HRRR模型，尤其在轻雨阈值下表现突出。


<details>
  <summary>Details</summary>
Motivation: 提供一种计算高效的替代方案，以改进传统HRRR模型在天气预测中的性能。

Method: 采用ResNet和GNN架构，结合DDIM进行概率预测，并通过多时间步训练和贪婪推断策略优化长时预测。

Result: 在轻雨阈值（20 dBZ）下优于HRRR，在中雨阈值（30 dBZ）下表现相当，且风暴定位和空间细节更优。

Conclusion: HRRRCast为高效、数据驱动的区域天气预测提供了新方向，具有竞争性精度和集合能力。

Abstract: The High-Resolution Rapid Refresh (HRRR) model is a convection-allowing model
used in operational weather forecasting across the contiguous United States
(CONUS). To provide a computationally efficient alternative, we introduce
HRRRCast, a data-driven emulator built with advanced machine learning
techniques. HRRRCast includes two architectures: a ResNet-based model (ResHRRR)
and a Graph Neural Network-based model (GraphHRRR). ResHRRR uses convolutional
neural networks enhanced with squeeze-and-excitation blocks and Feature-wise
Linear Modulation, and supports probabilistic forecasting via the Denoising
Diffusion Implicit Model (DDIM). To better handle longer lead times, we train a
single model to predict multiple lead times (1h, 3h, and 6h), then use a greedy
rollout strategy during inference. When evaluated on composite reflectivity
over the full CONUS domain using ensembles of 3 to 10 members, ResHRRR
outperforms HRRR forecast at light rainfall threshold (20 dBZ) and achieves
competitive performance at moderate thresholds (30 dBZ). Our work advances the
StormCast model of Pathak et al. [21] by: a) training on the full CONUS domain,
b) using multiple lead times to improve long-range skill, c) training on
analysis data instead of the +1h post-analysis data inadvertently used in
StormCast, and d) incorporating future GFS states as inputs, enabling
downscaling that improves long-lead accuracy. Grid-, neighborhood-, and
object-based metrics confirm better storm placement, lower frequency bias, and
higher success ratios than HRRR. HRRRCast ensemble forecasts also maintain
sharper spatial detail, with power spectra more closely matching HRRR analysis.
While GraphHRRR underperforms in its current form, it lays groundwork for
future graph-based forecasting. HRRRCast represents a step toward efficient,
data-driven regional weather prediction with competitive accuracy and ensemble
capability.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [3] [Evolutionary and Coevolutionary Multi-Agent Design Choices and Dynamics](https://arxiv.org/abs/2507.05534)
*Erik Hemberg,Eric Liu,Lucille Fuller,Stephen Moskal,Una-May O'Reilly*

Main category: cs.NE

TL;DR: 研究了两种控制器表示方法，结合不同进化算法（包括一种新颖的LLM支持的变异算子），通过网络安全场景评估团队性能，发现语法进化算法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索不同控制器表示和进化算法组合对团队性能的影响，并比较单边优化与协同进化的效果。

Method: 结合两种控制器表示与多种进化算法（包括LLM支持的变异算子），在网络安全场景中评估单边优化和协同进化的团队性能。

Result: 语法进化算法表现最佳；协同进化减少了性能波动，而单边优化能实现更高的性能峰值。

Conclusion: 控制器表示和算法组合对团队性能有显著影响，协同进化与单边优化各有优劣。

Abstract: We investigate two representation alternatives for the controllers of teams
of cyber agents. We combine these controller representations with different
evolutionary algorithms, one of which introduces a novel LLM-supported mutation
operator. Using a cyber security scenario, we evaluate agent learning when one
side is trained to compete against a side that does not evolve and when two
sides coevolve with each other. This allows us to quantify the relative merits
and tradeoffs of representation and algorithm combinations in terms of team
performance. Our versions of grammatical evolution algorithms using grammars
that allow a controller to be expressed in code-like logic can achieve the best
team performance. The scenario also allows us to compare the performance impact
and dynamics of coevolution versus evolution under different combinations.
Across the algorithms and representations, we observe that coevolution reduces
the performance highs and lows of both sides while it induces fluctuations on
both sides. In contrast, when only one-side is optimized, performance peaks are
higher and is more sustained than when both sides are optimized with
coevolution.

</details>


### [4] [A Universal Framework for Large-Scale Multi-Objective Optimization Based on Particle Drift and Diffusion](https://arxiv.org/abs/2507.05847)
*Jia-Cheng Li,Min-Rong Chen,Guo-Qiang Zeng,Jian Weng,Man Wang,Jia-Lin Mai*

Main category: cs.NE

TL;DR: 提出了一种基于粒子漂移和扩散的通用框架，用于解决大规模多目标优化问题，显著提升了收敛性和多样性。


<details>
  <summary>Details</summary>
Motivation: 大规模多目标优化问题因高维决策变量对现有进化算法的收敛性和多样性性能提出了挑战。

Method: 将优化过程分为三个子阶段（两个粗调和一个细调），并根据当前阶段执行不同的漂移-扩散策略。

Result: 实验表明，该框架显著提升了MOEAs的收敛性和多样性，并提高了计算效率。

Conclusion: 该框架为大规模多目标优化问题提供了一种有效的解决方案。

Abstract: Large-scale multi-objective optimization poses challenges to existing
evolutionary algorithms in maintaining the performances of convergence and
diversity because of high dimensional decision variables. Inspired by the
motion of particles in physics, we propose a universal framework for
large-scale multi-objective optimization based on particle drift and diffusion
to solve these challenges in this paper. This framework innovatively divides
the optimization process into three sub-stages: two coarse-tuning sub-stages
and one fine-tuning sub-stage. Different strategies of drift-diffusion
operations are performed on the guiding solutions according to the current
sub-stage, ingeniously simulating the movement of particles under diverse
environmental conditions. Finally, representative evolutionary algorithms are
embedded into the proposed framework, and their effectiveness are evaluated
through comparative experiments on various large-scale multi-objective problems
with 1000 to 5000 decision variables. Moreover, comparative algorithms are
conducted on neural network training problems to validate the effectiveness of
the proposed framework in the practical problems. The experimental results
demonstrate that the framework proposed in this paper significantly enhances
the performance of convergence and diversity of MOEAs, and improves the
computational efficiency of algorithms in solving large-scale multi-objective
optimization problems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [5] [Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)](https://arxiv.org/abs/2507.05300)
*Nicholas Merchant,Haitz Sáez de Ocáriz Borde,Andrei Cristian Popescu,Carlos Garcia Jurado Suarez*

Main category: cs.CV

TL;DR: 论文提出通过结构化标注改进文本到图像生成模型的提示对齐能力，并发布了一个高质量数据集Re-LAION-Caption 19M。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型因大规模数据集（如LAION-5B）的噪声和无结构特性，导致提示对齐困难，用户需依赖繁琐的提示工程。

Method: 提出在训练中强制使用结构化标注（四部分模板：主题、场景、美学、相机细节），并基于Mistral 7B Instruct生成高质量数据集Re-LAION-Caption 19M。对PixArt-Σ和Stable Diffusion 2进行微调，比较结构化与非结构化标注的效果。

Result: 结构化标注版本在视觉问答（VQA）模型评估中表现出更高的文本-图像对齐分数。

Conclusion: 结构化标注能显著提升生成模型的可控性和对齐能力，数据集已公开。

Abstract: We argue that generative text-to-image models often struggle with prompt
adherence due to the noisy and unstructured nature of large-scale datasets like
LAION-5B. This forces users to rely heavily on prompt engineering to elicit
desirable outputs. In this work, we propose that enforcing a consistent caption
structure during training can significantly improve model controllability and
alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of
Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by
a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part
template: subject, setting, aesthetics, and camera details. We fine-tune
PixArt-$\Sigma$ and Stable Diffusion 2 using both structured and randomly
shuffled captions, and show that structured versions consistently yield higher
text-image alignment scores using visual question answering (VQA) models. The
dataset is publicly available at
https://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M.

</details>


### [6] [CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection](https://arxiv.org/abs/2507.05302)
*Binjia Zhou,Hengrui Lou,Lizhe Chen,Haoyuan Li,Dawei Luo,Shuai Chen,Jie Lei,Zunlei Feng,Yijun Bei*

Main category: cs.CV

TL;DR: 提出了一种名为CorrDetail的视觉细节增强自校正框架，用于可解释的人脸伪造检测，解决了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的快速发展，面部深度伪造对安全领域构成重大挑战，亟需有效的检测方法。现有方法存在解释性不足或易产生幻觉的问题。

Method: 设计了CorrDetail框架，通过错误引导问题校正真实伪造细节，并引入视觉细粒度细节增强模块，结合融合决策策略提升模型性能。

Result: 实验表明，CorrDetail在性能上优于最新方法，能准确识别伪造细节，并具备强泛化能力。

Conclusion: CorrDetail为可解释的人脸伪造检测提供了有效解决方案，具有实际应用潜力。

Abstract: With the swift progression of image generation technology, the widespread
emergence of facial deepfakes poses significant challenges to the field of
security, thus amplifying the urgent need for effective deepfake
detection.Existing techniques for face forgery detection can broadly be
categorized into two primary groups: visual-based methods and multimodal
approaches. The former often lacks clear explanations for forgery details,
while the latter, which merges visual and linguistic modalities, is more prone
to the issue of hallucinations.To address these shortcomings, we introduce a
visual detail enhanced self-correction framework, designated CorrDetail, for
interpretable face forgery detection. CorrDetail is meticulously designed to
rectify authentic forgery details when provided with error-guided questioning,
with the aim of fostering the ability to uncover forgery details rather than
yielding hallucinated responses. Additionally, to bolster the reliability of
its findings, a visual fine-grained detail enhancement module is incorporated,
supplying CorrDetail with more precise visual forgery details. Ultimately, a
fusion decision strategy is devised to further augment the model's
discriminative capacity in handling extreme samples, through the integration of
visual information compensation and model bias reduction.Experimental results
demonstrate that CorrDetail not only achieves state-of-the-art performance
compared to the latest methodologies but also excels in accurately identifying
forged details, all while exhibiting robust generalization capabilities.

</details>


### [7] [SoftReMish: A Novel Activation Function for Enhanced Convolutional Neural Networks for Visual Recognition Performance](https://arxiv.org/abs/2507.06148)
*Mustafa Bayram Gücen*

Main category: cs.CV

TL;DR: 提出了一种新的激活函数SoftReMish，用于提升CNN在图像分类任务中的性能，实验表明其优于ReLU、Tanh和Mish。


<details>
  <summary>Details</summary>
Motivation: 改进CNN在图像分类任务中的性能，通过设计新的激活函数SoftReMish。

Method: 在标准CNN架构中使用SoftReMish替换其他激活函数，并在MNIST数据集上评估性能。

Result: SoftReMish实现了最低训练损失（3.14e-8）和最高验证准确率（99.41%）。

Conclusion: SoftReMish表现出更好的收敛性和泛化能力，适合视觉识别任务。

Abstract: In this study, SoftReMish, a new activation function designed to improve the
performance of convolutional neural networks (CNNs) in image classification
tasks, is proposed. Using the MNIST dataset, a standard CNN architecture
consisting of two convolutional layers, max pooling, and fully connected layers
was implemented. SoftReMish was evaluated against popular activation functions
including ReLU, Tanh, and Mish by replacing the activation function in all
trainable layers. The model performance was assessed in terms of minimum
training loss and maximum validation accuracy. Results showed that SoftReMish
achieved a minimum loss (3.14e-8) and a validation accuracy (99.41%),
outperforming all other functions tested. These findings demonstrate that
SoftReMish offers better convergence behavior and generalization capability,
making it a promising candidate for visual recognition tasks.

</details>


### [8] [YOLO-APD: Enhancing YOLOv8 for Robust Pedestrian Detection on Complex Road Geometries](https://arxiv.org/abs/2507.05376)
*Aquino Joctum,John Kandiri*

Main category: cs.CV

TL;DR: YOLO-APD是一种改进的深度学习架构，基于YOLOv8，专为复杂道路上的行人检测设计，结合多种创新模块，显著提升了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 解决复杂几何道路（如Type-S曲面）上行人检测的挑战，弥补RGB相机方法的局限性。

Method: 引入YOLO-APD架构，集成SimAM注意力机制、C3Ghost模块、SimSPPF模块、Mish激活函数和IGD模块，并利用车辆转向动态进行自适应感兴趣区域处理。

Result: 在CARLA数据集上达到77.7% mAP@0.5:0.95和96%以上的行人召回率，实时处理速度100 FPS，显著优于基线模型。

Conclusion: YOLO-APD为低成本传感器提供了高精度、高效且适应性强的感知系统，提升了自动驾驶在复杂环境中的安全性和可靠性。

Abstract: Autonomous vehicle perception systems require robust pedestrian detection,
particularly on geometrically complex roadways like Type-S curved surfaces,
where standard RGB camera-based methods face limitations. This paper introduces
YOLO-APD, a novel deep learning architecture enhancing the YOLOv8 framework
specifically for this challenge. YOLO-APD integrates several key architectural
modifications: a parameter-free SimAM attention mechanism, computationally
efficient C3Ghost modules, a novel SimSPPF module for enhanced multi-scale
feature pooling, the Mish activation function for improved optimization, and an
Intelligent Gather & Distribute (IGD) module for superior feature fusion in the
network's neck. The concept of leveraging vehicle steering dynamics for
adaptive region-of-interest processing is also presented. Comprehensive
evaluations on a custom CARLA dataset simulating complex scenarios demonstrate
that YOLO-APD achieves state-of-the-art detection accuracy, reaching 77.7%
mAP@0.5:0.95 and exceptional pedestrian recall exceeding 96%, significantly
outperforming baseline models, including YOLOv8. Furthermore, it maintains
real-time processing capabilities at 100 FPS, showcasing a superior balance
between accuracy and efficiency. Ablation studies validate the synergistic
contribution of each integrated component. Evaluation on the KITTI dataset
confirms the architecture's potential while highlighting the need for domain
adaptation. This research advances the development of highly accurate,
efficient, and adaptable perception systems based on cost-effective sensors,
contributing to enhanced safety and reliability for autonomous navigation in
challenging, less-structured driving environments.

</details>


### [9] [Foreground-aware Virtual Staining for Accurate 3D Cell Morphological Profiling](https://arxiv.org/abs/2507.05383)
*Alexandr A. Kalinin,Paula Llanos,Theresa Maria Sommer,Giovanni Sestini,Xinhai Hou,Jonathan Z. Sexton,Xiang Wan,Ivo D. Dinov,Brian D. Athey,Nicolas Rivron,Anne E. Carpenter,Beth Cimini,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: Spotlight是一种虚拟染色方法，通过机器学习和形状感知损失函数，专注于生物相关结构，提升3D显微镜图像的形态表示。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟染色方法在训练时对所有像素平等处理，导致背景噪声和伪影被复制，而非聚焦于生物相关信号。

Method: Spotlight使用基于直方图的前景估计来掩蔽像素级损失，并在软阈值预测上计算Dice损失，实现形状感知学习。

Result: 在3D基准数据集上，Spotlight改善了形态表示，同时保持像素级精度，生成的虚拟染色更适合分割和分析等下游任务。

Conclusion: Spotlight通过专注于生物相关结构，提升了虚拟染色的质量和实用性。

Abstract: Microscopy enables direct observation of cellular morphology in 3D, with
transmitted-light methods offering low-cost, minimally invasive imaging and
fluorescence microscopy providing specificity and contrast. Virtual staining
combines these strengths by using machine learning to predict fluorescence
images from label-free inputs. However, training of existing methods typically
relies on loss functions that treat all pixels equally, thus reproducing
background noise and artifacts instead of focusing on biologically meaningful
signals. We introduce Spotlight, a simple yet powerful virtual staining
approach that guides the model to focus on relevant cellular structures.
Spotlight uses histogram-based foreground estimation to mask pixel-wise loss
and to calculate a Dice loss on soft-thresholded predictions for shape-aware
learning. Applied to a 3D benchmark dataset, Spotlight improves morphological
representation while preserving pixel-level accuracy, resulting in virtual
stains better suited for downstream tasks such as segmentation and profiling.

</details>


### [10] [From General to Specialized: The Need for Foundational Models in Agriculture](https://arxiv.org/abs/2507.05390)
*Vishal Nedungadi,Xingguo Xiong,Aike Potze,Ron Van Bree,Tao Lin,Marc Rußwurm,Ioannis N. Athanasiadis*

Main category: cs.CV

TL;DR: 该论文探讨了基础模型在农业任务中的应用，提出了农业基础模型（CropFM）的需求框架，并评估了现有模型的适用性，强调需要专门针对农业的模型。


<details>
  <summary>Details</summary>
Motivation: 随着人口增长和气候变化加剧，粮食安全成为全球关注的问题，需要创新解决方案。基础模型在农业监测中的应用尚未充分探索。

Method: 论文定量评估现有基础模型在农业任务中的效果，提出CropFM的需求框架，并比较和评估两种代表性模型。

Result: 研究发现现有通用基础模型在农业任务中表现有限，需要专门针对农业的模型。

Conclusion: 论文呼吁开发专门针对农业的基础模型，以更好地满足农业监测的需求。

Abstract: Food security remains a global concern as population grows and climate change
intensifies, demanding innovative solutions for sustainable agricultural
productivity. Recent advances in foundation models have demonstrated remarkable
performance in remote sensing and climate sciences, and therefore offer new
opportunities for agricultural monitoring. However, their application in
challenges related to agriculture-such as crop type mapping, crop phenology
estimation, and crop yield estimation-remains under-explored. In this work, we
quantitatively evaluate existing foundational models to assess their
effectivity for a representative set of agricultural tasks. From an
agricultural domain perspective, we describe a requirements framework for an
ideal agricultural foundation model (CropFM). We then survey and compare
existing general-purpose foundational models in this framework and empirically
evaluate two exemplary of them in three representative agriculture specific
tasks. Finally, we highlight the need for a dedicated foundational model
tailored specifically to agriculture.

</details>


### [11] [Enhancing Underwater Images Using Deep Learning with Subjective Image Quality Integration](https://arxiv.org/abs/2507.05393)
*Jose M. Montero,Jose-Luis Lisani*

Main category: cs.CV

TL;DR: 本文提出了一种结合人类主观评估的深度学习方法来提升水下图像质量，通过分类器和GAN模型实现显著改进。


<details>
  <summary>Details</summary>
Motivation: 利用深度学习改进水下图像质量，同时结合人类主观评估以更贴近实际需求。

Method: 首先训练分类器区分图像质量，再用GAN模型基于不同增强标准优化低质量图像。

Result: 模型在颜色保真度和清晰度等标准下显著提升了图像质量，定量和定性分析均验证了其有效性。

Conclusion: 结合人类主观评估的深度学习方法能有效提升水下图像质量，尤其在颜色和清晰度方面表现突出。

Abstract: Recent advances in deep learning, particularly neural networks, have
significantly impacted a wide range of fields, including the automatic
enhancement of underwater images. This paper presents a deep learning-based
approach to improving underwater image quality by integrating human subjective
assessments into the training process. To this end, we utilize publicly
available datasets containing underwater images labeled by experts as either
high or low quality. Our method involves first training a classifier network to
distinguish between high- and low-quality images. Subsequently, generative
adversarial networks (GANs) are trained using various enhancement criteria to
refine the low-quality images. The performance of the GAN models is evaluated
using quantitative metrics such as PSNR, SSIM, and UIQM, as well as through
qualitative analysis. Results demonstrate that the proposed model --
particularly when incorporating criteria such as color fidelity and image
sharpness -- achieves substantial improvements in both perceived and measured
image quality.

</details>


### [12] [pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2507.05394)
*Sajjad Ghiasvand,Mahnoosh Alizadeh,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: pFedMMA是一种个性化的联邦学习框架，利用多模态适配器优化视觉语言任务，平衡个性化和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在个性化联邦学习中往往牺牲泛化能力，难以处理未见类别或领域。

Method: pFedMMA采用多模态适配器，包含模态特定的上下投影层和全局共享投影层，通过非对称优化策略实现本地个性化与全局泛化的平衡。

Result: 在11个数据集上的实验表明，pFedMMA在个性化和泛化之间取得了最佳平衡，优于现有联邦提示调优方法。

Conclusion: pFedMMA通过多模态适配器和共享投影设计，实现了高效的个性化联邦学习，同时保持泛化能力。

Abstract: Vision-Language Models (VLMs) like CLIP have demonstrated remarkable
generalization in zero- and few-shot settings, but adapting them efficiently to
decentralized, heterogeneous data remains a challenge. While prompt tuning has
emerged as a popular parameter-efficient approach in personalized federated
learning, existing methods often sacrifice generalization in favor of
personalization, struggling particularly on unseen classes or domains. In this
work, we propose pFedMMA, the first personalized federated learning framework
that leverages multi-modal adapters for vision-language tasks. Each adapter
contains modality-specific up- and down-projection layers alongside a globally
shared projection that aligns cross-modal features. Our asymmetric optimization
strategy allows clients to locally adapt to personalized data distributions
while collaboratively training the shared projection to improve global
generalization. This design is also communication-efficient, as only the shared
component is exchanged during rounds. Through extensive experiments across
eleven datasets, including domain- and label-shift scenarios, we show that
pFedMMA achieves state-of-the-art trade-offs between personalization and
generalization, outperforming recent federated prompt tuning methods. The code
is available at https://github.com/sajjad-ucsb/pFedMMA.

</details>


### [13] [Neural-Driven Image Editing](https://arxiv.org/abs/2507.05397)
*Pengfei Zhou,Jie Xia,Xiaopeng Peng,Wangbo Zhao,Zilong Ye,Zekai Li,Suorong Yang,Jiadong Pan,Yuanxiang Chen,Ziqiao Wang,Kai Wang,Qian Zheng,Xiaojun Chang,Gang Pan,Shurong Dong,Kaipeng Zhang,Yang You*

Main category: cs.CV

TL;DR: LoongX提出了一种基于多模态神经生理信号的无手图像编辑方法，结合扩散模型和脑机接口技术，性能接近文本驱动方法，并在某些情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统图像编辑依赖手动操作，对运动或语言能力有限的人不友好。LoongX旨在通过脑机接口和生成模型实现无障碍编辑。

Method: LoongX利用扩散模型和多模态神经信号（EEG、fNIRS等），通过CS3模块和DGF模块处理信号异质性，并结合对比学习预训练编码器。

Result: 实验表明，LoongX性能接近文本驱动方法（如CLIP-I和DINO），且在结合语音信号时表现更优。

Conclusion: LoongX展示了神经驱动生成模型在无障碍图像编辑中的潜力，为认知驱动创意技术开辟了新方向。

Abstract: Traditional image editing typically relies on manual prompting, making it
labor-intensive and inaccessible to individuals with limited motor control or
language abilities. Leveraging recent advances in brain-computer interfaces
(BCIs) and generative models, we propose LoongX, a hands-free image editing
approach driven by multimodal neurophysiological signals. LoongX utilizes
state-of-the-art diffusion models trained on a comprehensive dataset of 23,928
image editing pairs, each paired with synchronized electroencephalography
(EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography
(PPG), and head motion signals that capture user intent. To effectively address
the heterogeneity of these signals, LoongX integrates two key modules. The
cross-scale state space (CS3) module encodes informative modality-specific
features. The dynamic gated fusion (DGF) module further aggregates these
features into a unified latent space, which is then aligned with edit semantics
via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train
the encoders using contrastive learning to align cognitive states with semantic
intentions from embedded natural language. Extensive experiments demonstrate
that LoongX achieves performance comparable to text-driven methods (CLIP-I:
0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural
signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results
highlight the promise of neural-driven generative models in enabling
accessible, intuitive image editing and open new directions for
cognitive-driven creative technologies. Datasets and code will be released to
support future work and foster progress in this emerging area.

</details>


### [14] [Motion Generation: A Survey of Generative Approaches and Benchmarks](https://arxiv.org/abs/2507.05419)
*Aliasghar Khani,Arianna Rampini,Bruno Roy,Larasika Nadela,Noa Kaplan,Evan Atherton,Derek Cheung,Jacky Bibliowicz*

Main category: cs.CV

TL;DR: 本文综述了运动生成领域的最新进展，重点分析了不同生成方法（如GANs、自编码器、自回归模型和扩散技术）的优缺点，并提供了分类、评估指标和数据集的详细概述。


<details>
  <summary>Details</summary>
Motivation: 运动生成在计算机视觉、图形学和机器人学中有广泛应用，但现有方法的多样性使得需要从生成方法的角度进行系统综述。

Method: 对2023年以来顶级会议发表的论文进行分类，分析生成策略、架构原理、条件机制和生成设置，并整理评估指标和数据集。

Result: 提供了运动生成领域的全面分类和比较，明确了当前挑战和未来方向。

Conclusion: 本文为研究人员和实践者提供了及时且基础的参考，帮助理解运动生成领域的最新进展和开放问题。

Abstract: Motion generation, the task of synthesizing realistic motion sequences from
various conditioning inputs, has become a central problem in computer vision,
computer graphics, and robotics, with applications ranging from animation and
virtual agents to human-robot interaction. As the field has rapidly progressed
with the introduction of diverse modeling paradigms including GANs,
autoencoders, autoregressive models, and diffusion-based techniques, each
approach brings its own advantages and limitations. This growing diversity has
created a need for a comprehensive and structured review that specifically
examines recent developments from the perspective of the generative approach
employed.
  In this survey, we provide an in-depth categorization of motion generation
methods based on their underlying generative strategies. Our main focus is on
papers published in top-tier venues since 2023, reflecting the most recent
advancements in the field. In addition, we analyze architectural principles,
conditioning mechanisms, and generation settings, and compile a detailed
overview of the evaluation metrics and datasets used across the literature. Our
objective is to enable clearer comparisons and identify open challenges,
thereby offering a timely and foundational reference for researchers and
practitioners navigating the rapidly evolving landscape of motion generation.

</details>


### [15] [Mastering Regional 3DGS: Locating, Initializing, and Editing with Diverse 2D Priors](https://arxiv.org/abs/2507.05426)
*Lanqing Guo,Yufei Wang,Hezhen Hu,Yan Zheng,Yeying Jin,Siyu Huang,Zhangyang Wang*

Main category: cs.CV

TL;DR: 提出了一种基于2D扩散编辑和逆渲染的3D场景局部编辑方法，解决了3D语义解析性能不足的问题，实现了高效且精确的区域编辑。


<details>
  <summary>Details</summary>
Motivation: 3D语义解析性能较差，限制了3D场景局部编辑的精确性，需要一种更高效的方法来提升编辑质量。

Method: 结合2D扩散编辑和逆渲染技术，通过2D模型识别编辑区域，并利用深度图初始化粗粒度3D高斯分布，逐步优化结构和纹理。

Result: 实验显示，该方法在性能上达到最优，同时实现了4倍的加速。

Conclusion: 该方法为3D场景局部编辑提供了一种高效且精确的解决方案，显著提升了编辑质量和速度。

Abstract: Many 3D scene editing tasks focus on modifying local regions rather than the
entire scene, except for some global applications like style transfer, and in
the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a
series of Gaussians, this structure allows for precise regional edits, offering
enhanced control over specific areas of the scene; however, the challenge lies
in the fact that 3D semantic parsing often underperforms compared to its 2D
counterpart, making targeted manipulations within 3D spaces more difficult and
limiting the fidelity of edits, which we address by leveraging 2D diffusion
editing to accurately identify modification regions in each view, followed by
inverse rendering for 3D localization, then refining the frontal view and
initializing a coarse 3DGS with consistent views and approximate shapes derived
from depth maps predicted by a 2D foundation model, thereby supporting an
iterative, view-consistent editing process that gradually enhances structural
details and textures to ensure coherence across perspectives. Experiments
demonstrate that our method achieves state-of-the-art performance while
delivering up to a $4\times$ speedup, providing a more efficient and effective
approach to 3D scene local editing.

</details>


### [16] [OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts](https://arxiv.org/abs/2507.05427)
*Shiting Xiao,Rishabh Kabra,Yuhang Li,Donghyun Lee,Joao Carreira,Priyadarshini Panda*

Main category: cs.CV

TL;DR: OpenWorldSAM扩展了SAM2模型，通过集成轻量级视觉语言模型的多模态嵌入，实现了开放词汇场景下的对象分割。


<details>
  <summary>Details</summary>
Motivation: 解决基于开放语言提示的对象分割问题，将文本语义精确映射到空间掩码，并处理多样化和未见过的类别。

Method: 结合SAM2和VLM的预训练组件，仅训练少量参数，引入位置嵌入和交叉注意力层增强空间理解。

Result: 在多个基准测试中达到最先进的开放词汇语义、实例和全景分割性能。

Conclusion: OpenWorldSAM在开放词汇场景下表现出色，具有高效性和强大的零样本泛化能力。

Abstract: The ability to segment objects based on open-ended language prompts remains a
critical challenge, requiring models to ground textual semantics into precise
spatial masks while handling diverse and unseen categories. We present
OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model
v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings
extracted from a lightweight vision-language model (VLM). Our approach is
guided by four key principles: i) Unified prompting: OpenWorldSAM supports a
diverse range of prompts, including category-level and sentence-level language
descriptions, providing a flexible interface for various segmentation tasks.
ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we
train only 4.5 million parameters on the COCO-stuff dataset, achieving
remarkable resource efficiency. iii) Instance Awareness: We enhance the model's
spatial understanding through novel positional tie-breaker embeddings and
cross-attention layers, enabling effective segmentation of multiple instances.
iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities,
generalizing well on unseen categories and an open vocabulary of concepts
without additional training. Extensive experiments demonstrate that
OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic,
instance, and panoptic segmentation across multiple benchmarks, including
ADE20k, PASCAL, ScanNet, and SUN-RGBD.

</details>


### [17] [Robotic System with AI for Real Time Weed Detection, Canopy Aware Spraying, and Droplet Pattern Evaluation](https://arxiv.org/abs/2507.05432)
*Inayat Rasool,Pappu Kumar Yadav,Amee Parmar,Hasan Mirzakhaninafchi,Rikesh Budhathoki,Zain Ul Abideen Usmani,Supriya Paudel,Ivan Perez Olivera,Eric Jone*

Main category: cs.CV

TL;DR: 开发了一种基于AI的变量喷雾系统，通过实时检测杂草并调整喷雾量，减少除草剂使用。


<details>
  <summary>Details</summary>
Motivation: 解决除草剂过度使用导致的成本增加、环境污染和杂草抗性问题。

Method: 集成YOLO11n和YOLO11n-seg深度学习模型，部署在NVIDIA Jetson Orin Nano上，通过Arduino控制喷嘴。

Result: 模型检测精度高（mAP@50 0.98），喷雾覆盖率随冠层大小动态调整（16.22%-21.65%）。

Conclusion: 结合实时深度学习和低成本硬件，选择性喷雾潜力显著，未来将扩展杂草检测种类并进行田间验证。

Abstract: Uniform and excessive herbicide application in modern agriculture contributes
to increased input costs, environmental pollution, and the emergence of
herbicide resistant weeds. To address these challenges, we developed a vision
guided, AI-driven variable rate sprayer system capable of detecting weed
presence, estimating canopy size, and dynamically adjusting nozzle activation
in real time. The system integrates lightweight YOLO11n and YOLO11n-seg deep
learning models, deployed on an NVIDIA Jetson Orin Nano for onboard inference,
and uses an Arduino Uno-based relay interface to control solenoid actuated
nozzles based on canopy segmentation results. Indoor trials were conducted
using 15 potted Hibiscus rosa sinensis plants of varying canopy sizes to
simulate a range of weed patch scenarios. The YOLO11n model achieved a mean
average precision (mAP@50) of 0.98, with a precision of 0.99 and a recall close
to 1.0. The YOLO11n-seg segmentation model achieved a mAP@50 of 0.48, precision
of 0.55, and recall of 0.52. System performance was validated using water
sensitive paper, which showed an average spray coverage of 24.22% in zones
where canopy was present. An upward trend in mean spray coverage from 16.22%
for small canopies to 21.46% and 21.65% for medium and large canopies,
respectively, demonstrated the system's capability to adjust spray output based
on canopy size in real time. These results highlight the potential of combining
real time deep learning with low-cost embedded hardware for selective herbicide
application. Future work will focus on expanding the detection capabilities to
include three common weed species in South Dakota: water hemp (Amaranthus
tuberculatus), kochia (Bassia scoparia), and foxtail (Setaria spp.), followed
by further validation in both indoor and field trials within soybean and corn
production systems.

</details>


### [18] [Driving as a Diagnostic Tool: Scenario-based Cognitive Assessment in Older Drivers From Driving Video](https://arxiv.org/abs/2507.05463)
*Md Zahid Hasan,Guillermo Basulto-Elias,Jun Ha Chang,Sahuna Hallmark,Matthew Rizzo,Anuj Sharma,Soumik Sarkar*

Main category: cs.CV

TL;DR: 提出一种基于自然驾驶视频和大视觉模型的老年驾驶员认知状态识别方法，用于早期发现认知衰退。


<details>
  <summary>Details</summary>
Motivation: 当前认知衰退（如阿尔茨海默病和轻度认知障碍）的诊断方法耗时且昂贵，导致漏诊率高。通过分析真实驾驶行为，提取与认知衰退相关的“数字指纹”，实现早期检测。

Method: 利用大视觉模型分析自然驾驶视频，提取驾驶行为特征，分类认知状态并预测疾病进展。

Result: 方法能够识别功能衰退的早期预警信号，支持主动干预策略。

Conclusion: 该研究为开发可扩展、非侵入性的监测系统提供了基础，有助于减轻老龄化社会中认知衰退的社会和经济负担。

Abstract: We introduce scenario-based cognitive status identification in older drivers
from Naturalistic driving videos and large vision models. In recent times,
cognitive decline, including Alzheimer's disease (AD) and mild cognitive
impairment (MCI), is often underdiagnosed due to the time-consuming and costly
nature of current diagnostic methods. By analyzing real-world driving behavior
captured through in-vehicle systems, this research aims to extract "digital
fingerprints" that correlate with functional decline and clinical features of
MCI and AD. Moreover, modern large vision models can draw meaningful insights
from everyday driving patterns of older patients to early detect cognitive
decline. We propose a framework that uses large vision models and naturalistic
driving videos to analyze driver behavior, classify cognitive status and
predict disease progression. We leverage the strong relationship between
real-world driving behavior as an observation of the current cognitive status
of the drivers where the vehicle can be utilized as a "diagnostic tool". Our
method identifies early warning signs of functional impairment, contributing to
proactive intervention strategies. This work enhances early detection and
supports the development of scalable, non-invasive monitoring systems to
mitigate the growing societal and economic burden of cognitive decline in the
aging population.

</details>


### [19] [Cloud Diffusion Part 1: Theory and Motivation](https://arxiv.org/abs/2507.05496)
*Andrew Randono*

Main category: cs.CV

TL;DR: 论文提出了一种基于尺度不变噪声的扩散模型（Cloud Diffusion Model），用于图像生成，相比传统白噪声模型，可能带来更快的推理速度、更好的高频细节和更强的可控性。


<details>
  <summary>Details</summary>
Motivation: 自然图像的统计特性具有尺度不变性，而传统扩散模型使用的白噪声与之不符，因此提出改进。

Method: 将尺度不变噪声引入扩散模型，替代传统的白噪声。

Result: 预计新模型能提升推理速度、高频细节和可控性。

Conclusion: Cloud Diffusion Model有望成为传统扩散模型的改进方案，具体效果将在后续论文中验证。

Abstract: Diffusion models for image generation function by progressively adding noise
to an image set and training a model to separate out the signal from the noise.
The noise profile used by these models is white noise -- that is, noise based
on independent normal distributions at each point whose mean and variance is
independent of the scale. By contrast, most natural image sets exhibit a type
of scale invariance in their low-order statistical properties characterized by
a power-law scaling. Consequently, natural images are closer (in a quantifiable
sense) to a different probability distribution that emphasizes large scale
correlations and de-emphasizes small scale correlations. These scale invariant
noise profiles can be incorporated into diffusion models in place of white
noise to form what we will call a ``Cloud Diffusion Model". We argue that these
models can lead to faster inference, improved high-frequency details, and
greater controllability. In a follow-up paper, we will build and train a Cloud
Diffusion Model that uses scale invariance at a fundamental level and compare
it to classic, white noise diffusion models.

</details>


### [20] [LoomNet: Enhancing Multi-View Image Generation via Latent Space Weaving](https://arxiv.org/abs/2507.05499)
*Giulio Federico,Fabio Carrara,Claudio Gennaro,Giuseppe Amato,Marco Di Benedetto*

Main category: cs.CV

TL;DR: LoomNet提出了一种多视图扩散架构，通过并行应用同一扩散模型生成一致的多视图图像，显著提升了3D网格质量。


<details>
  <summary>Details</summary>
Motivation: 从单张图像生成一致的多视图图像具有挑战性，缺乏空间一致性会降低3D重建质量。

Method: LoomNet通过并行运行扩散模型，构建共享潜在空间以确保视图一致性，并通过正交平面投影和融合生成一致的多视图图像。

Result: LoomNet在15秒内生成16个高质量且一致的多视图图像，在图像质量和重建指标上优于现有方法。

Conclusion: LoomNet通过共享潜在空间和并行扩散模型，实现了高效、一致的多视图图像生成，展示了多样性和创造性。

Abstract: Generating consistent multi-view images from a single image remains
challenging. Lack of spatial consistency often degrades 3D mesh quality in
surface reconstruction. To address this, we propose LoomNet, a novel multi-view
diffusion architecture that produces coherent images by applying the same
diffusion model multiple times in parallel to collaboratively build and
leverage a shared latent space for view consistency. Each viewpoint-specific
inference generates an encoding representing its own hypothesis of the novel
view from a given camera pose, which is projected onto three orthogonal planes.
For each plane, encodings from all views are fused into a single aggregated
plane. These aggregated planes are then processed to propagate information and
interpolate missing regions, combining the hypotheses into a unified, coherent
interpretation. The final latent space is then used to render consistent
multi-view images. LoomNet generates 16 high-quality and coherent views in just
15 seconds. In our experiments, LoomNet outperforms state-of-the-art methods on
both image quality and reconstruction metrics, also showing creativity by
producing diverse, plausible novel views from the same input.

</details>


### [21] [Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model](https://arxiv.org/abs/2507.05513)
*Mengyao Xu,Gabriel Moreira,Ronay Ak,Radek Osmulski,Yauhen Babakhin,Zhiding Yu,Benedikt Schifferer,Even Oldridge*

Main category: cs.CV

TL;DR: 论文提出了一种跨模态检索模型llama-nemoretriever-colembed，通过改进NVIDIA Eagle2 VLM架构并集成ColBERT机制，实现了在多基准测试中的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 满足跨模态检索系统日益增长的需求。

Method: 改进NVIDIA Eagle2 VLM架构，替换因果注意力为双向注意力，并集成ColBERT机制；采用两阶段训练策略。

Result: 3B模型在ViDoRe V1和V2上分别取得NDCG@5 91.0和63.5，领先于其他模型。

Conclusion: 该模型在检索精度上表现优异，但在存储和效率上存在权衡。

Abstract: Motivated by the growing demand for retrieval systems that operate across
modalities, we introduce llama-nemoretriever-colembed, a unified text-image
retrieval model that delivers state-of-the-art performance across multiple
benchmarks. We release two model variants, 1B and 3B. The 3B model achieves
state of the art performance, scoring NDCG@5 91.0 on ViDoRe V1 and 63.5 on
ViDoRe V2, placing first on both leaderboards as of June 27, 2025.
  Our approach leverages the NVIDIA Eagle2 Vision-Language model (VLM),
modifies its architecture by replacing causal attention with bidirectional
attention, and integrates a ColBERT-style late interaction mechanism to enable
fine-grained multimodal retrieval in a shared embedding space. While this
mechanism delivers superior retrieval accuracy, it introduces trade-offs in
storage and efficiency. We provide a comprehensive analysis of these
trade-offs. Additionally, we adopt a two-stage training strategy to enhance the
model's retrieval capabilities.

</details>


### [22] [Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception](https://arxiv.org/abs/2507.05536)
*Moseli Mots'oehli,Feimei Chen,Hok Wai Chan,Itumeleng Tlali,Thulani Babeli,Kyungim Baek,Huaijin Chen*

Main category: cs.CV

TL;DR: 提出了一种增强低成本单目行车记录仪数据的流程，模拟非洲驾驶场景中的光学失真和天气干扰，并发布了工具包和基准数据集。


<details>
  <summary>Details</summary>
Motivation: 解决非洲等发展中地区自动驾驶数据集稀缺的问题，提升低资源环境下的感知能力。

Method: 使用折射模块模拟低质量镜头和空气湍流的光学效果，天气模块添加雾和镜头眩光，并通过三种图像恢复模型进行基准测试。

Result: 发布了包含失真工具包、增强数据集和基准结果的开源资源。

Conclusion: 该方法为非洲等低资源环境下的自动驾驶感知研究提供了低成本解决方案。

Abstract: The scarcity of autonomous vehicle datasets from developing regions,
particularly across Africa's diverse urban, rural, and unpaved roads, remains a
key obstacle to robust perception in low-resource settings. We present a
procedural augmentation pipeline that enhances low-cost monocular dashcam
footage with realistic refractive distortions and weather-induced artifacts
tailored to challenging African driving scenarios. Our refractive module
simulates optical effects from low-quality lenses and air turbulence, including
lens distortion, Perlin noise, Thin-Plate Spline (TPS), and divergence-free
(incompressible) warps. The weather module adds homogeneous fog, heterogeneous
fog, and lens flare. To establish a benchmark, we provide baseline performance
using three image restoration models. To support perception research in
underrepresented African contexts, without costly data collection, labeling, or
simulation, we release our distortion toolkit, augmented dataset splits, and
benchmark results.

</details>


### [23] [ReLayout: Integrating Relation Reasoning for Content-aware Layout Generation with Multi-modal Large Language Models](https://arxiv.org/abs/2507.05568)
*Jiaxu Tian,Xuehui Yu,Yaoxing Wang,Pan Wang,Guangqian Guo,Shan Gao*

Main category: cs.CV

TL;DR: ReLayout利用关系链式思维（relation-CoT）改进内容感知布局生成，通过明确关系定义和布局原型重平衡采样器，解决现有LLM方法在空间关系解释和多样性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的布局生成方法未能充分解释视觉主题与设计元素间的空间关系，导致生成布局结构不合理且缺乏多样性。

Method: 引入明确的关系定义（如区域、显著性和边距），将布局分解为结构化子布局；设计布局原型重平衡采样器，量化不同布局风格。

Result: 实验表明，ReLayout在结构性和多样性上优于基线方法，生成更符合人类审美且可解释性更强的布局。

Conclusion: ReLayout通过关系链式思维和原型重平衡，显著提升了布局生成的质量和多样性。

Abstract: Content-aware layout aims to arrange design elements appropriately on a given
canvas to convey information effectively. Recently, the trend for this task has
been to leverage large language models (LLMs) to generate layouts
automatically, achieving remarkable performance. However, existing LLM-based
methods fail to adequately interpret spatial relationships among visual themes
and design elements, leading to structural and diverse problems in layout
generation. To address this issue, we introduce ReLayout, a novel method that
leverages relation-CoT to generate more reasonable and aesthetically coherent
layouts by fundamentally originating from design concepts. Specifically, we
enhance layout annotations by introducing explicit relation definitions, such
as region, salient, and margin between elements, with the goal of decomposing
the layout into smaller, structured, and recursive layouts, thereby enabling
the generation of more structured layouts. Furthermore, based on these defined
relationships, we introduce a layout prototype rebalance sampler, which defines
layout prototype features across three dimensions and quantifies distinct
layout styles. This sampler addresses uniformity issues in generation that
arise from data bias in the prototype distribution balance process. Extensive
experimental results verify that ReLayout outperforms baselines and can
generate structural and diverse layouts that are more aligned with human
aesthetics and more explainable.

</details>


### [24] [Multi-Modal Face Anti-Spoofing via Cross-Modal Feature Transitions](https://arxiv.org/abs/2507.05575)
*Jun-Xiong Chong,Fang-Yu Hsu,Ming-Tsung Hsu,Yi-Ting Lin,Kai-Heng Chien,Chiou-Ting Hsu,Pei-Kai Huang*

Main category: cs.CV

TL;DR: 提出了一种跨模态转换引导网络（CTNet），用于解决多模态人脸防伪（FAS）中的分布差异和模态缺失问题。


<details>
  <summary>Details</summary>
Motivation: 多模态FAS中，不同模态的数据分布差异大，且测试时可能缺失某些模态。活体样本的跨模态特征转换更一致，而活体与伪造样本的转换不一致。

Method: 通过一致性的跨模态特征转换构建泛化特征空间，并学习不一致的转换以检测异常攻击。同时，从RGB模态学习补充的红外和深度特征以应对模态缺失。

Result: 实验表明，CTNet在大多数协议下优于现有的多模态FAS方法。

Conclusion: CTNet有效解决了多模态FAS中的分布差异和模态缺失问题，提升了检测性能。

Abstract: Multi-modal face anti-spoofing (FAS) aims to detect genuine human presence by
extracting discriminative liveness cues from multiple modalities, such as RGB,
infrared (IR), and depth images, to enhance the robustness of biometric
authentication systems. However, because data from different modalities are
typically captured by various camera sensors and under diverse environmental
conditions, multi-modal FAS often exhibits significantly greater distribution
discrepancies across training and testing domains compared to single-modal FAS.
Furthermore, during the inference stage, multi-modal FAS confronts even greater
challenges when one or more modalities are unavailable or inaccessible. In this
paper, we propose a novel Cross-modal Transition-guided Network (CTNet) to
tackle the challenges in the multi-modal FAS task. Our motivation stems from
that, within a single modality, the visual differences between live faces are
typically much smaller than those of spoof faces. Additionally, feature
transitions across modalities are more consistent for the live class compared
to those between live and spoof classes. Upon this insight, we first propose
learning consistent cross-modal feature transitions among live samples to
construct a generalized feature space. Next, we introduce learning the
inconsistent cross-modal feature transitions between live and spoof samples to
effectively detect out-of-distribution (OOD) attacks during inference. To
further address the issue of missing modalities, we propose learning
complementary infrared (IR) and depth features from the RGB modality as
auxiliary modalities. Extensive experiments demonstrate that the proposed CTNet
outperforms previous two-class multi-modal FAS methods across most protocols.

</details>


### [25] [Semi-Supervised Defect Detection via Conditional Diffusion and CLIP-Guided Noise Filtering](https://arxiv.org/abs/2507.05588)
*Shuai Li,Shihan Chen,Wanru Geng,Zhaohua Xu,Xiaolu Liu,Can Dong,Zhen Tian,Changlin Chen*

Main category: cs.CV

TL;DR: 提出了一种基于条件扩散的半监督缺陷检测框架（DSYM），通过两阶段协作训练和联合优化策略，显著提高了数据效率和检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统工业缺陷检测方法效率低、成本高且鲁棒性差，需要一种更高效、低依赖标注数据的解决方案。

Method: 采用半监督框架，结合条件扩散模型生成多尺度伪缺陷样本，并利用CLIP跨模态特征过滤噪声。

Result: 在NEU-DET数据集上，mAP@0.5达到78.4%（与传统监督方法相同标注量）和75.1%（仅需40%标注数据）。

Conclusion: DSYM为工业质检提供了一种高精度、低标注依赖的缺陷检测解决方案，已开源。

Abstract: In the realm of industrial quality inspection, defect detection stands as a
critical component, particularly in high-precision, safety-critical sectors
such as automotive components aerospace, and medical devices. Traditional
methods, reliant on manual inspection or early image processing algorithms,
suffer from inefficiencies, high costs, and limited robustness. This paper
introduces a semi-supervised defect detection framework based on conditional
diffusion (DSYM), leveraging a two-stage collaborative training mechanism and a
staged joint optimization strategy. The framework utilizes labeled data for
initial training and subsequently incorporates unlabeled data through the
generation of pseudo-labels. A conditional diffusion model synthesizes
multi-scale pseudo-defect samples, while a CLIP cross-modal feature-based noise
filtering mechanism mitigates label contamination. Experimental results on the
NEU-DET dataset demonstrate a 78.4% mAP@0.5 with the same amount of labeled
data as traditional supervised methods, and 75.1% mAP@0.5 with only 40% of the
labeled data required by the original supervised model, showcasing significant
advantages in data efficiency. This research provides a high-precision,
low-labeling-dependent solution for defect detection in industrial quality
inspection scenarios. The work of this article has been open-sourced at
https://github.com/cLin-c/Semisupervised-DSYM.

</details>


### [26] [GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field](https://arxiv.org/abs/2507.05594)
*Zhizhuo Pang,Zhihui Ke,Xiaobo Zhou,Tie Qiu*

Main category: cs.CV

TL;DR: 提出了一种基于2D高斯的视频表示方法GSVR，显著提高了解码速度和训练效率，同时保持高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于卷积网络的视频表示方法解码速度慢、训练时间长，限制了实际应用。

Method: 提出混合变形场建模视频动态，结合三平面运动和多项式运动；采用动态感知时间切片策略自适应分组；引入量化感知微调避免性能下降。

Result: 在Bunny数据集上达到800+FPS和35+PSNR，训练时间仅2秒/帧；解码速度比其他方法快10倍。

Conclusion: GSVR在视频插值和压缩任务中表现优异，优于现有方法。

Abstract: Implicit neural representations for video have been recognized as a novel and
promising form of video representation. Existing works pay more attention to
improving video reconstruction quality but little attention to the decoding
speed. However, the high computation of convolutional network used in existing
methods leads to low decoding speed. Moreover, these convolution-based video
representation methods also suffer from long training time, about 14 seconds
per frame to achieve 35+ PSNR on Bunny. To solve the above problems, we propose
GSVR, a novel 2D Gaussian-based video representation, which achieves 800+ FPS
and 35+ PSNR on Bunny, only needing a training time of $2$ seconds per frame.
Specifically, we propose a hybrid deformation field to model the dynamics of
the video, which combines two motion patterns, namely the tri-plane motion and
the polynomial motion, to deal with the coupling of camera motion and object
motion in the video. Furthermore, we propose a Dynamic-aware Time Slicing
strategy to adaptively divide the video into multiple groups of pictures(GOP)
based on the dynamic level of the video in order to handle large camera motion
and non-rigid movements. Finally, we propose quantization-aware fine-tuning to
avoid performance reduction after quantization and utilize image codecs to
compress Gaussians to achieve a compact representation. Experiments on the
Bunny and UVG datasets confirm that our method converges much faster than
existing methods and also has 10x faster decoding speed compared to other
methods. Our method has comparable performance in the video interpolation task
to SOTA and attains better video compression performance than NeRV.

</details>


### [27] [PaddleOCR 3.0 Technical Report](https://arxiv.org/abs/2507.05595)
*Cheng Cui,Ting Sun,Manhui Lin,Tingquan Gao,Yubo Zhang,Jiaxuan Liu,Xueqing Wang,Zelun Zhang,Changda Zhou,Hongen Liu,Yue Zhang,Wenyu Lv,Kui Huang,Yichao Zhang,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: PaddleOCR 3.0是一个开源的OCR工具包，提供多语言文本识别、文档解析和关键信息提取功能，参数少但性能媲美大型视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 满足大语言模型时代对文档理解的需求。

Method: 提出PP-OCRv5、PP-StructureV3和PP-ChatOCRv4三种解决方案，支持多语言文本识别、分层文档解析和关键信息提取。

Result: 参数少于1亿的模型在准确性和效率上媲美数十亿参数的视觉语言模型。

Conclusion: PaddleOCR 3.0为开发者提供了高质量的OCR模型库和高效工具，支持异构硬件加速，便于构建智能文档应用。

Abstract: This technical report introduces PaddleOCR 3.0, an Apache-licensed
open-source toolkit for OCR and document parsing. To address the growing demand
for document understanding in the era of large language models, PaddleOCR 3.0
presents three major solutions: (1) PP-OCRv5 for multilingual text recognition,
(2) PP-StructureV3 for hierarchical document parsing, and (3) PP-ChatOCRv4 for
key information extraction. Compared to mainstream vision-language models
(VLMs), these models with fewer than 100 million parameters achieve competitive
accuracy and efficiency, rivaling billion-parameter VLMs. In addition to
offering a high-quality OCR model library, PaddleOCR 3.0 provides efficient
tools for training, inference, and deployment, supports heterogeneous hardware
acceleration, and enables developers to easily build intelligent document
applications.

</details>


### [28] [Rethinking Layered Graphic Design Generation with a Top-Down Approach](https://arxiv.org/abs/2507.05601)
*Jingye Chen,Zhaowen Wang,Nanxuan Zhao,Li Zhang,Difan Liu,Jimei Yang,Qifeng Chen*

Main category: cs.CV

TL;DR: Accordion框架将AI生成的平面设计转换为可编辑的分层设计，并通过用户提示优化文本内容。


<details>
  <summary>Details</summary>
Motivation: AI生成的设计缺乏可编辑性，但能启发设计师。Accordion旨在解决这一问题。

Method: 基于视觉语言模型（VLM），分三阶段处理，结合SAM和元素移除模型。

Result: 在DesignIntention基准测试中表现优异，支持多种设计任务。

Conclusion: Accordion为AI生成设计提供可编辑性，提升设计师效率。

Abstract: Graphic design is crucial for conveying ideas and messages. Designers usually
organize their work into objects, backgrounds, and vectorized text layers to
simplify editing. However, this workflow demands considerable expertise. With
the rise of GenAI methods, an endless supply of high-quality graphic designs in
pixel format has become more accessible, though these designs often lack
editability. Despite this, non-layered designs still inspire human designers,
influencing their choices in layouts and text styles, ultimately guiding the
creation of layered designs. Motivated by this observation, we propose
Accordion, a graphic design generation framework taking the first attempt to
convert AI-generated designs into editable layered designs, meanwhile refining
nonsensical AI-generated text with meaningful alternatives guided by user
prompts. It is built around a vision language model (VLM) playing distinct
roles in three curated stages. For each stage, we design prompts to guide the
VLM in executing different tasks. Distinct from existing bottom-up methods
(e.g., COLE and Open-COLE) that gradually generate elements to create layered
designs, our approach works in a top-down manner by using the visually
harmonious reference image as global guidance to decompose each layer.
Additionally, it leverages multiple vision experts such as SAM and element
removal models to facilitate the creation of graphic layers. We train our
method using the in-house graphic design dataset Design39K, augmented with
AI-generated design images coupled with refined ground truth created by a
customized inpainting model. Experimental results and user studies by designers
show that Accordion generates favorable results on the DesignIntention
benchmark, including tasks such as text-to-template, adding text to background,
and text de-rendering, and also excels in creating design variations.

</details>


### [29] [Kernel Density Steering: Inference-Time Scaling via Mode Seeking for Image Restoration](https://arxiv.org/abs/2507.05604)
*Yuyang Hu,Kangfu Mei,Mojtaba Sahraee-Ardakan,Ulugbek S. Kamilov,Peyman Milanfar,Mauricio Delbracio*

Main category: cs.CV

TL;DR: Kernel Density Steering (KDS) 是一种新的推理时框架，通过显式局部模式搜索提升扩散模型的图像恢复质量，减少伪影。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像恢复中常出现不一致的保真度和不良伪影，KDS旨在解决这一问题。

Method: KDS使用N粒子扩散样本集合，通过计算局部核密度估计梯度，将样本引导至共享的高密度区域。

Result: KDS显著提升了超分辨率和图像修复任务的定量和定性性能。

Conclusion: KDS作为一种即插即用框架，无需重新训练或外部验证器，可有效提升扩散模型的输出质量。

Abstract: Diffusion models show promise for image restoration, but existing methods
often struggle with inconsistent fidelity and undesirable artifacts. To address
this, we introduce Kernel Density Steering (KDS), a novel inference-time
framework promoting robust, high-fidelity outputs through explicit local
mode-seeking. KDS employs an $N$-particle ensemble of diffusion samples,
computing patch-wise kernel density estimation gradients from their collective
outputs. These gradients steer patches in each particle towards shared,
higher-density regions identified within the ensemble. This collective local
mode-seeking mechanism, acting as "collective wisdom", steers samples away from
spurious modes prone to artifacts, arising from independent sampling or model
imperfections, and towards more robust, high-fidelity structures. This allows
us to obtain better quality samples at the expense of higher compute by
simultaneously sampling multiple particles. As a plug-and-play framework, KDS
requires no retraining or external verifiers, seamlessly integrating with
various diffusion samplers. Extensive numerical validations demonstrate KDS
substantially improves both quantitative and qualitative performance on
challenging real-world super-resolution and image inpainting tasks.

</details>


### [30] [Generative Head-Mounted Camera Captures for Photorealistic Avatars](https://arxiv.org/abs/2507.05620)
*Shaojie Bai,Seunghyeon Seo,Yida Wang,Chenghui Li,Owen Wang,Te-Li Wang,Tianyang Ma,Jason Saragih,Shih-En Wei,Nojun Kwak,Hyung Jun Kim*

Main category: cs.CV

TL;DR: 提出了一种名为GenHMC的生成方法，利用未配对的HMC数据生成高质量的合成HMC图像，解决了传统方法依赖配对数据的问题。


<details>
  <summary>Details</summary>
Motivation: 在VR/AR中实现逼真的虚拟角色动画面临挑战，主要因难以获取面部真实状态数据，且传统方法依赖昂贵的配对数据采集。

Method: 提出Generative HMC (GenHMC)，利用大量未配对的HMC数据，直接生成高质量合成HMC图像，解耦面部表情与外观。

Result: 方法能准确解耦输入条件信号，泛化到未见过的身份，并提升数据效率和准确性。

Conclusion: GenHMC通过生成合成HMC图像，解决了数据采集难题，提升了虚拟角色动画的逼真度和效率。

Abstract: Enabling photorealistic avatar animations in virtual and augmented reality
(VR/AR) has been challenging because of the difficulty of obtaining ground
truth state of faces. It is physically impossible to obtain synchronized images
from head-mounted cameras (HMC) sensing input, which has partial observations
in infrared (IR), and an array of outside-in dome cameras, which have full
observations that match avatars' appearance. Prior works relying on
analysis-by-synthesis methods could generate accurate ground truth, but suffer
from imperfect disentanglement between expression and style in their
personalized training. The reliance of extensive paired captures (HMC and dome)
for the same subject makes it operationally expensive to collect large-scale
datasets, which cannot be reused for different HMC viewpoints and lighting. In
this work, we propose a novel generative approach, Generative HMC (GenHMC),
that leverages large unpaired HMC captures, which are much easier to collect,
to directly generate high-quality synthetic HMC images given any conditioning
avatar state from dome captures. We show that our method is able to properly
disentangle the input conditioning signal that specifies facial expression and
viewpoint, from facial appearance, leading to more accurate ground truth.
Furthermore, our method can generalize to unseen identities, removing the
reliance on the paired captures. We demonstrate these breakthroughs by both
evaluating synthetic HMC images and universal face encoders trained from these
new HMC-avatar correspondences, which achieve better data efficiency and
state-of-the-art accuracy.

</details>


### [31] [AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework](https://arxiv.org/abs/2507.05621)
*Suoxiang Zhang,Xiaxi Li,Hongrui Chang,Zhuoyan Hou,Guoxin Wu,Ronghua Ji*

Main category: cs.CV

TL;DR: AdaptaGen是一个分层语义优化框架，通过矩阵提示优化和多视角理解解决领域特定图像生成的语义准确性和细节保真问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在领域特定图像生成中存在语义理解和视觉表示的分离，以及领域特定语义约束的不足，导致生成结果出现幻觉和语义偏差。

Method: 提出AdaptaGen框架，结合矩阵提示优化和多视角理解，设计跨模态适应机制和两阶段标题语义转换。

Result: 实验结果表明，AdaptaGen在40个类别上表现优异，仅需每类别16张图像即可显著提升图像质量、多样性和语义一致性。

Conclusion: AdaptaGen通过分层语义优化和跨模态适应，有效解决了领域特定图像生成的语义和视觉问题。

Abstract: Domain-specific image generation aims to produce high-quality visual content
for specialized fields while ensuring semantic accuracy and detail fidelity.
However, existing methods exhibit two critical limitations: First, current
approaches address prompt engineering and model adaptation separately,
overlooking the inherent dependence between semantic understanding and visual
representation in specialized domains. Second, these techniques inadequately
incorporate domain-specific semantic constraints during content synthesis,
resulting in generation outcomes that exhibit hallucinations and semantic
deviations. To tackle these issues, we propose AdaptaGen, a hierarchical
semantic optimization framework that integrates matrix-based prompt
optimization with multi-perspective understanding, capturing comprehensive
semantic relationships from both global and local perspectives. To mitigate
hallucinations in specialized domains, we design a cross-modal adaptation
mechanism, which, when combined with intelligent content synthesis, enables
preserving core thematic elements while incorporating diverse details across
images. Additionally, we introduce a two-phase caption semantic transformation
during the generation phase. This approach maintains semantic coherence while
enhancing visual diversity, ensuring the generated images adhere to
domain-specific constraints. Experimental results confirm our approach's
effectiveness, with our framework achieving superior performance across 40
categories from diverse datasets using only 16 images per category,
demonstrating significant improvements in image quality, diversity, and
semantic consistency.

</details>


### [32] [OFFSET: Segmentation-based Focus Shift Revision for Composed Image Retrieval](https://arxiv.org/abs/2507.05631)
*Zhiwei Chen,Yupeng Hu,Zixu Li,Zhiheng Fu,Xuemeng Song,Liqiang Nie*

Main category: cs.CV

TL;DR: 论文提出了一种基于焦点映射的特征提取器（OFFSET），用于解决组合图像检索中的视觉数据不均匀性和文本数据优先级问题。


<details>
  <summary>Details</summary>
Motivation: 组合图像检索（CIR）能灵活表达用户需求，但现有方法忽略了视觉数据的不均匀性和文本数据的优先级，导致查询特征退化和视觉焦点偏差。

Method: 提出OFFSET网络，包含主导部分分割和双焦点映射模块，以及文本引导的焦点修正模块，以减少噪声干扰并增强修改焦点的感知。

Result: 在四个基准数据集上的实验证明了OFFSET的优越性。

Conclusion: OFFSET通过改进特征提取和焦点修正，显著提升了组合图像检索的性能。

Abstract: Composed Image Retrieval (CIR) represents a novel retrieval paradigm that is
capable of expressing users' intricate retrieval requirements flexibly. It
enables the user to give a multimodal query, comprising a reference image and a
modification text, and subsequently retrieve the target image. Notwithstanding
the considerable advances made by prevailing methodologies, CIR remains in its
nascent stages due to two limitations: 1) inhomogeneity between dominant and
noisy portions in visual data is ignored, leading to query feature degradation,
and 2) the priority of textual data in the image modification process is
overlooked, which leads to a visual focus bias. To address these two
limitations, this work presents a focus mapping-based feature extractor, which
consists of two modules: dominant portion segmentation and dual focus mapping.
It is designed to identify significant dominant portions in images and guide
the extraction of visual and textual data features, thereby reducing the impact
of noise interference. Subsequently, we propose a textually guided focus
revision module, which can utilize the modification requirements implied in the
text to perform adaptive focus revision on the reference image, thereby
enhancing the perception of the modification focus on the composed features.
The aforementioned modules collectively constitute the segmentatiOn-based Focus
shiFt reviSion nETwork (\mbox{OFFSET}), and comprehensive experiments on four
benchmark datasets substantiate the superiority of our proposed method. The
codes and data are available on https://zivchen-ty.github.io/OFFSET.github.io/

</details>


### [33] [Knowledge-guided Complex Diffusion Model for PolSAR Image Classification in Contourlet Domain](https://arxiv.org/abs/2507.05666)
*Junfei Shi,Yu Cheng,Haiyan Jin,Junhuai Li,Zhaolin Xiao,Maoguo Gong,Weisi Lin*

Main category: cs.CV

TL;DR: 提出了一种基于Contourlet变换的结构知识引导的复杂扩散模型，用于PolSAR图像分类，解决了传统扩散模型在捕捉复杂相位信息和保留细节结构上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统实值扩散模型在PolSAR数据中难以捕捉复杂相位信息且细节保留不足，需改进。

Method: 结合Contourlet变换的多尺度多方向表示，设计复杂扩散网络，利用高低频子带特征引导扩散过程。

Result: 在三个真实PolSAR数据集上表现优于现有方法，尤其在边缘细节保留和区域同质性方面。

Conclusion: 该方法有效提升了PolSAR图像分类性能，尤其在复杂地形中表现突出。

Abstract: Diffusion models have demonstrated exceptional performance across various
domains due to their ability to model and generate complicated data
distributions. However, when applied to PolSAR data, traditional real-valued
diffusion models face challenges in capturing complex-valued phase
information.Moreover, these models often struggle to preserve fine structural
details. To address these limitations, we leverage the Contourlet transform,
which provides rich multiscale and multidirectional representations well-suited
for PolSAR imagery. We propose a structural knowledge-guided complex diffusion
model for PolSAR image classification in the Contourlet domain. Specifically,
the complex Contourlet transform is first applied to decompose the data into
low- and high-frequency subbands, enabling the extraction of statistical and
boundary features. A knowledge-guided complex diffusion network is then
designed to model the statistical properties of the low-frequency components.
During the process, structural information from high-frequency coefficients is
utilized to guide the diffusion process, improving edge preservation.
Furthermore, multiscale and multidirectional high-frequency features are
jointly learned to further boost classification accuracy. Experimental results
on three real-world PolSAR datasets demonstrate that our approach surpasses
state-of-the-art methods, particularly in preserving edge details and
maintaining region homogeneity in complex terrain.

</details>


### [34] [Dynamic Rank Adaptation for Vision-Language Models](https://arxiv.org/abs/2507.05668)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: 论文提出动态秩适应（DRA）方法，通过动态分配特征重要性来增强视觉语言模型在新类别上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示和适配器的方法在微调视觉语言模型时，难以平衡泛化能力和新类别识别性能，主要因为对所有特征平等处理导致过拟合。

Method: DRA通过令牌重要性分组和动态秩分配，优先保留重要特征，并引入通道响应机制和L1正则化以稳定训练。

Result: 实验表明DRA在多个基准测试中优于现有方法，显著提升了新类别的性能。

Conclusion: DRA有效解决了视觉语言模型在新类别上的泛化问题，具有广泛的应用潜力。

Abstract: Pre-trained large vision-language models (VLMs) like CLIP demonstrate
impressive generalization ability. Existing prompt-based and adapter-based
works have made significant progress in fine-tuning VLMs but still face the
challenges of maintaining strong generalization abilities, particularly towards
unseen new classes. This limitation partly arises from these methods treating
all tokens of the image and text encoder equally, which can lead to overfitting
on less informative features (e.g., background noise, template words) and
degrade the general representations that are crucial for novel concept
recognition. To address this issue, we propose Dynamic Rank Adaptation (DRA), a
novel adapter variant method, designed specifically to enhance new class
generalization. DRA dynamically allocates adaptation ranks based on the
importance of features during training to preserve general knowledge. DRA first
employs token importance grouping, using sequence attention to evaluate and
group tokens by their importance. Then, we adopt rank adaptation according to
the importance of each token group dynamically by assigning higher feature
ranks to the more important tokens. Also, we design a new channel response
mechanism to prioritize the preservation and adaptation of feature channels
identified as the most informative for each instance. In addition, a L1
regularization term is introduced to stabilize the training. Extensive
experiments demonstrate the effectiveness and superiority of our proposed DRA
over existing works, especially on enhancing the performance of new classes on
various benchmarks, including base-new classes, cross-datasets evaluation and
domain generalization. The source code will be published after the paper is
received.

</details>


### [35] [Modeling and Reversing Brain Lesions Using Diffusion Models](https://arxiv.org/abs/2507.05670)
*Omar Zamzam,Haleh Akrami,Anand Joshi,Richard Leahy*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的框架，用于分析和逆转脑损伤过程，通过分割异常区域、估计并逆转组织变形，最终估计健康脑组织。


<details>
  <summary>Details</summary>
Motivation: 现有脑损伤分割方法未区分受损和变形组织，限制了分析的准确性。

Method: 采用扩散模型框架，先分割异常区域，再估计并逆转组织变形，最后修复核心损伤区域。

Result: 相比传统方法，提高了损伤分割、表征和脑标记的准确性。

Conclusion: 该框架为脑损伤分析提供了更精确的工具，适用于临床和研究。

Abstract: Brain lesions are abnormalities or injuries in brain tissue that are often
detectable using magnetic resonance imaging (MRI), which reveals structural
changes in the affected areas. This broad definition of brain lesions includes
areas of the brain that are irreversibly damaged, as well as areas of brain
tissue that are deformed as a result of lesion growth or swelling. Despite the
importance of differentiating between damaged and deformed tissue, existing
lesion segmentation methods overlook this distinction, labeling both of them as
a single anomaly. In this work, we introduce a diffusion model-based framework
for analyzing and reversing the brain lesion process. Our pipeline first
segments abnormal regions in the brain, then estimates and reverses tissue
deformations by restoring displaced tissue to its original position, isolating
the core lesion area representing the initial damage. Finally, we inpaint the
core lesion area to arrive at an estimation of the pre-lesion healthy brain.
This proposed framework reverses a forward lesion growth process model that is
well-established in biomechanical studies that model brain lesions. Our results
demonstrate improved accuracy in lesion segmentation, characterization, and
brain labeling compared to traditional methods, offering a robust tool for
clinical and research applications in brain lesion analysis. Since pre-lesion
healthy versions of abnormal brains are not available in any public dataset for
validation of the reverse process, we simulate a forward model to synthesize
multiple lesioned brain images.

</details>


### [36] [R-VLM: Region-Aware Vision Language Model for Precise GUI Grounding](https://arxiv.org/abs/2507.05673)
*Joonhyung Park,Peng Tang,Sagnik Das,Srikar Appalaraju,Kunwar Yashraj Singh,R. Manmatha,Shabnam Ghadar*

Main category: cs.CV

TL;DR: 论文提出R-VLM方法，通过放大区域提案和IoU感知目标函数，提升GUI元素定位精度，显著提高自动化任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉GUI代理直接从杂乱屏幕截图中定位元素，处理大量无关信息且准确性不足，且使用的交叉熵损失无法有效衡量定位质量。

Method: 引入R-VLM方法，利用放大区域提案精确定位元素，并提出IoU感知目标函数以优化模型收敛。

Result: 在ScreenSpot和AgentStudio基准测试中，R-VLM将定位准确率提升13%；在AITW和Mind2Web导航任务中，绝对准确率提升3.2-9.7%。

Conclusion: R-VLM通过结合VLM与传统目标检测技术，显著提升了GUI自动化任务的准确性和效率。

Abstract: Visual agent models for automating human activities on Graphical User
Interfaces (GUIs) have emerged as a promising research direction, driven by
advances in large Vision Language Models (VLMs). A critical challenge in GUI
automation is the precise grounding of interface elements across diverse
platforms. Existing vision-only GUI agents directly ground elements from large
and cluttered screenshots, requiring them to process substantial irrelevant
information that compromises their accuracy. In addition, these approaches
typically employ basic cross-entropy loss for learning grounding objectives,
which fails to effectively capture grounding quality compared to established
object detection metrics like Intersection-over-Union (IoU). To address these
issues, we introduce R-VLM, a novel GUI grounding approach that leverages
zoomed-in region proposals for precise element localization. We also propose an
IoU-aware objective function that facilitates model convergence toward high IoU
predictions. Our approach bridges the gap between VLMs and conventional object
detection techniques, improving the state-of-the-art grounding accuracy by 13%
across diverse GUI platforms on the GUI grounding benchmarks ScreenSpot and
AgentStudio. In addition, our R-VLM approach shows 3.2-9.7% absolute accuracy
improvements in GUI navigation tasks on the AITW and Mind2Web benchmarks.

</details>


### [37] [MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos](https://arxiv.org/abs/2507.05675)
*Rongsheng Wang,Junying Chen,Ke Ji,Zhenyang Cai,Shunian Chen,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: 论文提出首个大规模医学视频生成数据集MedVideoCap-55K，并基于此开发了模型MedGen，显著提升了医学视频生成的视觉质量和医学准确性。


<details>
  <summary>Details</summary>
Motivation: 医学视频生成在临床培训和教育中至关重要，但现有模型因缺乏高质量医学数据集而表现不佳。

Method: 构建了包含55,000个医学视频片段的MedVideoCap-55K数据集，并开发了MedGen模型。

Result: MedGen在视觉质量和医学准确性上均达到领先水平，媲美商业系统。

Conclusion: MedVideoCap-55K和MedGen为医学视频生成提供了重要资源，有望推动该领域研究。

Abstract: Recent advances in video generation have shown remarkable progress in
open-domain settings, yet medical video generation remains largely
underexplored. Medical videos are critical for applications such as clinical
training, education, and simulation, requiring not only high visual fidelity
but also strict medical accuracy. However, current models often produce
unrealistic or erroneous content when applied to medical prompts, largely due
to the lack of large-scale, high-quality datasets tailored to the medical
domain. To address this gap, we introduce MedVideoCap-55K, the first
large-scale, diverse, and caption-rich dataset for medical video generation. It
comprises over 55,000 curated clips spanning real-world medical scenarios,
providing a strong foundation for training generalist medical video generation
models. Built upon this dataset, we develop MedGen, which achieves leading
performance among open-source models and rivals commercial systems across
multiple benchmarks in both visual quality and medical accuracy. We hope our
dataset and model can serve as a valuable resource and help catalyze further
research in medical video generation. Our code and data is available at
https://github.com/FreedomIntelligence/MedGen

</details>


### [38] [Integrated Structural Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.05677)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: 提出了一种集成结构提示（ISP）方法，通过自结构和跨结构提示模块增强视觉语言模型中文本和图像分支的信息交互，同时引入样本探测模块动态调整损失系数，提升模型在新类别上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法忽视了可学习提示与模态内及模态间令牌的结构关系，且难以平衡基类和新类的性能。

Method: 提出ISP方法，包含自结构和跨结构提示模块，以及动态调整损失系数的样本探测模块。

Result: 在基类到新类泛化、跨数据集评估和领域泛化三个广泛使用的设置上，ISP表现优于现有方法。

Conclusion: ISP通过结构关系和动态损失调整，显著提升了视觉语言模型在下游任务中的性能。

Abstract: Prompt learning methods have significantly extended the transferability of
pre-trained Vision-Language Models (VLMs) like CLIP for various downstream
tasks. These methods adopt handcraft templates or learnable vectors to provide
text or image instructions in fine-tuning VLMs. However, most existing works
ignore the structural relationships between learnable prompts and tokens within
and between modalities. Moreover, balancing the performance of base and new
classes remains a significant challenge. In this paper, we propose an
Integrated Structural Prompt (ISP) for VLMs to enhance the interaction of
information representations between the text and image branches. ISP introduces
self-structural and cross-structural prompt modules to model the structural
relationships between learnable prompts and frozen tokens within and across
modalities. This enables efficient information transfer while preserving
feature stability. Additionally, we propose a sample probing module that
dynamically adjusts loss coefficients based on sample difficulty, preventing
the mode from overfitting to simple samples and improving generalization
ability to new classes. Extensive experiments on three widely used settings:
base-to-new generalization, cross-dataset evaluation, and domain generalization
demonstrate that the proposed ISP achieves competitive performance against
state-of-the-art methods.

</details>


### [39] [LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion](https://arxiv.org/abs/2507.05678)
*Yisu Zhang,Chenjie Cao,Chaohui Yu,Jianke Zhu*

Main category: cs.CV

TL;DR: LiON-LoRA提出了一种改进的LoRA框架，通过线性可扩展性、正交性和范数一致性，实现对视频扩散模型中相机轨迹和物体运动的精确控制。


<details>
  <summary>Details</summary>
Motivation: 解决现有LoRA方法在视频扩散模型中融合不稳定和非线性可扩展性问题，以实现对相机轨迹和物体运动的精确控制。

Method: 分析LoRA特征的正交性，强制层间范数一致性，并引入可控令牌到扩散变换器中，线性调整运动幅度。

Result: LiON-LoRA在轨迹控制精度和运动强度调整上优于现有方法，且泛化能力强。

Conclusion: LiON-LoRA通过改进的LoRA框架，实现了对视频生成中空间和时间控制的统一，性能优越。

Abstract: Video Diffusion Models (VDMs) have demonstrated remarkable capabilities in
synthesizing realistic videos by learning from large-scale data. Although
vanilla Low-Rank Adaptation (LoRA) can learn specific spatial or temporal
movement to driven VDMs with constrained data, achieving precise control over
both camera trajectories and object motion remains challenging due to the
unstable fusion and non-linear scalability. To address these issues, we propose
LiON-LoRA, a novel framework that rethinks LoRA fusion through three core
principles: Linear scalability, Orthogonality, and Norm consistency. First, we
analyze the orthogonality of LoRA features in shallow VDM layers, enabling
decoupled low-level controllability. Second, norm consistency is enforced
across layers to stabilize fusion during complex camera motion combinations.
Third, a controllable token is integrated into the diffusion transformer (DiT)
to linearly adjust motion amplitudes for both cameras and objects with a
modified self-attention mechanism to ensure decoupled control. Additionally, we
extend LiON-LoRA to temporal generation by leveraging static-camera videos,
unifying spatial and temporal controllability. Experiments demonstrate that
LiON-LoRA outperforms state-of-the-art methods in trajectory control accuracy
and motion strength adjustment, achieving superior generalization with minimal
training data. Project Page: https://fuchengsu.github.io/lionlora.github.io/

</details>


### [40] [Event-RGB Fusion for Spacecraft Pose Estimation Under Harsh Lighting](https://arxiv.org/abs/2507.05698)
*Mohsi Jawaid,Marcus Märtens,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 论文提出了一种结合RGB和事件传感器的融合方法，用于解决航天器姿态估计中的光照挑战。


<details>
  <summary>Details</summary>
Motivation: 航天器姿态估计在自主太空操作中至关重要，但传统RGB传感器在极端光照条件下表现不佳，而事件传感器虽动态范围高，但空间分辨率低且低运动时信噪比差。

Method: 采用分束棱镜实现光学和时间对齐，开发RANSAC技术融合RGB和事件数据，并引入dropout不确定性估计检测极端条件。

Result: 在实验室多种挑战性光照条件下收集的数据集上验证了方法的有效性，支持事件传感器在姿态估计中的应用。

Conclusion: RGB与事件传感器融合方法有效提升了航天器姿态估计的鲁棒性，数据集将公开以促进社区研究。

Abstract: Spacecraft pose estimation is crucial for autonomous in-space operations,
such as rendezvous, docking and on-orbit servicing. Vision-based pose
estimation methods, which typically employ RGB imaging sensors, is a compelling
solution for spacecraft pose estimation, but are challenged by harsh lighting
conditions, which produce imaging artifacts such as glare, over-exposure,
blooming and lens flare. Due to their much higher dynamic range, neuromorphic
or event sensors are more resilient to extreme lighting conditions. However,
event sensors generally have lower spatial resolution and suffer from reduced
signal-to-noise ratio during periods of low relative motion. This work
addresses these individual sensor limitations by introducing a sensor fusion
approach combining RGB and event sensors. A beam-splitter prism was employed to
achieve precise optical and temporal alignment. Then, a RANSAC-based technique
was developed to fuse the information from the RGB and event channels to
achieve pose estimation that leveraged the strengths of the two modalities. The
pipeline was complemented by dropout uncertainty estimation to detect extreme
conditions that affect either channel. To benchmark the performance of the
proposed event-RGB fusion method, we collected a comprehensive real dataset of
RGB and event data for satellite pose estimation in a laboratory setting under
a variety of challenging illumination conditions. Encouraging results on the
dataset demonstrate the efficacy of our event-RGB fusion approach and further
supports the usage of event sensors for spacecraft pose estimation. To support
community research on this topic, our dataset will be released publicly.

</details>


### [41] [Hyperspectral Anomaly Detection Methods: A Survey and Comparative Study](https://arxiv.org/abs/2507.05730)
*Aayushma Pant,Arbind Agrahari Baniya,Tsz-Kwan Lee,Sunil Aryal*

Main category: cs.CV

TL;DR: 该论文综述了高光谱异常检测（HAD）技术，比较了统计模型、表示方法、经典机器学习和深度学习模型，发现深度学习精度最高，统计模型速度最快。


<details>
  <summary>Details</summary>
Motivation: 高光谱异常检测技术虽发展迅速，但仍面临计算复杂度高、噪声敏感和泛化能力有限等问题，需系统比较现有方法。

Method: 对17个基准数据集上的统计模型、表示方法、经典机器学习和深度学习模型进行了性能评估，使用ROC、AUC等指标。

Result: 深度学习模型检测精度最高，统计模型速度最快。

Conclusion: 研究为高光谱异常检测领域的未来研究提供了方向，强调了深度学习的潜力和统计模型的高效性。

Abstract: Hyperspectral images are high-dimensional datasets consisting of hundreds of
contiguous spectral bands, enabling detailed material and surface analysis.
Hyperspectral anomaly detection (HAD) refers to the technique of identifying
and locating anomalous targets in such data without prior information about a
hyperspectral scene or target spectrum. This technology has seen rapid
advancements in recent years, with applications in agriculture, defence,
military surveillance, and environmental monitoring. Despite this significant
progress, existing HAD methods continue to face challenges such as high
computational complexity, sensitivity to noise, and limited generalisation
across diverse datasets. This study presents a comprehensive comparison of
various HAD techniques, categorising them into statistical models,
representation-based methods, classical machine learning approaches, and deep
learning models. We evaluated these methods across 17 benchmarking datasets
using different performance metrics, such as ROC, AUC, and separability map to
analyse detection accuracy, computational efficiency, their strengths,
limitations, and directions for future research.The research shows that deep
learning models achieved the highest detection accuracy, while statistical
models demonstrated exceptional speed across all datasets. This study aims to
provide valuable insights for researchers and practitioners working to advance
the field of hyperspectral anomaly detection methods.

</details>


### [42] [SenseShift6D: Multimodal RGB-D Benchmarking for Robust 6D Pose Estimation across Environment and Sensor Variations](https://arxiv.org/abs/2507.05751)
*Yegyu Han,Taegyoon Yoon,Dayeon Woo,Sojeong Kim,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: SenseShift6D是一个新的RGB-D数据集，探索了真实世界中光照和传感器设置变化对6D物体姿态估计的影响，并展示了测试时传感器控制对性能的提升。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计数据集在固定光照和相机设置下采集，忽略了真实世界中的变化。SenseShift6D旨在填补这一空白，研究传感器控制对性能的影响。

Method: 通过物理调整13种RGB曝光、9种RGB增益、自动曝光、4种深度捕获模式和5种光照水平，生成101.9k RGB和10k深度图像，提供1,380种传感器-光照组合。

Result: 实验表明，测试时传感器控制比数字数据增强更有效，性能接近或优于增加真实训练数据。多模态RGB-D配置联合调整效果最佳。

Conclusion: SenseShift6D扩展了6D姿态评估范式，为自适应感知系统在不确定环境中的鲁棒性奠定了基础。

Abstract: Recent advances on 6D object-pose estimation has achieved high performance on
representative benchmarks such as LM-O, YCB-V, and T-Less. However, these
datasets were captured under fixed illumination and camera settings, leaving
the impact of real-world variations in illumination, exposure, gain or
depth-sensor mode - and the potential of test-time sensor control to mitigate
such variations - largely unexplored. To bridge this gap, we introduce
SenseShift6D, the first RGB-D dataset that physically sweeps 13 RGB exposures,
9 RGB gains, auto-exposure, 4 depth-capture modes, and 5 illumination levels.
For three common household objects (spray, pringles, and tincase), we acquire
101.9k RGB and 10k depth images, which can provide 1,380 unique sensor-lighting
permutations per object pose. Experiments with state-of-the-art models on our
dataset show that applying sensor control during test-time induces greater
performance improvement over digital data augmentation, achieving performance
comparable to or better than costly increases in real-world training data
quantity and diversity. Adapting either RGB or depth sensors individually is
effective, while jointly adapting multimodal RGB-D configurations yields even
greater improvements. SenseShift6D extends the 6D-pose evaluation paradigm from
data-centered to sensor-aware robustness, laying a foundation for adaptive,
self-tuning perception systems capable of operating robustly in uncertain
real-world environments. Our dataset is available at:
huggingface.co/datasets/Yegyu/SenseShift6D Associated scripts can be found at:
github.com/yegyu-han/SenseShift6D

</details>


### [43] [Normal Patch Retinex Robust Alghoritm for White Balancing in Digital Microscopy](https://arxiv.org/abs/2507.05757)
*Radoslaw Roszczyk,Artur Krupa,Izabella Antoniuk*

Main category: cs.CV

TL;DR: 本文提出了一种自动白平衡算法，用于校正显微镜图像的色彩，实验证明其优于传统摄影算法。


<details>
  <summary>Details</summary>
Motivation: 显微镜操作中获取色彩准确、平衡的图像具有挑战性，需要自动化解决方案。

Method: 提出了一种自动白平衡机制，并在200张显微镜图像上进行了实验验证。

Result: 算法在病理学常用染色图像上表现优于传统摄影白平衡算法。

Conclusion: 该自动白平衡算法在显微镜图像处理中更有效，尤其适用于特定染色技术。

Abstract: The acquisition of accurately coloured, balanced images in an optical
microscope can be a challenge even for experienced microscope operators. This
article presents an entirely automatic mechanism for balancing the white level
that allows the correction of the microscopic colour images adequately. The
results of the algorithm have been confirmed experimentally on a set of two
hundred microscopic images. The images contained scans of three microscopic
specimens commonly used in pathomorphology. Also, the results achieved were
compared with other commonly used white balance algorithms in digital
photography. The algorithm applied in this work is more effective than the
classical algorithms used in colour photography for microscopic images stained
with hematoxylin-phloxine-saffron and for immunohistochemical staining images.

</details>


### [44] [DreamArt: Generating Interactable Articulated Objects from a Single Image](https://arxiv.org/abs/2507.05763)
*Ruijie Lu,Yu Liu,Jiaxiang Tang,Junfeng Ni,Yuxiang Wang,Diwen Wan,Gang Zeng,Yixin Chen,Siyuan Huang*

Main category: cs.CV

TL;DR: DreamArt是一个从单视图图像生成高质量、可交互的铰接物体的框架，通过三阶段流程实现。


<details>
  <summary>Details</summary>
Motivation: 当前图像到3D的方法主要关注表面几何和纹理，忽略了部件分解和铰接建模，且现有神经重建方法依赖密集多视图或交互数据，难以扩展。

Method: 三阶段流程：1) 通过图像到3D生成、掩码提示的3D分割和部件补全重建部件分割的完整3D网格；2) 微调视频扩散模型以捕捉部件级铰接先验；3) 优化铰接运动并进行全局纹理细化。

Result: 实验表明，DreamArt能生成高质量的铰接物体，具有准确的部件形状、高保真外观和合理的铰接。

Conclusion: DreamArt为铰接物体生成提供了可扩展的解决方案。

Abstract: Generating articulated objects, such as laptops and microwaves, is a crucial
yet challenging task with extensive applications in Embodied AI and AR/VR.
Current image-to-3D methods primarily focus on surface geometry and texture,
neglecting part decomposition and articulation modeling. Meanwhile, neural
reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense
multi-view or interaction data, limiting their scalability. In this paper, we
introduce DreamArt, a novel framework for generating high-fidelity,
interactable articulated assets from single-view images. DreamArt employs a
three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D
object meshes through a combination of image-to-3D generation, mask-prompted 3D
segmentation, and part amodal completion. Second, we fine-tune a video
diffusion model to capture part-level articulation priors, leveraging movable
part masks as prompt and amodal images to mitigate ambiguities caused by
occlusion. Finally, DreamArt optimizes the articulation motion, represented by
a dual quaternion, and conducts global texture refinement and repainting to
ensure coherent, high-quality textures across all parts. Experimental results
demonstrate that DreamArt effectively generates high-quality articulated
objects, possessing accurate part shape, high appearance fidelity, and
plausible articulation, thereby providing a scalable solution for articulated
asset generation. Our project page is available at
https://dream-art-0.github.io/DreamArt/.

</details>


### [45] [TalkFashion: Intelligent Virtual Try-On Assistant Based on Multimodal Large Language Model](https://arxiv.org/abs/2507.05790)
*Yujie Hu,Xuanyu Zhang,Weiqi Li,Jian Zhang*

Main category: cs.CV

TL;DR: 本文提出TalkFashion，一种基于文本指令的多功能虚拟试穿系统，利用大语言模型分析用户指令并激活不同处理流程，实现全套装更换和局部编辑。


<details>
  <summary>Details</summary>
Motivation: 解决现有虚拟试穿方法功能单一、缺乏灵活性的问题，通过文本指令实现多功能试穿。

Method: 结合大语言模型分析指令，设计指令驱动的局部重绘模型，无需手动提供掩码，实现全自动局部编辑。

Result: 实验结果显示，该方法在语义一致性和视觉质量上优于现有方法。

Conclusion: TalkFashion通过文本指令实现了多功能虚拟试穿，提升了编辑任务的灵活性和自动化程度。

Abstract: Virtual try-on has made significant progress in recent years. This paper
addresses how to achieve multifunctional virtual try-on guided solely by text
instructions, including full outfit change and local editing. Previous methods
primarily relied on end-to-end networks to perform single try-on tasks, lacking
versatility and flexibility. We propose TalkFashion, an intelligent try-on
assistant that leverages the powerful comprehension capabilities of large
language models to analyze user instructions and determine which task to
execute, thereby activating different processing pipelines accordingly.
Additionally, we introduce an instruction-based local repainting model that
eliminates the need for users to manually provide masks. With the help of
multi-modal models, this approach achieves fully automated local editings,
enhancing the flexibility of editing tasks. The experimental results
demonstrate better semantic consistency and visual quality compared to the
current methods.

</details>


### [46] [SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning](https://arxiv.org/abs/2507.05798)
*Xin Hu,Ke Qin,Guiduo Duan,Ming Li,Yuan-Fang Li,Tao He*

Main category: cs.CV

TL;DR: SPADE框架通过空间感知去噪网络改进开放词汇PSG任务，显著提升空间关系预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练视觉语言模型（VLMs），但忽视了其在空间关系推理上的固有局限，导致关系预测不理想。

Method: SPADE框架包含两步：(1) 反演引导的UNet校准，(2) 空间感知上下文推理。通过轻量级LoRA微调和空间感知关系图Transformer提升性能。

Result: 在PSG和Visual Genome数据集上，SPADE在封闭和开放场景中均优于现有方法，尤其在空间关系预测上表现突出。

Conclusion: SPADE通过空间感知设计有效解决了VLMs在空间推理上的不足，为PSG任务提供了新思路。

Abstract: Panoptic Scene Graph Generation (PSG) integrates instance segmentation with
relation understanding to capture pixel-level structural relationships in
complex scenes. Although recent approaches leveraging pre-trained
vision-language models (VLMs) have significantly improved performance in the
open-vocabulary setting, they commonly ignore the inherent limitations of VLMs
in spatial relation reasoning, such as difficulty in distinguishing object
relative positions, which results in suboptimal relation prediction. Motivated
by the denoising diffusion model's inversion process in preserving the spatial
structure of input images, we propose SPADE (SPatial-Aware Denoising-nEtwork)
framework -- a novel approach for open-vocabulary PSG. SPADE consists of two
key steps: (1) inversion-guided calibration for the UNet adaptation, and (2)
spatial-aware context reasoning. In the first step, we calibrate a general
pre-trained teacher diffusion model into a PSG-specific denoising network with
cross-attention maps derived during inversion through a lightweight LoRA-based
fine-tuning strategy. In the second step, we develop a spatial-aware relation
graph transformer that captures both local and long-range contextual
information, facilitating the generation of high-quality relation queries.
Extensive experiments on benchmark PSG and Visual Genome datasets demonstrate
that SPADE outperforms state-of-the-art methods in both closed- and open-set
scenarios, particularly for spatial relationship prediction.

</details>


### [47] [DREAM: Document Reconstruction via End-to-end Autoregressive Model](https://arxiv.org/abs/2507.05805)
*Xin Li,Mingming Gong,Yunfei Wu,Jianxin Dai,Antai Guo,Xinghua Jiang,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun*

Main category: cs.CV

TL;DR: 本文提出了一种名为DREAM的自回归模型，用于端到端的文档重建，解决了现有方法中的错误传播和布局信息丢失问题，并在多个子任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有文档重建方法存在错误传播和布局信息丢失的问题，限制了性能。本文旨在通过端到端的自回归模型解决这些问题。

Method: 提出DREAM模型，将文本图像转换为文档重建序列，保留更多元素信息。同时定义了任务标准，并引入DSM指标和DocRec1K数据集。

Result: 实验证明DREAM在文档重建任务中表现卓越，并在多个子任务中具有竞争力。

Conclusion: DREAM模型在文档重建和相关子任务中表现出色，为未来研究提供了新方向。

Abstract: Document reconstruction constitutes a significant facet of document analysis
and recognition, a field that has been progressively accruing interest within
the scholarly community. A multitude of these researchers employ an array of
document understanding models to generate predictions on distinct subtasks,
subsequently integrating their results into a holistic document reconstruction
format via heuristic principles. Nevertheless, these multi-stage methodologies
are hindered by the phenomenon of error propagation, resulting in suboptimal
performance. Furthermore, contemporary studies utilize generative models to
extract the logical sequence of plain text, tables and mathematical expressions
in an end-to-end process. However, this approach is deficient in preserving the
information related to element layouts, which are vital for document
reconstruction. To surmount these aforementioned limitations, we in this paper
present an innovative autoregressive model specifically designed for document
reconstruction, referred to as Document Reconstruction via End-to-end
Autoregressive Model (DREAM). DREAM transmutes the text image into a sequence
of document reconstruction in a comprehensive, end-to-end process,
encapsulating a broader spectrum of document element information. In addition,
we establish a standardized definition of the document reconstruction task, and
introduce a novel Document Similarity Metric (DSM) and DocRec1K dataset for
assessing the performance of the task. Empirical results substantiate that our
methodology attains unparalleled performance in the realm of document
reconstruction. Furthermore, the results on a variety of subtasks, encompassing
document layout analysis, text recognition, table structure recognition,
formula recognition and reading order detection, indicate that our model is
competitive and compatible with various tasks.

</details>


### [48] [Towards Solar Altitude Guided Scene Illumination](https://arxiv.org/abs/2507.05812)
*Samed Doğan,Maximilian Hoh,Nico Leuze,Nicolas R. -Peña,Alfred Schöttl*

Main category: cs.CV

TL;DR: 论文提出了一种利用太阳高度角作为全局条件变量生成合成相机传感器数据的方法，解决了白天光照变化研究不足的问题。


<details>
  <summary>Details</summary>
Motivation: 真实世界数据采集成本高且受限，现有研究在白天光照变化方面存在明显不足。

Method: 使用太阳高度角作为全局条件变量，结合定制归一化方法，捕捉光照特性和图像噪声。

Result: 方法能够准确捕捉光照特性和图像噪声，适用于扩散模型。

Conclusion: 太阳高度角是一种高效且无需大量标注的解决方案，适用于合成数据生成。

Abstract: The development of safe and robust autonomous driving functions is heavily
dependent on large-scale, high-quality sensor data. However, real-word data
acquisition demands intensive human labor and is strongly limited by factors
such as labeling cost, driver safety protocols and diverse scenario coverage.
Thus, multiple lines of work focus on the conditional generation of synthetic
camera sensor data. We identify a significant gap in research regarding daytime
variation, presumably caused by the scarcity of available labels. Consequently,
we present the solar altitude as global conditioning variable. It is readily
computable from latitude-longitude coordinates and local time, eliminating the
need for extensive manual labeling. Our work is complemented by a tailored
normalization approach, targeting the sensitivity of daylight towards small
numeric changes in altitude. We demonstrate its ability to accurately capture
lighting characteristics and illumination-dependent image noise in the context
of diffusion models.

</details>


### [49] [Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework](https://arxiv.org/abs/2507.05814)
*Wang Wang,Mingyu Shi,Jun Jiang,Wenqian Ma,Chong Liu,Yasutaka Narazaki,Xuguang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种生成3D桥梁数据的系统框架，以解决现有合成数据方法泛化能力不足的问题，并支持分割和补全网络的训练。


<details>
  <summary>Details</summary>
Motivation: 桥梁作为关键交通基础设施，面临老化和损坏的挑战，传统人工检测效率低下，而3D点云技术因数据不完整（如缺失标签和扫描遮挡）受限。

Method: 提出一个系统框架，自动生成具有组件级实例标注、高保真颜色和精确法向量的完整点云，并可扩展为模拟多样且物理真实的非完整点云。

Result: 实验表明，使用合成数据训练的PointNet++模型在真实桥梁语义分割中达到84.2%的mIoU，KT-Net在组件补全任务中表现优异。

Conclusion: 该研究为桥梁结构的3D视觉分析提供了创新方法和基础数据集，对基础设施自动化管理与维护有重要意义。

Abstract: As critical transportation infrastructure, bridges face escalating challenges
from aging and deterioration, while traditional manual inspection methods
suffer from low efficiency. Although 3D point cloud technology provides a new
data-driven paradigm, its application potential is often constrained by the
incompleteness of real-world data, which results from missing labels and
scanning occlusions. To overcome the bottleneck of insufficient generalization
in existing synthetic data methods, this paper proposes a systematic framework
for generating 3D bridge data.
  This framework can automatically generate complete point clouds featuring
component-level instance annotations, high-fidelity color, and precise normal
vectors. It can be further extended to simulate the creation of diverse and
physically realistic incomplete point clouds, designed to support the training
of segmentation and completion networks, respectively. Experiments demonstrate
that a PointNet++ model trained with our synthetic data achieves a mean
Intersection over Union (mIoU) of 84.2% in real-world bridge semantic
segmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance
on the component completion task.
  This research offers an innovative methodology and a foundational dataset for
the 3D visual analysis of bridge structures, holding significant implications
for advancing the automated management and maintenance of infrastructure.

</details>


### [50] [2D Instance Editing in 3D Space](https://arxiv.org/abs/2507.05819)
*Yuhuan Xie,Aoxuan Pan,Ming-Xian Lin,Wei Huang,Yi-Hua Huang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 提出了一种“2D-3D-2D”框架，通过将2D对象提升为3D表示进行编辑，再重新投影回2D图像，解决了现有2D编辑方法在一致性和对象身份保持上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在2D图像编辑中虽表现优异，但在一致性和对象身份保持上存在局限，需通过3D环境改进。

Method: 采用“2D-3D-2D”框架，先将2D对象转换为3D表示进行编辑，再重新投影回2D图像。

Result: 实验表明，该方法在一致性和对象身份保持上优于现有2D编辑方法。

Conclusion: 通过3D环境编辑，显著提升了2D图像编辑的一致性和对象身份保持能力。

Abstract: Generative models have achieved significant progress in advancing 2D image
editing, demonstrating exceptional precision and realism. However, they often
struggle with consistency and object identity preservation due to their
inherent pixel-manipulation nature. To address this limitation, we introduce a
novel "2D-3D-2D" framework. Our approach begins by lifting 2D objects into 3D
representation, enabling edits within a physically plausible,
rigidity-constrained 3D environment. The edited 3D objects are then reprojected
and seamlessly inpainted back into the original 2D image. In contrast to
existing 2D editing methods, such as DragGAN and DragDiffusion, our method
directly manipulates objects in a 3D environment. Extensive experiments
highlight that our framework surpasses previous methods in general performance,
delivering highly consistent edits while robustly preserving object identity.

</details>


### [51] [Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs with Vision Foundation Models](https://arxiv.org/abs/2507.05822)
*L'ea Dubois,Klaus Schmidt,Chengyu Wang,Ji-Hoon Park,Lin Wang,Santiago Munoz*

Main category: cs.CV

TL;DR: 提出了一种结合视觉基础模型（VFM）和大语言模型（LLM）的新框架，通过融合模块将视觉特征与语言对齐，实现了视频理解中的高级认知任务。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型在高级认知任务（如因果推理和未来预测）上表现不足，缺乏常识知识。

Method: 采用两阶段训练策略：大规模视频-文本对齐预训练和针对推理任务的指令微调，融合模块将视觉特征转化为语言对齐表示。

Result: 模型在多个挑战性基准上达到最优性能，并展现出零样本泛化能力。

Conclusion: 该工作将机器感知从简单识别推向认知理解，为更智能的AI系统铺平了道路。

Abstract: Current video understanding models excel at recognizing "what" is happening
but fall short in high-level cognitive tasks like causal reasoning and future
prediction, a limitation rooted in their lack of commonsense world knowledge.
To bridge this cognitive gap, we propose a novel framework that synergistically
fuses a powerful Vision Foundation Model (VFM) for deep visual perception with
a Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our
key technical innovation is a sophisticated fusion module, inspired by the
Q-Former architecture, which distills complex spatiotemporal and object-centric
visual features into a concise, language-aligned representation. This enables
the LLM to effectively ground its inferential processes in direct visual
evidence. The model is trained via a two-stage strategy, beginning with
large-scale alignment pre-training on video-text data, followed by targeted
instruction fine-tuning on a curated dataset designed to elicit advanced
reasoning and prediction skills. Extensive experiments demonstrate that our
model achieves state-of-the-art performance on multiple challenging benchmarks.
Notably, it exhibits remarkable zero-shot generalization to unseen reasoning
tasks, and our in-depth ablation studies validate the critical contribution of
each architectural component. This work pushes the boundary of machine
perception from simple recognition towards genuine cognitive understanding,
paving the way for more intelligent and capable AI systems in robotics,
human-computer interaction, and beyond.

</details>


### [52] [I$^2$R: Inter and Intra-image Refinement in Few Shot Segmentation](https://arxiv.org/abs/2507.05838)
*Ourui Fu,Hangzhou He,Xinliang Zhang,Lei Zhu,Shuang Zeng,ZhaoHeng Xie,Yanye Lu*

Main category: cs.CV

TL;DR: 论文提出了一种新的少样本分割方法I²R，通过全局语义聚合和方向性掩码策略，解决了支持-查询图像间的语义差异和图像内相似区域误判问题，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 语义分割中的标注瓶颈促使了对少样本分割的研究，但现有方法因支持-查询图像间的语义差异和图像内相似区域的误判而受限。

Method: 提出I²R方法：1) 使用类别特定的高层表示聚合全局语义信息；2) 采用方向性掩码策略抑制不一致的像素对。

Result: 在PASCAL-5ⁱ和COCO-20ⁱ基准测试中，1-shot设置下mIoU分别提升了1.9%和2.1%。

Conclusion: I²R方法有效解决了现有少样本分割中的关键问题，性能显著优于现有技术。

Abstract: The annotation bottleneck in semantic segmentation has driven significant
interest in few-shot segmentation, which aims to develop segmentation models
capable of generalizing rapidly to novel classes using minimal exemplars.
Conventional training paradigms typically generate query prior maps by
extracting masked-area features from support images, followed by making
predictions guided by these prior maps. However, current approaches remain
constrained by two critical limitations stemming from inter- and intra-image
discrepancies, both of which significantly degrade segmentation performance: 1)
The semantic gap between support and query images results in mismatched
features and inaccurate prior maps; 2) Visually similar yet semantically
distinct regions within support or query images lead to false negative or false
positive predictions. We propose a novel FSS method called \textbf{I$^2$R}: 1)
Using category-specific high level representations which aggregate global
semantic cues from support and query images, enabling more precise inter-image
region localization and address the first limitation. 2) Directional masking
strategy that suppresses inconsistent support-query pixel pairs, which exhibit
high feature similarity but conflicting mask, to mitigate the second issue.
Experiments demonstrate that our method outperforms state-of-the-art
approaches, achieving improvements of 1.9\% and 2.1\% in mIoU under the 1-shot
setting on PASCAL-5$^i$ and COCO-20$^i$ benchmarks, respectively.

</details>


### [53] [USIGAN: Unbalanced Self-Information Feature Transport for Weakly Paired Image IHC Virtual Staining](https://arxiv.org/abs/2507.05843)
*Yue Peng,Bing Xiong,Fuqiang Chen,De Eybo,RanRan Zhang,Wanming Hu,Jing Cai,Wenjian Qin*

Main category: cs.CV

TL;DR: 论文提出了一种名为USIGAN的新方法，用于解决弱配对条件下IHC虚拟染色任务中的空间异质性问题，通过全局形态语义提取和优化传输机制提升生成结果的病理语义一致性。


<details>
  <summary>Details</summary>
Motivation: 解决弱配对条件下IHC虚拟染色任务中因空间异质性导致的不准确映射和病理语义不一致问题。

Method: 提出USIGAN方法，包括不平衡自信息特征传输、UOT-CTM机制和PC-SCM机制，以优化联合分布和提升一致性。

Result: 在两个公开数据集上验证，USIGAN在IoD和Pearson-R等临床指标上表现优异。

Conclusion: USIGAN有效解决了弱配对条件下的挑战，提升了IHC虚拟染色的临床相关性。

Abstract: Immunohistochemical (IHC) virtual staining is a task that generates virtual
IHC images from H\&E images while maintaining pathological semantic consistency
with adjacent slices. This task aims to achieve cross-domain mapping between
morphological structures and staining patterns through generative models,
providing an efficient and cost-effective solution for pathological analysis.
However, under weakly paired conditions, spatial heterogeneity between adjacent
slices presents significant challenges. This can lead to inaccurate one-to-many
mappings and generate results that are inconsistent with the pathological
semantics of adjacent slices. To address this issue, we propose a novel
unbalanced self-information feature transport for IHC virtual staining, named
USIGAN, which extracts global morphological semantics without relying on
positional correspondence.By removing weakly paired terms in the joint marginal
distribution, we effectively mitigate the impact of weak pairing on joint
distributions, thereby significantly improving the content consistency and
pathological semantic consistency of the generated results. Moreover, we design
the Unbalanced Optimal Transport Consistency (UOT-CTM) mechanism and the
Pathology Self-Correspondence (PC-SCM) mechanism to construct correlation
matrices between H\&E and generated IHC in image-level and real IHC and
generated IHC image sets in intra-group level.. Experiments conducted on two
publicly available datasets demonstrate that our method achieves superior
performance across multiple clinically significant metrics, such as IoD and
Pearson-R correlation, demonstrating better clinical relevance.

</details>


### [54] [DFYP: A Dynamic Fusion Framework with Spectral Channel Attention and Adaptive Operator learning for Crop Yield Prediction](https://arxiv.org/abs/2507.05849)
*Juli Zhang,Zeyu Yan,Jing Zhang,Qiguang Miao,Quan Wang*

Main category: cs.CV

TL;DR: DFYP是一种动态融合框架，通过结合光谱通道注意力、边缘自适应空间建模和学习融合机制，提高了作物产量预测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在空间建模能力和跨作物类型及年份的泛化能力上存在不足，DFYP旨在解决这些问题。

Method: DFYP包含三个关键组件：分辨率感知通道注意力模块（RCA）、自适应算子学习网络（AOL-Net）和双分支架构，分别用于增强光谱表示、改进空间特征提取和联合建模局部与全局信息。

Result: 在MODIS和Sentinel-2数据集上的实验表明，DFYP在RMSE、MAE和R2指标上均优于现有方法。

Conclusion: DFYP展示了在真实农业监测中的有效性和鲁棒性。

Abstract: Accurate remote sensing-based crop yield prediction remains a fundamental
challenging task due to complex spatial patterns, heterogeneous spectral
characteristics, and dynamic agricultural conditions. Existing methods often
suffer from limited spatial modeling capacity, weak generalization across crop
types and years. To address these challenges, we propose DFYP, a novel Dynamic
Fusion framework for crop Yield Prediction, which combines spectral channel
attention, edge-adaptive spatial modeling and a learnable fusion mechanism to
improve robustness across diverse agricultural scenarios. Specifically, DFYP
introduces three key components: (1) a Resolution-aware Channel Attention (RCA)
module that enhances spectral representation by adaptively reweighting input
channels based on resolution-specific characteristics; (2) an Adaptive Operator
Learning Network (AOL-Net) that dynamically selects operators for convolutional
kernels to improve edge-sensitive spatial feature extraction under varying crop
and temporal conditions; and (3) a dual-branch architecture with a learnable
fusion mechanism, which jointly models local spatial details and global
contextual information to support cross-resolution and cross-crop
generalization. Extensive experiments on multi-year datasets MODIS and
multi-crop dataset Sentinel-2 demonstrate that DFYP consistently outperforms
current state-of-the-art baselines in RMSE, MAE, and R2 across different
spatial resolutions, crop types, and time periods, showcasing its effectiveness
and robustness for real-world agricultural monitoring.

</details>


### [55] [D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos](https://arxiv.org/abs/2507.05859)
*Wenkang Zhang,Yan Zhao,Qiang Wang,Li Song,Zhengxue Cheng*

Main category: cs.CV

TL;DR: 提出了一种名为D-FCGS的前馈压缩框架，用于动态高斯点云序列的高效压缩，无需逐场景优化。


<details>
  <summary>Details</summary>
Motivation: 动态3D高斯点云的高效压缩是自由视点视频（FVV）的关键挑战，现有方法依赖优化编码，泛化性受限。

Method: 采用Group-of-Frames结构和I-P帧编码，通过稀疏控制点提取帧间运动，结合双先验熵模型进行压缩，并使用控制点引导的运动补偿和细化网络重建。

Result: 实验表明，D-FCGS在保持视觉质量的同时，实现了40倍以上的压缩，且处理时间少于2秒。

Conclusion: D-FCGS为动态3D高斯点云的压缩提供了高效的前馈解决方案，推动了FVV的可扩展传输与存储。

Abstract: Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient
compression of dynamic 3D representations remains a major challenge. Recent
advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have
enabled high-fidelity scene modeling. However, existing methods often couple
scene reconstruction with optimization-dependent coding, which limits
generalizability. This paper presents Feedforward Compression of Dynamic
Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing
temporally correlated Gaussian point cloud sequences. Our approach introduces a
Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame
motions are extracted via sparse control points. The resulting motion tensors
are compressed in a feedforward manner using a dual prior-aware entropy model
that combines hyperprior and spatial-temporal priors for accurate rate
estimation. For reconstruction, we perform control-point-guided motion
compensation and employ a refinement network to enhance view-consistent
fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS
generalizes across scenes without per-scene optimization. Experiments show that
it matches the rate-distortion performance of optimization-based methods,
achieving over 40 times compression in under 2 seconds while preserving visual
quality across viewpoints. This work advances feedforward compression for
dynamic 3DGS, paving the way for scalable FVV transmission and storage in
immersive applications.

</details>


### [56] [GeoMag: A Vision-Language Model for Pixel-level Fine-Grained Remote Sensing Image Parsing](https://arxiv.org/abs/2507.05887)
*Xianzhi Ma,Jianhui Li,Changhua Pei,Hao Liu*

Main category: cs.CV

TL;DR: GeoMag是一个端到端的通用大模型框架，通过动态调整注意力范围和分辨率，提升遥感图像多粒度解析能力，同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有遥感视觉语言模型（RS-VLMs）在像素级任务和小目标识别上表现不佳，且计算资源消耗高，限制了实际应用。

Method: 提出Task-driven Multi-granularity Resolution Adjustment (TMRA)和Prompt-guided Semantic-aware Cropping (PSC)，动态调整任务无关区域的分辨率并增强任务相关区域的视觉表示。

Result: 在10个基准测试中，GeoMag在像素级任务上表现优异，同时在其他粒度任务上保持竞争力。

Conclusion: GeoMag通过多粒度解析和计算优化，显著提升了遥感图像理解的性能和应用范围。

Abstract: The application of Vision-Language Models (VLMs) in remote sensing (RS) image
understanding has achieved notable progress, demonstrating the basic ability to
recognize and describe geographical entities. However, existing RS-VLMs are
mostly limited to image-level and region-level tasks, lacking the capability to
handle pixel-level tasks and performing poorly in small-object recognition
scenarios. Moreover, RS-VLMs consume significant computational resources when
processing high-resolution RS images, further restricting their practical
applicability. In this context, we propose GeoMag (Geographical Magnifier), an
end-to-end general-purpose large model framework for RS. GeoMag dynamically
focuses the attention scope based on prompt semantics to effectively perform
remote sensing image parsing across multiple levels of granularity. This method
introduces Task-driven Multi-granularity Resolution Adjustment (TMRA) and
Prompt-guided Semantic-aware Cropping (PSC), which adaptively reduce the
spatial resolution of task-irrelevant regions while enhancing the visual
representation of task-relevant areas. This approach improves the model's
perception of critical target regions, suppresses background redundancy, and
reduces the computational cost of interpreting high-resolution RS imagery.
Extensive comparative experiments on 10 benchmarks demonstrate that GeoMag not
only excels in handling pixel-level tasks but also maintains competitive
performance across tasks of other granularities compared to existing RS-VLMs.

</details>


### [57] [What You Have is What You Track: Adaptive and Robust Multimodal Tracking](https://arxiv.org/abs/2507.05899)
*Yuedong Tan,Jiawei Shao,Eduard Zamfir,Ruanjun Li,Zhaochong An,Chao Ma,Danda Paudel,Luc Van Gool,Radu Timofte,Zongwei Wu*

Main category: cs.CV

TL;DR: 该论文研究了多模态数据在视觉跟踪中的性能问题，提出了一个灵活框架以应对数据缺失，并在多个基准测试中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 多模态数据在视觉跟踪中能提升鲁棒性，但传感器同步问题导致数据缺失，现有跟踪器性能下降，缺乏适应性。

Method: 提出基于缺失率的动态计算单元激活机制，采用异构混合专家融合方法和视频级掩码策略。

Result: 模型在9个基准测试中表现最佳，适应不同缺失率和场景复杂度。

Conclusion: 该框架在多模态数据缺失情况下显著提升跟踪性能，代码和基准将公开。

Abstract: Multimodal data is known to be helpful for visual tracking by improving
robustness to appearance variations. However, sensor synchronization challenges
often compromise data availability, particularly in video settings where
shortages can be temporal. Despite its importance, this area remains
underexplored. In this paper, we present the first comprehensive study on
tracker performance with temporally incomplete multimodal data. Unsurprisingly,
under such a circumstance, existing trackers exhibit significant performance
degradation, as their rigid architectures lack the adaptability needed to
effectively handle missing modalities. To address these limitations, we propose
a flexible framework for robust multimodal tracking. We venture that a tracker
should dynamically activate computational units based on missing data rates.
This is achieved through a novel Heterogeneous Mixture-of-Experts fusion
mechanism with adaptive complexity, coupled with a video-level masking strategy
that ensures both temporal consistency and spatial completeness which is
critical for effective video tracking. Surprisingly, our model not only adapts
to varying missing rates but also adjusts to scene complexity. Extensive
experiments show that our model achieves SOTA performance across 9 benchmarks,
excelling in both conventional complete and missing modality settings. The code
and benchmark will be publicly available at
https://github.com/supertyd/FlexTrack/tree/main.

</details>


### [58] [On the Effectiveness of Methods and Metrics for Explainable AI in Remote Sensing Image Scene Classification](https://arxiv.org/abs/2507.05916)
*Jonas Klotz,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 本文研究了遥感（RS）图像场景分类中解释方法和评估指标的有效性，分析了十种解释指标和五种特征归因方法，发现其局限性，并提供了选择指南。


<details>
  <summary>Details</summary>
Motivation: 现有可解释人工智能（xAI）方法及其评估指标多针对自然图像设计，直接用于遥感图像可能不适用，因此需要研究其在RS中的有效性。

Method: 方法学和实验分析十种解释指标（分为五类）和五种特征归因方法，应用于三个RS数据集。

Result: 发现解释方法和指标的局限性，如基于扰动的方法依赖基线，梯度方法在多标签图像中表现不佳，部分评估指标不可靠。

Conclusion: 基于分析结果，提供了在RS图像场景分类中选择解释方法、指标和超参数的指南。

Abstract: The development of explainable artificial intelligence (xAI) methods for
scene classification problems has attracted great attention in remote sensing
(RS). Most xAI methods and the related evaluation metrics in RS are initially
developed for natural images considered in computer vision (CV), and their
direct usage in RS may not be suitable. To address this issue, in this paper,
we investigate the effectiveness of explanation methods and metrics in the
context of RS image scene classification. In detail, we methodologically and
experimentally analyze ten explanation metrics spanning five categories
(faithfulness, robustness, localization, complexity, randomization), applied to
five established feature attribution methods (Occlusion, LIME, GradCAM, LRP,
and DeepLIFT) across three RS datasets. Our methodological analysis identifies
key limitations in both explanation methods and metrics. The performance of
perturbation-based methods, such as Occlusion and LIME, heavily depends on
perturbation baselines and spatial characteristics of RS scenes. Gradient-based
approaches like GradCAM struggle when multiple labels are present in the same
image, while some relevance propagation methods (LRP) can distribute relevance
disproportionately relative to the spatial extent of classes. Analogously, we
find limitations in evaluation metrics. Faithfulness metrics share the same
problems as perturbation-based methods. Localization metrics and complexity
metrics are unreliable for classes with a large spatial extent. In contrast,
robustness metrics and randomization metrics consistently exhibit greater
stability. Our experimental results support these methodological findings.
Based on our analysis, we provide guidelines for selecting explanation methods,
metrics, and hyperparameters in the context of RS image scene classification.

</details>


### [59] [Exploring Partial Multi-Label Learning via Integrating Semantic Co-occurrence Knowledge](https://arxiv.org/abs/2507.05992)
*Xin Wu,Fei Teng,Yue Feng,Kaibo Shi,Zhuosheng Lin,Ji Zhang,James Wang*

Main category: cs.CV

TL;DR: SCINet提出了一种用于部分多标签学习的新框架，通过捕捉标签与实例的共现模式，利用多模态模型增强语义对齐，并通过跨模态融合和语义增强策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 部分多标签学习需要从不完全标注的数据中提取知识，核心挑战是准确识别标签与实例之间的模糊关系。

Method: 提出SCINet框架，包括双主导提示模块、跨模态融合模块和内在语义增强策略，以捕捉共现模式并增强语义理解。

Result: 在四个基准数据集上的实验表明，SCINet优于现有方法。

Conclusion: SCINet通过共现模式和多模态融合有效解决了部分多标签学习的挑战，性能显著提升。

Abstract: Partial multi-label learning aims to extract knowledge from incompletely
annotated data, which includes known correct labels, known incorrect labels,
and unknown labels. The core challenge lies in accurately identifying the
ambiguous relationships between labels and instances. In this paper, we
emphasize that matching co-occurrence patterns between labels and instances is
key to addressing this challenge. To this end, we propose Semantic
Co-occurrence Insight Network (SCINet), a novel and effective framework for
partial multi-label learning. Specifically, SCINet introduces a bi-dominant
prompter module, which leverages an off-the-shelf multimodal model to capture
text-image correlations and enhance semantic alignment. To reinforce
instance-label interdependencies, we develop a cross-modality fusion module
that jointly models inter-label correlations, inter-instance relationships, and
co-occurrence patterns across instance-label assignments. Moreover, we propose
an intrinsic semantic augmentation strategy that enhances the model's
understanding of intrinsic data semantics by applying diverse image
transformations, thereby fostering a synergistic relationship between label
confidence and sample difficulty. Extensive experiments on four widely-used
benchmark datasets demonstrate that SCINet surpasses state-of-the-art methods.

</details>


### [60] [High-Resolution Visual Reasoning via Multi-Turn Grounding-Based Reinforcement Learning](https://arxiv.org/abs/2507.05920)
*Xinyu Huang,Yuhao Dong,Weiwei Tian,Bo Li,Rui Feng,Ziwei Liu*

Main category: cs.CV

TL;DR: MGPO是一种基于强化学习的框架，通过多轮对话自动裁剪关键视觉区域，提升多模态模型在高分辨率图像上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型在处理高分辨率图像时，会生成大量无关的视觉标记，影响性能。

Method: 提出MGPO框架，利用强化学习在多轮对话中自动裁剪关键区域，仅需二元奖励函数。

Result: 在无标注数据上训练后，MGPO在多个基准测试中表现优于基线方法，甚至超越GPT-4o。

Conclusion: MGPO通过强化学习有效提升了多模态模型的视觉定位能力，且无需额外标注。

Abstract: State-of-the-art large multi-modal models (LMMs) face challenges when
processing high-resolution images, as these inputs are converted into enormous
visual tokens, many of which are irrelevant to the downstream task. In this
paper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an
end-to-end reinforcement learning (RL) framework that enables LMMs to
iteratively focus on key visual regions by automatically cropping sub-images,
based on model-predicted grounding coordinates within a multi-turn conversation
framework. Compared to supervised fine-tuning (SFT), which requires costly
additional grounding annotations, our approach highlights that LMMs can emerge
robust grounding abilities during the RL training process, leveraging only a
binary reward function derived from the correctness of the final answer.
Additionally, we observe that LMMs struggle to autonomously trigger visual
grounding during the rollout process. To address this cold start problem, we
design a multi-turn conversational template and restrict policy loss
computation to model outputs generated across multiple dialogue rounds, thereby
promoting stable optimization. Extensive experiments demonstrate that, when
trained on standard visual-question-short answering data without grounding
annotations, MGPO effectively elicits stronger grounding capabilities compared
to GRPO, leading to 5.4\% improvement on in-distribution MME-Realworld and
5.2\% improvement on the challenging out-of-distribution (OOD) V* Bench.
Notably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses
OpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at
https://github.com/EvolvingLMMs-Lab/MGPO.

</details>


### [61] [Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS](https://arxiv.org/abs/2507.05999)
*Xinyu Wang,Muhammad Ibrahim,Atif Mansoor,Ajmal Mian*

Main category: cs.CV

TL;DR: 提出了一种基于点云与卫星图像对齐的结构化地理配准方法，解决了GNSS信号缺失区域的定位问题，显著提升了精度。


<details>
  <summary>Details</summary>
Motivation: 在GNSS信号缺失的高楼和桥梁密集区域，现有依赖GNSS和IMU的方法因定位不稳定而失效，亟需不依赖先验定位的解决方案。

Method: 使用预训练Point Transformer分割道路点，提取道路骨架和交点进行全局刚性对齐，再通过RBF插值局部优化，最后基于SRTM地形数据校正高程。

Result: 在KITTI和Perth数据集上，平面配准精度分别提升55.3%和77.4%，高程相关性分别提升30.5%和50.4%。

Conclusion: 该方法有效解决了无GNSS支持下的点云地理配准问题，为城市规模3D地图重建提供了可靠方案。

Abstract: Accurate geo-registration of LiDAR point clouds presents significant
challenges in GNSS signal denied urban areas with high-rise buildings and
bridges. Existing methods typically rely on real-time GNSS and IMU data, that
require pre-calibration and assume stable positioning during data collection.
However, this assumption often fails in dense urban areas, resulting in
localization errors. To address this, we propose a structured geo-registration
and spatial correction method that aligns 3D point clouds with satellite
images, enabling frame-wise recovery of GNSS information and reconstruction of
city scale 3D maps without relying on prior localization. The proposed approach
employs a pre-trained Point Transformer model to segment the road points and
then extracts the road skeleton and intersection points from the point cloud as
well as the target map for alignment. Global rigid alignment of the two is
performed using the intersection points, followed by local refinement using
radial basis function (RBF) interpolation. Elevation correction is then applied
to the point cloud based on terrain information from SRTM dataset to resolve
vertical discrepancies. The proposed method was tested on the popular KITTI
benchmark and a locally collected Perth (Western Australia) CBD dataset. On the
KITTI dataset, our method achieved an average planimetric alignment standard
deviation (STD) of 0.84~m across sequences with intersections, representing a
55.3\% improvement over the original dataset. On the Perth dataset, which lacks
GNSS information, our method achieved an average STD of 0.96~m compared to the
GPS data extracted from Google Maps API. This corresponds to a 77.4\%
improvement from the initial alignment. Our method also resulted in elevation
correlation gains of 30.5\% on the KITTI dataset and 50.4\% on the Perth
dataset.

</details>


### [62] [Beyond Appearance: Geometric Cues for Robust Video Instance Segmentation](https://arxiv.org/abs/2507.05948)
*Quanzhu Niu,Yikang Zhou,Shihao Chen,Tao Zhang,Shunping Ji*

Main category: cs.CV

TL;DR: 该论文通过引入几何感知（深度估计）来增强视频实例分割（VIS）的鲁棒性，研究了三种集成方法，其中EDC和SV表现优异，EDC方法在OVIS基准测试中达到56.2 AP，创下新纪录。


<details>
  <summary>Details</summary>
Motivation: 视频实例分割（VIS）面临遮挡、运动模糊和外观变化等挑战，作者希望通过几何感知（深度估计）提升其鲁棒性。

Method: 研究了三种深度集成方法：EDC（扩展深度通道）、SV（共享ViT）和DS（深度监督）。其中EDC将深度图作为输入通道，SV设计统一ViT骨干网络，DS利用深度预测辅助训练。

Result: EDC和SV显著提升了VIS的鲁棒性，EDC方法在OVIS基准测试中达到56.2 AP，创下新纪录。

Conclusion: 深度信息是提升视频理解鲁棒性的关键因素。

Abstract: Video Instance Segmentation (VIS) fundamentally struggles with pervasive
challenges including object occlusions, motion blur, and appearance variations
during temporal association. To overcome these limitations, this work
introduces geometric awareness to enhance VIS robustness by strategically
leveraging monocular depth estimation. We systematically investigate three
distinct integration paradigms. Expanding Depth Channel (EDC) method
concatenates the depth map as input channel to segmentation networks; Sharing
ViT (SV) designs a uniform ViT backbone, shared between depth estimation and
segmentation branches; Depth Supervision (DS) makes use of depth prediction as
an auxiliary training guide for feature learning. Though DS exhibits limited
effectiveness, benchmark evaluations demonstrate that EDC and SV significantly
enhance the robustness of VIS. When with Swin-L backbone, our EDC method gets
56.2 AP, which sets a new state-of-the-art result on OVIS benchmark. This work
conclusively establishes depth cues as critical enablers for robust video
understanding.

</details>


### [63] [TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and OCR-Guided Supervision](https://arxiv.org/abs/2507.06033)
*Syeda Anshrah Gillani,Mirza Samad Ahmed Baig,Osama Ahmed Khan,Shahid Munir Shah,Umema Mujeeb,Maheen Ali*

Main category: cs.CV

TL;DR: 论文提出了一种新框架GCDA，通过改进文本编码器和引入字符感知注意力机制，显著提升了文本到图像生成模型中文本的可读性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在生成图像时无法生成可读且拼写正确的文本，限制了其在广告、学习和创意设计等实际应用中的使用。

Method: GCDA框架包括双流文本编码器（编码语义和字形信息）、字符感知注意力机制（避免失真）和OCR辅助的微调阶段（优化文本可读性）。

Result: 在MARIO-10M和T2I-CompBench数据集上的实验表明，GCDA在所有指标上均达到最新水平，如字符错误率（0.08 vs 0.21）和单词错误率（0.15 vs 0.25）。

Conclusion: GCDA通过改进文本生成能力，显著提升了文本到图像生成模型的实用性，为实际应用提供了更好的支持。

Abstract: The modern text-to-image diffusion models boom has opened a new era in
digital content production as it has proven the previously unseen ability to
produce photorealistic and stylistically diverse imagery based on the semantics
of natural-language descriptions. However, the consistent disadvantage of these
models is that they cannot generate readable, meaningful, and correctly spelled
text in generated images, which significantly limits the use of practical
purposes like advertising, learning, and creative design. This paper introduces
a new framework, namely Glyph-Conditioned Diffusion with Character-Aware
Attention (GCDA), using which a typical diffusion backbone is extended by three
well-designed modules. To begin with, the model has a dual-stream text encoder
that encodes both semantic contextual information and explicit glyph
representations, resulting in a character-aware representation of the input
text that is rich in nature. Second, an attention mechanism that is aware of
the character is proposed with a new attention segregation loss that aims to
limit the attention distribution of each character independently in order to
avoid distortion artifacts. Lastly, GCDA has an OCR-in-the-loop fine-tuning
phase, where a full text perceptual loss, directly optimises models to be
legible and accurately spell. Large scale experiments to benchmark datasets,
such as MARIO-10M and T2I-CompBench, reveal that GCDA sets a new
state-of-the-art on all metrics, with better character based metrics on text
rendering (Character Error Rate: 0.08 vs 0.21 for the previous best; Word Error
Rate: 0.15 vs 0.25), human perception, and comparable image synthesis quality
on high-fidelity (FID: 14.3).

</details>


### [64] [High-Fidelity and Generalizable Neural Surface Reconstruction with Sparse Feature Volumes](https://arxiv.org/abs/2507.05952)
*Aoxiang Fan,Corentin Dumery,Nicolas Talabot,Hieu Le,Pascal Fua*

Main category: cs.CV

TL;DR: 提出了一种稀疏表示方法，通过两阶段训练提高神经表面重建的分辨率和效率。


<details>
  <summary>Details</summary>
Motivation: 密集3D特征体积在高分辨率下效率低下，限制了重建质量，因此需要更高效的稀疏表示方法。

Method: 两阶段方法：预测体素占用率，仅在占用率高的体素中进行特征计算和体积渲染，并开发了高效的稀疏体积算法。

Result: 存储需求减少50倍以上，支持512^3分辨率重建，性能优于现有方法。

Conclusion: 稀疏表示方法显著提高了重建质量和效率，适用于标准硬件。

Abstract: Generalizable neural surface reconstruction has become a compelling technique
to reconstruct from few images without per-scene optimization, where dense 3D
feature volume has proven effective as a global representation of scenes.
However, the dense representation does not scale well to increasing voxel
resolutions, severely limiting the reconstruction quality. We thus present a
sparse representation method, that maximizes memory efficiency and enables
significantly higher resolution reconstructions on standard hardware. We
implement this through a two-stage approach: First training a network to
predict voxel occupancies from posed images and associated depth maps, then
computing features and performing volume rendering only in voxels with
sufficiently high occupancy estimates. To support this sparse representation,
we developed custom algorithms for efficient sampling, feature aggregation, and
querying from sparse volumes-overcoming the dense-volume assumptions inherent
in existing works. Experiments on public datasets demonstrate that our approach
reduces storage requirements by more than 50 times without performance
degradation, enabling reconstructions at $512^3$ resolution compared to the
typical $128^3$ on similar hardware, and achieving superior reconstruction
accuracy over current state-of-the-art methods.

</details>


### [65] [VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis](https://arxiv.org/abs/2507.06060)
*Alexandre Symeonidis-Herzig,Özge Mercanoğlu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: VisualSpeaker提出了一种基于光真实感可微分渲染的新方法，通过视觉语音识别监督改进3D面部动画，显著提升了动画质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 高保真的3D面部动画在人类交互和可访问性中至关重要，但现有方法受限于网格域，无法充分利用2D计算机视觉和图形的快速视觉创新。

Method: 使用光真实感可微分渲染，结合视觉语音识别监督，提出感知唇读损失函数，通过预训练的视觉自动语音识别模型优化3D高斯泼溅渲染。

Result: 在MEAD数据集上，Lip Vertex Error指标提升56.1%，同时保持网格驱动动画的可控性。

Conclusion: VisualSpeaker通过感知焦点支持准确的口型动作，对消除手语动画中的歧义至关重要。

Abstract: Realistic, high-fidelity 3D facial animations are crucial for expressive
avatar systems in human-computer interaction and accessibility. Although prior
methods show promising quality, their reliance on the mesh domain limits their
ability to fully leverage the rapid visual innovations seen in 2D computer
vision and graphics. We propose VisualSpeaker, a novel method that bridges this
gap using photorealistic differentiable rendering, supervised by visual speech
recognition, for improved 3D facial animation. Our contribution is a perceptual
lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting
avatar renders through a pre-trained Visual Automatic Speech Recognition model
during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker
improves both the standard Lip Vertex Error metric by 56.1% and the perceptual
quality of the generated animations, while retaining the controllability of
mesh-driven animation. This perceptual focus naturally supports accurate
mouthings, essential cues that disambiguate similar manual signs in sign
language avatars.

</details>


### [66] [Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation](https://arxiv.org/abs/2507.05963)
*Zhenghao Zhang,Junchao Liao,Xiangyu Meng,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: Tora2是Tora的增强版，通过解耦个性化提取器和门控自注意力机制，实现了多实体外观和运动的同步定制，显著提升了视频生成的多模态条件对齐和细节保留。


<details>
  <summary>Details</summary>
Motivation: 改进现有扩散变压器模型（如Tora）在视频生成中的外观和运动定制能力，解决多模态条件训练中的错位问题。

Method: 1. 解耦个性化提取器生成多实体嵌入；2. 门控自注意力机制整合轨迹、文本和视觉信息；3. 对比损失联合优化运动与个性化嵌入。

Result: Tora2在视频生成中实现了多实体外观和运动的同步定制，性能与现有方法相当，同时提供更先进的运动控制能力。

Conclusion: Tora2在多条件视频生成领域取得了重要进展，为外观和运动定制提供了新方法。

Abstract: Recent advances in diffusion transformer models for motion-guided video
generation, such as Tora, have shown significant progress. In this paper, we
present Tora2, an enhanced version of Tora, which introduces several design
improvements to expand its capabilities in both appearance and motion
customization. Specifically, we introduce a decoupled personalization extractor
that generates comprehensive personalization embeddings for multiple open-set
entities, better preserving fine-grained visual details compared to previous
methods. Building on this, we design a gated self-attention mechanism to
integrate trajectory, textual description, and visual information for each
entity. This innovation significantly reduces misalignment in multimodal
conditioning during training. Moreover, we introduce a contrastive loss that
jointly optimizes trajectory dynamics and entity consistency through explicit
mapping between motion and personalization embeddings. Tora2 is, to our best
knowledge, the first method to achieve simultaneous multi-entity customization
of appearance and motion for video generation. Experimental results demonstrate
that Tora2 achieves competitive performance with state-of-the-art customization
methods while providing advanced motion control capabilities, which marks a
critical advancement in multi-condition video generation. Project page:
https://github.com/alibaba/Tora .

</details>


### [67] [T-LoRA: Single Image Diffusion Model Customization Without Overfitting](https://arxiv.org/abs/2507.05964)
*Vera Soboleva,Aibek Alanov,Andrey Kuznetsov,Konstantin Sobolev*

Main category: cs.CV

TL;DR: T-LoRA是一种针对扩散模型个性化设计的时序依赖低秩适应框架，通过动态调整基于时间步的更新和正交初始化，解决了单图像定制中的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在有限训练样本下容易过拟合，影响泛化能力和输出多样性，本文旨在通过单图像定制解决这一问题。

Method: 提出T-LoRA框架，包括动态调整时间步相关的低秩更新和正交初始化的权重参数化技术。

Result: 实验表明T-LoRA在概念保真度和文本对齐上优于标准LoRA和其他方法，适用于数据有限场景。

Conclusion: T-LoRA在单图像定制中表现出色，平衡了概念保真度和文本对齐，具有实际应用潜力。

Abstract: While diffusion model fine-tuning offers a powerful approach for customizing
pre-trained models to generate specific objects, it frequently suffers from
overfitting when training samples are limited, compromising both generalization
capability and output diversity. This paper tackles the challenging yet most
impactful task of adapting a diffusion model using just a single concept image,
as single-image customization holds the greatest practical potential. We
introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework
specifically designed for diffusion model personalization. In our work we show
that higher diffusion timesteps are more prone to overfitting than lower ones,
necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates
two key innovations: (1) a dynamic fine-tuning strategy that adjusts
rank-constrained updates based on diffusion timesteps, and (2) a weight
parametrization technique that ensures independence between adapter components
through orthogonal initialization. Extensive experiments show that T-LoRA and
its individual components outperform standard LoRA and other diffusion model
personalization techniques. They achieve a superior balance between concept
fidelity and text alignment, highlighting the potential of T-LoRA in
data-limited and resource-constrained scenarios. Code is available at
https://github.com/ControlGenAI/T-LoRA.

</details>


### [68] [Automatic Synthesis of High-Quality Triplet Data for Composed Image Retrieval](https://arxiv.org/abs/2507.05970)
*Haiwen Li,Delong Liu,Zhaohui Hou,Zhicheng Zhao,Fei Su*

Main category: cs.CV

TL;DR: 提出了一种自动生成三元组的流程和合成数据集CIRHS，结合新框架CoAlign，显著提升了零样本和监督学习的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有CIR方法依赖昂贵人工标注三元组的问题，提升可扩展性和零样本能力。

Method: 利用LLM生成多样化提示，控制文本到图像生成模型生成图像对，构建CIRHS数据集；提出CoAlign框架实现全局对齐和局部推理。

Result: 在三个基准测试中取得优异零样本性能，监督训练下超越现有方法。

Conclusion: 首次证明完全合成数据集训练CIR模型的可行性，验证了CoAlign框架的有效性。

Abstract: As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)
aims to retrieve target images using multimodal (image+text) queries. Although
many existing CIR methods have attained promising performance, their reliance
on costly, manually labeled triplets hinders scalability and zero-shot
capability. To address this issue, we propose a scalable pipeline for automatic
triplet generation, along with a fully synthetic dataset named Composed Image
Retrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a
large language model (LLM) to generate diverse prompts, controlling a
text-to-image generative model to produce image pairs with identical elements
in each pair, which are then filtered and reorganized to form the CIRHS
dataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a
novel CIR framework, which can accomplish global alignment and local reasoning
within a broader context, enabling the model to learn more robust and
informative representations. By utilizing the synthetic CIRHS dataset, CoAlign
achieves outstanding zero-shot performance on three commonly used benchmarks,
demonstrating for the first time the feasibility of training CIR models on a
fully synthetic dataset. Furthermore, under supervised training, our method
outperforms all the state-of-the-art supervised CIR approaches, validating the
effectiveness of our proposed retrieval framework. The code and the CIRHS
dataset will be released soon.

</details>


### [69] [Ensemble-Based Deepfake Detection using State-of-the-Art Models with Robust Cross-Dataset Generalisation](https://arxiv.org/abs/2507.05996)
*Haroon Wahab,Hassan Ugail,Lujain Jaleel*

Main category: cs.CV

TL;DR: 论文提出一种基于集成学习的方法，提升深度伪造检测模型在分布外数据上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测模型在基准数据集上表现优异，但在分布外数据上性能显著下降，需要一种更稳健的解决方案。

Method: 通过结合多个先进非对称模型的预测概率，构建集成模型，并在两个不同的分布外数据集上进行实验验证。

Result: 实验表明，单一模型无法在所有场景中表现最优，而集成方法在所有情况下均能提供稳定且可靠的性能。

Conclusion: 非对称集成方法为现实世界中的深度伪造检测提供了一种稳健且可扩展的解决方案。

Abstract: Machine learning-based Deepfake detection models have achieved impressive
results on benchmark datasets, yet their performance often deteriorates
significantly when evaluated on out-of-distribution data. In this work, we
investigate an ensemble-based approach for improving the generalization of
deepfake detection systems across diverse datasets. Building on a recent
open-source benchmark, we combine prediction probabilities from several
state-of-the-art asymmetric models proposed at top venues. Our experiments span
two distinct out-of-domain datasets and demonstrate that no single model
consistently outperforms others across settings. In contrast, ensemble-based
predictions provide more stable and reliable performance in all scenarios. Our
results suggest that asymmetric ensembling offers a robust and scalable
solution for real-world deepfake detection where prior knowledge of forgery
type or quality is often unavailable.

</details>


### [70] [MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions by Disentangled Embedding](https://arxiv.org/abs/2507.06071)
*Chang Liu,Ye Pan,Chenyang Ding,Susanto Rahardja,Xiaokang Yang*

Main category: cs.CV

TL;DR: MEDTalk是一个用于细粒度和动态情感说话头生成的新框架，通过解耦内容和情感嵌入空间，结合音频和文本输入，生成更自然和多样化的面部动画。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常局限于静态和预定义的情感标签，缺乏多样性和自然性。MEDTalk旨在解决这些问题，提供更精细和动态的情感表达控制。

Method: 通过跨重建过程解耦内容和情感嵌入空间，结合音频和文本输入预测帧级强度变化，动态调整情感特征，并支持多模态输入（如文本描述和参考图像）以增强个性化控制。

Result: 生成的动画具有同步的唇部运动和生动的面部表情，能够方便地集成到工业生产线中。

Conclusion: MEDTalk在情感表达的多样性和自然性方面优于现有方法，适用于实际生产环境。

Abstract: Audio-driven emotional 3D facial animation aims to generate synchronized lip
movements and vivid facial expressions. However, most existing approaches focus
on static and predefined emotion labels, limiting their diversity and
naturalness. To address these challenges, we propose MEDTalk, a novel framework
for fine-grained and dynamic emotional talking head generation. Our approach
first disentangles content and emotion embedding spaces from motion sequences
using a carefully designed cross-reconstruction process, enabling independent
control over lip movements and facial expressions. Beyond conventional
audio-driven lip synchronization, we integrate audio and speech text,
predicting frame-wise intensity variations and dynamically adjusting static
emotion features to generate realistic emotional expressions. Furthermore, to
enhance control and personalization, we incorporate multimodal inputs-including
text descriptions and reference expression images-to guide the generation of
user-specified facial expressions. With MetaHuman as the priority, our
generated results can be conveniently integrated into the industrial production
pipeline.

</details>


### [71] [MCAM: Multimodal Causal Analysis Model for Ego-Vehicle-Level Driving Video Understanding](https://arxiv.org/abs/2507.06072)
*Tongtong Cheng,Rongzhen Li,Yixin Xiong,Tao Zhang,Jing Wang,Kai Liu*

Main category: cs.CV

TL;DR: MCAM模型通过多模态因果分析提升自动驾驶视频理解，解决了现有方法的浅层因果挖掘和虚假相关性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在自动驾驶视频理解中常忽略深层因果关系和多模态虚假相关，MCAM旨在解决这些问题。

Method: MCAM结合多级特征提取器、因果分析模块（DAG建模）和视觉-语言Transformer，实现多模态因果建模。

Result: 在BDD-X和CoVLA数据集上，MCAM在视觉-语言因果关系学习中达到SOTA性能。

Conclusion: MCAM有效捕捉视频序列中的因果关系，适用于自动驾驶应用。

Abstract: Accurate driving behavior recognition and reasoning are critical for
autonomous driving video understanding. However, existing methods often tend to
dig out the shallow causal, fail to address spurious correlations across
modalities, and ignore the ego-vehicle level causality modeling. To overcome
these limitations, we propose a novel Multimodal Causal Analysis Model (MCAM)
that constructs latent causal structures between visual and language
modalities. Firstly, we design a multi-level feature extractor to capture
long-range dependencies. Secondly, we design a causal analysis module that
dynamically models driving scenarios using a directed acyclic graph (DAG) of
driving states. Thirdly, we utilize a vision-language transformer to align
critical visual features with their corresponding linguistic expressions.
Extensive experiments on the BDD-X, and CoVLA datasets demonstrate that MCAM
achieves SOTA performance in visual-language causal relationship learning.
Furthermore, the model exhibits superior capability in capturing causal
characteristics within video sequences, showcasing its effectiveness for
autonomous driving applications. The code is available at
https://github.com/SixCorePeach/MCAM.

</details>


### [72] [Discontinuity-aware Normal Integration for Generic Central Camera Models](https://arxiv.org/abs/2507.06075)
*Francesco Milano,Manuel López-Antequera,Naina Dhingra,Roland Siegwart,Robert Thiel*

Main category: cs.CV

TL;DR: 提出了一种新的法线积分方法，显式建模深度不连续性并支持通用中央相机模型，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有法线积分方法对深度不连续性和相机模型支持有限，需改进。

Method: 基于局部平面假设，通过表面法线与射线方向约束建模。

Result: 在标准基准测试中达到最优，首次支持通用中央相机模型。

Conclusion: 新方法在法线积分中表现优异，拓展了应用范围。

Abstract: Recovering a 3D surface from its surface normal map, a problem known as
normal integration, is a key component for photometric shape reconstruction
techniques such as shape-from-shading and photometric stereo. The vast majority
of existing approaches for normal integration handle only implicitly the
presence of depth discontinuities and are limited to orthographic or ideal
pinhole cameras. In this paper, we propose a novel formulation that allows
modeling discontinuities explicitly and handling generic central cameras. Our
key idea is based on a local planarity assumption, that we model through
constraints between surface normals and ray directions. Compared to existing
methods, our approach more accurately approximates the relation between depth
and surface normals, achieves state-of-the-art results on the standard normal
integration benchmark, and is the first to directly handle generic central
camera models.

</details>


### [73] [ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models](https://arxiv.org/abs/2507.06078)
*Chihan Huang,Hao Tang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite the success of deep learning across various domains, it remains
vulnerable to adversarial attacks. Although many existing adversarial attack
methods achieve high success rates, they typically rely on $\ell_{p}$-norm
perturbation constraints, which do not align with human perceptual
capabilities. Consequently, researchers have shifted their focus toward
generating natural, unrestricted adversarial examples (UAEs). GAN-based
approaches suffer from inherent limitations, such as poor image quality due to
instability and mode collapse. Meanwhile, diffusion models have been employed
for UAE generation, but they still rely on iterative PGD perturbation
injection, without fully leveraging their central denoising capabilities. In
this paper, we introduce a novel approach for generating UAEs based on
diffusion models, named ScoreAdv. This method incorporates an interpretable
adversarial guidance mechanism to gradually shift the sampling distribution
towards the adversarial distribution, while using an interpretable saliency map
to inject the visual information of a reference image into the generated
samples. Notably, our method is capable of generating an unlimited number of
natural adversarial examples and can attack not only classification models but
also retrieval models. We conduct extensive experiments on ImageNet and CelebA
datasets, validating the performance of ScoreAdv across ten target models in
both black-box and white-box settings. Our results demonstrate that ScoreAdv
achieves state-of-the-art attack success rates and image quality. Furthermore,
the dynamic balance between denoising and adversarial perturbation enables
ScoreAdv to remain robust even under defensive measures.

</details>


### [74] [CAST-Phys: Contactless Affective States Through Physiological signals Database](https://arxiv.org/abs/2507.06080)
*Joaquim Comas,Alexander Joel Vera,Xavier Vives,Eleonora De Filippi,Alexandre Pereda,Federico Sukno*

Main category: cs.CV

TL;DR: 论文提出了一种新型无接触多模态情感识别数据集CAST-Phys，解决了现有数据集的不足，并展示了生理信号在情感识别中的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有情感识别数据集缺乏多模态数据，且接触式设备可能干扰真实情感反应，因此需要无接触的多模态情感识别方法。

Method: 构建了CAST-Phys数据集，包含PPG、EDA、RR等生理信号和高分辨率面部视频，支持远程信号恢复和多模态融合分析。

Result: 生理信号在情感识别中起关键作用，多模态融合显著提升了无接触情感识别的效果。

Conclusion: CAST-Phys数据集为无接触情感识别提供了高质量数据支持，推动了该领域的技术发展。

Abstract: In recent years, affective computing and its applications have become a
fast-growing research topic. Despite significant advancements, the lack of
affective multi-modal datasets remains a major bottleneck in developing
accurate emotion recognition systems. Furthermore, the use of contact-based
devices during emotion elicitation often unintentionally influences the
emotional experience, reducing or altering the genuine spontaneous emotional
response. This limitation highlights the need for methods capable of extracting
affective cues from multiple modalities without physical contact, such as
remote physiological emotion recognition. To address this, we present the
Contactless Affective States Through Physiological Signals Database
(CAST-Phys), a novel high-quality dataset explicitly designed for multi-modal
remote physiological emotion recognition using facial and physiological cues.
The dataset includes diverse physiological signals, such as
photoplethysmography (PPG), electrodermal activity (EDA), and respiration rate
(RR), alongside high-resolution uncompressed facial video recordings, enabling
the potential for remote signal recovery. Our analysis highlights the crucial
role of physiological signals in realistic scenarios where facial expressions
alone may not provide sufficient emotional information. Furthermore, we
demonstrate the potential of remote multi-modal emotion recognition by
evaluating the impact of individual and fused modalities, showcasing its
effectiveness in advancing contactless emotion recognition technologies.

</details>


### [75] [Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot Multi-Species Plant Identification](https://arxiv.org/abs/2507.06093)
*Murilo Gustineli,Anthony Miyaguchi,Adrian Cheung,Divyansh Khattak*

Main category: cs.CV

TL;DR: DS@GT团队在PlantCLEF 2025挑战赛中提出的第二名解决方案，结合了ViT模型、分块策略和领域先验优化，无需额外训练即实现了0.348的F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决多物种植物识别问题，尤其是在植被样方图像中，通过优化现有模型和策略提升性能。

Method: 使用ViTD2PC24All模型进行分块推理，4x4分块策略与模型感受野对齐，结合PaCMAP + K-Means聚类和地理过滤优化预测。

Result: 在私有排行榜上实现了0.348的宏观平均F1分数。

Conclusion: 该方法无需额外训练即可高效识别多物种植物，代码和脚本已开源。

Abstract: We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on
multi-species plant identification in vegetation quadrat images. Our pipeline
combines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level
inference, (ii) a 4x4 tiling strategy that aligns patch size with the network's
518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP +
K-Means visual clustering and geolocation filtering. Tile predictions are
aggregated by majority vote and re-weighted with cluster-specific Bayesian
priors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while
requiring no additional training. All code, configuration files, and
reproducibility scripts are publicly available at
https://github.com/dsgt-arc/plantclef-2025.

</details>


### [76] [Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering](https://arxiv.org/abs/2507.06103)
*Jiayi Song,Zihan Ye,Qingyuan Zhou,Weidong Yang,Ben Fei,Jingyi Xu,Ying He,Wanli Ouyang*

Main category: cs.CV

TL;DR: Ref-Unlock是一种基于3D高斯溅射的几何感知反射建模框架，通过显式分离透射和反射成分，提升复杂反射场景的渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如NeRF和3DGS）在处理反射表面时，常将反射误认为物理几何，导致重建质量下降。

Method: 采用双分支表示和高阶球谐函数捕捉高频反射细节，结合反射去除模块和几何感知的双边平滑约束。

Result: Ref-Unlock显著优于传统GS方法，与NeRF模型竞争，并支持灵活的反射编辑。

Conclusion: Ref-Unlock为反射场景的真实渲染提供了高效且通用的解决方案。

Abstract: Accurately rendering scenes with reflective surfaces remains a significant
challenge in novel view synthesis, as existing methods like Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections
as physical geometry, resulting in degraded reconstructions. Previous methods
rely on incomplete and non-generalizable geometric constraints, leading to
misalignment between the positions of Gaussian splats and the actual scene
geometry. When dealing with real-world scenes containing complex geometry, the
accumulation of Gaussians further exacerbates surface artifacts and results in
blurred reconstructions. To address these limitations, in this work, we propose
Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D
Gaussian Splatting, which explicitly disentangles transmitted and reflected
components to better capture complex reflections and enhance geometric
consistency in real-world scenes. Our approach employs a dual-branch
representation with high-order spherical harmonics to capture high-frequency
reflective details, alongside a reflection removal module providing pseudo
reflection-free supervision to guide clean decomposition. Additionally, we
incorporate pseudo-depth maps and a geometry-aware bilateral smoothness
constraint to enhance 3D geometric consistency and stability in decomposition.
Extensive experiments demonstrate that Ref-Unlock significantly outperforms
classical GS-based reflection methods and achieves competitive results with
NeRF-based models, while enabling flexible vision foundation models (VFMs)
driven reflection editing. Our method thus offers an efficient and
generalizable solution for realistic rendering of reflective scenes. Our code
is available at https://ref-unlock.github.io/.

</details>


### [77] [Omni-Video: Democratizing Unified Video Understanding and Generation](https://arxiv.org/abs/2507.06119)
*Zhiyu Tan,Hao Yang,Luozheng Qin,Jia Gong,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: Omni-Video是一个统一的视频理解和生成框架，通过多模态大语言模型（MLLMs）生成视觉线索，并结合扩散解码器生成高质量视频。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型主要关注图像处理，缺乏统一的视频理解和生成模型。

Method: 1) 轻量级架构设计，将视觉头与MLLMs结合，适配器连接扩散解码器；2) 高效多阶段训练方案。

Result: 模型在视频生成、编辑和理解任务中表现出良好的泛化能力。

Conclusion: Omni-Video为视频统一建模提供了高效解决方案。

Abstract: Notable breakthroughs in unified understanding and generation modeling have
led to remarkable advancements in image understanding, reasoning, production
and editing, yet current foundational models predominantly focus on processing
images, creating a gap in the development of unified models for video
understanding and generation. This report presents Omni-Video, an efficient and
effective unified framework for video understanding, generation, as well as
instruction-based editing. Our key insight is to teach existing multimodal
large language models (MLLMs) to produce continuous visual clues that are used
as the input of diffusion decoders, which produce high-quality videos
conditioned on these visual clues. To fully unlock the potential of our system
for unified video modeling, we integrate several technical improvements: 1) a
lightweight architectural design that respectively attaches a vision head on
the top of MLLMs and a adapter before the input of diffusion decoders, the
former produce visual tokens for the latter, which adapts these visual tokens
to the conditional space of diffusion decoders; and 2) an efficient multi-stage
training scheme that facilitates a fast connection between MLLMs and diffusion
decoders with limited data and computational resources. We empirically
demonstrate that our model exhibits satisfactory generalization abilities
across video generation, editing and understanding tasks.

</details>


### [78] [Prompt-Free Conditional Diffusion for Multi-object Image Augmentation](https://arxiv.org/abs/2507.06146)
*Haoyu Wang,Lei Zhang,Wei Wei,Chen Ding,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种无需提示的条件扩散框架，用于多目标图像增强，通过局部-全局语义融合策略和LoRA知识注入解决生成图像与原始数据的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成多目标图像时，要么依赖文本条件导致与原始数据偏差，要么依赖原始图像导致多样性不足。

Method: 采用局部-全局语义融合策略提取图像语义替代文本，通过LoRA注入知识，并设计基于计数的损失函数辅助训练。

Result: 实验表明，该方法在生成多样性和下游任务性能上优于现有基线，并展示了强泛化能力。

Conclusion: 提出的框架有效解决了多目标图像生成的偏差和多样性问题，具有实际应用潜力。

Abstract: Diffusion models has underpinned much recent advances of dataset augmentation
in various computer vision tasks. However, when involving generating
multi-object images as real scenarios, most existing methods either rely
entirely on text condition, resulting in a deviation between the generated
objects and the original data, or rely too much on the original images,
resulting in a lack of diversity in the generated images, which is of limited
help to downstream tasks. To mitigate both problems with one stone, we propose
a prompt-free conditional diffusion framework for multi-object image
augmentation. Specifically, we introduce a local-global semantic fusion
strategy to extract semantics from images to replace text, and inject knowledge
into the diffusion model through LoRA to alleviate the category deviation
between the original model and the target dataset. In addition, we design a
reward model based counting loss to assist the traditional reconstruction loss
for model training. By constraining the object counts of each category instead
of pixel-by-pixel constraints, bridging the quantity deviation between the
generated data and the original data while improving the diversity of the
generated data. Experimental results demonstrate the superiority of the
proposed method over several representative state-of-the-art baselines and
showcase strong downstream task gain and out-of-domain generalization
capabilities. Code is available at
\href{https://github.com/00why00/PFCD}{here}.

</details>


### [79] [Normalizing Diffusion Kernels with Optimal Transport](https://arxiv.org/abs/2507.06161)
*Nathan Kessler,Robin Magnet,Jean Feydy*

Main category: cs.CV

TL;DR: 论文提出了一种基于相似性或邻接矩阵的平滑算子，通过Sinkhorn算法归一化，使其具有类似Laplacian的性质，适用于不规则数据。


<details>
  <summary>Details</summary>
Motivation: 传统Laplacian算子需要严格定义的域结构，而简单卷积核和消息传递层对边界有偏差，因此需要一种更通用的平滑方法。

Method: 引入一类基于相似性或邻接矩阵的平滑算子，通过对称Sinkhorn算法归一化为扩散类算子，保留Laplacian的优良性质。

Result: 该方法能近似热扩散，保留Laplacian的谱信息，适用于点云、稀疏体素网格等不规则数据。

Conclusion: 提出的平滑算子填补了传统Laplacian与简单卷积核之间的空白，为不规则数据的处理提供了理论支持。

Abstract: Smoothing a signal based on local neighborhoods is a core operation in
machine learning and geometry processing. On well-structured domains such as
vector spaces and manifolds, the Laplace operator derived from differential
geometry offers a principled approach to smoothing via heat diffusion, with
strong theoretical guarantees. However, constructing such Laplacians requires a
carefully defined domain structure, which is not always available. Most
practitioners thus rely on simple convolution kernels and message-passing
layers, which are biased against the boundaries of the domain. We bridge this
gap by introducing a broad class of smoothing operators, derived from general
similarity or adjacency matrices, and demonstrate that they can be normalized
into diffusion-like operators that inherit desirable properties from
Laplacians. Our approach relies on a symmetric variant of the Sinkhorn
algorithm, which rescales positive smoothing operators to match the structural
behavior of heat diffusion. This construction enables Laplacian-like smoothing
and processing of irregular data such as point clouds, sparse voxel grids or
mixture of Gaussians. We show that the resulting operators not only approximate
heat diffusion but also retain spectral information from the Laplacian itself,
with applications to shape analysis and matching.

</details>


### [80] [OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion](https://arxiv.org/abs/2507.06165)
*Yunhan Yang,Yufan Zhou,Yuan-Chen Guo,Zi-Xin Zou,Yukun Huang,Ying-Tian Liu,Hao Xu,Ding Liang,Yan-Pei Cao,Xihui Liu*

Main category: cs.CV

TL;DR: OmniPart是一个新颖的框架，用于生成具有明确可编辑部分结构的3D对象，通过两阶段协同方法实现高语义解耦和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法通常只能生成单一整体形状，限制了交互应用的实用性，因此需要一种支持部分感知的3D生成方法。

Method: OmniPart分为两阶段：1) 自回归结构规划模块生成可控的3D部分边界框；2) 空间条件修正流模型同时合成所有3D部分。

Result: 实验表明，OmniPart在性能上达到领先水平，支持用户定义的部分粒度、精确定位和多样化下游应用。

Conclusion: OmniPart为更可解释、可编辑和多功能化的3D内容生成铺平了道路。

Abstract: The creation of 3D assets with explicit, editable part structures is crucial
for advancing interactive applications, yet most generative methods produce
only monolithic shapes, limiting their utility. We introduce OmniPart, a novel
framework for part-aware 3D object generation designed to achieve high semantic
decoupling among components while maintaining robust structural cohesion.
OmniPart uniquely decouples this complex task into two synergistic stages: (1)
an autoregressive structure planning module generates a controllable,
variable-length sequence of 3D part bounding boxes, critically guided by
flexible 2D part masks that allow for intuitive control over part decomposition
without requiring direct correspondences or semantic labels; and (2) a
spatially-conditioned rectified flow model, efficiently adapted from a
pre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and
consistently within the planned layout. Our approach supports user-defined part
granularity, precise localization, and enables diverse downstream applications.
Extensive experiments demonstrate that OmniPart achieves state-of-the-art
performance, paving the way for more interpretable, editable, and versatile 3D
content.

</details>


### [81] [Enhancing Scientific Visual Question Answering through Multimodal Reasoning and Ensemble Modeling](https://arxiv.org/abs/2507.06183)
*Prahitha Movva,Naga Harshita Marupaka*

Main category: cs.CV

TL;DR: 论文提出了一种针对科学图表视觉问答（SciVQA）的方法，通过优化提示、链式推理和集成模型，提升了模型在数值处理和一致性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉问答方法在科学数据解释中面临数值处理和多步推理的挑战，需要更精确的解决方案。

Method: 使用5B到8B参数的模型（如InternVL3）进行实验，并开发了集成模型。

Result: InternVL3在ROUGE-1和ROUGE-L F1得分上达到0.740，BERTScore为0.983；集成模型进一步提升了性能。

Conclusion: 优化提示、链式推理和集成建模能有效提升视觉问答能力，InternVL3表现最佳。

Abstract: Technical reports and articles often contain valuable information in the form
of semi-structured data like charts, and figures. Interpreting these and using
the information from them is essential for downstream tasks such as question
answering (QA). Current approaches to visual question answering often struggle
with the precision required for scientific data interpretation, particularly in
handling numerical values, multi-step reasoning over visual elements, and
maintaining consistency between visual observation and textual reasoning. We
present our approach to the SciVQA 2025 shared task, focusing on answering
visual and non-visual questions grounded in scientific figures from scholarly
articles.
  We conducted a series of experiments using models with 5B to 8B parameters.
Our strongest individual model, InternVL3, achieved ROUGE-1 and ROUGE-L F1
scores of \textbf{0.740} and a BERTScore of \textbf{0.983} on the SciVQA test
split. We also developed an ensemble model with multiple vision language models
(VLMs). Through error analysis on the validation split, our ensemble approach
improved performance compared to most individual models, though InternVL3
remained the strongest standalone performer. Our findings underscore the
effectiveness of prompt optimization, chain-of-thought reasoning and ensemble
modeling in improving the model's ability in visual question answering.

</details>


### [82] [CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions](https://arxiv.org/abs/2507.06210)
*Yuchen Huang,Zhiyuan Fan,Zhitao He,Sandeep Polisetty,Wenyan Li,Yi R. Fung*

Main category: cs.CV

TL;DR: 论文提出了一种方法，通过合成文化数据集CulTwin和定制对比学习，改进CLIP模型在细粒度文化概念识别上的表现，同时保持其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练视觉语言模型（如CLIP）在细粒度文化特征识别上表现不佳，主要由于缺乏高质量文化特定数据集和上下文知识。

Method: 设计了数据合成流程构建CulTwin数据集，并通过定制对比学习微调CLIP模型为CultureCLIP。

Result: CultureCLIP在文化相关基准测试中表现优于基础CLIP，某些任务上细粒度概念识别提升5.49%。

Conclusion: 通过数据合成和模型微调，CultureCLIP能有效捕捉文化细微差异，同时保持泛化能力。

Abstract: Pretrained vision-language models (VLMs) such as CLIP excel in multimodal
understanding but struggle with contextually relevant fine-grained visual
features, making it difficult to distinguish visually similar yet culturally
distinct concepts. This limitation stems from the scarcity of high-quality
culture-specific datasets, the lack of integrated contextual knowledge, and the
absence of hard negatives highlighting subtle distinctions. To address these
challenges, we first design a data curation pipeline that leverages
open-sourced VLMs and text-to-image diffusion models to construct CulTwin, a
synthetic cultural dataset. This dataset consists of paired
concept-caption-image triplets, where concepts visually resemble each other but
represent different cultural contexts. Then, we fine-tune CLIP on CulTwin to
create CultureCLIP, which aligns cultural concepts with contextually enhanced
captions and synthetic images through customized contrastive learning, enabling
finer cultural differentiation while preserving generalization capabilities.
Experiments on culturally relevant benchmarks show that CultureCLIP outperforms
the base CLIP, achieving up to a notable 5.49% improvement in fine-grained
concept recognition on certain tasks, while preserving CLIP's original
generalization ability, validating the effectiveness of our data synthesis and
VLM backbone training paradigm in capturing subtle cultural distinctions.

</details>


### [83] [Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion](https://arxiv.org/abs/2507.06230)
*Aleksandar Jevtić,Christoph Reich,Felix Wimbauer,Oliver Hahn,Christian Rupprecht,Stefan Roth,Daniel Cremers*

Main category: cs.CV

TL;DR: 论文提出了一种无监督的语义场景补全方法SceneDINO，利用自监督表示学习和多视角一致性，无需标注数据即可推断3D几何和语义。


<details>
  <summary>Details</summary>
Motivation: 现有语义场景补全方法依赖昂贵的标注数据，本文旨在探索无监督的解决方案。

Method: 采用自监督表示学习和多视角一致性训练，结合3D特征蒸馏技术，实现无监督的3D语义推断。

Result: 在无监督场景理解中达到最先进的分割精度，线性探测3D特征与监督方法相当，并展示了领域泛化和多视角一致性。

Conclusion: SceneDINO为单图像3D场景理解提供了强大的无监督基础。

Abstract: Semantic scene completion (SSC) aims to infer both the 3D geometry and
semantics of a scene from single images. In contrast to prior work on SSC that
heavily relies on expensive ground-truth annotations, we approach SSC in an
unsupervised setting. Our novel method, SceneDINO, adapts techniques from
self-supervised representation learning and 2D unsupervised scene understanding
to SSC. Our training exclusively utilizes multi-view consistency
self-supervision without any form of semantic or geometric ground truth. Given
a single input image, SceneDINO infers the 3D geometry and expressive 3D DINO
features in a feed-forward manner. Through a novel 3D feature distillation
approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised
scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy.
Linear probing our 3D features matches the segmentation accuracy of a current
supervised SSC approach. Additionally, we showcase the domain generalization
and multi-view consistency of SceneDINO, taking the first steps towards a
strong foundation for single image 3D scene understanding.

</details>


### [84] [RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models](https://arxiv.org/abs/2507.06231)
*Keyan Chen,Chenyang Liu,Bowen Chen,Jiafan Zhang,Zhengxia Zou,Zhenwei Shi*

Main category: cs.CV

TL;DR: RSRefSeg 2提出了一种解耦的双阶段框架，通过粗定位和细分割改进遥感图像分割的精度和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂语义关系和跨模态对齐时存在局限性，主要由于目标定位与边界划分的耦合机制。

Method: 采用CLIP和SAM的协作，通过双模态编码和级联二阶提示器优化语义提示，指导SAM生成精细分割。

Result: 在多个数据集上，RSRefSeg 2的gIoU提升了约3%，显著优于现有方法。

Conclusion: RSRefSeg 2通过解耦框架和基础模型协作，显著提升了分割精度和语义解释能力。

Abstract: Referring Remote Sensing Image Segmentation provides a flexible and
fine-grained framework for remote sensing scene analysis via vision-language
collaborative interpretation. Current approaches predominantly utilize a
three-stage pipeline encompassing dual-modal encoding, cross-modal interaction,
and pixel decoding. These methods demonstrate significant limitations in
managing complex semantic relationships and achieving precise cross-modal
alignment, largely due to their coupled processing mechanism that conflates
target localization with boundary delineation. This architectural coupling
amplifies error propagation under semantic ambiguity while restricting model
generalizability and interpretability. To address these issues, we propose
RSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow
into a collaborative dual-stage framework: coarse localization followed by fine
segmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with
SAM's segmentation generalizability through strategic foundation model
collaboration. Specifically, CLIP is employed as the dual-modal encoder to
activate target features within its pre-aligned semantic space and generate
localization prompts. To mitigate CLIP's misactivation challenges in
multi-entity scenarios described by referring texts, a cascaded second-order
prompter is devised, which enhances precision through implicit reasoning via
decomposition of text embeddings into complementary semantic subspaces. These
optimized semantic prompts subsequently direct the SAM to generate pixel-level
refined masks, thereby completing the semantic transmission pipeline. Extensive
experiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2
surpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex
semantic interpretation. Code is available at:
https://github.com/KyanChen/RSRefSeg2.

</details>


### [85] [Learning to Track Any Points from Human Motion](https://arxiv.org/abs/2507.06233)
*Inès Hyeonsu Kim,Seokju Cho,Jahyeok Koo,Junghyun Park,Jiahui Huang,Joon-Young Lee,Seungryong Kim*

Main category: cs.CV

TL;DR: AnthroTAP提出了一种自动化生成伪标签训练数据的流程，利用SMPL模型解决人类运动点跟踪数据标注困难的问题，并在少量数据和计算资源下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 人类运动数据标注困难，但适合训练鲁棒的点跟踪模型。

Method: 利用SMPL模型拟合视频中的人体，生成2D伪轨迹，并通过光线投射处理遮挡和光流一致性过滤不可靠轨迹。

Result: 在TAP-Vid基准测试中，AnthroTAP训练的模型性能优于其他模型，且仅需少量数据和计算资源。

Conclusion: AnthroTAP为点跟踪任务提供了一种高效的数据生成和训练方法。

Abstract: Human motion, with its inherent complexities, such as non-rigid deformations,
articulated movements, clothing distortions, and frequent occlusions caused by
limbs or other individuals, provides a rich and challenging source of
supervision that is crucial for training robust and generalizable point
trackers. Despite the suitability of human motion, acquiring extensive training
data for point tracking remains difficult due to laborious manual annotation.
Our proposed pipeline, AnthroTAP, addresses this by proposing an automated
pipeline to generate pseudo-labeled training data, leveraging the Skinned
Multi-Person Linear (SMPL) model. We first fit the SMPL model to detected
humans in video frames, project the resulting 3D mesh vertices onto 2D image
planes to generate pseudo-trajectories, handle occlusions using ray-casting,
and filter out unreliable tracks based on optical flow consistency. A point
tracking model trained on AnthroTAP annotated dataset achieves state-of-the-art
performance on the TAP-Vid benchmark, surpassing other models trained on real
videos while using 10,000 times less data and only 1 day in 4 GPUs, compared to
256 GPUs used in recent state-of-the-art.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [86] [Strongly Solving $7 \times 6$ Connect-Four on Consumer Grade Hardware](https://arxiv.org/abs/2507.05267)
*Markus Böck*

Main category: cs.AI

TL;DR: 本文提出了一种基于二元决策图的符号搜索方法，成功生成了一个89.6 GB的查找表，用于解决标准7×6棋盘的四子棋问题。


<details>
  <summary>Details</summary>
Motivation: 尽管四子棋已有数学解法，但传统认为强解形式的查找表不可行，本文旨在探索其可行性。

Method: 采用二元决策图的符号搜索方法，并结合高效的实现，生成查找表。同时，还包含一个alpha-beta搜索以优化胜负路径。

Result: 在单核CPU和128 GB内存的条件下，47小时内生成了89.6 GB的查找表。

Conclusion: 证明了强解形式的查找表在四子棋中的可行性，并提供了开源工具以支持进一步研究。

Abstract: While the game Connect-Four has been solved mathematically and the best move
can be effectively computed with search based methods, a strong solution in the
form of a look-up table was believed to be infeasible. In this paper, we
revisit a symbolic search method based on binary decision diagrams to produce
strong solutions. With our efficient implementation we were able to produce a
89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main
memory for the standard $7 \times 6$ board size. In addition to this
win-draw-loss evaluation, we include an alpha-beta search in our open source
artifact to find the move which achieves the fastest win or slowest loss.

</details>


### [87] [Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management](https://arxiv.org/abs/2507.05283)
*Yue Wang,Miao Zhou,Guijing Huang,Rui Zhuo,Chao Yi,Zhenliang Ma*

Main category: cs.AI

TL;DR: Chat2SPaT利用大型语言模型（LLMs）将用户对交通信号控制计划的半结构化描述转换为精确的信号相位与时间（SPaT）结果，准确率超过94%。


<details>
  <summary>Details</summary>
Motivation: 传统预定时交通信号控制计划需要繁琐的手动输入，Chat2SPaT旨在简化这一过程，提供用户友好的管理工具。

Method: 通过LLMs理解用户描述并生成JSON格式的相位序列和属性，再用Python脚本处理生成完整信号控制计划。

Result: 实验显示，Chat2SPaT在300多个测试案例中准确率超过94%，支持中英文。

Conclusion: Chat2SPaT为交通领域提供了一种高效、易用的信号控制计划管理工具，展示了LLMs在智能交通系统中的潜力。

Abstract: Pre-timed traffic signal control, commonly used for operating signalized
intersections and coordinated arterials, requires tedious manual work for
signaling plan creating and updating. When the time-of-day or day-of-week plans
are utilized, one intersection is often associated with multiple plans, leading
to further repetitive manual plan parameter inputting. To enable a
user-friendly traffic signal control plan management process, this study
proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous
descriptions on the signal control plan to exact signal phase and timing (SPaT)
results, which could further be transformed into structured stage-based or
ring-based plans to interact with intelligent transportation system (ITS)
software and traffic signal controllers. With curated prompts, Chat2SPaT first
leverages large language models' (LLMs) capability of understanding users' plan
descriptions and reformulate the plan as a combination of phase sequence and
phase attribute results in the json format. Based on LLM outputs, python
scripts are designed to locate phases in a cycle, address nuances of traffic
signal control, and finally assemble the complete traffic signal control plan.
Within a chat, the pipeline can be utilized iteratively to conduct further plan
editing. Experiments show that Chat2SPaT can generate plans with an accuracy of
over 94% for both English and Chinese cases, using a test dataset with over 300
plan descriptions. As the first benchmark for evaluating LLMs' capability of
understanding traffic signal control plan descriptions, Chat2SPaT provides an
easy-to-use plan management pipeline for traffic practitioners and researchers,
serving as a potential new building block for a more accurate and versatile
application of LLMs in the field of ITS. The source codes, prompts and test
dataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.

</details>


### [88] [Fuzzy Classification Aggregation for a Continuum of Agents](https://arxiv.org/abs/2507.05297)
*Zijun Meng*

Main category: cs.AI

TL;DR: 证明了对于连续个体分类的最优、独立且零一致的模糊分类聚合函数必须是加权算术平均。


<details>
  <summary>Details</summary>
Motivation: 研究模糊分类聚合函数的性质，特别是在多对象多类型分类中的最优解。

Method: 通过数学证明，分析满足最优、独立和零一致条件的模糊分类聚合函数。

Result: 发现唯一满足条件的函数是加权算术平均。

Conclusion: 加权算术平均是连续个体分类聚合的唯一有效方法。

Abstract: We prove that any optimal, independent, and zero unanimous fuzzy
classification aggregation function of a continuum of individual
classifications of $m\ge 3$ objects into $2\le p\le m$ types must be a weighted
arithmetic mean.

</details>


### [89] [OLG++: A Semantic Extension of Obligation Logic Graph](https://arxiv.org/abs/2507.05488)
*Subhasis Dasgupta,Jon Stephens,Amarnath Gupta*

Main category: cs.AI

TL;DR: OLG++是Obligation Logic Graph的语义扩展，用于建模法律规则，支持更丰富的节点和边类型，提升法律知识表示的表达能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有法律知识表示模型在空间、时间和逻辑分组等方面的不足，提供更细致的法律义务和例外表示。

Method: 引入空间、时间、党派组、可废止性和逻辑分组等新节点和边类型，支持结构化推理和复杂触发条件。

Result: 通过食品业务法规案例展示OLG++的表达能力，证明其优于现有图模型和LegalRuleML。

Conclusion: OLG++在法律知识表示中更具表达力，能更好地支持法律问题回答和规则推理。

Abstract: We present OLG++, a semantic extension of the Obligation Logic Graph (OLG)
for modeling regulatory and legal rules in municipal and interjurisdictional
contexts. OLG++ introduces richer node and edge types, including spatial,
temporal, party group, defeasibility, and logical grouping constructs, enabling
nuanced representations of legal obligations, exceptions, and hierarchies. The
model supports structured reasoning over rules with contextual conditions,
precedence, and complex triggers. We demonstrate its expressiveness through
examples from food business regulations, showing how OLG++ supports legal
question answering using property graph queries. OLG++ also improves over
LegalRuleML by providing native support for subClassOf, spatial constraints,
and reified exception structures. Our examples show that OLG++ is more
expressive than prior graph-based models for legal knowledge representation.

</details>


### [90] [Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents](https://arxiv.org/abs/2507.05495)
*Prahaladh Chandrahasan,Jiahe Jin,Zhihan Zhang,Tevin Wang,Andy Tang,Lucy Mo,Morteza Ziyadi,Leonardo F. R. Ribeiro,Zimeng Qiu,Markus Dreyer,Akari Asai,Chenyan Xiong*

Main category: cs.AI

TL;DR: 论文介绍了Deep Research Comparator平台，用于评估深度研究代理，支持报告对比、细粒度反馈和排名计算，并开发了Simple Deepresearch作为基线代理。


<details>
  <summary>Details</summary>
Motivation: 当前评估自主网络搜索和信息分析的深度研究代理存在挑战，尤其是对长报告和中间步骤的详细反馈需求。

Method: 提出Deep Research Comparator平台，支持代理报告对比、中间步骤评估和细粒度反馈；开发Simple Deepresearch作为基线代理。

Result: 收集了17位标注者对三个深度研究代理的真实偏好数据，验证了平台实用性。

Conclusion: 平台为深度研究代理开发提供了有效工具，支持报告对比和反馈收集。

Abstract: Effectively evaluating deep research agents that autonomously search the web,
analyze information, and generate reports remains a major challenge,
particularly when it comes to assessing long reports and giving detailed
feedback on their intermediate steps. To address these gaps, we introduce Deep
Research Comparator, a platform that offers a holistic framework for deep
research agent hosting, side-by-side comparison, fine-grained human feedback
collection, and ranking calculation. Given a user query, our platform displays
the final reports from two different agents along with their intermediate steps
during generation. Annotators can evaluate the overall quality of final reports
based on side-by-side comparison, and also provide detailed feedback separately
by assessing intermediate steps or specific text spans within the final report.
Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This
scaffold serves as a baseline that facilitates the easy integration of various
large language models to transform them into deep research agents for
evaluation. To demonstrate the platform's utility for deep research agent
development, we have collected real user preference data from 17 annotators on
three deep research agents. A demo video of our platform can be found at
https://www.youtube.com/watch?v=g4d2dnbdseg.

</details>


### [91] [Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality](https://arxiv.org/abs/2507.05515)
*Haochen Huang,Jiahuan Pei,Mohammad Aliannejadi,Xin Sun,Moonisa Ahsan,Pablo Cesar,Chuang Yu,Zhaochun Ren,Junxiao Wang*

Main category: cs.AI

TL;DR: 本文介绍了针对AR训练定制的数据集，评估了9种先进视觉语言模型（VLMs），发现即使是GPT-4o在细粒度任务上表现不佳，F1分数仅为40.54%。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在AR训练中的应用，并填补现有研究的空白。

Method: 构建了一个系统化的视觉语言任务数据集，并评估了9种VLMs的性能。

Result: 高级模型在细粒度任务上表现不佳，F1分数最高仅40.54%。

Conclusion: 需要改进数据集和基准测试以提升视觉语言对齐能力，同时研究对盲人和视障用户的社会意义。

Abstract: Vision-language models (VLMs) are essential for enabling AI-powered smart
assistants to interpret and reason in multimodal environments. However, their
application in augmented reality (AR) training remains largely unexplored. In
this work, we introduce a comprehensive dataset tailored for AR training,
featuring systematized vision-language tasks, and evaluate nine
state-of-the-art VLMs on it. Our results reveal that even advanced models,
including GPT-4o, struggle with fine-grained assembly tasks, achieving a
maximum F1 score of just 40.54% on state detection. These findings highlight
the demand for enhanced datasets, benchmarks, and further research to improve
fine-grained vision-language alignment. Beyond technical contributions, our
work has broader social implications, particularly in empowering blind and
visually impaired users with equitable access to AI-driven learning
opportunities. We provide all related resources, including the dataset, source
code, and evaluation results, to support the research community.

</details>


### [92] [Modeling (Deontic) Modal Operators With the s(CASP) Goal-directed Predicated Answer Set Programming System](https://arxiv.org/abs/2507.05519)
*Gopal Gupta,Abhiramon Rajasekharan,Alexis R. Tudor,Elmer Salazar,Joaquín Arias*

Main category: cs.AI

TL;DR: 使用ASP中的默认否定和强否定优雅表达义务模态逻辑，并通过全局约束解决其悖论。


<details>
  <summary>Details</summary>
Motivation: 解决义务模态逻辑的实现问题，尤其是其悖论。

Method: 利用ASP的默认否定和强否定表达模态算子，并用全局约束表示义务和禁止。

Result: 提出的方法优雅地解决了义务模态逻辑的多种悖论。

Conclusion: ASP技术为义务模态逻辑的实现提供了有效且优雅的解决方案。

Abstract: We consider the problem of implementing deontic modal logic. We show how
(deontic) modal operators can be expressed elegantly using default negation
(negation-as-failure) and strong negation present in answer set programming
(ASP). We propose using global constraints of ASP to represent obligations and
impermissibilities of deontic modal logic. We show that our proposed
representation results in the various paradoxes of deontic modal logic being
elegantly resolved.

</details>


### [93] [Cultivating Multimodal Intelligence: Interpretive Reasoning and Agentic RAG Approaches to Dermatological Diagnosis](https://arxiv.org/abs/2507.05520)
*Karishma Thakrar,Shreyas Basavatia,Akshay Daftardar*

Main category: cs.AI

TL;DR: 2025 ImageCLEF MEDIQA-MAGIC挑战赛的第二版聚焦于多模态皮肤病问答和分割任务，提出了一种结合多模态模型微调、结构化推理层和代理检索增强生成的方法，取得了高准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决远程医疗中基于有限输入的异步诊断决策问题，提高诊断的准确性和可解释性。

Method: 结合多模态模型微调、结构化推理层和代理检索增强生成（agentic RAG），整合患者图像、症状描述和外部数据库信息。

Result: 团队在比赛中获得第二名，提交的方案排名第六，表现出竞争力和高准确性。

Conclusion: 该架构通过模拟皮肤科医生的系统推理模式，为自动化诊断支持系统提供了更可靠的路径。

Abstract: The second edition of the 2025 ImageCLEF MEDIQA-MAGIC challenge, co-organized
by researchers from Microsoft, Stanford University, and the Hospital Clinic of
Barcelona, focuses on multimodal dermatology question answering and
segmentation, using real-world patient queries and images. This work addresses
the Closed Visual Question Answering (CVQA) task, where the goal is to select
the correct answer to multiple-choice clinical questions based on both
user-submitted images and accompanying symptom descriptions. The proposed
approach combines three core components: (1) fine-tuning open-source multimodal
models from the Qwen, Gemma, and LLaMA families on the competition dataset, (2)
introducing a structured reasoning layer that reconciles and adjudicates
between candidate model outputs, and (3) incorporating agentic
retrieval-augmented generation (agentic RAG), which adds relevant information
from the American Academy of Dermatology's symptom and condition database to
fill in gaps in patient context. The team achieved second place with a
submission that scored sixth, demonstrating competitive performance and high
accuracy. Beyond competitive benchmarks, this research addresses a practical
challenge in telemedicine: diagnostic decisions must often be made
asynchronously, with limited input and with high accuracy and interpretability.
By emulating the systematic reasoning patterns employed by dermatologists when
evaluating skin conditions, this architecture provided a pathway toward more
reliable automated diagnostic support systems.

</details>


### [94] [Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment](https://arxiv.org/abs/2507.05528)
*Jiahuan Pei,Fanghua Ye,Xin Sun,Wentao Deng,Koen Hindriks,Junxiao Wang*

Main category: cs.AI

TL;DR: 论文提出WikiHowAgent，一个基于大语言模型的多代理工作流，用于模拟教学互动，并通过数据集和评估协议验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在利用大规模课程内容和评估教学质量方面存在不足，缺乏可扩展性。

Method: 提出WikiHowAgent，整合教师和学习者代理、交互管理器和评估器，基于114,296条教学对话和14,287篇教程构建数据集。

Result: 实验证明该工作流在多样化场景中有效，并揭示了LLM在不同领域的潜力。

Conclusion: WikiHowAgent为教学互动提供了可扩展的解决方案，数据集和实现已开源。

Abstract: Large language models (LLMs) have advanced virtual educators and learners,
bridging NLP with AI4Education. Existing work often lacks scalability and fails
to leverage diverse, large-scale course content, with limited frameworks for
assessing pedagogic quality. To this end, we propose WikiHowAgent, a
multi-agent workflow leveraging LLMs to simulate interactive teaching-learning
conversations. It integrates teacher and learner agents, an interaction
manager, and an evaluator to facilitate procedural learning and assess
pedagogic quality. We introduce a dataset of 114,296 teacher-learner
conversations grounded in 14,287 tutorials across 17 domains and 727 topics.
Our evaluation protocol combines computational and rubric-based metrics with
human judgment alignment. Results demonstrate the workflow's effectiveness in
diverse setups, offering insights into LLM capabilities across domains. Our
datasets and implementations are fully open-sourced.

</details>


### [95] [Red Teaming AI Red Teaming](https://arxiv.org/abs/2507.05538)
*Subhabrata Majumdar,Brian Pendleton,Abhishek Gupta*

Main category: cs.AI

TL;DR: 本文批判性地审视了AI红队测试的实践，指出其当前在AI治理中的局限性，并提出一个涵盖宏观系统层面和微观模型层面的综合框架。


<details>
  <summary>Details</summary>
Motivation: AI红队测试目前过于关注模型级漏洞，忽视了更广泛的社会技术系统和涌现行为，需要更全面的方法。

Method: 提出一个两层次框架：宏观系统红队测试和微观模型红队测试，并结合网络安全经验和系统理论提出建议。

Result: 强调有效的AI红队测试需要多学科团队，关注涌现风险、系统性漏洞及技术与社会因素的相互作用。

Conclusion: AI红队测试需扩展视野，从模型级扩展到系统级，以更全面地应对AI治理中的挑战。

Abstract: Red teaming has evolved from its origins in military applications to become a
widely adopted methodology in cybersecurity and AI. In this paper, we take a
critical look at the practice of AI red teaming. We argue that despite its
current popularity in AI governance, there exists a significant gap between red
teaming's original intent as a critical thinking exercise and its narrow focus
on discovering model-level flaws in the context of generative AI. Current AI
red teaming efforts focus predominantly on individual model vulnerabilities
while overlooking the broader sociotechnical systems and emergent behaviors
that arise from complex interactions between models, users, and environments.
To address this deficiency, we propose a comprehensive framework
operationalizing red teaming in AI systems at two levels: macro-level system
red teaming spanning the entire AI development lifecycle, and micro-level model
red teaming. Drawing on cybersecurity experience and systems theory, we further
propose a set of recommendations. In these, we emphasize that effective AI red
teaming requires multifunctional teams that examine emergent risks, systemic
vulnerabilities, and the interplay between technical and social factors.

</details>


### [96] [SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation](https://arxiv.org/abs/2507.05541)
*Shovito Barua Soumma,Asiful Arefeen,Stephanie M. Carpenter,Melanie Hingle,Hassan Ghasemzadeh*

Main category: cs.AI

TL;DR: 论文探讨了使用GPT-4o-mini生成反事实解释（CFs）的零样本和三样本方法，在压力和心脏病数据集上表现优于传统方法，并提升了下游分类器性能。


<details>
  <summary>Details</summary>
Motivation: 反事实解释（CFs）能提供人类可理解的机器学习预测解释，用于异常预防和增强模型鲁棒性。

Method: 采用GPT-4o-mini在零样本和三样本设置下生成CFs，并与DiCE、CFNOW和NICE等传统方法对比。

Result: LLM生成的CFs在合理性（99%）、有效性（0.99）和稀疏性上表现优异，且作为增强数据提升了分类器准确率（平均5%）。

Conclusion: 基于提示的生成技术能显著提升临床和生理预测任务的可解释性和鲁棒性。

Abstract: Counterfactual explanations (CFs) offer human-centric insights into machine
learning predictions by highlighting minimal changes required to alter an
outcome. Therefore, CFs can be used as (i) interventions for abnormality
prevention and (ii) augmented data for training robust models. In this work, we
explore large language models (LLMs), specifically GPT-4o-mini, for generating
CFs in a zero-shot and three-shot setting. We evaluate our approach on two
datasets: the AI-Readi flagship dataset for stress prediction and a public
dataset for heart disease detection. Compared to traditional methods such as
DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high
plausibility (up to 99%), strong validity (up to 0.99), and competitive
sparsity. Moreover, using LLM-generated CFs as augmented samples improves
downstream classifier performance (an average accuracy gain of 5%), especially
in low-data regimes. This demonstrates the potential of prompt-based generative
techniques to enhance explainability and robustness in clinical and
physiological prediction tasks. Code base: github.com/anonymous/SenseCF.

</details>


### [97] [SingLoRA: Low Rank Adaptation Using a Single Matrix](https://arxiv.org/abs/2507.05566)
*David Bensaïd,Noam Rotstein,Roy Velich,Daniel Bensaïd,Ron Kimmel*

Main category: cs.AI

TL;DR: SingLoRA提出了一种改进的低秩适应方法，通过将权重更新分解为单个低秩矩阵及其转置的乘积，解决了LoRA中矩阵间尺度冲突的问题，提升了训练稳定性并减少了参数数量。


<details>
  <summary>Details</summary>
Motivation: LoRA在参数高效微调中表现出色，但矩阵间的尺度差异会导致训练不稳定和性能下降。SingLoRA旨在解决这一问题。

Method: SingLoRA将权重更新重新表述为单个低秩矩阵与其转置的乘积，从而消除尺度冲突并减少参数数量。

Result: 在多项任务中，SingLoRA表现优于LoRA和LoRA+，例如在MNLI任务中达到91.3%的准确率，且参数数量仅为60%。

Conclusion: SingLoRA通过简化低秩适应设计，显著提升了训练稳定性和性能，同时减少了参数需求。

Abstract: Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient
fine-tuning of large pretrained models. LoRA augments the pre-trained weights
of a model by adding the product of two smaller matrices that together form a
low-rank matrix update. Recent research has shown that scale disparities
between these two matrices often cause unstable training dynamics, leading to
suboptimal performance. In this paper, we propose SingLoRA, which reformulates
low-rank adaptation by learning the weights update as a decomposition of a
single low-rank matrix multiplied by its transpose. This simple design
inherently removes inter-matrix scale conflicts, ensuring stable optimization,
and roughly halves the parameter count. We analyze SingLoRA within the
infinite-width neural network framework, showing that it guarantees stable
feature learning by construction. Extensive experiments on multiple tasks
validate these benefits. In common sense reasoning, fine-tuning LLama 7B on
MNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+
(90.2%) - while using only 60% of their parameter budget. In image generation,
fine-tuning Stable Diffusion with SingLoRA significantly improves image
fidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to
scores of 0.148 and 0.143 for DoRA and LoRA, respectively.

</details>


### [98] [Towards Measurement Theory for Artificial Intelligence](https://arxiv.org/abs/2507.05587)
*Elija Perrier*

Main category: cs.AI

TL;DR: 提出一个关于人工智能测量的正式理论框架，旨在通过标准化测量促进系统比较、风险分析及能力评估。


<details>
  <summary>Details</summary>
Motivation: 为AI研究、实践和监管提供统一的测量标准，以支持系统比较、风险分析及能力评估的透明度。

Method: 提出分层的测量框架，区分直接与间接可观测性，并构建可校准的AI现象分类体系。

Result: 为AI测量提供理论基础，促进系统间比较、风险分析及能力评估的标准化。

Conclusion: 通过正式测量理论，AI领域可实现更透明、可比较和可校准的评估体系。

Abstract: We motivate and outline a programme for a formal theory of measurement of
artificial intelligence. We argue that formalising measurement for AI will
allow researchers, practitioners, and regulators to: (i) make comparisons
between systems and the evaluation methods applied to them; (ii) connect
frontier AI evaluations with established quantitative risk analysis techniques
drawn from engineering and safety science; and (iii) foreground how what counts
as AI capability is contingent upon the measurement operations and scales we
elect to use. We sketch a layered measurement stack, distinguish direct from
indirect observables, and signpost how these ingredients provide a pathway
toward a unified, calibratable taxonomy of AI phenomena.

</details>


### [99] [MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models](https://arxiv.org/abs/2507.05591)
*Wei Zhang,Juan Chen,En Zhu,Wenhong Cheng,YunPeng Li,Yanbo J. Wang*

Main category: cs.AI

TL;DR: 提出了一种新型多模态大语言模型（MLlm-DR），用于可解释的抑郁症诊断，通过整合小型LLMs和轻量级查询模块（LQ-former），在CMDC和E-DAIC-WOZ数据集上取得最优效果。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症诊断方法缺乏解释性，且多模态LLMs未针对访谈数据训练，导致诊断性能不佳。

Method: MLlm-DR结合小型LLMs生成抑郁症评分及解释，LQ-former从语音和视觉数据提取特征，通过精细训练数据集优化逻辑推理。

Result: 在CMDC和E-DAIC-WOZ数据集上达到最优性能。

Conclusion: MLlm-DR实现了可解释且高效的抑郁症诊断，为临床实践提供了新工具。

Abstract: Automated depression diagnosis aims to analyze multimodal information from
interview videos to predict participants' depression scores. Previous studies
often lack clear explanations of how these scores were determined, limiting
their adoption in clinical practice. While the advent of LLMs provides a
possible pathway for explainable depression diagnosis, current LLMs capable of
processing multimodal data lack training on interview data, resulting in poor
diagnostic performance when used directly. In this paper, we propose a novel
multimodal large language model (MLlm-DR) that can understand multimodal
information inputs and supports explainable depression diagnosis. MLlm-DR
integrates a smaller LLMs and a lightweight query module (LQ-former).
Specifically, the smaller LLMs is designed to generate depression scores and
corresponding evaluation rationales. To enhance its logical reasoning for
domain-specific tasks while maintaining practicality, we constructed a robust
training dataset to fine-tune it. Meanwhile, the LQ-former captures
depression-related features from speech and visual data, aiding the model's
ability to process multimodal information, to achieve comprehensive depression
diagnosis. Our approach achieves state-of-the-art results on two
interview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its
effectiveness and superiority.

</details>


### [100] [Domain adaptation of large language models for geotechnical applications](https://arxiv.org/abs/2507.05613)
*Lei Fan,Fangxue Liu,Cheng Chen*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型（LLMs）在岩土工程中的适应与应用，探讨了关键方法、应用领域及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的发展，其在岩土工程中的应用潜力巨大，但需领域特定适应，本文旨在填补这一研究空白。

Method: 通过提示工程、检索增强生成、领域自适应预训练和微调等方法，将LLMs适应于岩土工程领域。

Result: LLMs在岩土工程中的应用包括地质解释、地下特征描述、设计计算等，同时分析了其优势与局限。

Conclusion: 本文为实践者提供了整合LLMs的指导，并为学术界的进一步研究奠定了基础。

Abstract: Recent developments in large language models (LLMs) are opening up new
opportunities in geotechnical engineering and engineering geology. While
general-purpose LLMs possess broad capabilities, effective application in
geotechnics often requires domain-specific adaptation. Such tailored LLMs are
increasingly employed to streamline geotechnical workflows. This paper presents
the first survey of the adaptation and application of LLMs in geotechnical
engineering. It outlines key methodologies for adaptation to geotechnical
domain, including prompt engineering, retrieval-augmented generation,
domain-adaptive pretraining, and fine-tuning. The survey examines the
state-of-the-art applications of geotechnical-adapted LLMs, including
geological interpretation, subsurface characterization, site planning, design
calculations, numerical modeling, safety and risk assessment, and educational
tutoring. It also analyzes benefits and limitations of geotechnical-adapted
LLMs, and identifies promising directions for future research in this
interdisciplinary discipline. The findings serve as a valuable resource for
practitioners seeking to integrate LLMs into geotechnical practice, while also
providing a foundation to stimulate further investigation within the academic
community.

</details>


### [101] [ADMC: Attention-based Diffusion Model for Missing Modalities Feature Completion](https://arxiv.org/abs/2507.05624)
*Wei Zhang,Juan Chen,Yanbo J. Wang,En Zhu,Xuan Yang,Yiduo Wang*

Main category: cs.AI

TL;DR: 提出了一种基于注意力的扩散模型（ADMC），用于解决多模态情感和意图识别中缺失模态的问题，通过独立训练特征提取网络和生成缺失模态特征，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感和意图识别中，传感器故障或数据不完整导致模态缺失，传统方法存在过耦合和生成不精确的问题。

Method: 采用独立训练的特征提取网络和注意力扩散网络（ADN），生成与真实多模态分布一致的缺失模态特征。

Result: 在IEMOCAP和MIntRec基准测试中取得了最先进的结果，适用于缺失和完整模态场景。

Conclusion: ADMC框架有效解决了模态缺失问题，提升了多模态情感和意图识别的性能。

Abstract: Multimodal emotion and intent recognition is essential for automated
human-computer interaction, It aims to analyze users' speech, text, and visual
information to predict their emotions or intent. One of the significant
challenges is that missing modalities due to sensor malfunctions or incomplete
data. Traditional methods that attempt to reconstruct missing information often
suffer from over-coupling and imprecise generation processes, leading to
suboptimal outcomes. To address these issues, we introduce an Attention-based
Diffusion model for Missing Modalities feature Completion (ADMC). Our framework
independently trains feature extraction networks for each modality, preserving
their unique characteristics and avoiding over-coupling. The Attention-based
Diffusion Network (ADN) generates missing modality features that closely align
with authentic multimodal distribution, enhancing performance across all
missing-modality scenarios. Moreover, ADN's cross-modal generation offers
improved recognition even in full-modality contexts. Our approach achieves
state-of-the-art results on the IEMOCAP and MIntRec benchmarks, demonstrating
its effectiveness in both missing and complete modality scenarios.

</details>


### [102] [Enhancing Student Learning with LLM-Generated Retrieval Practice Questions: An Empirical Study in Data Science Courses](https://arxiv.org/abs/2507.05629)
*Yuan An,John Liu,Niyam Acharya,Ruhma Hashmi*

Main category: cs.AI

TL;DR: 研究表明，LLM生成的检索练习问题能显著提高学生知识保留率（89% vs 73%），但需人工验证质量。


<details>
  <summary>Details</summary>
Motivation: 解决教师生成高质量检索练习问题耗时的问题，探索LLM自动生成的可行性。

Method: 在两个大学数据科学课程中，对比学生使用LLM生成的多选题与无练习时的学习效果。

Result: 使用LLM生成问题的学生知识保留率显著更高（89% vs 73%）。

Conclusion: LLM生成的检索练习问题有效支持学习，但需人工验证以确保质量。

Abstract: Retrieval practice is a well-established pedagogical technique known to
significantly enhance student learning and knowledge retention. However,
generating high-quality retrieval practice questions is often time-consuming
and labor intensive for instructors, especially in rapidly evolving technical
subjects. Large Language Models (LLMs) offer the potential to automate this
process by generating questions in response to prompts, yet the effectiveness
of LLM-generated retrieval practice on student learning remains to be
established. In this study, we conducted an empirical study involving two
college-level data science courses, with approximately 60 students. We compared
learning outcomes during one week in which students received LLM-generated
multiple-choice retrieval practice questions to those from a week in which no
such questions were provided. Results indicate that students exposed to
LLM-generated retrieval practice achieved significantly higher knowledge
retention, with an average accuracy of 89%, compared to 73% in the week without
such practice. These findings suggest that LLM-generated retrieval questions
can effectively support student learning and may provide a scalable solution
for integrating retrieval practice into real-time teaching. However, despite
these encouraging outcomes and the potential time-saving benefits, cautions
must be taken, as the quality of LLM-generated questions can vary. Instructors
must still manually verify and revise the generated questions before releasing
them to students.

</details>


### [103] [LLMs are Introvert](https://arxiv.org/abs/2507.05638)
*Litian Zhang,Xiaoming Zhang,Bingyu Yan,Ziyi Zhou,Bo Zhang,Zhenyu Guan,Xi Zhang,Chaozhuo Li*

Main category: cs.AI

TL;DR: 论文提出了一种基于SIP-CoT和情感记忆的LLM模拟方法，用于改进社交信息传播的仿真效果。


<details>
  <summary>Details</summary>
Motivation: 社交媒体的快速发展和生成式AI的普及加速了错误信息的传播，传统模型无法充分捕捉在线互动的复杂性，LLM虽具潜力但存在行为与真实人类动态的差距。

Method: 引入基于LLM的模拟环境，结合SIP-CoT机制和情感记忆，改进社交信息处理。

Result: 实验表明，SIP-CoT增强的LLM代理能更有效地处理社交信息，行为更接近真实人类互动。

Conclusion: 研究揭示了当前LLM模拟的局限性，并证明SIP-CoT和情感记忆能显著提升LLM代理的社交智能和真实性。

Abstract: The exponential growth of social media and generative AI has transformed
information dissemination, fostering connectivity but also accelerating the
spread of misinformation. Understanding information propagation dynamics and
developing effective control strategies is essential to mitigate harmful
content. Traditional models, such as SIR, provide basic insights but
inadequately capture the complexities of online interactions. Advanced methods,
including attention mechanisms and graph neural networks, enhance accuracy but
typically overlook user psychology and behavioral dynamics. Large language
models (LLMs), with their human-like reasoning, offer new potential for
simulating psychological aspects of information spread. We introduce an
LLM-based simulation environment capturing agents' evolving attitudes,
emotions, and responses. Initial experiments, however, revealed significant
gaps between LLM-generated behaviors and authentic human dynamics, especially
in stance detection and psychological realism. A detailed evaluation through
Social Information Processing Theory identified major discrepancies in
goal-setting and feedback evaluation, stemming from the lack of emotional
processing in standard LLM training. To address these issues, we propose the
Social Information Processing-based Chain of Thought (SIP-CoT) mechanism
enhanced by emotion-guided memory. This method improves the interpretation of
social cues, personalization of goals, and evaluation of feedback. Experimental
results confirm that SIP-CoT-enhanced LLM agents more effectively process
social information, demonstrating behaviors, attitudes, and emotions closer to
real human interactions. In summary, this research highlights critical
limitations in current LLM-based propagation simulations and demonstrates how
integrating SIP-CoT and emotional memory significantly enhances the social
intelligence and realism of LLM agents.

</details>


### [104] [City-Level Foreign Direct Investment Prediction with Tabular Learning on Judicial Data](https://arxiv.org/abs/2507.05651)
*Tianxing Wu,Lizhe Cao,Shuang Wang,Jiming Wang,Shutong Zhu,Yerong Wu,Yuqing Feng*

Main category: cs.AI

TL;DR: 论文提出了一种基于司法数据的城市级外国直接投资（FDI）预测方法（TLJD），通过整合大规模司法数据和经济数据，提高了预测的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统基于经济数据（如GDP）的FDI预测可能因数据操纵而不准确，司法数据能反映投资安全和回报，因此被用于改进预测。

Method: 构建了一个基于1200万份公开判决文件的司法绩效评估指标体系，并提出TLJD方法，结合行数据和列数据，利用专家混合模型调整指标权重。

Result: 在跨城市和跨时间任务中，TLJD的预测效果优于其他十种先进基线方法（R2至少达到0.92）。

Conclusion: TLJD通过司法数据显著提升了FDI预测的准确性，为地方政府提供了更可靠的决策支持。

Abstract: To advance the United Nations Sustainable Development Goal on promoting
sustained, inclusive, and sustainable economic growth, foreign direct
investment (FDI) plays a crucial role in catalyzing economic expansion and
fostering innovation. Precise city-level FDI prediction is quite important for
local government and is commonly studied based on economic data (e.g., GDP).
However, such economic data could be prone to manipulation, making predictions
less reliable. To address this issue, we try to leverage large-scale judicial
data which reflects judicial performance influencing local investment security
and returns, for city-level FDI prediction. Based on this, we first build an
index system for the evaluation of judicial performance over twelve million
publicly available adjudication documents according to which a tabular dataset
is reformulated. We then propose a new Tabular Learning method on Judicial Data
(TLJD) for city-level FDI prediction. TLJD integrates row data and column data
in our built tabular dataset for judicial performance indicator encoding, and
utilizes a mixture of experts model to adjust the weights of different
indicators considering regional variations. To validate the effectiveness of
TLJD, we design cross-city and cross-time tasks for city-level FDI predictions.
Extensive experiments on both tasks demonstrate the superiority of TLJD (reach
to at least 0.92 R2) over the other ten state-of-the-art baselines in different
evaluation metrics.

</details>


### [105] [Divergent Realities: A Comparative Analysis of Human Expert vs. Artificial Intelligence Based Generation and Evaluation of Treatment Plans in Dermatology](https://arxiv.org/abs/2507.05716)
*Dipayan Sengupta,Saumya Panda*

Main category: cs.AI

TL;DR: 研究比较了人类专家和两种AI模型（通用型和推理型）生成的皮肤病治疗计划，发现评估结果因评估者（人类或AI）而异，揭示了临床经验与算法逻辑之间的差距。


<details>
  <summary>Details</summary>
Motivation: 评估AI生成的治疗计划质量，尤其是在新推理模型出现后，探索人类与AI评估的差异。

Method: 十位皮肤科医生、通用AI（GPT-4o）和推理AI（o3）为五个复杂皮肤病案例生成治疗计划，匿名后由人类专家和高级AI（Gemini 2.5 Pro）分别评分。

Result: 人类专家评分显著高于AI计划，而AI评估者则完全相反，推理AI（o3）被AI评为最佳。

Conclusion: 临床计划的质量感知取决于评估者特性，未来需构建可解释的人机协同系统以弥合理念差距。

Abstract: Background: Evaluating AI-generated treatment plans is a key challenge as AI
expands beyond diagnostics, especially with new reasoning models. This study
compares plans from human experts and two AI models (a generalist and a
reasoner), assessed by both human peers and a superior AI judge.
  Methods: Ten dermatologists, a generalist AI (GPT-4o), and a reasoning AI
(o3) generated treatment plans for five complex dermatology cases. The
anonymized, normalized plans were scored in two phases: 1) by the ten human
experts, and 2) by a superior AI judge (Gemini 2.5 Pro) using an identical
rubric.
  Results: A profound 'evaluator effect' was observed. Human experts scored
peer-generated plans significantly higher than AI plans (mean 7.62 vs. 7.16;
p=0.0313), ranking GPT-4o 6th (mean 7.38) and the reasoning model, o3, 11th
(mean 6.97). Conversely, the AI judge produced a complete inversion, scoring AI
plans significantly higher than human plans (mean 7.75 vs. 6.79; p=0.0313). It
ranked o3 1st (mean 8.20) and GPT-4o 2nd, placing all human experts lower.
  Conclusions: The perceived quality of a clinical plan is fundamentally
dependent on the evaluator's nature. An advanced reasoning AI, ranked poorly by
human experts, was judged as superior by a sophisticated AI, revealing a deep
gap between experience-based clinical heuristics and data-driven algorithmic
logic. This paradox presents a critical challenge for AI integration,
suggesting the future requires synergistic, explainable human-AI systems that
bridge this reasoning gap to augment clinical care.

</details>


### [106] [An autonomous agent for auditing and improving the reliability of clinical AI models](https://arxiv.org/abs/2507.05755)
*Lukas Kuhn,Florian Buettner*

Main category: cs.AI

TL;DR: ModelAuditor是一个自省代理工具，用于识别和修复AI模型在临床实践中因数据分布变化导致的性能下降。


<details>
  <summary>Details</summary>
Motivation: AI模型在临床实践中可能因数据分布变化（如设备、光照或人口统计差异）而性能骤降，但缺乏高效、可解释的可靠性审计工具。

Method: ModelAuditor通过与用户对话、选择任务特定指标、模拟临床相关分布变化，生成可解释报告，识别失败模式和缓解策略。

Result: 在三个真实临床场景中，ModelAuditor成功识别失败模式，并通过针对性建议恢复15-25%的性能损失，优于基线方法和最新增强技术。

Conclusion: ModelAuditor提供了一种高效、低成本的方法，显著提升了AI模型在临床实践中的可靠性。

Abstract: The deployment of AI models in clinical practice faces a critical challenge:
models achieving expert-level performance on benchmarks can fail
catastrophically when confronted with real-world variations in medical imaging.
Minor shifts in scanner hardware, lighting or demographics can erode accuracy,
but currently reliability auditing to identify such catastrophic failure cases
before deployment is a bespoke and time-consuming process. Practitioners lack
accessible and interpretable tools to expose and repair hidden failure modes.
Here we introduce ModelAuditor, a self-reflective agent that converses with
users, selects task-specific metrics, and simulates context-dependent,
clinically relevant distribution shifts. ModelAuditor then generates
interpretable reports explaining how much performance likely degrades during
deployment, discussing specific likely failure modes and identifying root
causes and mitigation strategies. Our comprehensive evaluation across three
real-world clinical scenarios - inter-institutional variation in
histopathology, demographic shifts in dermatology, and equipment heterogeneity
in chest radiography - demonstrates that ModelAuditor is able correctly
identify context-specific failure modes of state-of-the-art models such as the
established SIIM-ISIC melanoma classifier. Its targeted recommendations recover
15-25% of performance lost under real-world distribution shift, substantially
outperforming both baseline models and state-of-the-art augmentation methods.
These improvements are achieved through a multi-agent architecture and execute
on consumer hardware in under 10 minutes, costing less than US$0.50 per audit.

</details>


### [107] [Real-time monitoring of the SoH of lithium-ion batteries](https://arxiv.org/abs/2507.05765)
*Bruno Jammes,Edgar Hernando Sepúlveda-Oviedo,Corinne Alonso*

Main category: cs.AI

TL;DR: 提出一种基于充电结束阶段放电脉冲分析的新方法，用于实时监测电池健康状态（SoH），在实验中表现出高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 微电网中电池健康状态的实时监测面临挑战，传统方法受限。

Method: 通过分析充电结束阶段的放电脉冲，利用等效电路模型参数估计SoH。

Result: 实验数据显示，该方法预测电池退化时平均绝对误差约1%，可解释性得分接近0.9。

Conclusion: 该方法有望集成到电池管理系统（BMS）中，优化持续运行下的电池管理。

Abstract: Real-time monitoring of the state of health (SoH) of batteries remains a
major challenge, particularly in microgrids where operational constraints limit
the use of traditional methods. As part of the 4BLife project, we propose an
innovative method based on the analysis of a discharge pulse at the end of the
charge phase. The parameters of the equivalent electrical model describing the
voltage evolution across the battery terminals during this current pulse are
then used to estimate the SoH. Based on the experimental data acquired so far,
the initial results demonstrate the relevance of the proposed approach. After
training using the parameters of two batteries with a capacity degradation of
around 85%, we successfully predicted the degradation of two other batteries,
cycled down to approximately 90% SoH, with a mean absolute error of around 1%
in the worst case, and an explainability score of the estimator close to 0.9.
If these performances are confirmed, this method can be easily integrated into
battery management systems (BMS) and paves the way for optimized battery
management under continuous operation.

</details>


### [108] [GTA1: GUI Test-time Scaling Agent](https://arxiv.org/abs/2507.05791)
*Yan Yang,Dongxu Li,Yutong Dai,Yuhao Yang,Ziyang Luo,Zirui Zhao,Zhiyuan Hu,Junzhe Huang,Amrita Saha,Zeyuan Chen,Ran Xu,Liyuan Pan,Caiming Xiong,Junnan Li*

Main category: cs.AI

TL;DR: 论文提出了一种名为GTA1的GUI代理，通过测试时缩放方法和强化学习解决任务规划和视觉元素定位的挑战，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决GUI代理在任务规划中的模糊性和复杂界面中视觉元素定位的准确性两大挑战。

Method: 引入测试时缩放方法选择最优动作提案，并利用强化学习模型提高视觉元素定位的准确性。

Result: GTA1-7B在多个基准测试中表现优异，例如在Screenspot-Pro、Screenspot-V2和OSWorld-G上的准确率分别为50.1%、92.4%和67.7%。

Conclusion: GTA1通过测试时缩放和强化学习方法显著提升了GUI代理的性能，并在实验中验证了其有效性。

Abstract: Graphical user interface (GUI) agents autonomously operate across platforms
(e.g., Linux) to complete tasks by interacting with visual elements.
Specifically, a user instruction is decomposed into a sequence of action
proposals, each corresponding to an interaction with the GUI. After each
action, the agent observes the updated GUI environment to plan the next step.
However, two main challenges arise: i) resolving ambiguity in task planning
(i.e., the action proposal sequence), where selecting an appropriate plan is
non-trivial, as many valid ones may exist; ii) accurately grounding actions in
complex and high-resolution interfaces, i.e., precisely interacting with visual
targets.
  This paper investigates the two aforementioned challenges with our GUI
Test-time Scaling Agent, namely GTA1. First, to select the most appropriate
action proposal, we introduce a test-time scaling method. At each step, we
sample multiple candidate action proposals and leverage a judge model to
evaluate and select the most suitable one. It trades off computation for better
decision quality by concurrent sampling, shortening task execution steps, and
improving overall performance. Second, we propose a model that achieves
improved accuracy when grounding the selected action proposal to its
corresponding visual elements. Our key insight is that reinforcement learning
(RL) facilitates visual grounding through inherent objective alignments,
rewarding successful clicks on interface elements.
  Experimentally, our method establishes state-of-the-art performance across
diverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%
accuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When
paired with a planner applying our test-time scaling strategy, it exhibits
state-of-the-art agentic performance (e.g., 45.2% task success rate on
OSWorld). We open-source our code and models here.

</details>


### [109] [Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity](https://arxiv.org/abs/2507.05816)
*Shuai Zhao,Yulin Zhang,Luwei Xiao,Xinyi Wu,Yanhao Jia,Zhongliang Guo,Xiaobao Wu,Cong-Duy Nguyen,Guoming Zhang,Anh Tuan Luu*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLM）在预测早产儿视网膜病变（ROP）风险中的能力，提出了CROP数据集和Affective-ROPTester评估框架，发现外部知识增强和情感提示工程对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在ROP风险预测中的潜力，填补相关研究空白。

Method: 使用CROP数据集，结合Instruction、Chain-of-Thought和In-Context Learning三种提示策略，并引入情感元素评估模型性能。

Result: LLM在仅依赖内部知识时表现有限，外部知识显著提升性能；情感偏见明显，正面情感提示有助于减少偏见。

Conclusion: 情感敏感的提示工程对提升诊断可靠性至关重要，Affective-ROPTester框架可有效评估和减少临床语言模型中的情感偏见。

Abstract: Despite the remarkable progress of large language models (LLMs) across
various domains, their capacity to predict retinopathy of prematurity (ROP)
risk remains largely unexplored. To address this gap, we introduce a novel
Chinese benchmark dataset, termed CROP, comprising 993 admission records
annotated with low, medium, and high-risk labels. To systematically examine the
predictive capabilities and affective biases of LLMs in ROP risk
stratification, we propose Affective-ROPTester, an automated evaluation
framework incorporating three prompting strategies: Instruction-based,
Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme
assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and
ICL schemes leverage external medical knowledge to enhance predictive accuracy.
Crucially, we integrate emotional elements at the prompt level to investigate
how different affective framings influence the model's ability to predict ROP
and its bias patterns. Empirical results derived from the CROP dataset yield
two principal observations. First, LLMs demonstrate limited efficacy in ROP
risk prediction when operating solely on intrinsic knowledge, yet exhibit
marked performance gains when augmented with structured external inputs.
Second, affective biases are evident in the model outputs, with a consistent
inclination toward overestimating medium- and high-risk cases. Third, compared
to negative emotions, positive emotional framing contributes to mitigating
predictive bias in model outputs. These findings highlight the critical role of
affect-sensitive prompt engineering in enhancing diagnostic reliability and
emphasize the utility of Affective-ROPTester as a framework for evaluating and
mitigating affective bias in clinical language modeling systems.

</details>


### [110] [CogniPlay: a work-in-progress Human-like model for General Game Playing](https://arxiv.org/abs/2507.05868)
*Aloïs Rautureau,Éric Piette*

Main category: cs.AI

TL;DR: 论文探讨了AI系统在游戏中表现优异但缺乏人类直觉决策的问题，提出了基于认知心理学的模型CogniPlay。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在游戏中表现卓越，但其决策过程与人类直觉模式不符，研究旨在填补这一差距。

Method: 结合认知心理学和前人研究，提出了适用于通用游戏（GGP）的模型CogniPlay。

Result: 提出了一个基于人类认知的工作进展模型。

Conclusion: CogniPlay模型有望更接近人类直觉决策，但仍需进一步研究。

Abstract: While AI systems have equaled or surpassed human performance in a wide
variety of games such as Chess, Go, or Dota 2, describing these systems as
truly "human-like" remains far-fetched. Despite their success, they fail to
replicate the pattern-based, intuitive decision-making processes observed in
human cognition. This paper presents an overview of findings from cognitive
psychology and previous efforts to model human-like behavior in artificial
agents, discusses their applicability to General Game Playing (GGP) and
introduces our work-in-progress model based on these observations: CogniPlay.

</details>


### [111] [Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc -- and We Can Do Better](https://arxiv.org/abs/2507.05886)
*Aaron Bembenek*

Main category: cs.AI

TL;DR: 提出了一种名为神经符号转换系统的计算模型，旨在结合符号算法和大型语言模型（LLMs），以构建更强大的自动推理工具。


<details>
  <summary>Details</summary>
Motivation: 当前构建神经符号自动推理系统的方法缺乏传统符号算法的强保证，且未能充分利用LLM的潜力。

Method: 提出神经符号转换系统，将符号状态与直觉配对，并在符号和直觉上并行执行状态转换。

Result: 该模型有望扩展逻辑推理能力，同时保留符号算法的强保证。

Conclusion: 该计算模型可具体化为逻辑编程语言，为神经符号自动推理工具提供理论基础。

Abstract: There is growing excitement about building software verifiers, synthesizers,
and other Automated Reasoning (AR) tools by combining traditional symbolic
algorithms and Large Language Models (LLMs). Unfortunately, the current
practice for constructing such neurosymbolic AR systems is an ad hoc
programming model that does not have the strong guarantees of traditional
symbolic algorithms, nor a deep enough synchronization of neural networks and
symbolic reasoning to unlock the full potential of LLM-powered reasoning. I
propose Neurosymbolic Transition Systems as a principled computational model
that can underlie infrastructure for building neurosymbolic AR tools. In this
model, symbolic state is paired with intuition, and state transitions operate
over symbols and intuition in parallel. I argue why this new paradigm can scale
logical reasoning beyond current capabilities while retaining the strong
guarantees of symbolic algorithms, and I sketch out how the computational model
I propose can be reified in a logic programming language.

</details>


### [112] [Decomposing the Time Series Forecasting Pipeline: A Modular Approach for Time Series Representation, Information Extraction, and Projection](https://arxiv.org/abs/2507.05891)
*Robert Leppich,Michael Stenger,André Bauer,Samuel Kounev*

Main category: cs.AI

TL;DR: 论文提出了一种分解时间序列预测流程的方法，通过三个阶段（输入序列表示、信息提取与记忆构建、目标投影）提升预测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: Transformer在时间序列预测中取得进展，但仍面临序列表示、记忆构建和目标投影的挑战，需针对不同任务解决特定问题。

Method: 将预测流程分解为三个阶段，研究不同模块（如卷积层和自注意力机制）的配置，并在七个基准数据集上评估。

Result: 模型在预测精度上达到最优，同时显著提升计算效率，减少训练和推理时间及参数数量。

Conclusion: 通过系统分解和模块优化，实现了高效且准确的时间序列预测，代码已开源。

Abstract: With the advent of Transformers, time series forecasting has seen significant
advances, yet it remains challenging due to the need for effective sequence
representation, memory construction, and accurate target projection. Time
series forecasting remains a challenging task, demanding effective sequence
representation, meaningful information extraction, and precise future
projection. Each dataset and forecasting configuration constitutes a distinct
task, each posing unique challenges the model must overcome to produce accurate
predictions. To systematically address these task-specific difficulties, this
work decomposes the time series forecasting pipeline into three core stages:
input sequence representation, information extraction and memory construction,
and final target projection. Within each stage, we investigate a range of
architectural configurations to assess the effectiveness of various modules,
such as convolutional layers for feature extraction and self-attention
mechanisms for information extraction, across diverse forecasting tasks,
including evaluations on seven benchmark datasets. Our models achieve
state-of-the-art forecasting accuracy while greatly enhancing computational
efficiency, with reduced training and inference times and a lower parameter
count. The source code is available at
https://github.com/RobertLeppich/REP-Net.

</details>


### [113] [MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation](https://arxiv.org/abs/2507.05894)
*Fathinah Izzati,Xinyue Li,Yuxuan Wu,Gus Xia*

Main category: cs.AI

TL;DR: 论文提出MusiScene模型，通过音乐场景想象（MSI）任务生成与音乐相关的场景描述，并用于提升视频背景音乐生成（VBMG）。


<details>
  <summary>Details</summary>
Motivation: 人类能从音乐中想象出场景，但现有音乐描述模型仅关注音乐元素。论文旨在探索音乐语言模型是否能实现类似任务。

Method: 构建大规模视频-音频描述数据集（3,371对），微调Music Understanding LLaMA（MU-LLaMA）以创建MusiScene模型。

Result: MusiScene在生成上下文相关描述上优于MU-LLaMA，并成功用于提升VBMG任务。

Conclusion: MusiScene能有效实现音乐场景想象，为跨模态音乐理解与应用提供新方向。

Abstract: Humans can imagine various atmospheres and settings when listening to music,
envisioning movie scenes that complement each piece. For example, slow,
melancholic music might evoke scenes of heartbreak, while upbeat melodies
suggest celebration. This paper explores whether a Music Language Model, e.g.
MU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI),
which requires cross-modal information from video and music to train. To
improve upon existing music captioning models which focusing solely on musical
elements, we introduce MusiScene, a music captioning model designed to imagine
scenes that complement each music. In this paper, (1) we construct a
large-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music
Understanding LLaMA for the MSI task to create MusiScene, and (3) we conduct
comprehensive evaluations and prove that our MusiScene is more capable of
generating contextually relevant captions compared to MU-LLaMA. We leverage the
generated MSI captions to enhance Video Background Music Generation (VBMG) from
text.

</details>


### [114] [BlueLM-2.5-3B Technical Report](https://arxiv.org/abs/2507.05934)
*Baojiao Xiong,Boheng Chen,Chengzhi Wang,Daxiong Luo,Dongsheng Xu,Dongyang Liu,Fan Yang,Fangyuan Li,Fei Teng,Feng Wang,Fukang Qin,Fuquan Peng,Guanxin Tan,Guozhi Wang,Haibo Yu,Haohao Gao,Heng Liu,Hongbo Yang,Hongjian Zou,Houzheng Shen,Hu Meng,Huan Li,Hui Tan,Jiali Chen,Jianzhao Chen,Jinliang Zhu,Kai Wang,Lei Wu,Liangbing Liu,Liuyang Bian,Liyan He,Long Liu,Peiwen Li,Penggang Shi,Qi Ding,Rui Hu,Shuai Cao,Shuai Ren,Shuang Peng,Teng Xie,Weiji Chen,Weilin Xiang,Weixin Wu,Xi Yin,Xiaoxin Chen,Xu Chen,Yafei Wen,Yan Hu,Yanzhou Yang,Yina Xie,Yinghao Chen,Yixuan Liao,Yu Geng,Yuanjiang Ouyang,Yuanzhuo Yang,Yuehua He,Yushuai Peng,Zhaoxiong Wang,Zheng Wang,Zhibo Zhou,Ziyang Wu*

Main category: cs.AI

TL;DR: BlueLM-2.5-3B是一个紧凑的多模态大语言模型，支持思维和非思维模式，并在边缘设备上高效部署，性能优于同类模型。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效、紧凑的多模态大语言模型，适用于边缘设备部署，同时具备强大的通用和推理能力。

Method: 通过多样化数据整理、关键数据重采样、混合异构强化学习和高性能训练基础设施开发模型。

Result: 在文本和多模态基准测试中表现优异，性能接近或超越更大规模的模型，且数据效率高。

Conclusion: BlueLM-2.5-3B为高性能边缘设备多模态模型的发展提供了重要贡献和研究启示。

Abstract: We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large
Language Model (MLLM) designed for efficient edge-device deployment, offering
strong general-purpose and reasoning capabilities. To the best of our
knowledge, this is the first 3B-scale MLLM to support both thinking and
non-thinking modes, while also enabling explicit control over thinking token
budget. BlueLM-2.5-3B is developed through diversified data curation, key data
resampling, hybrid heterogeneous reinforcement learning, and a high-performance
training infrastructure. Our model achieves superior multimodal capacity while
preserving competitive pure-text performance with only 2.9 billion parameters.
We conduct comprehensive evaluations across a broad range of multimodal and
text-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable
performance to Qwen3-4B on text-only benchmarks, and trails the larger
Kimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In
non-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal
benchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency.
All of the aforementioned performance is achieved with substantially less total
training data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to
the advancement of high-performance, on-device MLLMs and provides meaningful
insights to the research community.

</details>


### [115] [A Wireless Foundation Model for Multi-Task Prediction](https://arxiv.org/abs/2507.05938)
*Yucheng Sheng,Jiacheng Wang,Xingyu Zhou,Le Liang,Hao Ye,Shi Jin,Geoffrey Ye Li*

Main category: cs.AI

TL;DR: 提出了一种统一的基础模型，用于无线网络中的多任务预测，支持不同预测区间，并通过分解、编码和Transformer架构实现高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 移动通信网络的复杂性和动态性增加，传统深度学习方法难以泛化到不同场景和任务。

Method: 采用单变量分解统一异构任务，编码粒度实现区间感知，使用因果Transformer架构，并引入补丁掩码策略支持任意输入长度。

Result: 在大规模数据集上训练后，模型在未见场景中表现出强泛化能力，并在新任务上实现零样本性能超越传统基线。

Conclusion: 提出的基础模型为无线网络中的多任务预测提供了高效且泛化能力强的解决方案。

Abstract: With the growing complexity and dynamics of the mobile communication
networks, accurately predicting key system parameters, such as channel state
information (CSI), user location, and network traffic, has become essential for
a wide range of physical (PHY)-layer and medium access control (MAC)-layer
tasks. Although traditional deep learning (DL)-based methods have been widely
applied to such prediction tasks, they often struggle to generalize across
different scenarios and tasks. In response, we propose a unified foundation
model for multi-task prediction in wireless networks that supports diverse
prediction intervals. The proposed model enforces univariate decomposition to
unify heterogeneous tasks, encodes granularity for interval awareness, and uses
a causal Transformer backbone for accurate predictions. Additionally, we
introduce a patch masking strategy during training to support arbitrary input
lengths. After trained on large-scale datasets, the proposed foundation model
demonstrates strong generalization to unseen scenarios and achieves zero-shot
performance on new tasks that surpass traditional full-shot baselines.

</details>


### [116] [Enhancing the Interpretability of Rule-based Explanations through Information Retrieval](https://arxiv.org/abs/2507.05976)
*Alessandro Umbrico,Guido Bologna,Luca Coraci,Francesca Fracasso,Silvia Gola,Gabriella Cortellessa*

Main category: cs.AI

TL;DR: 提出一种基于属性的方法，提升可解释AI在乳腺癌淋巴水肿风险评估中的透明度和可接受性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的AI技术缺乏透明度，限制了其在医疗决策中的可解释性和接受度。

Method: 采用基于规则的预测模型，结合信息检索技术，统计分析属性的相关性。

Result: 用户研究表明，该方法比原始可解释AI模型输出更具可解释性和实用性。

Conclusion: 提出的方法能有效提升AI预测在医疗决策中的透明度和实用性。

Abstract: The lack of transparency of data-driven Artificial Intelligence techniques
limits their interpretability and acceptance into healthcare decision-making
processes. We propose an attribution-based approach to improve the
interpretability of Explainable AI-based predictions in the specific context of
arm lymphedema's risk assessment after lymph nodal radiotherapy in breast
cancer. The proposed method performs a statistical analysis of the attributes
in the rule-based prediction model using standard metrics from Information
Retrieval techniques. This analysis computes the relevance of each attribute to
the prediction and provides users with interpretable information about the
impact of risk factors. The results of a user study that compared the output
generated by the proposed approach with the raw output of the Explainable AI
model suggested higher levels of interpretability and usefulness in the context
of predicting lymphedema risk.

</details>


### [117] [Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening](https://arxiv.org/abs/2507.05984)
*Zhijun Guo,Alvina Lai,Julia Ive,Alexandru Petcu,Yutong Wang,Luyuan Qi,Johan H Thygesen,Kezhi Li*

Main category: cs.AI

TL;DR: HopeBot是一款基于LLM的聊天机器人，用于抑郁症筛查，相比传统PHQ-9问卷更具互动性和适应性，用户反馈积极。


<details>
  <summary>Details</summary>
Motivation: 传统PHQ-9问卷缺乏互动性和适应性，HopeBot旨在通过聊天机器人技术改进抑郁症筛查体验。

Method: 开发HopeBot，采用检索增强生成和实时澄清技术，进行用户研究（132名参与者）。

Result: HopeBot与PHQ-9得分一致性高（ICC=0.91），71%用户更信任聊天机器人，87.1%愿意重复使用或推荐。

Conclusion: 基于语音的LLM聊天机器人可作为抑郁症筛查的可扩展、低负担辅助工具。

Abstract: Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively
screen depression but lack interactivity and adaptability. We developed
HopeBot, a chatbot powered by a large language model (LLM) that administers the
PHQ-9 using retrieval-augmented generation and real-time clarification. In a
within-subject study, 132 adults in the United Kingdom and China completed both
self-administered and chatbot versions. Scores demonstrated strong agreement
(ICC = 0.91; 45% identical). Among 75 participants providing comparative
feedback, 71% reported greater trust in the chatbot, highlighting clearer
structure, interpretive guidance, and a supportive tone. Mean ratings (0-10)
were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,
and 7.4 for recommendation helpfulness; the latter varied significantly by
employment status and prior mental-health service use (p < 0.05). Overall,
87.1% expressed willingness to reuse or recommend HopeBot. These findings
demonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden
adjuncts for routine depression screening.

</details>


### [118] [CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation](https://arxiv.org/abs/2507.06013)
*Kushal Gajjar,Harshit Sikchi,Arpit Singh Gautam,Marc Hammons,Saurabh Jha*

Main category: cs.AI

TL;DR: CogniSQL-R1-Zero是一个基于强化学习的框架，通过轻量级奖励信号生成准确的SQL，在Text-to-SQL任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言转SQL（Text-to-SQL）任务中复杂查询生成正确且可执行SQL的挑战。

Method: 采用强化学习框架，利用执行正确性和格式标签合规性作为奖励信号，避免中间监督和复杂奖励设计。

Result: 在Text2SQL基准测试中达到最先进的执行准确率，优于多个大型模型。

Conclusion: 强化学习方法在高效和可扩展的Text-to-SQL生成中具有潜力，并发布了两个数据集支持进一步研究。

Abstract: Translating natural language into SQL (Text-to-SQL) remains a core challenge
at the intersection of language understanding and structured data access.
Although large language models (LLMs) have improved fluency, generating correct
and executable SQL, especially for complex queries, continues to be
challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL)
framework and model that produces accurate SQL using a lightweight reward
signal based on execution correctness and format-tag compliance. By avoiding
intermediate supervision, hybrid pipelines and complex reward shaping, our
method encourages stable learning and stronger alignment with the ultimate task
objective-producing executable programs. CogniSQL-R1-Zero achieves
state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench,
outperforming prior supervised and instruction-tuned baselines including SFT
CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a
significantly smaller 7B backbone. This result underscores the scalability and
efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs
(40 GB VRAM each). To support further research in efficient and interpretable
Text-to-SQL modeling, we release two curated datasets: (i) a collection of
5,024 reasoning traces with varying context lengths, and (ii) a
positive-sampled corpus of 36,356 corpus of weakly supervised queries, each
annotated with six semantically diverse reasoning paths. Together, these
contributions advance scalable, execution-aligned Text-to-SQL generation.

</details>


### [119] [Feature-Guided Neighbor Selection for Non-Expert Evaluation of Model Predictions](https://arxiv.org/abs/2507.06029)
*Courtney Ford,Mark T. Keane*

Main category: cs.AI

TL;DR: FGNS是一种后处理方法，通过结合局部和全局特征重要性选择代表性样本，提升非专家用户对模型输出的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法对非专家用户生成的解释不够清晰，FGNS旨在通过改进样本选择提升解释的可理解性。

Method: FGNS结合局部和全局特征重要性，选择更具类别代表性的邻居样本。

Result: 在用户研究中，FGNS显著提升非专家识别模型错误的能力，决策更快更准，且邻居选择更符合类别特征。

Conclusion: FGNS为提升模型解释与人类理解对齐提供了有效方法，但解释质量与用户信任之间的差距仍需进一步研究。

Abstract: Explainable AI (XAI) methods often struggle to generate clear, interpretable
outputs for users without domain expertise. We introduce Feature-Guided
Neighbor Selection (FGNS), a post hoc method that enhances interpretability by
selecting class-representative examples using both local and global feature
importance. In a user study (N = 98) evaluating Kannada script classifications,
FGNS significantly improved non-experts' ability to identify model errors while
maintaining appropriate agreement with correct predictions. Participants made
faster and more accurate decisions compared to those given traditional k-NN
explanations. Quantitative analysis shows that FGNS selects neighbors that
better reflect class characteristics rather than merely minimizing
feature-space distance, leading to more consistent selection and tighter
clustering around class prototypes. These results support FGNS as a step toward
more human-aligned model assessment, although further work is needed to address
the gap between explanation quality and perceived trust.

</details>


### [120] [On Lockean beliefs that are deductively closed and minimal change](https://arxiv.org/abs/2507.06042)
*Tommaso Flaminio,Lluis Godo,Ramón Pino Pérez,Lluis Subirana*

Main category: cs.AI

TL;DR: 论文在Lockean理论框架下，研究了基于概率置信度的信念集，并提出了两种使其在经典逻辑演绎下封闭的特征化方法，同时提出了一种最小化修正的更新方法。


<details>
  <summary>Details</summary>
Motivation: 解决Lockean信念集在经典逻辑演绎下不封闭的问题，并探索如何通过最小修正实现信念集的演绎封闭。

Method: 提出了两种特征化方法，描述封闭的信念集；并提出一种概率更新方法，实现最小修正。

Result: 展示了如何通过最小修正使信念集在演绎下封闭。

Conclusion: 论文为Lockean信念集的封闭性和最小修正提供了理论支持和方法论指导。

Abstract: Within the formal setting of the Lockean thesis, an agent belief set is
defined in terms of degrees of confidence and these are described in
probabilistic terms. This approach is of established interest, notwithstanding
some limitations that make its use troublesome in some contexts, like, for
instance, in belief change theory. Precisely, Lockean belief sets are not
generally closed under (classical) logical deduction. The aim of the present
paper is twofold: on one side we provide two characterizations of those belief
sets that are closed under classical logic deduction, and on the other we
propose an approach to probabilistic update that allows us for a minimal
revision of those beliefs, i.e., a revision obtained by making the fewest
possible changes to the existing belief set while still accommodating the new
information. In particular, we show how we can deductively close a belief set
via a minimal revision.

</details>


### [121] [FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models](https://arxiv.org/abs/2507.06057)
*Bo Pang,Yalu Ouyang,Hangfei Xu,Ziqi Jia,Panpan Li,Shengzhao Wen,Lu Wang,Shiyong Li,Yanpeng Wang*

Main category: cs.AI

TL;DR: FEVO框架通过多阶段增强（CPT、SFT、RL）提升LLM在金融领域的性能，并在多个基准测试中取得最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究在金融领域应用LLM的进展有限，需要结合领域知识和结构化推理。

Method: 使用CPT扩展金融知识，SFT注入结构化推理，RL整合知识与推理，并利用高质量数据集FEVO-Train进行训练。

Result: FEVO-R32B在五个金融基准测试中表现最优，优于更大模型和专用模型。

Conclusion: FEVO框架有效提升了LLM在金融领域的性能，验证了金融知识扩展和结构化推理的重要性。

Abstract: Advancements in reasoning for large language models (LLMs) have lead to
significant performance improvements for LLMs in various fields such as
mathematics and programming. However, research applying these advances to the
financial domain, where considerable domain-specific knowledge is necessary to
complete tasks, remains limited. To address this gap, we introduce FEVO
(Financial Evolution), a multi-stage enhancement framework developed to enhance
LLM performance in the financial domain. FEVO systemically enhances LLM
performance by using continued pre-training (CPT) to expand financial domain
knowledge, supervised fine-tuning (SFT) to instill structured, elaborate
reasoning patterns, and reinforcement learning (RL) to further integrate the
expanded financial domain knowledge with the learned structured reasoning. To
ensure effective and efficient training, we leverage frontier reasoning models
and rule-based filtering to curate FEVO-Train, high-quality datasets
specifically designed for the different post-training phases. Using our
framework, we train the FEVO series of models -- C32B, S32B, R32B -- from
Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and
general capabilities, with results showing that FEVO-R32B achieves
state-of-the-art performance on five financial benchmarks against much larger
models as well as specialist models. More significantly, FEVO-R32B demonstrates
markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct
using only RL), thus validating the effectiveness of financial domain knowledge
expansion and structured, logical reasoning distillation

</details>


### [122] [AI-Based Demand Forecasting and Load Balancing for Optimising Energy use in Healthcare Systems: A real case study](https://arxiv.org/abs/2507.06077)
*Iman Rahimi,Isha Patel*

Main category: cs.AI

TL;DR: 该论文提出了一种结合LSTM、遗传算法和SHAP的AI框架，用于医疗设施的高效能源管理，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 医疗设施能源需求波动大，传统方法效率低且成本高，亟需更高效的解决方案。

Method: 采用LSTM进行时间序列预测，遗传算法优化参数和负载平衡，SHAP增强模型可解释性。

Result: LSTM在预测性能上显著优于ARIMA和Prophet模型（MAE和RMSE更低），遗传算法和SHAP进一步提升了效率和透明度。

Conclusion: 该框架为医疗设施能源管理提供了高效、可扩展的解决方案，未来可探索实时部署和强化学习结合。

Abstract: This paper tackles the urgent need for efficient energy management in
healthcare facilities, where fluctuating demands challenge operational
efficiency and sustainability. Traditional methods often prove inadequate,
causing inefficiencies and higher costs. To address this, the study presents an
AI-based framework combining Long Short-Term Memory (LSTM), genetic algorithm
(GA), and SHAP (Shapley Additive Explanations), specifically designed for
healthcare energy management. Although LSTM is widely used for time-series
forecasting, its application in healthcare energy prediction remains
underexplored. The results reveal that LSTM significantly outperforms ARIMA and
Prophet models in forecasting complex, non-linear demand patterns. LSTM
achieves a Mean Absolute Error (MAE) of 21.69 and Root Mean Square Error (RMSE)
of 29.96, far better than Prophet (MAE: 59.78, RMSE: 81.22) and ARIMA (MAE:
87.73, RMSE: 125.22), demonstrating superior performance. The genetic algorithm
is applied to optimize model parameters and improve load balancing strategies,
enabling adaptive responses to real-time energy fluctuations. SHAP analysis
further enhances model transparency by explaining the influence of different
features on predictions, fostering trust in decision-making processes. This
integrated LSTM-GA-SHAP approach offers a robust solution for improving
forecasting accuracy, boosting energy efficiency, and advancing sustainability
in healthcare facilities. Future research may explore real-time deployment and
hybridization with reinforcement learning for continuous optimization. Overall,
the study establishes a solid foundation for using AI in healthcare energy
management, highlighting its scalability, efficiency, and resilience potential.

</details>


### [123] [OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety](https://arxiv.org/abs/2507.06134)
*Sanidhya Vijayvargiya,Aditya Bharat Soni,Xuhui Zhou,Zora Zhiruo Wang,Nouha Dziri,Graham Neubig,Maarten Sap*

Main category: cs.AI

TL;DR: OpenAgentSafety是一个评估AI代理安全性的框架，支持真实工具和多任务测试，发现主流LLM在51.2%至72.7%的任务中存在不安全行为。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖模拟环境或狭窄任务域，无法全面检测AI代理的不安全行为，需要更严谨的评估框架。

Method: OpenAgentSafety框架结合真实工具（如浏览器、代码执行环境）和350多项多轮多用户任务，采用规则分析和LLM评估检测不安全行为。

Result: 测试显示，Claude-Sonnet-3.7和o3-mini等主流LLM在51.2%至72.7%的安全漏洞任务中表现不安全。

Conclusion: OpenAgentSafety揭示了AI代理的安全漏洞，强调实际部署前需加强安全保障。

Abstract: Recent advances in AI agents capable of solving complex, everyday tasks, from
scheduling to customer service, have enabled deployment in real-world settings,
but their possibilities for unsafe behavior demands rigorous evaluation. While
prior benchmarks have attempted to assess agent safety, most fall short by
relying on simulated environments, narrow task domains, or unrealistic tool
abstractions. We introduce OpenAgentSafety, a comprehensive and modular
framework for evaluating agent behavior across eight critical risk categories.
Unlike prior work, our framework evaluates agents that interact with real
tools, including web browsers, code execution environments, file systems, bash
shells, and messaging platforms; and supports over 350 multi-turn, multi-user
tasks spanning both benign and adversarial user intents. OpenAgentSafety is
designed for extensibility, allowing researchers to add tools, tasks, websites,
and adversarial strategies with minimal effort. It combines rule-based analysis
with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors.
Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe
behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7%
with o3-mini, highlighting critical safety vulnerabilities and the need for
stronger safeguards before real-world deployment.

</details>


### [124] [The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains](https://arxiv.org/abs/2507.06187)
*Scott Geng,Hamish Ivison,Chun-Liang Li,Maarten Sap,Jerry Li,Ranjay Krishna,Pang Wei Koh*

Main category: cs.AI

TL;DR: 论文提出delta学习假设，表明即使数据点质量较弱，通过配对偏好数据也能提升模型性能。实验验证了该假设，并展示了其在高性能模型训练中的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在强监督稀缺的情况下，如何通过弱数据点的配对偏好数据提升语言模型性能。

Method: 方法包括提出delta学习假设，并通过实验验证，包括在8B模型上使用3B和1.5B模型的配对数据进行后训练。

Result: 结果显示，该方法在11个基准测试中与Tulu 3性能相当，且成本更低。

Conclusion: 结论表明，delta学习为高性能模型训练提供了一种更简单、更经济的方法。

Abstract: Improvements in language models are often driven by improving the quality of
the data we train them on, which can be limiting when strong supervision is
scarce. In this work, we show that paired preference data consisting of
individually weak data points can enable gains beyond the strength of each
individual data point. We formulate the delta learning hypothesis to explain
this phenomenon, positing that the relative quality delta between points
suffices to drive learning via preference tuning--even when supervised
finetuning on the weak data hurts. We validate our hypothesis in controlled
experiments and at scale, where we post-train 8B models on preference data
generated by pairing a small 3B model's responses with outputs from an even
smaller 1.5B model to create a meaningful delta. Strikingly, on a standard
11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the
performance of Tulu 3, a state-of-the-art open model tuned from the same base
model while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta
learning enables simpler and cheaper open recipes for state-of-the-art
post-training. To better understand delta learning, we prove in logistic
regression that the performance gap between two weak teacher models provides
useful signal for improving a stronger student. Overall, our work shows that
models can learn surprisingly well from paired data that might typically be
considered weak.

</details>


### [125] [Identifiability in Causal Abstractions: A Hierarchy of Criteria](https://arxiv.org/abs/2507.06213)
*Clément Yvernes,Emilie Devijver,Marianne Clausel,Eric Gaussier*

Main category: cs.AI

TL;DR: 论文探讨了在缺乏完整因果图的情况下，如何通过因果抽象（简化的因果表示）来识别因果效应，并提出了一个层次化的可识别性标准框架。


<details>
  <summary>Details</summary>
Motivation: 在复杂或高维环境中，完整的因果图通常未知，限制了因果效应的识别。因果抽象提供了一种部分保留因果信息的简化方法。

Method: 研究基于因果图的集合形式化因果抽象，提出并形式化了多种可识别性标准，并将这些标准组织成层次结构。

Result: 构建了一个层次化的可识别性标准框架，明确了不同因果知识水平下的可识别内容，并通过文献示例和工具展示了其应用。

Conclusion: 该框架为在缺乏完整因果知识时推理可识别性提供了工具，并有助于理解不同抽象水平下的因果效应识别能力。

Abstract: Identifying the effect of a treatment from observational data typically
requires assuming a fully specified causal diagram. However, such diagrams are
rarely known in practice, especially in complex or high-dimensional settings.
To overcome this limitation, recent works have explored the use of causal
abstractions-simplified representations that retain partial causal information.
In this paper, we consider causal abstractions formalized as collections of
causal diagrams, and focus on the identifiability of causal queries within such
collections. We introduce and formalize several identifiability criteria under
this setting. Our main contribution is to organize these criteria into a
structured hierarchy, highlighting their relationships. This hierarchical view
enables a clearer understanding of what can be identified under varying levels
of causal knowledge. We illustrate our framework through examples from the
literature and provide tools to reason about identifiability when full causal
knowledge is unavailable.

</details>


### [126] [Aligned Textual Scoring Rules](https://arxiv.org/abs/2507.06221)
*Yuxuan Lu,Yifan Wu,Jason Hartline,Michael J. Curry*

Main category: cs.AI

TL;DR: 本文提出了一种对齐评分规则（ASR），用于文本信息获取，通过优化与参考分数（如人类评分）的均方误差，在保持评分规则正确性的同时更好地与人类偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 现有评分规则虽能保证正确性，但未必与人类偏好对齐，因此需要设计一种既能保持正确性又能与人类偏好一致的评分规则。

Method: 设计对齐评分规则（ASR），通过最小化其与参考分数（如人类评分）的均方误差，优化评分规则。

Result: 实验表明，ASR在保持评分规则正确性的同时，显著优于先前方法，更符合人类偏好。

Conclusion: ASR是一种有效的评分规则，能够在保持正确性的同时更好地与人类偏好对齐。

Abstract: Scoring rules elicit probabilistic predictions from a strategic agent by
scoring the prediction against a ground truth state. A scoring rule is proper
if, from the agent's perspective, reporting the true belief maximizes the
expected score. With the development of language models, Wu and Hartline (2024)
proposes a reduction from textual information elicitation to the numerical
(i.e. probabilistic) information elicitation problem, which achieves provable
properness for textual elicitation. However, not all proper scoring rules are
well aligned with human preference over text. Our paper designs the Aligned
Scoring rule (ASR) for text by optimizing and minimizing the mean squared error
between a proper scoring rule and a reference score (e.g. human score). Our
experiments show that our ASR outperforms previous methods in aligning with
human preference while maintaining properness.

</details>
