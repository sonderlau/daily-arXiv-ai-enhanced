{"id": "2508.12198", "categories": ["physics.ao-ph", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12198", "abs": "https://arxiv.org/abs/2508.12198", "authors": ["ChangJae Lee", "Heecheol Yang", "Jonghak Choi"], "title": "Exploring Multimodal AI Reasoning for Meteorological Forecasting from Skew-T Diagrams", "comment": "24 pages, 3 figures, 9 tables", "summary": "Forecasting from atmospheric soundings is a fundamental task in operational\nmeteorology, often requiring structured visual reasoning over Skew-T log-P\ndiagrams by human forecasters. While recent advances in Vision-Language Models\n(VLMs) have shown promise in other scientific domains, their application to\nmeteorological diagram interpretation remains largely unexplored. In this\nstudy, we present a lightweight AI assistant that interprets Skew-T diagrams\nusing a small language model (LM) and a small VLM fine-tuned to emulate human\nforecasters. Using a curriculum learning framework, we first train the models\nto identify key atmospheric features from diagrams through visual question\nanswering, followed by chain-of-thought reasoning tasks that estimate\nprecipitation probability based on the derived visual groundings. Model inputs\ninclude either textual summaries or generated Skew-T diagrams derived from\noperational Numerical Weather Prediction (NWP) forecasts, paired with\nthree-hour precipitation observations from South Korea's Auto Weather Stations\nnetwork. Evaluation results demonstrate that the fine-tuned VLM achieves skill\ncomparable to an operational NWP model, despite relying solely on static\natmospheric profiles. Ablation studies reveal that visual grounding and\nreasoning supervision are critical for performance, while attention map\nanalysis confirms that the model learns to focus on relevant meteorological\nfeatures. These findings highlight the potential of compact, interpretable\nmultimodal models to support weather forecasting tasks. The approach offers a\ncomputationally efficient alternative to large-scale systems, and future work\ncould extend it to more complex applications.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u591a\u6a21\u6001AI\u52a9\u624b\uff0c\u901a\u8fc7\u7ec6\u8c03\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u6765\u89e3\u91ca\u5929\u6c14\u97f3\u54cd\u56fe\uff0c\u8fbe\u5230\u4e86\u4e0e\u64cd\u4f5c\u6570\u503c\u9884\u62a5\u6a21\u578b\u76f8\u5f53\u7684\u9884\u62a5\u6280\u80fd\u3002", "motivation": "\u5929\u6c14\u97f3\u54cd\u56fe\u9884\u62a5\u662f\u6c14\u8c61\u9884\u62a5\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u6c14\u8c61\u56fe\u89e3\u91ca\u65b9\u9762\u5e94\u7528\u4e0d\u591f\u3002\u9700\u8981\u5f00\u53d1\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u89e3\u91ca\u6027\u6a21\u578b\u6765\u652f\u6301\u5929\u6c14\u9884\u62a5\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u5148\u8bad\u7ec3\u6a21\u578b\u8bc6\u522b\u5929\u6c14\u7279\u5f81\uff0c\u7136\u540e\u8fdb\u884c\u94fe\u5f0f\u601d\u7eea\u63a8\u7406\u4f30\u7b97\u964d\u96e8\u6982\u7387\u3002\u8f93\u5165\u5305\u62ec\u6587\u672c\u6458\u8981\u6216\u751f\u6210\u7684Skew-T\u56fe\u3002", "result": "\u7ec6\u8c03\u540e\u7684VLM\u8fbe\u5230\u4e86\u4e0e\u64cd\u4f5cNWP\u6a21\u578b\u76f8\u5f53\u7684\u6280\u80fd\uff0c\u867d\u7136\u4ec5\u4f9d\u8d56\u9759\u6001\u5929\u6c14\u914d\u7f6e\u3002\u89c6\u89c9\u57fa\u7840\u548c\u63a8\u7406\u76d1\u7763\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u7ec6\u8c03\u591a\u6a21\u6001\u6a21\u578b\u5728\u5929\u6c14\u9884\u62a5\u4e2d\u7684\u6f5c\u529b\uff0c\u63d0\u4f9b\u4e86\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u5e94\u7528\u3002"}}
{"id": "2508.12481", "categories": ["physics.ao-ph", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2508.12481", "abs": "https://arxiv.org/abs/2508.12481", "authors": ["Mauricio L\u00f3pez-Reyes", "M. L. Mart\u00edn-P\u00e9rez", "C. Calvo-Sancho", "J. J. Gonz\u00e1lez-Alem\u00e1n"], "title": "Dynamic Forcing Behind Rapid Intensification of Hurricane Lidia", "comment": null, "summary": "This study examines Hurricane Lidia rapid intensification (RI) in the\nunderstudied northeastern Pacific, focusing on its interaction with an\nupper-level trough. Using IFS-ECMWF ensemble forecasts and ERA5 reanalysis, we\nanalyze the large-scale dynamical mechanisms driving Lidias intensification.\nResults show that the trough played a crucial role in promoting RI by enhancing\nsynoptic-scale ascent, upper-level divergence, and eddy flux convergence. In\nthe higher-intensification ensemble group, stronger Trenberth forcing emerged\nprior to RI onset, suggesting a causative role in preconditioning the storm\nenvironment. This dynamical forcing likely triggered latent heat release, which\nin turn modified the upper-level potential vorticity structure and contributed\nto a subsequent reduction in vertical wind shear. In contrast, the\nlower-intensification group exhibited weaker forcing, higher shear, and a lack\nof sustained ventilation. These findings highlight the importance of diagnosing\nearly dynamical triggers for RI, particularly in regions where operational\naccess to high-resolution models is limited. This approach provides a\ncost-effective framework for anticipating RI using ensemble-based diagnostics\nand could serve as a valuable forecasting tool in data-sparse areas such as the\nPacific coast of Mexico. Future studies should combine this large-scale\nmethodology with high-resolution simulations to better capture storm-scale\nprocesses and validate multi-scale interactions in RI events.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5317\u592a\u5e73\u6d0b\u53f0\u98ceLidia\u7684\u5feb\u901f\u52a0\u5f3a\u8fc7\u7a0b\uff0c\u53d1\u73b0\u4e0a\u5c42\u6e2f\u6c9f\u901a\u8fc7\u589e\u5f3a\u5929\u6c14\u5347\u51fd\u3001\u9ad8\u5c42\u6566\u5ea6\u548c\u6fc0\u5b50\u901a\u91cf\u805a\u96c6\u6765\u4fc3\u8fdb\u53f0\u98ce\u5feb\u901f\u52a0\u5f3a\uff0c\u4e3a\u6570\u636e\u7a00\u7f29\u5730\u533a\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u7684\u9884\u62a5\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u5317\u592a\u5e73\u6d0b\u4e1c\u5317\u90e8\u533a\u57df\u53f0\u98ce\u5feb\u901f\u52a0\u5f3a\u7684\u52a8\u529b\u5b66\u673a\u5236\uff0c\u7279\u522b\u662f\u53f0\u98ce\u4e0e\u4e0a\u5c42\u6e2f\u6c9f\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u6d1e\u5b50\u8fd0\u884c\u6a21\u578b\u8bbf\u95ee\u6709\u9650\u7684\u5730\u533a\u63d0\u4f9b\u9884\u62a4\u5de5\u5177\u3002", "method": "\u4f7f\u7528IFS-ECMWF\u96c6\u5408\u9884\u62a5\u548cERA5\u518d\u5206\u6790\u6570\u636e\uff0c\u5206\u6790\u5927\u89c4\u6a21\u52a8\u529b\u5b66\u673a\u5236\uff0c\u5bf9\u6bd4\u9ad8\u5f3a\u5ea6\u96c6\u5408\u4e0e\u4f4e\u5f3a\u5ea6\u96c6\u5408\u7684\u5dee\u5f02\u3002", "result": "\u6e2f\u6c9f\u5728\u5feb\u901f\u52a0\u5f3a\u524d\u4ea7\u751f\u66f4\u5f3a\u7684Trenberth\u8feb\u52bf\uff0c\u89e6\u53d1\u6f5c\u70ed\u91ca\u653e\u5e76\u6539\u5584\u4e0a\u5c42\u6f5c\u6e29\u7ed3\u6784\uff0c\u51cf\u5c11\u5782\u76f4\u98ce\u5207\u53d8\u3002\u5f31\u8feb\u52bf\u96c6\u5408\u5219\u663e\u793a\u66f4\u9ad8\u7684\u98ce\u5207\u53d8\u548c\u7f3a\u4e4f\u6301\u7eed\u901a\u98ce\u3002", "conclusion": "\u8bc6\u522b\u65e9\u671f\u52a8\u529b\u5b66\u89e6\u53d1\u56e0\u7d20\u5bf9\u5feb\u901f\u52a0\u5f3a\u9884\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u96c6\u5408\u57fa\u7840\u8bca\u65ad\u65b9\u6cd5\u4e3a\u6570\u636e\u7a00\u7f29\u5730\u533a\u63d0\u4f9b\u4e86\u6210\u672c\u6548\u76ca\u9ad8\u7684\u9884\u62a5\u6846\u67b6\u3002"}}
{"id": "2508.13120", "categories": ["physics.ao-ph", "math.PR", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2508.13120", "abs": "https://arxiv.org/abs/2508.13120", "authors": ["Justin Finkel", "Paul A. O'Gorman"], "title": "Rare event sampling for moving targets: extremes of temperature and daily precipitation in a general circulation model", "comment": "19 pages, 5 figures, submitted to AGU Advances", "summary": "Extreme weather events epitomize high cost: to society through their physical\nimpacts, and to computer servers that are used to simulate them to provide\ninformation to mitigate those impacts. It costs hundreds of years to sample a\nfew once-per-century events with straightforward model integration, but that\ncost can be much reduced with rare event sampling, which nudges ensembles of\nsimulations to convert moderate events to severe ones, e.g., by steering a\ncyclone directly through a region of interest. With proper statistical\naccounting, rare event algorithms can provide quantitative climate risk\nassessment at reduced cost. But this can only work if ensemble members diverge\nfast enough. Sudden, transient events characteristic of Earth's midlatitude\nstorm track regions, such as heavy precipitation and heat extremes, pose a\nparticular challenge because they come and go faster than an ensemble can\nexplore the possibilities. Here we extend standard rare event algorithms to\nhandle this challenging case in an idealized atmospheric general circulation\nmodel, achieving 5-10 times sped-up estimation of long return periods, such as\n100-150 years from only 20 years of simulation for extremes of daily\nprecipitation and surface temperature. The algorithm, called TEAMS\n(``trying-early adaptive multilevel splitting''), was developed previously in\nFinkel and O'Gorman (2024) using a toy chaotic system, and relies on a key\nparameter -- the advance split time -- which may be estimated based on simple\ndiagnostics of ensemble dispersion rates. The results are promising for\naccelerated risk assessment across a wide range of physical hazards using more\nrealistic and complex models with acute computational constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86TEAMS\u7b97\u6cd5\uff0c\u901a\u8fc7\u63d0\u524d\u5206\u88c2\u65f6\u95f4\u53c2\u6570\u4f18\u5316\u7f55\u89c1\u4e8b\u4ef6\u91c7\u6837\uff0c\u5728\u7406\u60f3\u5316\u5927\u6c14\u73af\u6d41\u6a21\u578b\u4e2d\u5b9e\u73b05-10\u500d\u52a0\u901f\uff0c\u4ec5\u752820\u5e74\u6a21\u62df\u5c31\u80fd\u4f30\u7b97100-150\u5e74\u91cd\u73b0\u671f\u7684\u6781\u7aef\u964d\u6c34\u6e29\u5ea6\u4e8b\u4ef6", "motivation": "\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u6a21\u62df\u6210\u672c\u9ad8\u6602\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u6570\u767e\u5e74\u624d\u80fd\u91c7\u6837\u5230\u767e\u5e74\u4e00\u9047\u4e8b\u4ef6\u3002\u4e2d\u7eac\u5ea6\u98ce\u66b4\u5e26\u7684\u7a81\u53d1\u6027\u6781\u7aef\u4e8b\u4ef6\uff08\u5982\u5f3a\u964d\u6c34\u548c\u70ed\u6d6a\uff09\u7531\u4e8e\u6301\u7eed\u65f6\u95f4\u77ed\uff0c\u4f20\u7edf\u7f55\u89c1\u4e8b\u4ef6\u91c7\u6837\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406", "method": "\u6269\u5c55\u6807\u51c6\u7f55\u89c1\u4e8b\u4ef6\u7b97\u6cd5TEAMS\uff08trying-early adaptive multilevel splitting\uff09\uff0c\u5229\u7528\u63d0\u524d\u5206\u88c2\u65f6\u95f4\u53c2\u6570\uff0c\u57fa\u4e8e\u96c6\u5408\u79bb\u6563\u7387\u8bca\u65ad\u6765\u4f18\u5316\u91c7\u6837\u8fc7\u7a0b\uff0c\u5728\u7406\u60f3\u5316\u5927\u6c14\u73af\u6d41\u6a21\u578b\u4e2d\u8fdb\u884c\u6d4b\u8bd5", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e865-10\u500d\u7684\u52a0\u901f\u6548\u679c\uff0c\u4ec5\u752820\u5e74\u6a21\u62df\u5c31\u80fd\u51c6\u786e\u4f30\u7b97100-150\u5e74\u91cd\u73b0\u671f\u7684\u6781\u7aef\u4e8b\u4ef6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c", "conclusion": "TEAMS\u7b97\u6cd5\u4e3a\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u590d\u6742\u6a21\u578b\u4e2d\u52a0\u901f\u5404\u79cd\u7269\u7406\u707e\u5bb3\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.11659", "categories": ["cs.NE", "cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.11659", "abs": "https://arxiv.org/abs/2508.11659", "authors": ["Zhuo Liu", "Tao Chen"], "title": "Toward Practical Equilibrium Propagation: Brain-inspired Recurrent Neural Network with Feedback Regulation and Residual Connections", "comment": null, "summary": "Brain-like intelligent systems need brain-like learning methods. Equilibrium\nPropagation (EP) is a biologically plausible learning framework with strong\npotential for brain-inspired computing hardware. However, existing\nim-plementations of EP suffer from instability and prohibi-tively high\ncomputational costs. Inspired by the structure and dynamics of the brain, we\npropose a biologically plau-sible Feedback-regulated REsidual recurrent neural\nnetwork (FRE-RNN) and study its learning performance in EP framework. Feedback\nregulation enables rapid convergence by reducing the spectral radius. The\nimprovement in con-vergence property reduces the computational cost and\ntrain-ing time of EP by orders of magnitude, delivering perfor-mance on par\nwith backpropagation (BP) in benchmark tasks. Meanwhile, residual connections\nwith brain-inspired topologies help alleviate the vanishing gradient problem\nthat arises when feedback pathways are weak in deep RNNs. Our approach\nsubstantially enhances the applicabil-ity and practicality of EP in large-scale\nnetworks that un-derpin artificial intelligence. The techniques developed here\nalso offer guidance to implementing in-situ learning in physical neural\nnetworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u7269\u542f\u53d1\u7684\u53cd\u9988\u8c03\u8282\u6b8b\u5dee\u5faa\u73af\u795e\u7ecf\u7f51\u7edc(FRE-RNN)\uff0c\u89e3\u51b3\u4e86\u5e73\u8861\u4f20\u64ad(EP)\u7b97\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u4f7fEP\u5728\u5927\u89c4\u6a21\u7f51\u7edc\u4e2d\u5177\u6709\u5b9e\u7528\u6027", "motivation": "\u8111\u542f\u53d1\u667a\u80fd\u7cfb\u7edf\u9700\u8981\u8111\u542f\u53d1\u7684\u5b66\u4e60\u65b9\u6cd5\u3002\u5e73\u8861\u4f20\u64ad(EP)\u662f\u4e00\u79cd\u751f\u7269\u53ef\u4fe1\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u4f46\u73b0\u6709\u5b9e\u73b0\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u548c\u8fc7\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u95ee\u9898", "method": "\u8bbe\u8ba1\u751f\u7269\u542f\u53d1\u7684\u53cd\u9988\u8c03\u8282\u6b8b\u5dee\u5faa\u73af\u795e\u7ecf\u7f51\u7edc(FRE-RNN)\uff0c\u901a\u8fc7\u53cd\u9988\u8c03\u8282\u964d\u4f4e\u8c31\u534a\u5f84\u5b9e\u73b0\u5feb\u901f\u6536\u655b\uff0c\u6b8b\u5dee\u8fde\u63a5\u7f13\u89e3\u6df1\u5ea6RNN\u4e2d\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898", "result": "\u5c06EP\u7684\u8ba1\u7b97\u6210\u672c\u548c\u8bad\u7ec3\u65f6\u95f4\u964d\u4f4e\u4e86\u6570\u91cf\u7ea7\uff0c\u5728\u57fa\u51c6\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e0e\u53cd\u5411\u4f20\u64ad(BP)\u76f8\u5f53\u7684\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86EP\u5728\u5927\u89c4\u6a21\u7f51\u7edc\u4e2d\u7684\u9002\u7528\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u7269\u7406\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u539f\u4f4d\u5b66\u4e60\u5b9e\u73b0\u63d0\u4f9b\u4e86\u6307\u5bfc"}}
{"id": "2508.11836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11836", "abs": "https://arxiv.org/abs/2508.11836", "authors": ["Dave Goel", "Matthew Guzdial", "Anurag Sarkar"], "title": "Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video", "comment": null, "summary": "World models are defined as a compressed spatial and temporal learned\nrepresentation of an environment. The learned representation is typically a\nneural network, making transfer of the learned environment dynamics and\nexplainability a challenge. In this paper, we propose an approach, Finite\nAutomata Extraction (FAE), that learns a neuro-symbolic world model from\ngameplay video represented as programs in a novel domain-specific language\n(DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more\nprecise model of the environment and more general code than prior DSL-based\napproaches.", "AI": {"tldr": "\u63d0\u51faFAE\u65b9\u6cd5\u4ece\u6e38\u620f\u89c6\u9891\u4e2d\u5b66\u4e60\u795e\u7ecf\u7b26\u53f7\u4e16\u754c\u6a21\u578b\uff0c\u7528\u65b0\u7684DSL Retro Coder\u8868\u793a\uff0c\u76f8\u6bd4\u4e4b\u524d\u65b9\u6cd5\u5b66\u4e60\u66f4\u7cbe\u786e\u7684\u73af\u5883\u6a21\u578b\u548c\u66f4\u901a\u7528\u7684\u4ee3\u7801", "motivation": "\u73b0\u6709\u4e16\u754c\u6a21\u578b\u901a\u5e38\u662f\u795e\u7ecf\u7f51\u7edc\u8868\u793a\uff0c\u96be\u4ee5\u8fc1\u79fb\u5b66\u4e60\u5230\u7684\u73af\u5883\u52a8\u6001\u548c\u89e3\u91ca\u6027\uff0c\u9700\u8981\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u548c\u53ef\u8fc1\u79fb\u7684\u4e16\u754c\u6a21\u578b\u8868\u793a\u65b9\u6cd5", "method": "FAE\u65b9\u6cd5\u4ece\u6e38\u620f\u89c6\u9891\u4e2d\u5b66\u4e60\u795e\u7ecf\u7b26\u53f7\u4e16\u754c\u6a21\u578b\uff0c\u4f7f\u7528\u65b0\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00Retro Coder\u5c06\u6a21\u578b\u8868\u793a\u4e3a\u7a0b\u5e8f", "result": "\u76f8\u6bd4\u4e4b\u524d\u7684\u4e16\u754c\u6a21\u578b\u65b9\u6cd5\uff0cFAE\u5b66\u4e60\u4e86\u66f4\u7cbe\u786e\u7684\u73af\u5883\u6a21\u578b\uff1b\u76f8\u6bd4\u4e4b\u524d\u7684DSL\u65b9\u6cd5\uff0c\u751f\u6210\u4e86\u66f4\u901a\u7528\u7684\u4ee3\u7801", "conclusion": "FAE\u65b9\u6cd5\u80fd\u591f\u5b66\u4e60\u5230\u66f4\u7cbe\u786e\u548c\u53ef\u89e3\u91ca\u7684\u4e16\u754c\u6a21\u578b\u8868\u793a\uff0c\u5728\u795e\u7ecf\u7b26\u53f7\u5efa\u6a21\u65b9\u9762\u53d6\u5f97\u4e86\u6539\u8fdb"}}
{"id": "2508.11696", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11696", "abs": "https://arxiv.org/abs/2508.11696", "authors": ["Sami Sadat", "Mohammad Irtiza Hossain", "Junaid Ahmed Sifat", "Suhail Haque Rafi", "Md. Waseq Alauddin Alvi", "Md. Khalilur Rhaman"], "title": "A Deep Learning-Based CCTV System for Automatic Smoking Detection in Fire Exit Zones", "comment": null, "summary": "A deep learning real-time smoking detection system for CCTV surveillance of\nfire exit areas is proposed due to critical safety requirements. The dataset\ncontains 8,124 images from 20 different scenarios along with 2,708 raw samples\ndemonstrating low-light areas. We evaluated three advanced object detection\nmodels: YOLOv8, YOLOv11, and YOLOv12, followed by development of a custom model\nderived from YOLOv8 with added structures for challenging surveillance\ncontexts. The proposed model outperformed the others, achieving a recall of\n78.90 percent and mAP at 50 of 83.70 percent, delivering optimal object\ndetection across varied environments. Performance evaluation on multiple edge\ndevices using multithreaded operations showed the Jetson Xavier NX processed\ndata at 52 to 97 milliseconds per inference, establishing its suitability for\ntime-sensitive operations. This system offers a robust and adaptable platform\nfor monitoring public safety and enabling automatic regulatory compliance.", "AI": {"tldr": "\u57fa\u4e8eYOLOv8\u7684\u81ea\u5b9a\u4e49\u6a21\u578b\u5728CCTV\u76d1\u63a7\u4e2d\u5b9e\u65f6\u68c0\u6d4b\u706b\u707e\u9014\u5f84\u5438\u70df\u884c\u4e3a\uff0c\u8fbe\u523078.90%\u56de\u7387\u548c83.70% mAP\u6027\u80fd\uff0c\u5728Jetson Xavier NX\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5904\u7406\u901f\u5ea652-97ms/\u6b21\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5b89\u5168\u76d1\u63a7\u3002", "motivation": "\u706b\u707e\u9014\u5f84\u7684\u5438\u70df\u884c\u4e3a\u5b58\u5728\u91cd\u5927\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u590d\u6742\u76d1\u63a7\u73af\u5883\u4e0b\u5b9e\u65f6\u68c0\u6d4b\u7684\u7cfb\u7edf\uff0c\u786e\u4fdd\u516c\u5171\u5b89\u5168\u548c\u81ea\u52a8\u9075\u89c4\u6267\u884c\u3002", "method": "\u4f7f\u75288,124\u5f20\u56fe\u7247\u548c2,708\u4e2a\u660e\u6697\u73af\u5883\u6837\u672c\u6784\u5efa\u6570\u636e\u96c6\uff0c\u8bc4\u4f30YOLOv8\u3001YOLOv11\u3001YOLOv12\u4e09\u79cd\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u5728YOLOv8\u57fa\u7840\u4e0a\u589e\u52a0\u7279\u6b8a\u7ed3\u6784\u6784\u5efa\u81ea\u5b9a\u4e49\u6a21\u578b\u4ee5\u5e94\u5bf9\u76d1\u63a7\u6311\u6218\u3002", "result": "\u81ea\u5b9a\u4e49\u6a21\u578b\u8868\u73b0\u6700\u4f18\uff0c\u8fbe\u523078.90%\u56de\u7387\u548c83.70% mAP\u3002\u5728Jetson Xavier NX\u8fb9\u7f18\u8bbe\u5907\u4e0a\u901a\u8fc7\u591a\u7ebf\u7a0b\u8fd0\u7b97\uff0c\u6bcf\u6b21\u63a8\u7406\u8017\u65f652-97\u6beb\u79d2\uff0c\u9002\u5408\u5b9e\u65f6\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u5e73\u53f0\uff0c\u80fd\u591f\u5728\u591a\u6837\u5316\u73af\u5883\u4e0b\u6709\u6548\u76d1\u63a7\u516c\u5171\u5b89\u5168\uff0c\u5e76\u652f\u6301\u81ea\u52a8\u9075\u89c4\u6267\u884c\u3002"}}
{"id": "2508.11674", "categories": ["cs.NE", "cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.11674", "abs": "https://arxiv.org/abs/2508.11674", "authors": ["Zofia Rudnicka", "Janusz Szczepanski", "Agnieszka Pregowska"], "title": "Learning Internal Biological Neuron Parameters and Complexity-Based Encoding for Improved Spiking Neural Networks Performance", "comment": null, "summary": "This study introduces a novel approach by replacing the traditional\nperceptron neuron model with a biologically inspired probabilistic meta neuron,\nwhere the internal neuron parameters are jointly learned, leading to improved\nclassification accuracy of spiking neural networks (SNNs). To validate this\ninnovation, we implement and compare two SNN architectures: one based on\nstandard leaky integrate-and-fire (LIF) neurons and another utilizing the\nproposed probabilistic meta neuron model. As a second key contribution, we\npresent a new biologically inspired classification framework that uniquely\nintegrates SNNs with Lempel-Ziv complexity (LZC) a measure closely related to\nentropy rate. By combining the temporal precision and biological plausibility\nof SNNs with the capacity of LZC to capture structural regularity, the proposed\napproach enables efficient and interpretable classification of spatiotemporal\nneural data, an aspect not addressed in existing works. We consider learning\nalgorithms such as backpropagation, spike-timing-dependent plasticity (STDP),\nand the Tempotron learning rule. To explore neural dynamics, we use Poisson\nprocesses to model neuronal spike trains, a well-established method for\nsimulating the stochastic firing behavior of biological neurons. Our results\nreveal that depending on the training method, the classifier's efficiency can\nimprove by up to 11.00%, highlighting the advantage of learning additional\nneuron parameters beyond the traditional focus on weighted inputs alone.", "AI": {"tldr": "\u63d0\u51fa\u7528\u6982\u7387\u5143\u795e\u7ecf\u5143\u66ff\u4ee3\u4f20\u7edf\u611f\u77e5\u5668\u795e\u7ecf\u5143\uff0c\u7ed3\u5408Lempel-Ziv\u590d\u6742\u5ea6\u6784\u5efa\u65b0\u578bSNN\u5206\u7c7b\u6846\u67b6\uff0c\u8bad\u7ec3\u6548\u7387\u6700\u9ad8\u63d0\u534711%", "motivation": "\u4f20\u7edfSNN\u795e\u7ecf\u5143\u6a21\u578b\u53c2\u6570\u5b66\u4e60\u6709\u9650\uff0c\u9700\u8981\u66f4\u751f\u7269\u542f\u53d1\u7684\u795e\u7ecf\u5143\u6a21\u578b\u548c\u5206\u7c7b\u6846\u67b6\u6765\u63d0\u5347\u65f6\u7a7a\u795e\u7ecf\u6570\u636e\u5206\u7c7b\u6027\u80fd", "method": "\u4f7f\u7528\u6982\u7387\u5143\u795e\u7ecf\u5143\u66ff\u4ee3LIF\u795e\u7ecf\u5143\uff0c\u7ed3\u5408LZC\u590d\u6742\u5ea6\u6784\u5efa\u5206\u7c7b\u6846\u67b6\uff0c\u91c7\u7528BP\u3001STDP\u3001Tempotron\u7b49\u591a\u79cd\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u6cca\u677e\u8fc7\u7a0b\u6a21\u62df\u795e\u7ecf\u5143\u53d1\u653e", "result": "\u6839\u636e\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5206\u7c7b\u5668\u6548\u7387\u6700\u9ad8\u53ef\u63d0\u534711.00%\uff0c\u8bc1\u660e\u5b66\u4e60\u989d\u5916\u795e\u7ecf\u5143\u53c2\u6570\u7684\u4f18\u52bf", "conclusion": "\u6982\u7387\u5143\u795e\u7ecf\u5143\u6a21\u578b\u548cLZC-SNN\u7ed3\u5408\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347SNN\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u65f6\u7a7a\u795e\u7ecf\u6570\u636e\u5206\u6790\u63d0\u4f9b\u65b0\u9014\u5f84"}}
{"id": "2508.11850", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11850", "abs": "https://arxiv.org/abs/2508.11850", "authors": ["Milad Yazdani", "Mahdi Mostajabdaveh", "Samin Aref", "Zirui Zhou"], "title": "EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models", "comment": null, "summary": "Integer programming lies at the heart of crucial combinatorial optimization\ntasks but remains challenging due to its NP-hard nature. An effective approach\nfor practically solving integer programs is the manual design of acceleration\ncuts, i.e. inequalities that improve solver performance. However, this creative\nprocess demands deep expertise and is yet to be automated. Our proposed\nframework, EvoCut, automates the generation of acceleration cuts by combining\nlarge language models (LLMs) with an evolutionary search. EvoCut (i)\ninitializes a diverse population of candidate cuts via an LLM-based initializer\nagent; (ii) for each cut empirically evaluates both preservation of the optimal\nsolution and its ability to cut off fractional solutions across a verification\nset; and (iii) iteratively refines the population through evolutionary\ncrossover and mutation agents. We quantify each cut's utility by its relative\nreduction in the solver's optimality gap. Our comparisons against standard\ninteger programming practice show that EvoCut reduces optimality gap by 17-57%\nwithin a fixed time. It obtains the same solutions up to 4 times as fast, and\nobtains higher-quality solutions within the same time limit. Requiring no human\nexpert input, EvoCut reliably generates, improves, and empirically verifies\ncuts that generalize to unseen instances. The code is available at\nhttps://github.com/milad1378yz/EvoCut.", "AI": {"tldr": "EvoCut\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u751f\u6210\u6574\u6570\u89c4\u5212\u52a0\u901f\u5272\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8fdb\u5316\u641c\u7d22\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u53ef\u663e\u8457\u63d0\u5347\u6c42\u89e3\u5668\u6027\u80fd", "motivation": "\u6574\u6570\u89c4\u5212\u662f\u7ec4\u5408\u4f18\u5316\u7684\u6838\u5fc3\u4f46NP\u96be\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u624b\u5de5\u8bbe\u8ba1\u52a0\u901f\u5272\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u9700\u8981\u6df1\u539a\u4e13\u4e1a\u77e5\u8bc6\u4e14\u96be\u4ee5\u81ea\u52a8\u5316", "method": "\u7ed3\u5408LLM\u548c\u8fdb\u5316\u641c\u7d22\uff1a1\uff09LLM\u521d\u59cb\u5316\u591a\u6837\u5316\u5019\u9009\u5272\uff1b2\uff09\u9a8c\u8bc1\u96c6\u8bc4\u4f30\u5272\u7684\u8d28\u91cf\uff08\u4fdd\u6301\u6700\u4f18\u89e3\u548c\u5207\u5272\u5206\u6570\u89e3\u80fd\u529b\uff09\uff1b3\uff09\u901a\u8fc7\u8fdb\u5316\u4ea4\u53c9\u53d8\u5f02\u8fed\u4ee3\u4f18\u5316\u79cd\u7fa4", "result": "\u76f8\u6bd4\u6807\u51c6\u65b9\u6cd5\uff0cEvoCut\u5728\u56fa\u5b9a\u65f6\u95f4\u5185\u5c06\u6700\u4f18\u6027\u95f4\u9699\u964d\u4f4e17-57%\uff0c\u83b7\u5f97\u76f8\u540c\u89e3\u7684\u901f\u5ea6\u63d0\u53474\u500d\uff0c\u5728\u76f8\u540c\u65f6\u95f4\u5185\u83b7\u5f97\u66f4\u9ad8\u8d28\u91cf\u89e3", "conclusion": "EvoCut\u80fd\u591f\u53ef\u9760\u5730\u751f\u6210\u3001\u6539\u8fdb\u548c\u9a8c\u8bc1\u53ef\u6cdb\u5316\u5230\u672a\u89c1\u5b9e\u4f8b\u7684\u5272\uff0c\u5b8c\u5168\u81ea\u52a8\u5316\u4e14\u65e0\u9700\u4eba\u5de5\u4e13\u5bb6\u8f93\u5165"}}
{"id": "2508.11697", "categories": ["cs.CV", "cs.AI", "I.5.1"], "pdf": "https://arxiv.org/pdf/2508.11697", "abs": "https://arxiv.org/abs/2508.11697", "authors": ["Adri\u00e1n Rodr\u00edguez-Mu\u00f1oz", "Manel Baradad", "Phillip Isola", "Antonio Torralba"], "title": "Separating Knowledge and Perception with Procedural Data", "comment": "17 pages, 18 figures, 3 tables, to be published in ICML 2025", "summary": "We train representation models with procedural data only, and apply them on\nvisual similarity, classification, and semantic segmentation tasks without\nfurther training by using visual memory -- an explicit database of reference\nimage embeddings. Unlike prior work on visual memory, our approach achieves\nfull compartmentalization with respect to all real-world images while retaining\nstrong performance. Compared to a model trained on Places, our procedural model\nperforms within $1\\%$ on NIGHTS visual similarity, outperforms by $8\\%$ and\n$15\\%$ on CUB200 and Flowers102 fine-grained classification, and is within\n$10\\%$ on ImageNet-1K classification. It also demonstrates strong zero-shot\nsegmentation, achieving an $R^2$ on COCO within $10\\%$ of the models trained on\nreal data. Finally, we analyze procedural versus real data models, showing that\nparts of the same object have dissimilar representations in procedural models,\nresulting in incorrect searches in memory and explaining the remaining\nperformance gap.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u7a0b\u5e8f\u5316\u6570\u636e\u8bad\u7ec3\u8868\u5f81\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u8bb0\u5fc6\u673a\u5236\u5728\u89c6\u89c9\u76f8\u4f3c\u6027\u3001\u5206\u7c7b\u548c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e0a\u5b9e\u73b0\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u5f3a\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u5b8c\u5168\u4f7f\u7528\u7a0b\u5e8f\u5316\u6570\u636e\u6784\u5efa\u89c6\u89c9\u8868\u5f81\u6a21\u578b\u7684\u53ef\u80fd\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u4e0e\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u7684\u5b8c\u5168\u9694\u79bb\u3002", "method": "\u4f7f\u7528\u7a0b\u5e8f\u5316\u6570\u636e\u8bad\u7ec3\u8868\u5f81\u6a21\u578b\uff0c\u5e76\u5e94\u7528\u89c6\u89c9\u8bb0\u5fc6\u673a\u5236\u2014\u2014\u4e00\u4e2a\u663e\u5f0f\u7684\u53c2\u8003\u56fe\u50cf\u5d4c\u5165\u6570\u636e\u5e93\uff0c\u5728\u591a\u4e2a\u89c6\u89c9\u4efb\u52a1\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u63a8\u7406\u3002", "result": "\u5728NIGHTS\u89c6\u89c9\u76f8\u4f3c\u6027\u4efb\u52a1\u4e0a\u4ec5\u5dee1%\uff0c\u5728CUB200\u548cFlowers102\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e0a\u5206\u522b\u8d85\u8d8a8%\u548c15%\uff0c\u5728ImageNet-1K\u5206\u7c7b\u4e0a\u5dee\u8ddd10%\uff0c\u5728COCO\u96f6\u6837\u672c\u5206\u5272\u4efb\u52a1\u4e0aR\u00b2\u5dee\u8ddd10%\u3002", "conclusion": "\u7a0b\u5e8f\u5316\u6570\u636e\u6a21\u578b\u80fd\u591f\u8fbe\u5230\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f46\u540c\u4e00\u7269\u4f53\u7684\u4e0d\u540c\u90e8\u5206\u5728\u7a0b\u5e8f\u5316\u6a21\u578b\u4e2d\u7684\u8868\u5f81\u5b58\u5728\u5dee\u5f02\uff0c\u5bfc\u81f4\u5185\u5b58\u641c\u7d22\u9519\u8bef\uff0c\u8fd9\u662f\u5269\u4f59\u6027\u80fd\u5dee\u8ddd\u7684\u539f\u56e0\u3002"}}
{"id": "2508.11689", "categories": ["cs.NE", "cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.11689", "abs": "https://arxiv.org/abs/2508.11689", "authors": ["Eduardo Calle-Ortiz", "Hui Guan", "Deepak Ganesan", "Phuc Nguyen"], "title": "Adaptive Spiking with Plasticity for Energy Aware Neuromorphic Systems", "comment": "14 pages", "summary": "This paper presents ASPEN, a novel energy-aware technique for neuromorphic\nsystems that could unleash the future of intelligent, always-on,\nultra-low-power, and low-burden wearables. Our main research objectives are to\nexplore the feasibility of neuromorphic computing for wearables, identify open\nresearch directions, and demonstrate the feasibility of developing an adaptive\nspiking technique for energy-aware computation, which can be game-changing for\nresource-constrained devices in always-on applications. As neuromorphic\ncomputing systems operate based on spike events, their energy consumption is\nclosely related to spiking activity, i.e., each spike incurs computational and\npower costs; consequently, minimizing the number of spikes is a critical\nstrategy for operating under constrained energy budgets. To support this goal,\nASPEN utilizes stochastic perturbations to the neuronal threshold during\ntraining to not only enhance the network's robustness across varying\nthresholds, which can be controlled at inference time, but also act as a\nregularizer that improves generalization, reduces spiking activity, and enables\nenergy control without the need for complex retraining or pruning. More\nspecifically, ASPEN adaptively adjusts intrinsic neuronal parameters as a\nlightweight and scalable technique for dynamic energy control without\nreconfiguring the entire model. Our evaluation on neuromorphic emulator and\nhardware shows that ASPEN significantly reduces spike counts and energy\nconsumption while maintaining accuracy comparable to state-of-the-art methods.", "AI": {"tldr": "ASPEN\u662f\u4e00\u79cd\u65b0\u578b\u7684\u795e\u7ecf\u5f62\u6001\u7cfb\u7edf\u80fd\u91cf\u611f\u77e5\u6280\u672f\uff0c\u901a\u8fc7\u968f\u673a\u6270\u52a8\u795e\u7ecf\u5143\u9608\u503c\u6765\u51cf\u5c11\u8109\u51b2\u6d3b\u52a8\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u80fd\u91cf\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5f00\u53d1\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u81ea\u9002\u5e94\u8109\u51b2\u6280\u672f\uff0c\u89e3\u51b3\u59cb\u7ec8\u5f00\u542f\u5e94\u7528\u4e2d\u80fd\u91cf\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u8bad\u7ec3\u671f\u95f4\u5bf9\u795e\u7ecf\u5143\u9608\u503c\u7684\u968f\u673a\u6270\u52a8\uff0c\u589e\u5f3a\u7f51\u7edc\u9c81\u68d2\u6027\uff0c\u4f5c\u4e3a\u6b63\u5219\u5316\u5668\u51cf\u5c11\u8109\u51b2\u6d3b\u52a8\uff0c\u65e0\u9700\u590d\u6742\u91cd\u65b0\u8bad\u7ec3\u6216\u526a\u679d\u5373\u53ef\u5b9e\u73b0\u80fd\u91cf\u63a7\u5236\u3002", "result": "\u5728\u795e\u7ecf\u5f62\u6001\u4eff\u771f\u5668\u548c\u786c\u4ef6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cASPEN\u663e\u8457\u51cf\u5c11\u4e86\u8109\u51b2\u8ba1\u6570\u548c\u80fd\u91cf\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "conclusion": "ASPEN\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u53ef\u6269\u5c55\u7684\u52a8\u6001\u80fd\u91cf\u63a7\u5236\u6280\u672f\uff0c\u65e0\u9700\u91cd\u65b0\u914d\u7f6e\u6574\u4e2a\u6a21\u578b\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u59cb\u7ec8\u5f00\u542f\u5e94\u7528\u5e26\u6765\u4e86\u7a81\u7834\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11860", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11860", "abs": "https://arxiv.org/abs/2508.11860", "authors": ["Frazier N. Baker", "Daniel Adu-Ampratwum", "Reza Averly", "Botao Yu", "Huan Sun", "Xia Ning"], "title": "LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework", "comment": "24 pages, 5 figures", "summary": "Large language model (LLM) agent evaluators leverage specialized tools to\nground the rational decision-making of LLMs, making them well-suited to aid in\nscientific discoveries, such as constrained retrosynthesis planning.\nConstrained retrosynthesis planning is an essential, yet challenging, process\nwithin chemistry for identifying synthetic routes from commercially available\nstarting materials to desired target molecules, subject to practical\nconstraints. Here, we present LARC, the first LLM-based Agentic framework for\nRetrosynthesis planning under Constraints. LARC incorporates agentic constraint\nevaluation, through an Agent-as-a-Judge, directly into the retrosynthesis\nplanning process, using agentic feedback grounded in tool-based reasoning to\nguide and constrain route generation. We rigorously evaluate LARC on a\ncarefully curated set of 48 constrained retrosynthesis planning tasks across 3\nconstraint types. LARC achieves a 72.9% success rate on these tasks, vastly\noutperforming LLM baselines and approaching human expert-level success in\nsubstantially less time. The LARC framework is extensible, and serves as a\nfirst step towards an effective agentic tool or a co-scientist to human experts\nfor constrained retrosynthesis.", "AI": {"tldr": "LARC\u662f\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u7ea6\u675f\u9006\u5408\u6210\u89c4\u5212\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5de5\u5177\u5316\u63a8\u7406\u7684\u4ee3\u7406\u53cd\u9988\u6765\u6307\u5bfc\u8def\u7ebf\u751f\u6210\uff0c\u572848\u4e2a\u7ea6\u675f\u4efb\u52a1\u4e0a\u8fbe\u523072.9%\u7684\u6210\u529f\u7387\uff0c\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\u4f46\u8017\u65f6\u66f4\u5c11\u3002", "motivation": "\u7ea6\u675f\u9006\u5408\u6210\u89c4\u5212\u662f\u5316\u5b66\u4e2d\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u8fc7\u7a0b\uff0c\u9700\u8981\u4ece\u5546\u4e1a\u53ef\u7528\u8d77\u59cb\u6750\u6599\u5230\u76ee\u6807\u5206\u5b50\u7684\u5408\u6210\u8def\u7ebf\u8bc6\u522b\uff0c\u540c\u65f6\u6ee1\u8db3\u5b9e\u9645\u7ea6\u675f\u6761\u4ef6\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u590d\u6742\u7ea6\u675f\u3002", "method": "LARC\u6846\u67b6\u91c7\u7528\u4ee3\u7406\u4f5c\u4e3a\u8bc4\u5224\u5458\uff08Agent-as-a-Judge\uff09\uff0c\u5c06\u4ee3\u7406\u7ea6\u675f\u8bc4\u4f30\u76f4\u63a5\u6574\u5408\u5230\u9006\u5408\u6210\u89c4\u5212\u8fc7\u7a0b\u4e2d\uff0c\u4f7f\u7528\u57fa\u4e8e\u5de5\u5177\u63a8\u7406\u7684\u4ee3\u7406\u53cd\u9988\u6765\u6307\u5bfc\u548c\u7ea6\u675f\u8def\u7ebf\u751f\u6210\u3002", "result": "\u5728\u7cbe\u5fc3\u7b56\u5212\u768448\u4e2a\u7ea6\u675f\u9006\u5408\u6210\u89c4\u5212\u4efb\u52a1\uff08\u6db5\u76d63\u79cd\u7ea6\u675f\u7c7b\u578b\uff09\u4e0a\uff0cLARC\u8fbe\u523072.9%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8eLLM\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u66f4\u77ed\u65f6\u95f4\u5185\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\u3002", "conclusion": "LARC\u6846\u67b6\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u662f\u671d\u7740\u4e3a\u4eba\u7c7b\u4e13\u5bb6\u5f00\u53d1\u6709\u6548\u4ee3\u7406\u5de5\u5177\u6216\u5408\u4f5c\u79d1\u5b66\u5bb6\u7684\u7b2c\u4e00\u6b65\uff0c\u53ef\u7528\u4e8e\u7ea6\u675f\u9006\u5408\u6210\u89c4\u5212\u3002"}}
{"id": "2508.11721", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11721", "abs": "https://arxiv.org/abs/2508.11721", "authors": ["Ke Zou", "Jocelyn Hui Lin Goh", "Yukun Zhou", "Tian Lin", "Samantha Min Er Yew", "Sahana Srinivasan", "Meng Wang", "Rui Santos", "Gabor M. Somfai", "Huazhu Fu", "Haoyu Chen", "Pearse A. Keane", "Ching-Yu Cheng", "Yih Chung Tham"], "title": "FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis", "comment": "12 pages, 3 figures", "summary": "Foundation models (FMs) have shown great promise in medical image analysis by\nimproving generalization across diverse downstream tasks. In ophthalmology,\nseveral FMs have recently emerged, but there is still no clear answer to\nfundamental questions: Which FM performs the best? Are they equally good across\ndifferent tasks? What if we combine all FMs together? To our knowledge, this is\nthe first study to systematically evaluate both single and fused ophthalmic\nFMs. To address these questions, we propose FusionFM, a comprehensive\nevaluation suite, along with two fusion approaches to integrate different\nophthalmic FMs. Our framework covers both ophthalmic disease detection\n(glaucoma, diabetic retinopathy, and age-related macular degeneration) and\nsystemic disease prediction (diabetes and hypertension) based on retinal\nimaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM,\nRetiZero, and DINORET) using standardized datasets from multiple countries and\nevaluated their performance using AUC and F1 metrics. Our results show that\nDINORET and RetiZero achieve superior performance in both ophthalmic and\nsystemic disease tasks, with RetiZero exhibiting stronger generalization on\nexternal datasets. Regarding fusion strategies, the Gating-based approach\nprovides modest improvements in predicting glaucoma, AMD, and hypertension.\nDespite these advances, predicting systemic diseases, especially hypertension\nin external cohort remains challenging. These findings provide an\nevidence-based evaluation of ophthalmic FMs, highlight the benefits of model\nfusion, and point to strategies for enhancing their clinical applicability.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e86\u56db\u79cd\u773c\u79d1\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u63d0\u51faFusionFM\u878d\u5408\u6846\u67b6\u3002DINORET\u548cRetiZero\u8868\u73b0\u6700\u4f73\uff0cRetiZero\u5177\u6709\u66f4\u5f3a\u7684\u5916\u90e8\u6570\u636e\u96c6\u6cd5\u5316\u80fd\u529b\u3002\u878d\u5408\u7b56\u7565\u5728\u67d0\u4e9b\u75be\u75c5\u9884\u6d4b\u4e2d\u53ef\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u7cfb\u7edf\u6027\u75be\u75c5\u9884\u6d4b\u4ecd\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u773c\u79d1\u9886\u57df\u51fa\u73b0\u4e86\u591a\u4e2a\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u6765\u56de\u7b54\u54ea\u4e2a\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3001\u662f\u5426\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u826f\u597d\u4ee5\u53ca\u878d\u5408\u6240\u6709\u6a21\u578b\u7684\u6548\u679c\u5982\u4f55\u7b49\u57fa\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51faFusionFM\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u79cd\u878d\u5408\u65b9\u6cd5\u6765\u6574\u5408\u4e0d\u540c\u773c\u79d1\u57fa\u7840\u6a21\u578b\u3002\u8bc4\u4f30\u8303\u56f4\u6db5\u76d6\u773c\u79d1\u75be\u75c5\u68c0\u6d4b\uff08\u9752\u5149\u773c\u3001\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u3001\u9e7f\u9e7f\u773c\uff09\u548c\u7cfb\u7edf\u6027\u75be\u75c5\u9884\u6d4b\uff08\u7cd6\u5c3f\u75c5\u548c\u9ad8\u8840\u538b\uff09\u3002\u5bf9\u56db\u79cd\u5148\u8fdb\u6a21\u578b\uff08RETFound\u3001VisionFM\u3001RetiZero\u3001DINORET\uff09\u8fdb\u884c\u6807\u51c6\u5316\u6d4b\u8bd5\uff0c\u4f7f\u7528AUC\u548cF1\u6307\u6807\u8bc4\u4f30\u6027\u80fd\u3002", "result": "DINORET\u548cRetiZero\u5728\u773c\u79d1\u548c\u7cfb\u7edf\u6027\u75be\u75c5\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u6700\u4f73\uff0c\u5176\u4e2dRetiZero\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u6cd5\u5316\u80fd\u529b\u3002\u95e8\u63a7\u878d\u5408\u7b56\u7565\u5728\u9884\u6d4b\u9752\u5149\u773c\u3001AMD\u548c\u9ad8\u8840\u538b\u65f6\u80fd\u63d0\u4f9b\u8f7b\u5fae\u7684\u6027\u80fd\u63d0\u5347\u3002\u7136\u800c\uff0c\u9884\u6d4b\u7cfb\u7edf\u6027\u75be\u75c5\uff08\u7279\u522b\u662f\u5916\u90e8\u7fa4\u4f53\u4e2d\u7684\u9ad8\u8840\u538b\uff09\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u773c\u79d1\u57fa\u7840\u6a21\u578b\u8bc4\u4f30\uff0c\u5f3a\u8c03\u4e86\u6a21\u578b\u878d\u5408\u7684\u4f18\u52bf\uff0c\u5e76\u6307\u51fa\u4e86\u63d0\u9ad8\u4e34\u5e8a\u5e94\u7528\u6027\u7684\u7b56\u7565\u65b9\u5411\u3002"}}
{"id": "2508.11703", "categories": ["cs.NE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11703", "abs": "https://arxiv.org/abs/2508.11703", "authors": ["Vasileios Saketos", "Sebastian Kaltenbach", "Sergey Litvinov", "Petros Koumoutsakos"], "title": "Data-Driven Discovery of Interpretable Kalman Filter Variants through Large Language Models and Genetic Programming", "comment": null, "summary": "Algorithmic discovery has traditionally relied on human ingenuity and\nextensive experimentation. Here we investigate whether a prominent scientific\ncomputing algorithm, the Kalman Filter, can be discovered through an automated,\ndata-driven, evolutionary process that relies on Cartesian Genetic Programming\n(CGP) and Large Language Models (LLM). We evaluate the contributions of both\nmodalities (CGP and LLM) in discovering the Kalman filter under varying\nconditions. Our results demonstrate that our framework of CGP and LLM-assisted\nevolution converges to near-optimal solutions when Kalman optimality\nassumptions hold. When these assumptions are violated, our framework evolves\ninterpretable alternatives that outperform the Kalman filter. These results\ndemonstrate that combining evolutionary algorithms and generative models for\ninterpretable, data-driven synthesis of simple computational modules is a\npotent approach for algorithmic discovery in scientific computing.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.11894", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11894", "abs": "https://arxiv.org/abs/2508.11894", "authors": ["Ao Li", "Bin Yan", "Bingfeng Cai", "Chenxi Li", "Cunzhong Zhao", "Fugen Yao", "Gaoqiang Liu", "Guanjun Jiang", "Jian Xu", "Liang Dong", "Liansheng Sun", "Rongshen Zhang", "Xiaolei Gui", "Xin Liu", "Xin Shang", "Yao Wu", "Yu Cao", "Zhenxin Ma", "Zhuang Jia"], "title": "QuarkMed Medical Foundation Model Technical Report", "comment": "20 pages", "summary": "Recent advancements in large language models have significantly accelerated\ntheir adoption in healthcare applications, including AI-powered medical\nconsultations, diagnostic report assistance, and medical search tools. However,\nmedical tasks often demand highly specialized knowledge, professional accuracy,\nand customization capabilities, necessitating a robust and reliable foundation\nmodel. QuarkMed addresses these needs by leveraging curated medical data\nprocessing, medical-content Retrieval-Augmented Generation (RAG), and a\nlarge-scale, verifiable reinforcement learning pipeline to develop a\nhigh-performance medical foundation model. The model achieved 70% accuracy on\nthe Chinese Medical Licensing Examination, demonstrating strong generalization\nacross diverse medical benchmarks. QuarkMed offers a powerful yet versatile\npersonal medical AI solution, already serving over millions of users at\nai.quark.cn.", "AI": {"tldr": "QuarkMed\u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u533b\u7597\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u533b\u5b66\u6570\u636e\u5904\u7406\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5927\u89c4\u6a21\u53ef\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u4e2d\u56fd\u6267\u4e1a\u533b\u5e08\u8003\u8bd5\u4e2d\u8fbe\u523070%\u51c6\u786e\u7387\uff0c\u5df2\u670d\u52a1\u6570\u767e\u4e07\u7528\u6237\u3002", "motivation": "\u533b\u7597\u4efb\u52a1\u9700\u8981\u9ad8\u5ea6\u4e13\u4e1a\u7684\u77e5\u8bc6\u3001\u4e13\u4e1a\u51c6\u786e\u6027\u548c\u5b9a\u5236\u80fd\u529b\uff0c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u5e94\u7528\u4e2d\u9700\u8981\u66f4\u53ef\u9760\u7684\u57fa\u7840\u6a21\u578b\u652f\u6301\u3002", "method": "\u5229\u7528\u7cbe\u9009\u533b\u5b66\u6570\u636e\u5904\u7406\u3001\u533b\u5b66\u5185\u5bb9\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u548c\u5927\u89c4\u6a21\u53ef\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u7ba1\u9053\u6765\u5f00\u53d1\u533b\u7597\u57fa\u7840\u6a21\u578b\u3002", "result": "\u5728\u4e2d\u56fd\u533b\u5b66\u6267\u7167\u8003\u8bd5\u4e2d\u8fbe\u523070%\u7684\u51c6\u786e\u7387\uff0c\u5728\u591a\u6837\u5316\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "QuarkMed\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u800c\u591a\u529f\u80fd\u7684\u4e2a\u4eba\u533b\u7597AI\u89e3\u51b3\u65b9\u6848\uff0c\u5df2\u7ecf\u5728ai.quark.cn\u670d\u52a1\u8d85\u8fc7\u767e\u4e07\u7528\u6237\u3002"}}
{"id": "2508.11728", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11728", "abs": "https://arxiv.org/abs/2508.11728", "authors": ["Chunxia Ren", "Ning Zhu", "Yue Lai", "Gui Chen", "Ruijie Wang", "Yangyi Hu", "Suyao Liu", "Shuwen Mao", "Hong Su", "Yu Zhang", "Li Xiao"], "title": "UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction", "comment": "23 pages, 6 figures", "summary": "Dentocraniofacial hard tissue defects profoundly affect patients'\nphysiological functions, facial aesthetics, and psychological well-being,\nposing significant challenges for precise reconstruction. Current deep learning\nmodels are limited to single-tissue scenarios and modality-specific imaging\ninputs, resulting in poor generalizability and trade-offs between anatomical\nfidelity, computational efficiency, and cross-tissue adaptability. Here we\nintroduce UniDCF, a unified framework capable of reconstructing multiple\ndentocraniofacial hard tissues through multimodal fusion encoding of point\nclouds and multi-view images. By leveraging the complementary strengths of each\nmodality and incorporating a score-based denoising module to refine surface\nsmoothness, UniDCF overcomes the limitations of prior single-modality\napproaches. We curated the largest multimodal dataset, comprising intraoral\nscans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated\ninstances. Evaluations demonstrate that UniDCF outperforms existing\nstate-of-the-art methods in terms of geometric precision, structural\ncompleteness, and spatial accuracy. Clinical simulations indicate UniDCF\nreduces reconstruction design time by 99% and achieves clinician-rated\nacceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and\nhigh-fidelity reconstruction, supporting personalized and precise restorative\ntreatments, streamlining clinical workflows, and enhancing patient outcomes.", "AI": {"tldr": "UniDCF\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u70b9\u4e91\u548c\u591a\u89c6\u89d2\u56fe\u50cf\u7684\u878d\u5408\u7f16\u7801\uff0c\u91cd\u5efa\u591a\u79cd\u7259\u988c\u9762\u786c\u7ec4\u7ec7\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7259\u988c\u9762\u786c\u7ec4\u7ec7\u7f3a\u635f\u4e25\u91cd\u5f71\u54cd\u60a3\u8005\u751f\u7406\u529f\u80fd\u3001\u9762\u90e8\u7f8e\u89c2\u548c\u5fc3\u7406\u5065\u5eb7\uff0c\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ec5\u9650\u4e8e\u5355\u7ec4\u7ec7\u548c\u7279\u5b9a\u6a21\u6001\u8f93\u5165\uff0c\u5bfc\u81f4\u6cdb\u5316\u6027\u5dee\uff0c\u9700\u8981\u5728\u89e3\u5256\u4fdd\u771f\u5ea6\u3001\u8ba1\u7b97\u6548\u7387\u548c\u8de8\u7ec4\u7ec7\u9002\u5e94\u6027\u4e4b\u95f4\u6743\u8861\u3002", "method": "\u63d0\u51faUniDCF\u6846\u67b6\uff0c\u901a\u8fc7\u70b9\u4e91\u548c\u591a\u89c6\u89d2\u56fe\u50cf\u7684\u591a\u6a21\u6001\u878d\u5408\u7f16\u7801\uff0c\u5229\u7528\u5404\u6a21\u6001\u4e92\u8865\u4f18\u52bf\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5206\u6570\u7684\u53bb\u566a\u6a21\u5757\u6765\u4f18\u5316\u8868\u9762\u5e73\u6ed1\u5ea6\u3002\u6784\u5efa\u4e86\u6700\u5927\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b6,609\u540d\u60a3\u8005\u7684\u53e3\u5185\u626b\u63cf\u3001CBCT\u548cCT\u6570\u636e\uff0c\u517154,555\u4e2a\u6807\u6ce8\u5b9e\u4f8b\u3002", "result": "\u8bc4\u4f30\u663e\u793aUniDCF\u5728\u51e0\u4f55\u7cbe\u5ea6\u3001\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u7a7a\u95f4\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u4e34\u5e8a\u6a21\u62df\u8868\u660e\uff0cUniDCF\u5c06\u91cd\u5efa\u8bbe\u8ba1\u65f6\u95f4\u51cf\u5c1199%\uff0c\u4e34\u5e8a\u533b\u751f\u63a5\u53d7\u5ea6\u8d85\u8fc794%\u3002", "conclusion": "UniDCF\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u81ea\u52a8\u5316\u548c\u9ad8\u4fdd\u771f\u5ea6\u7684\u91cd\u5efa\uff0c\u652f\u6301\u4e2a\u6027\u5316\u548c\u7cbe\u786e\u7684\u4fee\u590d\u6cbb\u7597\uff0c\u7b80\u5316\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6539\u5584\u60a3\u8005\u6cbb\u7597\u6548\u679c\u3002"}}
{"id": "2508.11871", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2508.11871", "abs": "https://arxiv.org/abs/2508.11871", "authors": ["Zhen-Song Chen", "Hong-Wei Ding", "Xian-Jia Wang", "Witold Pedrycz"], "title": "LLM4CMO: Large Language Model-aided Algorithm Design for Constrained Multiobjective Optimization", "comment": null, "summary": "Constrained multi-objective optimization problems (CMOPs) frequently arise in\nreal-world applications where multiple conflicting objectives must be optimized\nunder complex constraints. Existing dual-population two-stage algorithms have\nshown promise by leveraging infeasible solutions to improve solution quality.\nHowever, designing high-performing constrained multi-objective evolutionary\nalgorithms (CMOEAs) remains a challenging task due to the intricacy of\nalgorithmic components. Meanwhile, large language models (LLMs) offer new\nopportunities for assisting with algorithm design; however, their effective\nintegration into such tasks remains underexplored. To address this gap, we\npropose LLM4CMO, a novel CMOEA based on a dual-population, two-stage framework.\nIn Stage 1, the algorithm identifies both the constrained Pareto front (CPF)\nand the unconstrained Pareto front (UPF). In Stage 2, it performs targeted\noptimization using a combination of hybrid operators (HOps), an epsilon-based\nconstraint-handling method, and a classification-based UPF-CPF relationship\nstrategy, along with a dynamic resource allocation (DRA) mechanism. To reduce\ndesign complexity, the core modules, including HOps, epsilon decay function,\nand DRA, are decoupled and designed through prompt template engineering and\nLLM-human interaction. Experimental results on six benchmark test suites and\nten real-world CMOPs demonstrate that LLM4CMO outperforms eleven\nstate-of-the-art baseline algorithms. Ablation studies further validate the\neffectiveness of the LLM-aided modular design. These findings offer preliminary\nevidence that LLMs can serve as efficient co-designers in the development of\ncomplex evolutionary optimization algorithms. The code associated with this\narticle is available at https://anonymous.4open.science/r/LLM4CMO971.", "AI": {"tldr": "LLM4CMO\u662f\u4e00\u4e2a\u57fa\u4e8e\u53cc\u79cd\u7fa4\u4e24\u9636\u6bb5\u6846\u67b6\u7684\u7ea6\u675f\u591a\u76ee\u6807\u8fdb\u5316\u7b97\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u8bbe\u8ba1\u6838\u5fc3\u6a21\u5757\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u95ee\u9898\u4e0a\u4f18\u4e8e11\u79cd\u5148\u8fdb\u57fa\u7ebf\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u53cc\u79cd\u7fa4\u4e24\u9636\u6bb5\u7b97\u6cd5\u5728\u5229\u7528\u4e0d\u53ef\u884c\u89e3\u63d0\u5347\u89e3\u8d28\u91cf\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u7b97\u6cd5\u7ec4\u4ef6\u8bbe\u8ba1\u590d\u6742\uff1b\u540c\u65f6\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b97\u6cd5\u8bbe\u8ba1\u8f85\u52a9\u65b9\u9762\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u53cc\u79cd\u7fa4\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bc6\u522b\u7ea6\u675f\u548c\u975e\u7ea6\u675f\u5e15\u7d2f\u6258\u524d\u6cbf\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u6df7\u5408\u7b97\u5b50\u3001epsilon\u7ea6\u675f\u5904\u7406\u3001\u5206\u7c7b\u7b56\u7565\u548c\u52a8\u6001\u8d44\u6e90\u5206\u914d\u673a\u5236\u8fdb\u884c\u9488\u5bf9\u6027\u4f18\u5316\uff0c\u6838\u5fc3\u6a21\u5757\u901a\u8fc7\u63d0\u793a\u6a21\u677f\u5de5\u7a0b\u548cLLM-\u4eba\u7c7b\u4ea4\u4e92\u8bbe\u8ba1\u3002", "result": "\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u548c10\u4e2a\u5b9e\u9645\u7ea6\u675f\u591a\u76ee\u6807\u95ee\u9898\u4e0a\uff0cLLM4CMO\u4f18\u4e8e11\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u7b97\u6cd5\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86LLM\u8f85\u52a9\u6a21\u5757\u5316\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u590d\u6742\u8fdb\u5316\u4f18\u5316\u7b97\u6cd5\u5f00\u53d1\u7684\u9ad8\u6548\u534f\u540c\u8bbe\u8ba1\u8005\uff0c\u4e3a\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.11944", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11944", "abs": "https://arxiv.org/abs/2508.11944", "authors": ["Hongtao Liu", "Zhicheng Du", "Zihe Wang", "Weiran Shen"], "title": "CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs", "comment": null, "summary": "Game-playing ability serves as an indicator for evaluating the strategic\nreasoning capability of large language models (LLMs). While most existing\nstudies rely on utility performance metrics, which are not robust enough due to\nvariations in opponent behavior and game structure. To address this limitation,\nwe propose \\textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation\nframework inspired by the cognitive hierarchy models from behavioral economics.\nWe hypothesize that agents have bounded rationality -- different agents behave\nat varying reasoning depths/levels. We evaluate LLMs' strategic reasoning\nthrough a three-phase systematic framework, utilizing behavioral data from six\nstate-of-the-art LLMs across fifteen carefully selected normal-form games.\nExperiments show that LLMs exhibit consistent strategic reasoning levels across\ndiverse opponents, confirming the framework's robustness and generalization\ncapability. We also analyze the effects of two key mechanisms (Chat Mechanism\nand Memory Mechanism) on strategic reasoning performance. Results indicate that\nthe Chat Mechanism significantly degrades strategic reasoning, whereas the\nMemory Mechanism enhances it. These insights position CHBench as a promising\ntool for evaluating LLM capabilities, with significant potential for future\nresearch and practical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86CHBench\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u4e8e\u8ba4\u77e5\u5c42\u6b21\u6a21\u578b\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6218\u7565\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u804a\u5929\u673a\u5236\u4f1a\u964d\u4f4e\u6218\u7565\u63a8\u7406\uff0c\u800c\u8bb0\u5fc6\u673a\u5236\u80fd\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u6548\u7528\u6027\u80fd\u6307\u6807\u6765\u8bc4\u4f30LLMs\u7684\u6e38\u620f\u80fd\u529b\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u4e0d\u591f\u7a33\u5065\uff0c\u53d7\u5bf9\u624b\u884c\u4e3a\u548c\u6e38\u620f\u7ed3\u6784\u53d8\u5316\u5f71\u54cd\u8f83\u5927\u3002\u9700\u8981\u66f4\u7a33\u5065\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u8861\u91cfLLMs\u7684\u6218\u7565\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u8ba4\u77e5\u5c42\u6b21\u57fa\u51c6(CHBench)\uff0c\u57fa\u4e8e\u884c\u4e3a\u7ecf\u6d4e\u5b66\u4e2d\u7684\u8ba4\u77e5\u5c42\u6b21\u6a21\u578b\uff0c\u5047\u8bbe\u667a\u80fd\u4f53\u5177\u6709\u6709\u9650\u7406\u6027\u3002\u901a\u8fc7\u4e09\u9636\u6bb5\u7cfb\u7edf\u6846\u67b6\uff0c\u572815\u4e2a\u7cbe\u9009\u7684\u6b63\u89c4\u5f62\u5f0f\u6e38\u620f\u4e2d\u4f7f\u75286\u4e2a\u6700\u5148\u8fdbLLMs\u7684\u884c\u4e3a\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLMs\u5728\u4e0d\u540c\u5bf9\u624b\u95f4\u5c55\u73b0\u51fa\u4e00\u81f4\u7684\u6218\u7565\u63a8\u7406\u6c34\u5e73\uff0c\u8bc1\u5b9e\u4e86\u6846\u67b6\u7684\u7a33\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u804a\u5929\u673a\u5236\u663e\u8457\u964d\u4f4e\u6218\u7565\u63a8\u7406\u6027\u80fd\uff0c\u800c\u8bb0\u5fc6\u673a\u5236\u80fd\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "CHBench\u662f\u8bc4\u4f30LLM\u80fd\u529b\u7684\u6709\u524d\u666f\u5de5\u5177\uff0c\u5177\u6709\u91cd\u8981\u7684\u672a\u6765\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u6218\u7565\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u6846\u67b6\u3002"}}
{"id": "2508.11737", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11737", "abs": "https://arxiv.org/abs/2508.11737", "authors": ["Shiyin Lu", "Yang Li", "Yu Xia", "Yuwei Hu", "Shanshan Zhao", "Yanqing Ma", "Zhichao Wei", "Yinglun Li", "Lunhao Duan", "Jianshan Zhao", "Yuxuan Han", "Haijun Li", "Wanying Chen", "Junke Tang", "Chengkun Hou", "Zhixing Du", "Tianli Zhou", "Wenjie Zhang", "Huping Ding", "Jiahe Li", "Wen Li", "Gui Hu", "Yiliang Gu", "Siran Yang", "Jiamang Wang", "Hailong Sun", "Yibo Wang", "Hui Sun", "Jinlong Huang", "Yuping He", "Shengze Shi", "Weihong Zhang", "Guodong Zheng", "Junpeng Jiang", "Sensen Gao", "Yi-Feng Wu", "Sijia Chen", "Yuhui Chen", "Qing-Guo Chen", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "title": "Ovis2.5 Technical Report", "comment": null, "summary": "We present Ovis2.5, a successor to Ovis2 designed for native-resolution\nvisual perception and strong multimodal reasoning. Ovis2.5 integrates a\nnative-resolution vision transformer that processes images at their native,\nvariable resolutions, avoiding the degradation from fixed-resolution tiling and\npreserving both fine detail and global layout -- crucial for visually dense\ncontent like complex charts. To strengthen reasoning, we train the model to\nmove beyond linear chain-of-thought and perform reflection -- including\nself-checking and revision. This advanced capability is exposed as an optional\n\"thinking mode\" at inference time, allowing users to trade latency for enhanced\naccuracy on difficult inputs. The model is trained via a comprehensive\nfive-phase curriculum that progressively builds its skills. The process begins\nwith foundational visual and multimodal pretraining, advances through\nlarge-scale instruction tuning, and culminates in alignment and reasoning\nenhancement using DPO and GRPO. To scale these upgrades efficiently, we employ\nmultimodal data packing and hybrid parallelism, yielding a significant\nend-to-end speedup. We release two open-source models: Ovis2.5-9B and\nOvis2.5-2B. The latter continues the \"small model, big performance\" philosophy\nof Ovis2, making it ideal for resource-constrained, on-device scenarios. On the\nOpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a\nsubstantial improvement over its predecessor, Ovis2-8B, and achieving\nstate-of-the-art results among open-source MLLMs in the sub-40B parameter\nrange; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate\nscores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong\ncapabilities on grounding and video tasks, and achieves open-source SOTA at its\nscale for complex chart analysis.", "AI": {"tldr": "Ovis2.5\u662f\u4e00\u4e2a\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u539f\u751f\u5206\u8fa8\u7387\u89c6\u89c9\u5904\u7406\u548c\u53cd\u5c04\u63a8\u7406\u80fd\u529b\uff0c\u5728OpenCompass\u591a\u6a21\u6001\u6392\u884c\u699c\u4e0a\u53d6\u5f97\u4e8678.3\u5206\u7684\u4f18\u5f02\u6210\u7ee9\uff0c\u572840B\u53c2\u6570\u4ee5\u4e0b\u7684\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230SOTA\u6c34\u5e73\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u56fa\u5b9a\u5206\u8fa8\u7387\u56fe\u50cf\u5904\u7406\u5bfc\u81f4\u7684\u7ec6\u8282\u4e22\u5931\u95ee\u9898\uff0c\u4ee5\u53ca\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u89c6\u89c9\u5185\u5bb9\uff08\u5982\u590d\u6742\u56fe\u8868\uff09\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u539f\u751f\u5206\u8fa8\u7387\u56fe\u50cf\u5e76\u5177\u5907\u81ea\u6211\u68c0\u67e5\u548c\u4fee\u6b63\u80fd\u529b\u7684\u591a\u6a21\u6001\u6a21\u578b\u3002", "method": "\u91c7\u7528\u539f\u751f\u5206\u8fa8\u7387\u89c6\u89c9Transformer\u5904\u7406\u53ef\u53d8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u5f15\u5165\u53cd\u5c04\u63a8\u7406\uff08\u5305\u62ec\u81ea\u6211\u68c0\u67e5\u548c\u4fee\u8ba2\uff09\u4f5c\u4e3a\u53ef\u9009\u7684\"\u601d\u8003\u6a21\u5f0f\"\uff0c\u901a\u8fc7\u4e94\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff08\u5305\u62ec\u89c6\u89c9\u9884\u8bad\u7ec3\u3001\u5927\u89c4\u6a21\u6307\u4ee4\u8c03\u4f18\u3001DPO\u548cGRPO\u5bf9\u9f50\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u6570\u636e\u6253\u5305\u548c\u6df7\u5408\u5e76\u884c\u6280\u672f\u63d0\u5347\u6548\u7387\u3002", "result": "Ovis2.5-9B\u5728OpenCompass\u6392\u884c\u699c\u5e73\u5747\u5f97\u520678.3\uff0c\u76f8\u6bd4\u524d\u4ee3Ovis2-8B\u6709\u663e\u8457\u63d0\u5347\uff1bOvis2.5-2B\u5f97\u520673.9\uff0c\u5728\u540c\u7b49\u89c4\u6a21\u6a21\u578b\u4e2d\u8fbe\u5230SOTA\u3002\u5728STEM\u57fa\u51c6\u6d4b\u8bd5\u3001\u63a5\u5730\u4efb\u52a1\u3001\u89c6\u9891\u4efb\u52a1\u548c\u590d\u6742\u56fe\u8868\u5206\u6790\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u9886\u5148\u7ed3\u679c\u3002", "conclusion": "Ovis2.5\u901a\u8fc7\u539f\u751f\u5206\u8fa8\u7387\u89c6\u89c9\u5904\u7406\u548c\u53cd\u5c04\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u89c6\u89c9\u5185\u5bb9\u5206\u6790\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u7684\u5c0f\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12133", "categories": ["cs.NE", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2508.12133", "abs": "https://arxiv.org/abs/2508.12133", "authors": ["Saem Hasan", "Muhammad Ali Nayeem", "M. Sohel Rahman"], "title": "Improving MSA Estimation through Adaptive Weight Vectors in MOEA/D", "comment": null, "summary": "Accurate phylogenetic inference from biological sequences depends critically\non the quality of multiple sequence alignments, yet optimal alignment for many\nsequences is computationally intractable and sensitive to scoring choices. In\nthis work we introduce MOEA/D-ADF, a novel variant of MOEA/D that adaptively\nadjusts subproblem weight vectors based on fitness variance to improve the\nexploration-exploitation trade-off. We combine MOEA/D-ADF with PMAO (PASTA with\nmany application-aware optimization criteria) to form PMAO++, where\nPMAO-generated solutions are used to seed MOEA/D-ADF, which then evolves a\npopulation using 30 weight vectors to produce a diverse ensemble of\nalignment-tree pairs. PMAO++ outperforms the original PMAO on a majority of\nbenchmark cases, achieving better false-negative (FN) rates on 12 of 17\nBAliBASE-derived datasets and producing superior best-case trees, including\nseveral instances with zero FN rate. Beyond improving single best alignments,\nthe rich set of alignment-tree pairs produced by PMAO++ is especially valuable\nfor downstream summary methods (for example, consensus and summary-tree\napproaches), allowing more robust phylogenetic inference by integrating signal\nacross multiple plausible alignments and trees. Certain dataset features, such\nas large terminal N/C extensions found in the RV40 group, remain challenging,\nbut overall PMAO++ demonstrates clear advantages for sequence-based\nphylogenetic analysis. Future work will explore parameter tuning, larger\nbenchmark suites, and tighter integration with summary-tree pipelines to\nfurther enhance applicability for biological sequence studies.", "AI": {"tldr": "\u63d0\u51fa\u4e86MOEA/D-ADF\u7b97\u6cd5\u6539\u8fdb\u63a2\u7d22-\u5229\u7528\u5e73\u8861\uff0c\u7ed3\u5408PMAO\u5f62\u6210PMAO++\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u6bd4\u5bf9-\u6811\u5bf9\u96c6\u5408\u6765\u63d0\u5347\u7cfb\u7edf\u53d1\u80b2\u63a8\u65ad\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u751f\u7269\u5e8f\u5217\u7684\u7cfb\u7edf\u53d1\u80b2\u63a8\u65ad\u4e25\u91cd\u4f9d\u8d56\u4e8e\u591a\u91cd\u5e8f\u5217\u6bd4\u5bf9\u7684\u8d28\u91cf\uff0c\u4f46\u6700\u4f18\u6bd4\u5bf9\u5bf9\u4e8e\u8bb8\u591a\u5e8f\u5217\u6765\u8bf4\u662f\u8ba1\u7b97\u4e0d\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u5bf9\u8bc4\u5206\u9009\u62e9\u654f\u611f\u3002", "method": "\u5f00\u53d1\u4e86MOEA/D-ADF\u7b97\u6cd5\uff08\u57fa\u4e8e\u9002\u5e94\u5ea6\u65b9\u5dee\u81ea\u9002\u5e94\u8c03\u6574\u6743\u91cd\u5411\u91cf\u7684MOEA/D\u53d8\u4f53\uff09\uff0c\u4e0ePMAO\u7ed3\u5408\u5f62\u6210PMAO++\uff0c\u4f7f\u752830\u4e2a\u6743\u91cd\u5411\u91cf\u8fdb\u5316\u79cd\u7fa4\u751f\u6210\u591a\u6837\u5316\u7684\u6bd4\u5bf9-\u6811\u5bf9\u96c6\u5408\u3002", "result": "PMAO++\u5728\u5927\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u539f\u59cbPMAO\uff0c\u572817\u4e2aBAliBASE\u6570\u636e\u96c6\u4e2d12\u4e2a\u83b7\u5f97\u66f4\u597d\u7684\u5047\u9634\u6027\u7387\uff0c\u4ea7\u751f\u591a\u4e2a\u96f6\u5047\u9634\u6027\u7387\u7684\u6700\u4f73\u6811\u3002", "conclusion": "PMAO++\u4ea7\u751f\u7684\u4e30\u5bcc\u6bd4\u5bf9-\u6811\u5bf9\u96c6\u5408\u5bf9\u4e8e\u4e0b\u6e38\u6c47\u603b\u65b9\u6cd5\u7279\u522b\u6709\u4ef7\u503c\uff0c\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u53ef\u4fe1\u6bd4\u5bf9\u548c\u6811\u7684\u4fe1\u53f7\u6765\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u7cfb\u7edf\u53d1\u80b2\u63a8\u65ad\u3002"}}
{"id": "2508.11953", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11953", "abs": "https://arxiv.org/abs/2508.11953", "authors": ["Yuan Li", "Zhengzhong Liu", "Eric Xing"], "title": "Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models", "comment": null, "summary": "Optimizing data mixtures for supervised fine-tuning (SFT) of large language\nmodels (LLMs) is critical for developing general-purpose models, yet this area\nremains underexplored. In this paper, we frame data mixing as an optimization\nproblem and introduce a novel method designed to minimize validation loss. Our\napproach parametrizes the loss by modeling effective data transferred and\nleveraging scaling laws for fine-tuning. By experimenting with various\nsmall-scale data mixtures, we fit these parameters and derive the optimal\nweights. We provide both mathematical proofs and empirical results\ndemonstrating that our algorithm achieves excellent overall and individual\nperformance across all domains. Through controlled experiments, we show that\nmodels trained with our optimized weights perform on par with those using\noptimal weights determined via grid search, with per-domain loss only 0.66%\nhigher than the best domain loss from grid search on average. Additionally, we\nshow that reweighting popular SFT datasets using our method improves both\nvalidation loss and downstream performance. Finally, we discuss how our method\ncan generalize to guide data selection for domain-specific models and provide\ninsights into SFT.", "AI": {"tldr": "\u901a\u8fc7\u5efa\u6a21\u6709\u6548\u6570\u636e\u8f6c\u79fb\u548c\u5229\u7528\u7f29\u653e\u5b9a\u5f8b\u6765\u4f18\u5316SFT\u6570\u636e\u6df7\u5408\u914d\u7f6e\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u8fbe\u5230\u4e0e\u7f51\u683c\u641c\u7d22\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6539\u5584\u9a8c\u8bc1\u635f\u5931\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u76ee\u524d\u5728\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u76d1\u7763\u7cbe\u8c03\u65f6\uff0c\u5982\u4f55\u4f18\u5316\u6570\u636e\u6df7\u5408\u914d\u7f6e\u4ecd\u7136\u662f\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u91cd\u8981\u9886\u57df\uff0c\u9700\u8981\u66f4\u79d1\u5b66\u7684\u65b9\u6cd5\u6765\u6700\u5c0f\u5316\u9a8c\u8bc1\u635f\u5931\u3002", "method": "\u5c06\u6570\u636e\u6df7\u5408\u95ee\u9898\u6846\u67b6\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5efa\u6a21\u6709\u6548\u6570\u636e\u8f6c\u79fb\u548c\u5229\u7528\u7cbe\u8c03\u7f29\u653e\u5b9a\u5f8b\u6765\u53c2\u6570\u5316\u635f\u5931\u51fd\u6570\uff0c\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u6df7\u5408\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u62df\u5408\u53c2\u6570\u5e76\u6c42\u89e3\u6700\u4f18\u6743\u91cd\u3002", "result": "\u7b97\u6cd5\u5728\u6240\u6709\u9886\u57df\u90fd\u53d6\u5f97\u4e86\u4f18\u79c0\u7684\u603b\u4f53\u548c\u4e2a\u522b\u6027\u80fd\uff0c\u4e0e\u7f51\u683c\u641c\u7d22\u786e\u5b9a\u7684\u6700\u4f18\u6743\u91cd\u8868\u73b0\u76f8\u5f53\uff0c\u6bcf\u4e2a\u9886\u57df\u635f\u5931\u4ec5\u6bd4\u7f51\u683c\u641c\u7d22\u7684\u6700\u4f73\u57df\u635f\u5931\u5e73\u5747\u9ad8\u51fa0.66%\uff0c\u91cd\u65b0\u52a0\u6743\u5e38\u7528SFT\u6570\u636e\u96c6\u540e\u80fd\u591f\u6539\u5584\u9a8c\u8bc1\u635f\u5931\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u6709\u6548\u4f18\u5316SFT\u6570\u636e\u6df7\u5408\uff0c\u8fd8\u53ef\u4ee5\u63a8\u5e7f\u5230\u6307\u5bfc\u9886\u57df\u7279\u5b9a\u6a21\u578b\u7684\u6570\u636e\u9009\u62e9\uff0c\u4e3aSFT\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u6df1\u5ea6\u89c1\u89e3\u3002"}}
{"id": "2508.11801", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11801", "abs": "https://arxiv.org/abs/2508.11801", "authors": ["Ming Cheng", "Tong Wu", "Jiazhen Hu", "Jiaying Gong", "Hoda Eldardiry"], "title": "VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models", "comment": "5 pages, 2 figures, 5 tables, accepted in CIKM 2025", "summary": "Attribute Value Extraction (AVE) is important for structuring product\ninformation in e-commerce. However, existing AVE datasets are primarily limited\nto text-to-text or image-to-text settings, lacking support for product videos,\ndiverse attribute coverage, and public availability. To address these gaps, we\nintroduce VideoAVE, the first publicly available video-to-text e-commerce AVE\ndataset across 14 different domains and covering 172 unique attributes. To\nensure data quality, we propose a post-hoc CLIP-based Mixture of Experts\nfiltering system (CLIP-MoE) to remove the mismatched video-product pairs,\nresulting in a refined dataset of 224k training data and 25k evaluation data.\nIn order to evaluate the usability of the dataset, we further establish a\ncomprehensive benchmark by evaluating several state-of-the-art video vision\nlanguage models (VLMs) under both attribute-conditioned value prediction and\nopen attribute-value pair extraction tasks. Our results analysis reveals that\nvideo-to-text AVE remains a challenging problem, particularly in open settings,\nand there is still room for developing more advanced VLMs capable of leveraging\neffective temporal information. The dataset and benchmark code for VideoAVE are\navailable at: https://github.com/gjiaying/VideoAVE", "AI": {"tldr": "VideoAVE\u662f\u9996\u4e2a\u516c\u5f00\u7684\u89c6\u9891\u5230\u6587\u672c\u7535\u5546\u5c5e\u6027\u503c\u63d0\u53d6\u6570\u636e\u96c6\uff0c\u6db5\u76d614\u4e2a\u9886\u57df\u548c172\u4e2a\u5c5e\u6027\uff0c\u5305\u542b224k\u8bad\u7ec3\u6570\u636e\u548c25k\u8bc4\u4f30\u6570\u636e\uff0c\u5e76\u5efa\u7acb\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709AVE\u6570\u636e\u96c6\u7f3a\u4e4f\u89c6\u9891\u652f\u6301\u3001\u5c5e\u6027\u8986\u76d6\u4e0d\u8db3\u548c\u516c\u5f00\u53ef\u7528\u6027\u7684\u95ee\u9898\uff0c\u586b\u8865\u89c6\u9891\u7535\u5546\u5c5e\u6027\u63d0\u53d6\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eCLIP\u7684\u6df7\u5408\u4e13\u5bb6\u8fc7\u6ee4\u7cfb\u7edf(CLIP-MoE)\u6765\u79fb\u9664\u4e0d\u5339\u914d\u7684\u89c6\u9891-\u4ea7\u54c1\u5bf9\uff0c\u786e\u4fdd\u6570\u636e\u8d28\u91cf\uff1b\u5efa\u7acb\u5305\u542b\u5c5e\u6027\u6761\u4ef6\u503c\u9884\u6d4b\u548c\u5f00\u653e\u5c5e\u6027\u503c\u5bf9\u63d0\u53d6\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u89c6\u9891\u5230\u6587\u672cAVE\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5f00\u653e\u8bbe\u7f6e\u4e2d\uff0c\u73b0\u6709\u89c6\u9891\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5229\u7528\u6709\u6548\u65f6\u5e8f\u4fe1\u606f\u65b9\u9762\u8fd8\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "VideoAVE\u6570\u636e\u96c6\u586b\u8865\u4e86\u89c6\u9891\u7535\u5546\u5c5e\u6027\u63d0\u53d6\u7684\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u5c55\u793a\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u6f5c\u529b\u3002"}}
{"id": "2508.12609", "categories": ["cs.NE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12609", "abs": "https://arxiv.org/abs/2508.12609", "authors": ["Qingyan Meng", "Mingqing Xiao", "Zhengyu Ma", "Huihui Zhou", "Yonghong Tian", "Zhouchen Lin"], "title": "A Self-Ensemble Inspired Approach for Effective Training of Binary-Weight Spiking Neural Networks", "comment": null, "summary": "Spiking Neural Networks (SNNs) are a promising approach to low-power\napplications on neuromorphic hardware due to their energy efficiency. However,\ntraining SNNs is challenging because of the non-differentiable spike generation\nfunction. To address this issue, the commonly used approach is to adopt the\nbackpropagation through time framework, while assigning the gradient of the\nnon-differentiable function with some surrogates. Similarly, Binary Neural\nNetworks (BNNs) also face the non-differentiability problem and rely on\napproximating gradients. However, the deep relationship between these two\nfields and how their training techniques can benefit each other has not been\nsystematically researched. Furthermore, training binary-weight SNNs is even\nmore difficult. In this work, we present a novel perspective on the dynamics of\nSNNs and their close connection to BNNs through an analysis of the\nbackpropagation process. We demonstrate that training a feedforward SNN can be\nviewed as training a self-ensemble of a binary-activation neural network with\nnoise injection. Drawing from this new understanding of SNN dynamics, we\nintroduce the Self-Ensemble Inspired training method for (Binary-Weight) SNNs\n(SEI-BWSNN), which achieves high-performance results with low latency even for\nthe case of the 1-bit weights. Specifically, we leverage a structure of\nmultiple shortcuts and a knowledge distillation-based training technique to\nimprove the training of (binary-weight) SNNs. Notably, by binarizing FFN layers\nin a Transformer architecture, our approach achieves 82.52% accuracy on\nImageNet with only 2 time steps, indicating the effectiveness of our\nmethodology and the potential of binary-weight SNNs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89d2\uff0c\u5c06\u8109\u51b2\u795e\u7ecf\u7f51\u7edc(SNNs)\u8bad\u7ec3\u89c6\u4e3a\u5e26\u6709\u566a\u58f0\u6ce8\u5165\u7684\u4e8c\u8fdb\u5236\u6fc0\u6d3b\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u6211\u96c6\u6210\u8bad\u7ec3\uff0c\u5e76\u63d0\u51faSEI-BWSNN\u65b9\u6cd5\u6765\u8bad\u7ec3\u4e8c\u8fdb\u5236\u6743\u91cdSNNs\uff0c\u5728ImageNet\u4e0a\u8fbe\u523082.52%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc(SNNs)\u548c\u4e8c\u8fdb\u5236\u795e\u7ecf\u7f51\u7edc(BNNs)\u90fd\u9762\u4e34\u975e\u53ef\u5fae\u7684\u6fc0\u6d3b\u51fd\u6570\u95ee\u9898\uff0c\u4f46\u4e24\u8005\u4e4b\u95f4\u7684\u6df1\u5c42\u5173\u8054\u548c\u8bad\u7ec3\u6280\u672f\u7684\u76f8\u4e92\u5956\u52b1\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002\u7279\u522b\u662f\u4e8c\u8fdb\u5236\u6743\u91cdSNNs\u7684\u8bad\u7ec3\u66f4\u52a0\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u5bf9\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\u7684\u5206\u6790\uff0c\u5c06SNN\u8bad\u7ec3\u89c6\u4e3a\u5e26\u6709\u566a\u58f0\u6ce8\u5165\u7684\u4e8c\u8fdb\u5236\u6fc0\u6d3b\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u6211\u96c6\u6210\u8bad\u7ec3\u3002\u63d0\u51faSEI-BWSNN\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u91cd\u77ed\u63a5\u7ed3\u6784\u548c\u77e5\u8bc6\u84c4\u7c89\u57fa\u7840\u7684\u8bad\u7ec3\u6280\u672f\u6765\u6539\u5584(binary-weight) SNNs\u7684\u8bad\u7ec3\u3002", "result": "\u5728Transformer\u67b6\u6784\u4e2d\u4e8c\u8fdb\u5236\u5316FFN\u5c42\u540e\uff0c\u4ec5\u75282\u4e2a\u65f6\u95f4\u6b65\u957f\u5c31\u5728ImageNet\u4e0a\u8fbe\u523082.52%\u7684\u51c6\u786e\u7387\uff0c\u663e\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4e8c\u8fdb\u5236\u6743\u91cdSNNs\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aSNNs\u548cBNNs\u4e4b\u95f4\u7684\u5173\u8054\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u89e3\u89c6\u89d2\uff0c\u5e76\u901a\u8fc7SEI-BWSNN\u65b9\u6cd5\u6210\u529f\u5730\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u4e8c\u8fdb\u5236\u6743\u91cdSNNs\u8bad\u7ec3\uff0c\u4e3a\u4f4e\u529f\u8017\u795e\u7ecf\u7eb3\u7c73\u786c\u4ef6\u5e94\u7528\u5f00\u542f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.11954", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11954", "abs": "https://arxiv.org/abs/2508.11954", "authors": ["Sehyuk Park", "Soyeon Caren Han", "Eduard Hovy"], "title": "UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting", "comment": null, "summary": "Time series forecasting is a foundational task across domains, such as\nfinance, healthcare, and environmental monitoring. While recent advances in\nTime Series Foundation Models (TSFMs) have demonstrated strong generalisation\nthrough large-scale pretraining, existing models operate predominantly in a\nunimodal setting, ignoring the rich multimodal context, such as visual and\ntextual signals, that often accompanies time series data in real-world\nscenarios. This paper introduces a novel parameter-efficient multimodal\nframework, UniCast, that extends TSFMs to jointly leverage time series, vision,\nand text modalities for enhanced forecasting performance. Our method integrates\nmodality-specific embeddings from pretrained Vision and Text Encoders with a\nfrozen TSFM via soft prompt tuning, enabling efficient adaptation with minimal\nparameter updates. This design not only preserves the generalisation strength\nof the foundation model but also enables effective cross-modal interaction.\nExtensive experiments across diverse time-series forecasting benchmarks\ndemonstrate that UniCast consistently and significantly outperforms all\nexisting TSFM baselines. The findings highlight the critical role of multimodal\ncontext in advancing the next generation of general-purpose time series\nforecasters.", "AI": {"tldr": "UniCast\u662f\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u7684\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u6765\u589e\u5f3a\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u5728\u5355\u6a21\u6001\u8bbe\u7f6e\u4e0b\u8fd0\u884c\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u5e38\u4f34\u968f\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u4e30\u5bcc\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\uff08\u5982\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u53f7\uff09\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51faUniCast\u6846\u67b6\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\u63d0\u53d6\u6a21\u6001\u7279\u5b9a\u5d4c\u5165\uff0c\u901a\u8fc7\u8f6f\u63d0\u793a\u8c03\u4f18\u4e0e\u51bb\u7ed3\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u96c6\u6210\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u6700\u5c0f\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5728\u591a\u4e2a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniCast\u59cb\u7ec8\u663e\u8457\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u57fa\u7ebf\u3002", "conclusion": "\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5728\u63a8\u52a8\u4e0b\u4e00\u4ee3\u901a\u7528\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5668\u53d1\u5c55\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0cUniCast\u6846\u67b6\u4e3a\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u63d0\u4f9b\u4e86\u53c2\u6570\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11803", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11803", "abs": "https://arxiv.org/abs/2508.11803", "authors": ["Azam Nouri"], "title": "An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation", "comment": "5 pages, No figure", "summary": "This study investigates whether second-order geometric cues - planar\ncurvature magnitude, curvature sign, and gradient orientation - are sufficient\non their own to drive a multilayer perceptron (MLP) classifier for handwritten\ncharacter recognition (HCR), offering an alternative to convolutional neural\nnetworks (CNNs). Using these three handcrafted feature maps as inputs, our\ncurvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89\npercent on EMNIST letters. These results underscore the discriminative power of\ncurvature-based representations for handwritten character images and\ndemonstrate that the advantages of deep learning can be realized even with\ninterpretable, hand-engineered features.", "AI": {"tldr": "\u7814\u7a76\u8bc1\u660e\u4ec5\u4f7f\u7528\u4e8c\u9636\u51e0\u4f55\u7279\u5f81\uff08\u5e73\u9762\u66f2\u7387\u3001\u66f2\u7387\u7b26\u53f7\u548c\u68af\u5ea6\u65b9\u5411\uff09\u5c31\u80fd\u8ba9MLP\u5728\u624b\u5199\u5b57\u7b26\u8bc6\u522b\u4e0a\u8fbe\u5230\u9ad8\u51c6\u786e\u7387\uff0c\u4e3aCNN\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848", "motivation": "\u63a2\u7d22\u662f\u5426\u4ec5\u51ed\u4e8c\u9636\u51e0\u4f55\u7279\u5f81\u5c31\u80fd\u5b9e\u73b0\u9ad8\u6548\u7684\u624b\u5199\u5b57\u7b26\u8bc6\u522b\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u66f4\u5f3a\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u4f7f\u7528\u624b\u5de5\u8bbe\u8ba1\u7684\u4e09\u4e2a\u7279\u5f81\u56fe\uff08\u66f2\u7387\u5927\u5c0f\u3001\u66f2\u7387\u7b26\u53f7\u3001\u68af\u5ea6\u65b9\u5411\uff09\u4f5c\u4e3a\u8f93\u5165\uff0c\u6784\u5efa\u591a\u5c42\u611f\u77e5\u673a\u5206\u7c7b\u5668", "result": "\u5728MNIST\u6570\u5b57\u4e0a\u8fbe\u523097%\u51c6\u786e\u7387\uff0c\u5728EMNIST\u5b57\u6bcd\u4e0a\u8fbe\u523089%\u51c6\u786e\u7387", "conclusion": "\u66f2\u7387\u7279\u5f81\u5728\u624b\u5199\u5b57\u7b26\u8bc6\u522b\u4e2d\u5177\u6709\u5f3a\u5927\u5224\u522b\u80fd\u529b\uff0c\u6df1\u5ea6\u5b66\u4e60\u4f18\u52bf\u53ef\u4ee5\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u624b\u5de5\u7279\u5f81\u5b9e\u73b0"}}
{"id": "2508.12846", "categories": ["cs.NE", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.12846", "abs": "https://arxiv.org/abs/2508.12846", "authors": ["Wiktor J. Szczerek", "Artur Podobas"], "title": "IzhiRISC-V -- a RISC-V-based Processor with Custom ISA Extension for Spiking Neuron Networks Processing with Izhikevich Neurons", "comment": null, "summary": "Spiking Neural Network processing promises to provide high energy efficiency\ndue to the sparsity of the spiking events. However, when realized on\ngeneral-purpose hardware -- such as a RISC-V processor -- this promise can be\nundermined and overshadowed by the inefficient code, stemming from repeated\nusage of basic instructions for updating all the neurons in the network. One of\nthe possible solutions to this issue is the introduction of a custom ISA\nextension with neuromorphic instructions for spiking neuron updating, and\nrealizing those instructions in bespoke hardware expansion to the existing ALU.\nIn this paper, we present the first step towards realizing a large-scale system\nbased on the RISC-V-compliant processor called IzhiRISC-V, supporting the\ncustom neuromorphic ISA extension.", "AI": {"tldr": "\u57fa\u4e8eRISC-V\u5904\u7406\u5668\u7684\u810f\u6b7b\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u6548\u7387\u4f4e\u4e0b\uff0c\u63d0\u51fa\u901a\u8fc7\u5b9a\u5236ISA\u6269\u5c55\u548c\u786c\u4ef6\u6269\u5c55\u6765\u63d0\u9ad8\u6548\u7387\u7684\u65b9\u6848\u3002", "motivation": "\u901a\u7528\u786c\u4ef6\u4e0a\u8fd0\u884c\u810f\u6b7b\u795e\u7ecf\u7f51\u7edc\u5bfc\u81f4\u4ee3\u7801\u6548\u7387\u4f4e\u4e0b\uff0c\u5f71\u54cd\u4e86\u810f\u6b7b\u4e8b\u4ef6\u7a00\u758f\u6027\u5e26\u6765\u7684\u80fd\u6e90\u6548\u7387\u4f18\u52bf\u3002", "method": "\u4e3aRISC-V\u5904\u7406\u5668\u5f15\u5165\u5b9a\u5236\u7684\u795e\u7ecf\u6a21\u5f0fISA\u6269\u5c55\u6307\u4ee4\uff0c\u5e76\u5728\u73b0\u6709ALU\u4e0a\u5b9e\u73b0\u4e13\u95e8\u7684\u786c\u4ef6\u6269\u5c55\u6765\u652f\u6301\u8fd9\u4e9b\u6307\u4ee4\u3002", "result": "\u5f00\u53d1\u4e86\u79f0\u4e3aIzhiRISC-V\u7684RISC-V\u517c\u5bb9\u5904\u7406\u5668\uff0c\u652f\u6301\u5b9a\u5236\u795e\u7ecf\u6a21\u5f0fISA\u6269\u5c55\uff0c\u4e3a\u5927\u89c4\u6a21\u7cfb\u7edf\u5b9e\u73b0\u5960\u5b9a\u4e86\u7b2c\u4e00\u6b65\u57fa\u7840\u3002", "conclusion": "\u901a\u8fc7\u5b9a\u5236\u786c\u4ef6\u6269\u5c55\u548cISA\u6307\u4ee4\u96c6\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u901a\u7528\u786c\u4ef6\u4e0a\u810f\u6b7b\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u80fd\u810f\u6b7b\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u63d0\u4f9b\u4e86\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2508.11959", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11959", "abs": "https://arxiv.org/abs/2508.11959", "authors": ["Xuanxiang Huang", "Olivier L\u00e9toff\u00e9", "Joao Marques-Silva"], "title": "Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index", "comment": null, "summary": "Feature attribution methods based on game theory are ubiquitous in the field\nof eXplainable Artificial Intelligence (XAI). Recent works proposed rigorous\nfeature attribution using logic-based explanations, specifically targeting\nhigh-stakes uses of machine learning (ML) models. Typically, such works exploit\nweak abductive explanation (WAXp) as the characteristic function to assign\nimportance to features. However, one possible downside is that the contribution\nof non-WAXp sets is neglected. In fact, non-WAXp sets can also convey important\ninformation, because of the relationship between formal explanations (XPs) and\nadversarial examples (AExs). Accordingly, this paper leverages Shapley value\nand Banzhaf index to devise two novel feature importance scores. We take into\naccount non-WAXp sets when computing feature contribution, and the novel scores\nquantify how effective each feature is at excluding AExs. Furthermore, the\npaper identifies properties and studies the computational complexity of the\nproposed scores.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u65b0\u578b\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u975e\u5f31\u6eaf\u56e0\u89e3\u91ca\u96c6\u6765\u91cf\u5316\u7279\u5f81\u5728\u6392\u9664\u5bf9\u6297\u6837\u672c\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5f31\u6eaf\u56e0\u89e3\u91ca(WAXp)\uff0c\u4f46\u5ffd\u7565\u4e86\u975eWAXp\u96c6\u7684\u91cd\u8981\u4fe1\u606f\u3002\u7531\u4e8e\u5f62\u5f0f\u5316\u89e3\u91ca(XPs)\u4e0e\u5bf9\u6297\u6837\u672c(AExs)\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u975eWAXp\u96c6\u4e5f\u80fd\u4f20\u9012\u91cd\u8981\u4fe1\u606f\u3002", "method": "\u5229\u7528Shapley\u503c\u548cBanzhaf\u6307\u6570\u8bbe\u8ba1\u4e24\u79cd\u65b0\u9896\u7684\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\u65b9\u6cd5\uff0c\u5728\u8ba1\u7b97\u7279\u5f81\u8d21\u732e\u65f6\u8003\u8651\u975eWAXp\u96c6\uff0c\u91cf\u5316\u6bcf\u4e2a\u7279\u5f81\u5728\u6392\u9664\u5bf9\u6297\u6837\u672c\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u7279\u5f81\u8d21\u732e\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u4e9b\u8bc4\u5206\u65b9\u6cd5\u7684\u6027\u8d28\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u8003\u8651\u975eWAXp\u96c6\uff0c\u65b0\u7684\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u7279\u5f81\u5f52\u56e0\u5206\u6790\uff0c\u7279\u522b\u662f\u5728\u8bc6\u522b\u548c\u6392\u9664\u5bf9\u6297\u6837\u672c\u65b9\u9762\u5177\u6709\u66f4\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2508.11808", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY", "cs.MM", "I.2.7; I.2.10"], "pdf": "https://arxiv.org/pdf/2508.11808", "abs": "https://arxiv.org/abs/2508.11808", "authors": ["Sahajpreet Singh", "Rongxin Ouyang", "Subhayan Mukerjee", "Kokil Jaidka"], "title": "Labels or Input? Rethinking Augmentation in Multimodal Hate Detection", "comment": "13 pages, 2 figures, 7 tables", "summary": "The modern web is saturated with multimodal content, intensifying the\nchallenge of detecting hateful memes, where harmful intent is often conveyed\nthrough subtle interactions between text and image under the guise of humor or\nsatire. While recent advances in Vision-Language Models (VLMs) show promise,\nthese models lack support for fine-grained supervision and remain susceptible\nto implicit hate speech. In this paper, we present a dual-pronged approach to\nimprove multimodal hate detection. First, we propose a prompt optimization\nframework that systematically varies prompt structure, supervision granularity,\nand training modality. We show that prompt design and label scaling both\ninfluence performance, with structured prompts improving robustness even in\nsmall models, and InternVL2 achieving the best F1-scores across binary and\nscaled settings. Second, we introduce a multimodal data augmentation pipeline\nthat generates 2,479 counterfactually neutral memes by isolating and rewriting\nthe hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,\nsuccessfully reduces spurious correlations and improves classifier\ngeneralization. Our approaches inspire new directions for building synthetic\ndata to train robust and fair vision-language models. Our findings demonstrate\nthat prompt structure and data composition are as critical as model size, and\nthat targeted augmentation can support more trustworthy and context-sensitive\nhate detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u53cc\u91cd\u65b9\u6cd5\u6539\u8fdb\u591a\u6a21\u6001\u4ec7\u6068\u68c0\u6d4b\uff1a\u63d0\u793a\u4f18\u5316\u6846\u67b6\u548c\u591a\u6a21\u6001\u6570\u636e\u589e\u5f3a\u7ba1\u9053\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u548c\u53cd\u4e8b\u5b9e\u6570\u636e\u751f\u6210\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u4ee3\u7f51\u7edc\u5145\u65a5\u7740\u591a\u6a21\u6001\u5185\u5bb9\uff0c\u4ec7\u6068\u8868\u60c5\u5305\u68c0\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u6709\u5bb3\u610f\u56fe\u5f80\u5f80\u901a\u8fc7\u6587\u672c\u548c\u56fe\u50cf\u7684\u5fae\u5999\u4e92\u52a8\u4ee5\u5e7d\u9ed8\u6216\u8bbd\u523a\u5f62\u5f0f\u5448\u73b0\u3002\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u76d1\u7763\u652f\u6301\uff0c\u5bb9\u6613\u53d7\u5230\u9690\u542b\u4ec7\u6068\u8a00\u8bba\u7684\u5f71\u54cd\u3002", "method": "1) \u63d0\u51fa\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u7cfb\u7edf\u53d8\u5316\u63d0\u793a\u7ed3\u6784\u3001\u76d1\u7763\u7c92\u5ea6\u548c\u8bad\u7ec3\u6a21\u6001\uff1b2) \u5f15\u5165\u591a\u6a21\u6001\u6570\u636e\u589e\u5f3a\u7ba1\u9053\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53LLM-VLM\u8bbe\u7f6e\u751f\u62102479\u4e2a\u53cd\u4e8b\u5b9e\u4e2d\u6027\u8868\u60c5\u5305\uff0c\u9694\u79bb\u5e76\u91cd\u5199\u4ec7\u6068\u6a21\u6001\u3002", "result": "\u7ed3\u6784\u5316\u63d0\u793a\u5373\u4f7f\u5728\u5c0f\u578b\u6a21\u578b\u4e2d\u4e5f\u80fd\u63d0\u9ad8\u9c81\u68d2\u6027\uff0cInternVL2\u5728\u4e8c\u5143\u548c\u5206\u7ea7\u8bbe\u7f6e\u4e2d\u5747\u83b7\u5f97\u6700\u4f73F1\u5206\u6570\u3002\u6570\u636e\u589e\u5f3a\u7ba1\u9053\u6210\u529f\u51cf\u5c11\u865a\u5047\u76f8\u5173\u6027\u5e76\u63d0\u9ad8\u5206\u7c7b\u5668\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u793a\u7ed3\u6784\u548c\u6570\u636e\u7ec4\u6210\u4e0e\u6a21\u578b\u89c4\u6a21\u540c\u7b49\u91cd\u8981\uff0c\u9488\u5bf9\u6027\u589e\u5f3a\u53ef\u4ee5\u652f\u6301\u66f4\u53ef\u4fe1\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u4ec7\u6068\u68c0\u6d4b\uff0c\u4e3a\u6784\u5efa\u5408\u6210\u6570\u636e\u8bad\u7ec3\u9c81\u68d2\u516c\u5e73\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.11975", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11975", "abs": "https://arxiv.org/abs/2508.11975", "authors": ["Gongyao Jiang", "Qiong Luo"], "title": "Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven Synthesis and Candidate-Conditioned Answering", "comment": "Accepted to CIKM 2025", "summary": "Vision Language Models (VLMs) often struggle with chart understanding tasks,\nparticularly in accurate chart description and complex reasoning. Synthetic\ndata generation is a promising solution, while usually facing the challenge of\nnoise labels. To address this challenge, we first introduce a chart synthesis\npipeline that generates aligned chart-question-answer triplets through code\ngeneration and execution, ensuring the reliability of synthetic data without\nhuman intervention. Furthermore, inspired by test-time scaling that increases\ninference budget and thereby improves performance, we design a\ncandidate-conditioned answering process. The VLM first generates multiple\nresponses per query, and then synthesizes the final answer by contextualizing\nthese candidates. Experiments demonstrate significant improvements, with up to\n15.50 points accuracy gain over the initial VLM, in a fully self-improving\nparadigm without either human-labeled data or external models.", "AI": {"tldr": "\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u548c\u6267\u884c\u7684\u56fe\u8868\u5408\u6210\u6d41\u6c34\u7ebf\uff0c\u7ed9\u51fa\u5bf9\u9f50\u7684\u56fe\u8868-\u95ee\u9898-\u7b54\u6848\u4e09\u5143\u7ec4\uff0c\u5e76\u8bbe\u8ba1\u5019\u9009\u6761\u4ef6\u5316\u7b54\u9898\u8fc7\u7a0b\uff0c\u5728\u65e0\u4eba\u5de5\u6807\u6ce8\u6216\u5916\u90e8\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u6539\u8fdb", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u9047\u5230\u56f0\u96be\uff0c\u7279\u522b\u662f\u51c6\u786e\u7684\u56fe\u8868\u63cf\u8ff0\u548c\u590d\u6742\u63a8\u7406\u3002\u5408\u6210\u6570\u636e\u751f\u6210\u662f\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u901a\u5e38\u9762\u4e34\u566a\u58f0\u6807\u7b7e\u7684\u6311\u6218", "method": "\u9996\u5148\u5f15\u5165\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u548c\u6267\u884c\u7684\u56fe\u8868\u5408\u6210\u6d41\u6c34\u7ebf\uff0c\u751f\u6210\u5bf9\u9f50\u7684\u56fe\u8868-\u95ee\u9898-\u7b54\u6848\u4e09\u5143\u7ec4\uff0c\u786e\u4fdd\u5408\u6210\u6570\u636e\u7684\u53ef\u9760\u6027\u3002\u8fd8\u8bbe\u8ba1\u4e86\u5019\u9009\u6761\u4ef6\u5316\u7b54\u9898\u8fc7\u7a0b\uff0c\u8ba9VLM\u5148\u751f\u6210\u591a\u4e2a\u56de\u5e94\uff0c\u7136\u540e\u901a\u8fc7\u4e0a\u4e0b\u6587\u5316\u8fd9\u4e9b\u5019\u9009\u6765\u5408\u6210\u6700\u7ec8\u7b54\u6848", "result": "\u5b9e\u9a8c\u8868\u660e\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5728\u5b8c\u5168\u81ea\u6211\u6539\u8fdb\u8303\u5f0f\u4e0b\uff0c\u6bd4\u521d\u59cbVLM\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u6700\u591a15.50\u4e2a\u767e\u5206\u70b9", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u6216\u5916\u90e8\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u5019\u9009\u6761\u4ef6\u5316\u7b54\u9898\u673a\u5236\uff0c\u6709\u6548\u5730\u6539\u5584\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u8868\u73b0"}}
{"id": "2508.11825", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11825", "abs": "https://arxiv.org/abs/2508.11825", "authors": ["Sherlon Almeida da Silva", "Davi Geiger", "Luiz Velho", "Moacir Antonelli Ponti"], "title": "Towards Understanding 3D Vision: the Role of Gaussian Curvature", "comment": null, "summary": "Recent advances in computer vision have predominantly relied on data-driven\napproaches that leverage deep learning and large-scale datasets. Deep neural\nnetworks have achieved remarkable success in tasks such as stereo matching and\nmonocular depth reconstruction. However, these methods lack explicit models of\n3D geometry that can be directly analyzed, transferred across modalities, or\nsystematically modified for controlled experimentation. We investigate the role\nof Gaussian curvature in 3D surface modeling. Besides Gaussian curvature being\nan invariant quantity under change of observers or coordinate systems, we\ndemonstrate using the Middlebury stereo dataset that it offers: (i) a sparse\nand compact description of 3D surfaces, (ii) state-of-the-art monocular and\nstereo methods seem to implicitly consider it, but no explicit module of such\nuse can be extracted, (iii) a form of geometric prior that can inform and\nimprove 3D surface reconstruction, and (iv) a possible use as an unsupervised\nmetric for stereo methods.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u9ad8\u65af\u66f2\u7387\u57283D\u8868\u9762\u5efa\u6a21\u4e2d\u7684\u4f5c\u7528\uff0c\u6307\u51fa\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u663e\u5f0f\u7684\u51e0\u4f55\u6a21\u578b\uff0c\u9ad8\u65af\u66f2\u7387\u4f5c\u4e3a\u4e00\u79cd\u4e0d\u53d8\u91cf\u53ef\u4ee5\u63d0\u4f9b\u7a00\u758f\u7f29\u7565\u76843D\u8868\u9762\u63cf\u8ff0\uff0c\u5e76\u4f5c\u4e3a\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\u6539\u5584\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u5728\u7acb\u4f53\u5339\u914d\u548c\u5355\u76ee\u6df1\u5ea6\u91cd\u5efa\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u7f3a\u4e4f\u53ef\u4ee5\u76f4\u63a5\u5206\u6790\u3001\u8de8\u6a21\u6001\u8f6c\u79fb\u6216\u7cfb\u7edf\u6027\u4fee\u6539\u7684\u663e\u5f0f3D\u51e0\u4f55\u6a21\u578b\u3002\u7814\u7a76\u4eba\u5458\u60f3\u8981\u63a2\u7d22\u9ad8\u65af\u66f2\u7387\u5728\u8868\u9762\u5efa\u6a21\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u7814\u7a76\u4eba\u5458\u4f7f\u7528Middlebury\u7acb\u4f53\u6570\u636e\u96c6\u6765\u7814\u7a76\u9ad8\u65af\u66f2\u7387\u7684\u7279\u6027\u3002\u9ad8\u65af\u66f2\u7387\u662f\u4e00\u79cd\u5728\u89c2\u5bdf\u8005\u53d8\u6362\u6216\u5750\u6807\u7cfb\u53d8\u6362\u4e0b\u4fdd\u6301\u4e0d\u53d8\u7684\u91cf\uff0c\u7814\u7a76\u4eba\u5458\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u5b83\u7684\u51e0\u9879\u5173\u952e\u7279\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(i)\u9ad8\u65af\u66f2\u7387\u80fd\u63d0\u4f9b\u7a00\u758f\u7f29\u7565\u76843D\u8868\u9762\u63cf\u8ff0\uff1b(ii)\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5355\u76ee\u548c\u7acb\u4f53\u65b9\u6cd5\u4f3c\u4e4e\u90fd\u9690\u5f0f\u8003\u8651\u4e86\u9ad8\u65af\u66f2\u7387\uff0c\u4f46\u65e0\u6cd5\u63d0\u53d6\u663e\u5f0f\u6a21\u5757\uff1b(iii)\u9ad8\u65af\u66f2\u7387\u53ef\u4f5c\u4e3a\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\u6765\u63d0\u53473D\u8868\u9762\u91cd\u5efa\u7684\u6548\u679c\uff1b(iv)\u5b83\u8fd8\u53ef\u80fd\u4f5c\u4e3a\u7acb\u4f53\u65b9\u6cd5\u7684\u65e0\u76d1\u7763\u8bc4\u91cf\u6307\u6807\u3002", "conclusion": "\u9ad8\u65af\u66f2\u7387\u57283D\u8868\u9762\u5efa\u6a21\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u5b83\u4e0d\u4ec5\u662f\u4e00\u79cd\u7a00\u758f\u7f29\u7565\u7684\u8868\u5f81\u65b9\u5f0f\uff0c\u8fd8\u53ef\u4ee5\u4f5c\u4e3a\u51e0\u4f55\u5148\u9a8c\u6765\u6539\u5584\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u4e3a\u7acb\u4f53\u65b9\u6cd5\u63d0\u4f9b\u65e0\u76d1\u7763\u8bc4\u4f30\u3002\u8fd9\u4e3a\u5f00\u53d1\u66f4\u53ef\u89e3\u91ca\u548c\u53ef\u63a7\u7684\u51e0\u4f55\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2508.11987", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11987", "abs": "https://arxiv.org/abs/2508.11987", "authors": ["Zhiyuan Zeng", "Jiashuo Liu", "Siyuan Chen", "Tianci He", "Yali Liao", "Jinpeng Wang", "Zaiyuan Wang", "Yang Yang", "Lingyue Yin", "Mingren Yin", "Zhenwei Zhu", "Tianle Cai", "Zehui Chen", "Jiecao Chen", "Yantao Du", "Xiang Gao", "Jiacheng Guo", "Liang Hu", "Jianpeng Jiao", "Xiangsheng Li", "Jingkai Liu", "Shuang Ni", "Zhoufutu Wen", "Ge Zhang", "Kaiyuan Zhang", "Xin Zhou", "Jose Blanchet", "Xipeng Qiu", "Mengdi Wang", "Wenhao Huang"], "title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction", "comment": "Technical report, 51 pages", "summary": "Future prediction is a complex task for LLM agents, requiring a high level of\nanalytical thinking, information gathering, contextual understanding, and\ndecision-making under uncertainty. Agents must not only gather and interpret\nvast amounts of dynamic information but also integrate diverse data sources,\nweigh uncertainties, and adapt predictions based on emerging trends, just as\nhuman experts do in fields like politics, economics, and finance. Despite its\nimportance, no large-scale benchmark exists for evaluating agents on future\nprediction, largely due to challenges in handling real-time updates and\nretrieving timely, accurate answers. To address this, we introduce\n$\\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically\ndesigned for LLM agents performing future prediction tasks. FutureX is the\nlargest and most diverse live benchmark for future prediction, supporting\nreal-time daily updates and eliminating data contamination through an automated\npipeline for question gathering and answer collection. We evaluate 25 LLM/agent\nmodels, including those with reasoning, search capabilities, and integration of\nexternal tools such as the open-source Deep Research Agent and closed-source\nDeep Research models. This comprehensive evaluation assesses agents' adaptive\nreasoning and performance in dynamic environments. Additionally, we provide\nin-depth analyses of agents' failure modes and performance pitfalls in\nfuture-oriented tasks, including the vulnerability to fake web pages and the\ntemporal validity. Our goal is to establish a dynamic, contamination-free\nevaluation standard that drives the development of LLM agents capable of\nperforming at the level of professional human analysts in complex reasoning and\npredictive thinking.", "AI": {"tldr": "FutureX\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3aLLM\u667a\u80fd\u4f53\u8bbe\u8ba1\u7684\u52a8\u6001\u5b9e\u65f6\u672a\u6765\u9884\u6d4b\u8bc4\u4f30\u57fa\u51c6\uff0c\u652f\u6301\u6bcf\u65e5\u5b9e\u65f6\u66f4\u65b0\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u907f\u514d\u6570\u636e\u6c61\u67d3\uff0c\u8bc4\u4f30\u4e8625\u4e2a\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u672a\u6765\u9884\u6d4b\u5bf9LLM\u667a\u80fd\u4f53\u6765\u8bf4\u662f\u4e00\u9879\u590d\u6742\u4efb\u52a1\uff0c\u9700\u8981\u9ad8\u6c34\u5e73\u5206\u6790\u601d\u7ef4\u548c\u4fe1\u606f\u5904\u7406\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e3b\u8981\u7531\u4e8e\u5904\u7406\u5b9e\u65f6\u66f4\u65b0\u548c\u83b7\u53d6\u53ca\u65f6\u51c6\u786e\u7b54\u6848\u7684\u6311\u6218\u3002", "method": "\u6784\u5efaFutureX\u52a8\u6001\u5b9e\u65f6\u8bc4\u4f30\u57fa\u51c6\uff0c\u652f\u6301\u6bcf\u65e5\u5b9e\u65f6\u66f4\u65b0\uff0c\u91c7\u7528\u81ea\u52a8\u5316\u95ee\u9898\u6536\u96c6\u548c\u7b54\u6848\u6536\u96c6\u6d41\u7a0b\uff0c\u8bc4\u4f3025\u4e2aLLM/\u667a\u80fd\u4f53\u6a21\u578b\uff08\u5305\u62ec\u5177\u6709\u63a8\u7406\u3001\u641c\u7d22\u80fd\u529b\u548c\u5916\u90e8\u5de5\u5177\u96c6\u6210\u7684\u6a21\u578b\uff09\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9\u667a\u80fd\u4f53\u5728\u672a\u6765\u5bfc\u5411\u4efb\u52a1\u4e2d\u5931\u8d25\u6a21\u5f0f\u548c\u6027\u80fd\u7f3a\u9677\u7684\u6df1\u5165\u5206\u6790\uff0c\u5305\u62ec\u5bf9\u865a\u5047\u7f51\u9875\u7684\u8106\u5f31\u6027\u548c\u65f6\u95f4\u6709\u6548\u6027\u7b49\u95ee\u9898\u3002", "conclusion": "\u76ee\u6807\u662f\u5efa\u7acb\u4e00\u4e2a\u52a8\u6001\u3001\u65e0\u6c61\u67d3\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u63a8\u52a8LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u63a8\u7406\u548c\u9884\u6d4b\u601d\u7ef4\u65b9\u9762\u8fbe\u5230\u4e13\u4e1a\u4eba\u7c7b\u5206\u6790\u5e08\u7684\u6c34\u5e73\u3002"}}
{"id": "2508.11826", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11826", "abs": "https://arxiv.org/abs/2508.11826", "authors": ["Dehn Xu", "Tim Katzke", "Emmanuel M\u00fcller"], "title": "From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images", "comment": null, "summary": "Graph Neural Networks (GNNs) have emerged as a powerful approach for\ngraph-based machine learning tasks. Previous work applied GNNs to image-derived\ngraph representations for various downstream tasks such as classification or\nanomaly detection. These transformations include segmenting images, extracting\nfeatures from segments, mapping them to nodes, and connecting them. However, to\nthe best of our knowledge, no study has rigorously compared the effectiveness\nof the numerous potential image-to-graph transformation approaches for\nGNN-based graph-level anomaly detection (GLAD). In this study, we\nsystematically evaluate the efficacy of multiple segmentation schemes, edge\nconstruction strategies, and node feature sets based on color, texture, and\nshape descriptors to produce suitable image-derived graph representations to\nperform graph-level anomaly detection. We conduct extensive experiments on\ndermoscopic images using state-of-the-art GLAD models, examining performance\nand efficiency in purely unsupervised, weakly supervised, and fully supervised\nregimes. Our findings reveal, for example, that color descriptors contribute\nthe best standalone performance, while incorporating shape and texture features\nconsistently enhances detection efficacy. In particular, our best unsupervised\nconfiguration using OCGTL achieves a competitive AUC-ROC score of up to 0.805\nwithout relying on pretrained backbones like comparable image-based approaches.\nWith the inclusion of sparse labels, the performance increases substantially to\n0.872 and with full supervision to 0.914 AUC-ROC.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e86\u591a\u79cd\u56fe\u50cf\u5230\u56fe\u8c61\u8f6c\u6362\u65b9\u6cd5\u5728\u56fe\u795e\u7ecf\u7f51\u7edc\u56fe\u7ea7\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u989c\u8272\u63cf\u8ff0\u7b26\u6700\u91cd\u8981\uff0c\u7ed3\u5408\u5f62\u72b6\u548c\u7eb9\u7406\u7279\u5f81\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u867d\u7136GNN\u5df2\u5e94\u7528\u4e8e\u56fe\u50cf\u6f14\u7b97\u7684\u56fe\u8868\u793a\uff0c\u4f46\u8fd8\u6ca1\u6709\u7814\u7a76\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u56fe\u50cf\u5230\u56fe\u8c61\u8f6c\u6362\u65b9\u6cd5\u5728\u56fe\u7ea7\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u591a\u79cd\u5206\u5272\u65b9\u6848\u3001\u8fb9\u7f18\u6784\u5efa\u7b56\u7565\u548c\u8282\u70b9\u7279\u5f81\u96c6\uff08\u5305\u62ec\u989c\u8272\u3001\u7eb9\u7406\u3001\u5f62\u72b6\u63cf\u8ff0\u7b26\uff09\uff0c\u5728\u76ae\u80a4\u955c\u56fe\u50cf\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u4e86\u65e0\u76d1\u7763\u3001\u5f31\u76d1\u7763\u548c\u5168\u76d1\u7763\u4e09\u79cd\u6a21\u5f0f\u3002", "result": "\u989c\u8272\u63cf\u8ff0\u7b26\u5355\u72ec\u6027\u80fd\u6700\u597d\uff0c\u7ed3\u5408\u5f62\u72b6\u548c\u7eb9\u7406\u7279\u5f81\u80fd\u4e00\u81f4\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002\u6700\u4f73\u65e0\u76d1\u7763\u914d\u7f6e\u8fbe\u5230AUC-ROC 0.805\uff0c\u52a0\u5165\u7a00\u758f\u6807\u7b7e\u540e\u63d0\u5347\u52300.872\uff0c\u5168\u76d1\u7763\u65f6\u8fbe\u52300.914\u3002", "conclusion": "\u56fe\u50cf\u5230\u56fe\u8c61\u8f6c\u6362\u65b9\u6cd5\u7684\u9009\u62e9\u5bf9GNN\u56fe\u7ea7\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u989c\u8272\u7279\u5f81\u662f\u6700\u91cd\u8981\u7684\u5355\u72ec\u56e0\u7d20\uff0c\u591a\u7279\u5f81\u878d\u5408\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2508.11991", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11991", "abs": "https://arxiv.org/abs/2508.11991", "authors": ["Weihao Sun"], "title": "Modeling Relational Logic Circuits for And-Inverter Graph Convolutional Network", "comment": null, "summary": "The automation of logic circuit design enhances chip performance, energy\nefficiency, and reliability, and is widely applied in the field of Electronic\nDesign Automation (EDA).And-Inverter Graphs (AIGs) efficiently represent,\noptimize, and verify the functional characteristics of digital circuits,\nenhancing the efficiency of EDA development.Due to the complex structure and\nlarge scale of nodes in real-world AIGs, accurate modeling is challenging,\nleading to existing work lacking the ability to jointly model functional and\nstructural characteristics, as well as insufficient dynamic information\npropagation capability.To address the aforementioned challenges, we propose\nAIGer.Specifically, AIGer consists of two components: 1) Node logic feature\ninitialization embedding component and 2) AIGs feature learning network\ncomponent.The node logic feature initialization embedding component projects\nlogic nodes, such as AND and NOT, into independent semantic spaces, to enable\neffective node embedding for subsequent processing.Building upon this, the AIGs\nfeature learning network component employs a heterogeneous graph convolutional\nnetwork, designing dynamic relationship weight matrices and differentiated\ninformation aggregation approaches to better represent the original structure\nand information of AIGs.The combination of these two components enhances\nAIGer's ability to jointly model functional and structural characteristics and\nimproves its message passing capability. Experimental results indicate that\nAIGer outperforms the current best models in the Signal Probability Prediction\n(SSP) task, improving MAE and MSE by 18.95\\% and 44.44\\%, respectively. In the\nTruth Table Distance Prediction (TTDP) task, AIGer achieves improvements of\n33.57\\% and 14.79\\% in MAE and MSE, respectively, compared to the\nbest-performing models.", "AI": {"tldr": "AIGer\u662f\u4e00\u4e2a\u7528\u4e8eAIG\u56fe\u8868\u793a\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8282\u70b9\u903b\u8f91\u7279\u5f81\u521d\u59cb\u5316\u548c\u5f02\u6784\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u5728\u4fe1\u53f7\u6982\u7387\u9884\u6d4b\u548c\u771f\u503c\u8868\u8ddd\u79bb\u9884\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u5efa\u6a21AIG\u56fe\u7684\u529f\u80fd\u548c\u7ed3\u6784\u7279\u5f81\uff0c\u4ee5\u53ca\u52a8\u6001\u4fe1\u606f\u4f20\u64ad\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ee5\u63d0\u5347EDA\u9886\u57df\u7684\u7535\u8def\u8bbe\u8ba1\u81ea\u52a8\u5316\u6548\u7387\u3002", "method": "\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1)\u8282\u70b9\u903b\u8f91\u7279\u5f81\u521d\u59cb\u5316\u5d4c\u5165\u7ec4\u4ef6\uff0c\u5c06\u903b\u8f91\u8282\u70b9\u6295\u5f71\u5230\u72ec\u7acb\u8bed\u4e49\u7a7a\u95f4\uff1b2)AIG\u7279\u5f81\u5b66\u4e60\u7f51\u7edc\u7ec4\u4ef6\uff0c\u4f7f\u7528\u5f02\u6784\u56fe\u5377\u79ef\u7f51\u7edc\u8bbe\u8ba1\u52a8\u6001\u5173\u7cfb\u6743\u91cd\u77e9\u9635\u548c\u5dee\u5f02\u5316\u4fe1\u606f\u805a\u5408\u65b9\u6cd5\u3002", "result": "\u5728\u4fe1\u53f7\u6982\u7387\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cMAE\u548cMSE\u5206\u522b\u63d0\u534718.95%\u548c44.44%\uff1b\u5728\u771f\u503c\u8868\u8ddd\u79bb\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cMAE\u548cMSE\u5206\u522b\u63d0\u534733.57%\u548c14.79%\u3002", "conclusion": "AIGer\u901a\u8fc7\u6709\u6548\u7ed3\u5408\u529f\u80fd\u4e0e\u7ed3\u6784\u7279\u5f81\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86AIG\u56fe\u7684\u8868\u793a\u5b66\u4e60\u6027\u80fd\uff0c\u4e3aEDA\u9886\u57df\u7684\u7535\u8def\u8bbe\u8ba1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2508.11834", "categories": ["cs.CV", "cs.AI", "cs.RO", "cs.SY", "eess.IV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11834", "abs": "https://arxiv.org/abs/2508.11834", "authors": ["Hamza Kheddar", "Yassine Habchi", "Mohamed Chahine Ghanem", "Mustapha Hemis", "Dusit Niyato"], "title": "Recent Advances in Transformer and Large Language Models for UAV Applications", "comment": null, "summary": "The rapid advancement of Transformer-based models has reshaped the landscape\nof uncrewed aerial vehicle (UAV) systems by enhancing perception,\ndecision-making, and autonomy. This review paper systematically categorizes and\nevaluates recent developments in Transformer architectures applied to UAVs,\nincluding attention mechanisms, CNN-Transformer hybrids, reinforcement learning\nTransformers, and large language models (LLMs). Unlike previous surveys, this\nwork presents a unified taxonomy of Transformer-based UAV models, highlights\nemerging applications such as precision agriculture and autonomous navigation,\nand provides comparative analyses through structured tables and performance\nbenchmarks. The paper also reviews key datasets, simulators, and evaluation\nmetrics used in the field. Furthermore, it identifies existing gaps in the\nliterature, outlines critical challenges in computational efficiency and\nreal-time deployment, and offers future research directions. This comprehensive\nsynthesis aims to guide researchers and practitioners in understanding and\nadvancing Transformer-driven UAV technologies.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u5206\u7c7b\u548c\u8bc4\u4f30\u4e86Transformer\u67b6\u6784\u5728\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u7684\u6700\u65b0\u5e94\u7528\u8fdb\u5c55\uff0c\u5305\u62ec\u6ce8\u610f\u529b\u673a\u5236\u3001CNN-Transformer\u6df7\u5408\u6a21\u578b\u3001\u5f3a\u5316\u5b66\u4e60Transformer\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u5206\u7c7b\u4f53\u7cfb\u548c\u6027\u80fd\u57fa\u51c6\u5206\u6790\u3002", "motivation": "\u968f\u7740Transformer\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u5728\u65e0\u4eba\u673a\u611f\u77e5\u3001\u51b3\u7b56\u548c\u81ea\u4e3b\u6027\u65b9\u9762\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u7efc\u8ff0\u6765\u7edf\u4e00\u5206\u7c7b\u548c\u8bc4\u4f30\u8fd9\u4e9b\u8fdb\u5c55\uff0c\u9700\u8981\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u5168\u9762\u7684\u6280\u672f\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5bf9Transformer\u5728\u65e0\u4eba\u673a\u9886\u57df\u7684\u5e94\u7528\u8fdb\u884c\u5206\u7c7b\u6574\u7406\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u683c\u548c\u6027\u80fd\u57fa\u51c6\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\uff0c\u5e76\u56de\u987e\u5173\u952e\u6570\u636e\u96c6\u3001\u6a21\u62df\u5668\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5efa\u7acb\u4e86Transformer\u5728\u65e0\u4eba\u673a\u5e94\u7528\u7684\u7edf\u4e00\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u5305\u62ec\u7cbe\u51c6\u519c\u4e1a\u548c\u81ea\u4e3b\u5bfc\u822a\u7b49\u65b0\u5174\u5e94\u7528\u9886\u57df\uff0c\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u6027\u80fd\u6bd4\u8f83\u548c\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u7814\u7a76\u4e2d\u7684\u6280\u672f\u7a7a\u767d\u3002", "conclusion": "\u8bba\u6587\u4e3aTransformer\u9a71\u52a8\u7684\u65e0\u4eba\u673a\u6280\u672f\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6280\u672f\u8def\u7ebf\u56fe\uff0c\u6307\u51fa\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u5b9e\u65f6\u90e8\u7f72\u7b49\u5173\u952e\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2508.11995", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.11995", "abs": "https://arxiv.org/abs/2508.11995", "authors": ["Xuyang Zhao", "Shiwan Zhao", "Hualong Yu", "Liting Zhang", "Qicheng Li"], "title": "AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning", "comment": null, "summary": "Multi-agent systems (MAS) powered by large language models (LLMs) hold\nsignificant promise for solving complex decision-making tasks. However, the\ncore process of collaborative decision-making (CDM) within these systems\nremains underexplored. Existing approaches often rely on either ``dictatorial\"\nstrategies that are vulnerable to the cognitive biases of a single agent, or\n``voting-based\" methods that fail to fully harness collective intelligence. To\naddress these limitations, we propose \\textbf{AgentCDM}, a structured framework\nfor enhancing collaborative decision-making in LLM-based multi-agent systems.\nDrawing inspiration from the Analysis of Competing Hypotheses (ACH) in\ncognitive science, AgentCDM introduces a structured reasoning paradigm that\nsystematically mitigates cognitive biases and shifts decision-making from\npassive answer selection to active hypothesis evaluation and construction. To\ninternalize this reasoning process, we develop a two-stage training paradigm:\nthe first stage uses explicit ACH-inspired scaffolding to guide the model\nthrough structured reasoning, while the second stage progressively removes this\nscaffolding to encourage autonomous generalization. Experiments on multiple\nbenchmark datasets demonstrate that AgentCDM achieves state-of-the-art\nperformance and exhibits strong generalization, validating its effectiveness in\nimproving the quality and robustness of collaborative decisions in MAS.", "AI": {"tldr": "AgentCDM\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u534f\u4f5c\u51b3\u7b56\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8303\u5f0f\u7f13\u89e3\u8ba4\u77e5\u504f\u89c1\uff0c\u4ece\u88ab\u52a8\u7b54\u6848\u9009\u62e9\u8f6c\u5411\u4e3b\u52a8\u5047\u8bbe\u8bc4\u4f30\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u534f\u4f5c\u51b3\u7b56\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677\uff1a\u8981\u4e48\u4f9d\u8d56\u5355\u4e00\u667a\u80fd\u4f53\u7684\"\u72ec\u88c1\"\u7b56\u7565\uff08\u6613\u53d7\u8ba4\u77e5\u504f\u89c1\u5f71\u54cd\uff09\uff0c\u8981\u4e48\u4f7f\u7528\"\u6295\u7968\"\u65b9\u6cd5\uff08\u65e0\u6cd5\u5145\u5206\u5229\u7528\u96c6\u4f53\u667a\u6167\uff09\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u534f\u4f5c\u51b3\u7b56\u6846\u67b6\u3002", "method": "\u53d7\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7ade\u4e89\u5047\u8bbe\u5206\u6790(ACH)\u542f\u53d1\uff0c\u63d0\u51fa\u7ed3\u6784\u5316\u63a8\u7406\u8303\u5f0f\uff1a1)\u4f7f\u7528\u663e\u5f0fACH\u811a\u624b\u67b6\u6307\u5bfc\u6a21\u578b\u8fdb\u884c\u7ed3\u6784\u5316\u63a8\u7406\uff1b2)\u9010\u6b65\u79fb\u9664\u811a\u624b\u67b6\u4ee5\u4fc3\u8fdb\u81ea\u4e3b\u6cdb\u5316\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAgentCDM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AgentCDM\u6709\u6548\u63d0\u9ad8\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u534f\u4f5c\u51b3\u7b56\u7684\u8d28\u91cf\u548c\u9c81\u68d2\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u7f13\u89e3\u8ba4\u77e5\u504f\u89c1\u3001\u4fc3\u8fdb\u4e3b\u52a8\u5047\u8bbe\u8bc4\u4f30\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.11854", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11854", "abs": "https://arxiv.org/abs/2508.11854", "authors": ["Matthew Hull", "Haoyang Yang", "Pratham Mehta", "Mansi Phute", "Aeree Cho", "Haorang Wang", "Matthew Lau", "Wenke Lee", "Wilian Lunardi", "Martin Andreoni", "Polo Chau"], "title": "ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages", "comment": "7 pages, 6 figures", "summary": "As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks\nfor efficient novel-view synthesis from static images, how might an adversary\ntamper images to cause harm? We introduce ComplicitSplat, the first attack that\nexploits standard 3DGS shading methods to create viewpoint-specific camouflage\n- colors and textures that change with viewing angle - to embed adversarial\ncontent in scene objects that are visible only from specific viewpoints and\nwithout requiring access to model architecture or weights. Our extensive\nexperiments show that ComplicitSplat generalizes to successfully attack a\nvariety of popular detector - both single-stage, multi-stage, and\ntransformer-based models on both real-world capture of physical objects and\nsynthetic scenes. To our knowledge, this is the first black-box attack on\ndownstream object detectors using 3DGS, exposing a novel safety risk for\napplications like autonomous navigation and other mission-critical robotic\nsystems.", "AI": {"tldr": "\u9996\u4e2a\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5ComplicitSplat\uff0c\u5229\u75283D\u9ad8\u65af\u62df\u5408\u7684\u8f6e\u5ed3\u6e32\u67d3\u6280\u672f\u5728\u7279\u5b9a\u89c6\u89d2\u4e0b\u5d4c\u5165\u963f\u5c14\u6cd5\u5185\u5bb9\uff0c\u653b\u51fb\u5404\u79cd\u76ee\u6807\u68c0\u6d4b\u5668\u800c\u65e0\u9700\u6a21\u578b\u6743\u91cd\u3002", "motivation": "\u968f\u77403D\u9ad8\u65af\u62df\u5408\u6280\u672f\u5728\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\u5feb\u901f\u5e94\u7528\uff0c\u9700\u8981\u7814\u7a76\u6076\u610f\u653b\u51fb\u8005\u5982\u4f55\u7be1\u6539\u56fe\u50cf\u9020\u6210\u5bb3\uff0c\u66dd\u9732\u65b0\u7684\u5b89\u5168\u98ce\u9669\u3002", "method": "\u5229\u7528\u6807\u51c63DGS\u6e32\u67d3\u65b9\u6cd5\u521b\u5efa\u89c6\u89d2\u7279\u5f02\u6027\u8c1c\u8272\u7b49\u6548\u679c\uff0c\u5728\u573a\u666f\u5bf9\u8c61\u4e2d\u5d4c\u5165\u4ec5\u5728\u7279\u5b9a\u89c6\u89d2\u53ef\u89c1\u7684\u963f\u5c14\u6cd5\u5185\u5bb9\uff0c\u65e0\u9700\u6a21\u578b\u67b6\u6784\u6216\u6743\u91cd\u8bbf\u95ee\u3002", "result": "\u5b9e\u9a8c\u8868\u660eComplicitSplat\u6210\u529f\u653b\u51fb\u591a\u79cd\u6d41\u884c\u68c0\u6d4b\u5668\uff08\u5355\u9636\u6bb5\u3001\u591a\u9636\u6bb5\u3001Transformer\u57fa\u7840\uff09\uff0c\u5728\u771f\u5b9e\u7269\u7406\u5bf9\u8c61\u548c\u5408\u6210\u573a\u666f\u4e2d\u90fd\u6709\u6548\u3002", "conclusion": "\u8be5\u653b\u51fb\u66dd\u9732\u4e863DGS\u5728\u81ea\u4e3b\u5bfc\u822a\u7b49\u4efb\u52a1\u5173\u952e\u7cfb\u7edf\u4e2d\u7684\u65b0\u5b89\u5168\u98ce\u9669\uff0c\u662f\u9996\u4e2a\u57fa\u4e8e3DGS\u7684\u9ed1\u76d2\u4e0b\u6e38\u76ee\u6807\u68c0\u6d4b\u5668\u653b\u51fb\u3002"}}
{"id": "2508.12022", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12022", "abs": "https://arxiv.org/abs/2508.12022", "authors": ["Dorsa Macky Aleagha", "Payam Zohari", "Mostafa Haghir Chehreghani"], "title": "AI Models for Depressive Disorder Detection and Diagnosis: A Review", "comment": null, "summary": "Major Depressive Disorder is one of the leading causes of disability\nworldwide, yet its diagnosis still depends largely on subjective clinical\nassessments. Integrating Artificial Intelligence (AI) holds promise for\ndeveloping objective, scalable, and timely diagnostic tools. In this paper, we\npresent a comprehensive survey of state-of-the-art AI methods for depression\ndetection and diagnosis, based on a systematic review of 55 key studies. We\nintroduce a novel hierarchical taxonomy that structures the field by primary\nclinical task (diagnosis vs. prediction), data modality (text, speech,\nneuroimaging, multimodal), and computational model class (e.g., graph neural\nnetworks, large language models, hybrid approaches). Our in-depth analysis\nreveals three major trends: the predominance of graph neural networks for\nmodeling brain connectivity, the rise of large language models for linguistic\nand conversational data, and an emerging focus on multimodal fusion,\nexplainability, and algorithmic fairness. Alongside methodological insights, we\nprovide an overview of prominent public datasets and standard evaluation\nmetrics as a practical guide for researchers. By synthesizing current advances\nand highlighting open challenges, this survey offers a comprehensive roadmap\nfor future innovation in computational psychiatry.", "AI": {"tldr": "\u672c\u6587\u5bf955\u9879\u5173\u952e\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e34\u5e8a\u4efb\u52a1\u3001\u6570\u636e\u6a21\u6001\u548c\u8ba1\u7b97\u6a21\u578b\u7684\u5206\u5c42\u5206\u7c7b\u6cd5\uff0c\u603b\u7ed3\u4e86\u6291\u90c1\u75c7AI\u8bca\u65ad\u9886\u57df\u7684\u4e09\u5927\u8d8b\u52bf\uff1a\u56fe\u795e\u7ecf\u7f51\u7edc\u4e3b\u5bfc\u8111\u8fde\u63a5\u5efa\u6a21\u3001\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u8bed\u8a00\u6570\u636e\u3001\u591a\u6a21\u6001\u878d\u5408\u548c\u53ef\u89e3\u91ca\u6027\u6210\u4e3a\u65b0\u5174\u7126\u70b9\u3002", "motivation": "\u6291\u90c1\u75c7\u662f\u5168\u7403\u4e3b\u8981\u81f4\u6b8b\u539f\u56e0\uff0c\u4f46\u76ee\u524d\u8bca\u65ad\u4ecd\u4f9d\u8d56\u4e3b\u89c2\u4e34\u5e8a\u8bc4\u4f30\u3002AI\u6280\u672f\u6709\u671b\u5f00\u53d1\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u548c\u53ca\u65f6\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u8be5\u9886\u57df\u7684\u7814\u7a76\u73b0\u72b6\u548c\u53d1\u5c55\u8d8b\u52bf\u3002", "method": "\u901a\u8fc7\u5bf955\u9879\u5173\u952e\u7814\u7a76\u7684\u7cfb\u7edf\u7efc\u8ff0\uff0c\u5efa\u7acb\u4e86\u5206\u5c42\u5206\u7c7b\u6cd5\uff08\u4e34\u5e8a\u4efb\u52a1\u00d7\u6570\u636e\u6a21\u6001\u00d7\u8ba1\u7b97\u6a21\u578b\uff09\uff0c\u6df1\u5165\u5206\u6790\u5f53\u524d\u6700\u5148\u8fdb\u7684AI\u65b9\u6cd5\uff0c\u5305\u62ec\u56fe\u795e\u7ecf\u7f51\u7edc\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6df7\u5408\u65b9\u6cd5\u7b49\u3002", "result": "\u8bc6\u522b\u51fa\u4e09\u5927\u4e3b\u8981\u8d8b\u52bf\uff1a\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u8111\u8fde\u63a5\u5efa\u6a21\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u6570\u636e\u5904\u7406\u4e2d\u5d1b\u8d77\uff0c\u591a\u6a21\u6001\u878d\u5408\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7b97\u6cd5\u516c\u5e73\u6027\u6210\u4e3a\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002\u540c\u65f6\u63d0\u4f9b\u4e86\u516c\u5171\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u7684\u5b9e\u7528\u6307\u5357\u3002", "conclusion": "\u672c\u7efc\u8ff0\u7efc\u5408\u4e86\u5f53\u524d\u8fdb\u5c55\u5e76\u7a81\u51fa\u5f00\u653e\u6311\u6218\uff0c\u4e3a\u8ba1\u7b97\u7cbe\u795e\u75c5\u5b66\u9886\u57df\u7684\u672a\u6765\u521b\u65b0\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8def\u7ebf\u56fe\uff0c\u63a8\u52a8\u6291\u90c1\u75c7\u8bca\u65ad\u5411\u66f4\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2508.11864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11864", "abs": "https://arxiv.org/abs/2508.11864", "authors": ["Yucheng Tang", "Pawel Rajwa", "Alexander Ng", "Yipei Wang", "Wen Yan", "Natasha Thorley", "Aqua Asif", "Clare Allen", "Louise Dickinson", "Francesco Giganti", "Shonit Punwani", "Daniel C. Alexander", "Veeru Kasivisvanathan", "Yipeng Hu"], "title": "Impact of Clinical Image Quality on Efficient Foundation Model Finetuning", "comment": null, "summary": "Foundation models in medical imaging have shown promising label efficiency,\nachieving high downstream performance with only a fraction of annotated data.\nHere, we evaluate this in prostate multiparametric MRI using ProFound, a\ndomain-specific vision foundation model pretrained on large-scale prostate MRI\ndatasets. We investigate how variable image quality affects label-efficient\nfinetuning by measuring the generalisability of finetuned models. Experiments\nsystematically vary high-/low-quality image ratios in finetuning and evaluation\nsets. Our findings indicate that image quality distribution and its\nfinetune-and-test mismatch significantly affect model performance. In\nparticular: a) Varying the ratio of high- to low-quality images between\nfinetuning and test sets leads to notable differences in downstream\nperformance; and b) The presence of sufficient high-quality images in the\nfinetuning set is critical for maintaining strong performance, whilst the\nimportance of matched finetuning and testing distribution varies between\ndifferent downstream tasks, such as automated radiology reporting and prostate\ncancer detection.When quality ratios are consistent, finetuning needs far less\nlabeled data than training from scratch, but label efficiency depends on image\nquality distribution. Without enough high-quality finetuning data, pretrained\nmodels may fail to outperform those trained without pretraining. This\nhighlights the importance of assessing and aligning quality distributions\nbetween finetuning and deployment, and the need for quality standards in\nfinetuning data for specific downstream tasks. Using ProFound, we show the\nvalue of quantifying image quality in both finetuning and deployment to fully\nrealise the data and compute efficiency benefits of foundation models.", "AI": {"tldr": "\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u5f62\u6001\u5b66\u4e2d\u663e\u793a\u4e86\u6807\u7b7e\u6548\u7387\u4f18\u52bf\uff0c\u4f46\u56fe\u50cf\u8d28\u91cf\u5206\u5e03\u548c\u5b66\u4e60-\u6d4b\u8bd5\u96c6\u7684\u8d28\u91cf\u5339\u914d\u5f88\u91cd\u8981\u3002\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u5145\u8db3\u6027\u5f3a\u523b\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u8d28\u91cf\u5206\u5e03\u4e0d\u4e00\u81f4\u4f1a\u5bfc\u81f4\u6027\u80fd\u5dee\u5f02\uff0c\u800c\u4e14\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u5bf9\u8d28\u91cf\u5339\u914d\u7684\u9700\u6c42\u4e0d\u540c\u3002", "motivation": "\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u5f62\u6001\u5b66\u4e2d\u7684\u6807\u7b7e\u6548\u7387\uff0c\u5e76\u7814\u7a76\u56fe\u50cf\u8d28\u91cf\u53d8\u5316\u5982\u4f55\u5f71\u54cd\u7cbe\u8c03\u6a21\u578b\u7684\u666e\u9002\u6027\u3002\u91cd\u70b9\u5173\u6ce8\u9ad8/\u4f4e\u8d28\u91cf\u56fe\u50cf\u6bd4\u4f8b\u5728\u5b66\u4e60\u548c\u6d4b\u8bd5\u96c6\u4e2d\u7684\u5339\u914d\u95ee\u9898\u3002", "method": "\u4f7f\u7528ProFound\u57fa\u7840\u6a21\u578b\uff08\u9886\u57df\u7279\u5b9a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u5728\u5927\u89c4\u6a21\u524d\u5217\u8170\u90e8MRI\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\uff09\u8fdb\u884c\u5b9e\u9a8c\u3002\u7cfb\u7edf\u6027\u53d8\u5316\u5b66\u4e60\u96c6\u548c\u8bc4\u4f30\u96c6\u4e2d\u9ad8/\u4f4e\u8d28\u91cf\u56fe\u50cf\u6bd4\u4f8b\uff0c\u6d4b\u91cf\u7cbe\u8c03\u6a21\u578b\u7684\u666e\u9002\u6027\u3002", "result": "\u56fe\u50cf\u8d28\u91cf\u5206\u5e03\u548c\u5b66\u4e60-\u6d4b\u8bd5\u96c6\u8d28\u91cf\u4e0d\u5339\u914d\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002\u5b66\u4e60\u96c6\u4e2d\u5145\u8db3\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u5bf9\u7ef4\u6301\u5f3a\u52b2\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u81ea\u52a8\u5316\u5f62\u6001\u5b66\u62a5\u544a\u548c\u524d\u5217\u8170\u764c\u68c0\u6d4b\uff09\u5bf9\u8d28\u91cf\u5339\u914d\u7684\u9700\u6c42\u4e0d\u540c\u3002\u8d28\u91cf\u6bd4\u4f8b\u4e00\u81f4\u65f6\uff0c\u7cbe\u8c03\u9700\u8981\u7684\u6807\u7b7e\u6570\u636e\u6bd4\u4ece\u5934\u8bad\u7ec3\u5c11\u5f97\u591a\uff0c\u4f46\u6807\u7b7e\u6548\u7387\u4f9d\u8d56\u4e8e\u56fe\u50cf\u8d28\u91cf\u5206\u5e03\u3002", "conclusion": "\u8d28\u91cf\u5206\u5e03\u7684\u8bc4\u4f30\u548c\u5bf9\u9f50\u5bf9\u4e8e\u5b8c\u5168\u5b9e\u73b0\u57fa\u7840\u6a21\u578b\u7684\u6570\u636e\u548c\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\u81f3\u5173\u91cd\u8981\u3002\u5982\u679c\u7cbe\u8c03\u6570\u636e\u4e2d\u6ca1\u6709\u5145\u8db3\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u8d85\u8fc7\u4ece\u5934\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u9700\u8981\u4e3a\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u5236\u5b9a\u7cbe\u8c03\u6570\u636e\u7684\u8d28\u91cf\u6807\u51c6\u3002"}}
{"id": "2508.12026", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12026", "abs": "https://arxiv.org/abs/2508.12026", "authors": ["Szymon Pawlonka", "Miko\u0142aj Ma\u0142ki\u0144ski", "Jacek Ma\u0144dziuk"], "title": "Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems", "comment": null, "summary": "Bongard Problems (BPs) provide a challenging testbed for abstract visual\nreasoning (AVR), requiring models to identify visual concepts fromjust a few\nexamples and describe them in natural language. Early BP benchmarks featured\nsynthetic black-and-white drawings, which might not fully capture the\ncomplexity of real-world scenes. Subsequent BP datasets employed real-world\nimages, albeit the represented concepts are identifiable from high-level image\nfeatures, reducing the task complexity. Differently, the recently released\nBongard-RWR dataset aimed at representing abstract concepts formulated in the\noriginal BPs using fine-grained real-world images. Its manual construction,\nhowever, limited the dataset size to just $60$ instances, constraining\nevaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset\ncomposed of $5\\,400$ instances that represent original BP abstract concepts\nusing real-world-like images generated via a vision language model (VLM)\npipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually\ncurated images and generate new descriptions aligned with the underlying\nconcepts, use Flux.1-dev to synthesize images from these descriptions, and\nmanually verify that the generated images faithfully reflect the intended\nconcepts. We evaluate state-of-the-art VLMs across diverse BP formulations,\nincluding binary and multiclass classification, as well as textual answer\ngeneration. Our findings reveal that while VLMs can recognize coarse-grained\nvisual concepts, they consistently struggle with discerning fine-grained\nconcepts, highlighting limitations in their reasoning capabilities.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Bongard-RWR+\u6570\u636e\u96c6\uff0c\u5305\u542b5400\u4e2a\u5b9e\u4f8b\uff0c\u4f7f\u7528VLM\u751f\u6210\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6765\u8868\u793a\u539f\u59cbBongard\u95ee\u9898\u7684\u62bd\u8c61\u6982\u5ff5\uff0c\u8bc4\u4f30\u53d1\u73b0VLM\u5728\u7ec6\u7c92\u5ea6\u6982\u5ff5\u8bc6\u522b\u4e0a\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u73b0\u6709\u7684Bongard\u95ee\u9898\u6570\u636e\u96c6\u8981\u4e48\u4f7f\u7528\u5408\u6210\u56fe\u50cf\u7f3a\u4e4f\u771f\u5b9e\u590d\u6742\u6027\uff0c\u8981\u4e48\u4f7f\u7528\u771f\u5b9e\u56fe\u50cf\u4f46\u6982\u5ff5\u8fc7\u4e8e\u7b80\u5355\uff0c\u4e14\u624b\u52a8\u6784\u5efa\u7684Bongard-RWR\u6570\u636e\u96c6\u89c4\u6a21\u592a\u5c0f\uff08\u4ec560\u4e2a\u5b9e\u4f8b\uff09\uff0c\u9650\u5236\u4e86\u8bc4\u4f30\u7684\u9c81\u68d2\u6027\u3002", "method": "\u57fa\u4e8eBongard-RWR\uff0c\u4f7f\u7528Pixtral-12B\u63cf\u8ff0\u624b\u52a8\u7b5b\u9009\u7684\u56fe\u50cf\u5e76\u751f\u6210\u65b0\u63cf\u8ff0\uff0c\u7528Flux.1-dev\u4ece\u63cf\u8ff0\u5408\u6210\u56fe\u50cf\uff0c\u624b\u52a8\u9a8c\u8bc1\u751f\u6210\u56fe\u50cf\u662f\u5426\u5fe0\u5b9e\u53cd\u6620\u76ee\u6807\u6982\u5ff5\uff0c\u6784\u5efa\u4e865400\u4e2a\u5b9e\u4f8b\u7684\u6570\u636e\u96c6\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u6700\u5148\u8fdb\u7684VLM\u80fd\u591f\u8bc6\u522b\u7c97\u7c92\u5ea6\u89c6\u89c9\u6982\u5ff5\uff0c\u4f46\u5728\u8fa8\u522b\u7ec6\u7c92\u5ea6\u6982\u5ff5\u65b9\u9762\u6301\u7eed\u5b58\u5728\u56f0\u96be\uff0c\u7a81\u663e\u4e86\u5176\u63a8\u7406\u80fd\u529b\u7684\u5c40\u9650\u6027\u3002", "conclusion": "Bongard-RWR+\u6570\u636e\u96c6\u4e3a\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u5f53\u524dVLM\u5728\u7ec6\u7c92\u5ea6\u6982\u5ff5\u7406\u89e3\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.11870", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11870", "abs": "https://arxiv.org/abs/2508.11870", "authors": ["Ying Huang", "Yuanbin Man", "Wenqi Jia", "Zhengzhong Tu", "Junzhou Huang", "Miao Yin"], "title": "AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition", "comment": null, "summary": "Adapter-based fine-tuning has gained remarkable attention in adapting large\npre-trained vision language models (VLMs) for a wide range of downstream tasks\nefficiently. In this paradigm, only the inserted adapters are fine-tuned,\nwithout the need for training the original VLM backbone. Existing works scale\nadapters by integrating them into every layer of VLMs to increase the capacity\nof adapters. However, these methods face two primary limitations: 1) limited\ncompression rate due to ignoring cross-layer redundancy, and 2) limited\nrepresentational capacity across homogeneous adapters. In this paper, we\npropose a novel vision-language fine-tuning framework based on cross-layer\ntensor ring decomposition (TRD) with the integration and collaboration of\ndiverse adapters, called AdaRing, achieving ultra-light parameter-efficient\nadaptation of VLMs on various tasks. To remove the high redundancy that exists\namong adapters across layers, we exploit the tensor-level low-rankness to\nformulate adapters as layer-shared tensor cores and layer-specific slices.\nMoreover, guided by generalization-aware fine-tuning, diverse rank-driven\nadapters cooperate to handle tasks that require different representations. Our\nexperiments show that the proposed AdaRing achieves the state-of-the-art\nperformance while reducing average training parameters by 90%.", "AI": {"tldr": "\u57fa\u4e8e\u5f20\u91cf\u73af\u5206\u89e3\u7684\u8de8\u5c42\u9002\u914d\u5668\u6846\u67b6AdaRing\uff0c\u901a\u8fc7\u5229\u7528\u5f20\u91cf\u4f4e\u79e9\u6027\u548c\u591a\u6837\u5316\u9002\u914d\u5668\u534f\u4f5c\uff0c\u5728\u51cf\u5c1190%\u53c2\u6570\u7684\u540c\u65f6\u8fbe\u5230\u6700\u4f73\u6027\u80fd", "motivation": "\u89e3\u51b3\u73b0\u6709\u9002\u914d\u5668\u65b9\u6cd5\u7684\u4e24\u5927\u95ee\u9898\uff1a1)\u8de8\u5c42\u5197\u4f59\u5bfc\u81f4\u538b\u7f29\u7387\u6709\u9650\uff1b2)\u540c\u8d28\u5316\u9002\u914d\u5668\u8868\u5f81\u80fd\u529b\u4e0d\u8db3", "method": "\u91c7\u7528\u5f20\u91cf\u73af\u5206\u89e3(TRD)\u628a\u9002\u914d\u5668\u6784\u9020\u4e3a\u5c42\u5171\u4eab\u7684\u5f20\u91cf\u6838\u548c\u5c42\u7279\u5f02\u5207\u7247\uff0c\u5e76\u5728\u6cdb\u5316\u611f\u77e5\u6307\u5bfc\u4e0b\u8ba9\u4e0d\u540c\u79e9\u7684\u9002\u914d\u5668\u534f\u540c\u5de5\u4f5c", "result": "\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u8fbe\u5230state-of-the-art\u6027\u80fd\uff0c\u5e73\u5747\u8bad\u7ec3\u53c2\u6570\u51cf\u5c1190%", "conclusion": "AdaRing\u6846\u67b6\u901a\u8fc7\u5229\u7528\u8de8\u5c42\u5197\u4f59\u548c\u591a\u6837\u5316\u9002\u914d\u5668\u534f\u4f5c\uff0c\u5b9e\u73b0\u4e86\u8d85\u8f7b\u91cf\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u9ad8\u6548\u8c03\u6574"}}
{"id": "2508.12027", "categories": ["cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.12027", "abs": "https://arxiv.org/abs/2508.12027", "authors": ["Filippo Torresan", "Keisuke Suzuki", "Ryota Kanai", "Manuel Baltieri"], "title": "Active inference for action-unaware agents", "comment": "59 pages, 47 figures", "summary": "Active inference is a formal approach to study cognition based on the notion\nthat adaptive agents can be seen as engaging in a process of approximate\nBayesian inference, via the minimisation of variational and expected free\nenergies. Minimising the former provides an account of perceptual processes and\nlearning as evidence accumulation, while minimising the latter describes how\nagents select their actions over time. In this way, adaptive agents are able to\nmaximise the likelihood of preferred observations or states, given a generative\nmodel of the environment. In the literature, however, different strategies have\nbeen proposed to describe how agents can plan their future actions. While they\nall share the notion that some kind of expected free energy offers an\nappropriate way to score policies, sequences of actions, in terms of their\ndesirability, there are different ways to consider the contribution of past\nmotor experience to the agent's future behaviour. In some approaches, agents\nare assumed to know their own actions, and use such knowledge to better plan\nfor the future. In other approaches, agents are unaware of their actions, and\nmust infer their motor behaviour from recent observations in order to plan for\nthe future. This difference reflects a standard point of departure in two\nleading frameworks in motor control based on the presence, or not, of an\nefference copy signal representing knowledge about an agent's own actions. In\nthis work we compare the performances of action-aware and action-unaware agents\nin two navigations tasks, showing how action-unaware agents can achieve\nperformances comparable to action-aware ones while at a severe disadvantage.", "AI": {"tldr": "\u4e3b\u52a8\u63a8\u65ad\u662f\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u63a8\u65ad\u7684\u8ba4\u77e5\u5f62\u5f0f\u63cf\u8ff0\uff0c\u672c\u6587\u6bd4\u8f83\u4e86\u52a8\u4f5c\u610f\u8bc6\u548c\u52a8\u4f5c\u65e0\u610f\u8bc6\u4ee3\u7406\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4e3b\u52a8\u63a8\u65ad\u9886\u57df\u4e2d\u5b58\u5728\u4e0d\u540c\u7684\u52a8\u4f5c\u89c4\u5212\u7b56\u7565\uff0c\u6709\u4e9b\u5047\u8bbe\u4ee3\u7406\u77e5\u9053\u81ea\u5df1\u7684\u52a8\u4f5c\uff0c\u800c\u6709\u4e9b\u5219\u9700\u8981\u4ece\u89c2\u5bdf\u4e2d\u63a8\u65ad\u52a8\u4f5c\u884c\u4e3a\u3002\u672c\u6587\u60f3\u8981\u6bd4\u8f83\u8fd9\u4e24\u7c7b\u4ee3\u7406\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5728\u4e24\u4e2a\u5bfc\u822a\u4efb\u52a1\u4e2d\u6bd4\u8f83\u52a8\u4f5c\u610f\u8bc6\u4ee3\u7406\u548c\u52a8\u4f5c\u65e0\u610f\u8bc6\u4ee3\u7406\u7684\u8868\u73b0\uff0c\u4ee5\u4e86\u89e3\u52a8\u4f5c\u77e5\u8bc6\u5bf9\u89c4\u5212\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u52a8\u4f5c\u65e0\u610f\u8bc6\u4ee3\u7406\u867d\u7136\u5904\u4e8e\u4e25\u91cd\u4e0d\u5229\u5730\u4f4d\uff0c\u4f46\u4ecd\u80fd\u8fbe\u5230\u4e0e\u52a8\u4f5c\u610f\u8bc6\u4ee3\u7406\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u8fd9\u4e00\u7ed3\u679c\u8868\u660e\u4e86\u4e3b\u52a8\u63a8\u65ad\u6846\u67b6\u4e2d\u4e0d\u540c\u52a8\u4f5c\u89c4\u5212\u7b56\u7565\u7684\u7070\u5ea6\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u7406\u89e3\u8ba4\u77e5\u8fc7\u7a0b\u4e2d\u7684\u52a8\u4f5c\u5904\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2508.11886", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.11886", "abs": "https://arxiv.org/abs/2508.11886", "authors": ["Wenhui Zhu", "Xiwen Chen", "Zhipeng Wang", "Shao Tang", "Sayan Ghosh", "Xuanzhao Dong", "Rajat Koner", "Yalin Wang"], "title": "EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models", "comment": null, "summary": "Instructed Visual Segmentation (IVS) tasks require segmenting objects in\nimages or videos based on natural language instructions. While recent\nmultimodal large language models (MLLMs) have achieved strong performance on\nIVS, their inference cost remains a major bottleneck, particularly in video. We\nempirically analyze visual token sampling in MLLMs and observe a strong\ncorrelation between subset token coverage and segmentation performance. This\nmotivates our design of a simple and effective token pruning method that\nselects a compact yet spatially representative subset of tokens to accelerate\ninference. In this paper, we introduce a novel visual token pruning method for\nIVS, called EVTP-IV, which builds upon the k-center by integrating spatial\ninformation to ensure better coverage. We further provide an\ninformation-theoretic analysis to support our design. Experiments on standard\nIVS benchmarks show that our method achieves up to 5X speed-up on video tasks\nand 3.5X on image tasks, while maintaining comparable accuracy using only 20%\nof the tokens. Our method also consistently outperforms state-of-the-art\npruning baselines under varying pruning ratios.", "AI": {"tldr": "\u901a\u8fc7\u7a7a\u95f4\u4fe1\u606f\u4f18\u5316\u7684\u89c6\u89c9token\u526a\u679d\u65b9\u6cd5EVTP-IV\uff0c\u5728\u6307\u4ee4\u5f0f\u89c6\u89c9\u5206\u5272\u4efb\u52a1\u4e2d\u5b9e\u73b05\u500d\u901f\u5ea6\u63d0\u5347\uff0c\u4fdd\u6301\u7c97\u7cd5\u7cbe\u5ea6", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u8981\u901a\u8fc7token\u526a\u679d\u6765\u52a0\u901f\u63a8\u7406", "method": "\u63d0\u51faEVTP-IV\u65b9\u6cd5\uff0c\u57fa\u4e8ek-center\u7b97\u6cd5\u96c6\u6210\u7a7a\u95f4\u4fe1\u606f\uff0c\u9009\u62e9\u7a7a\u95f4\u4ee3\u8868\u6027\u5f3a\u7684\u7b80\u7ea6token\u5b50\u96c6", "result": "\u5728\u6807\u51c6IVS\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e86\u89c6\u9891\u4efb\u52a15\u500d\u52a0\u901f\u3001\u56fe\u7247\u4efb\u52a13.5\u500d\u52a0\u901f\uff0c\u4ec5\u4f7f\u752820%token\u5374\u4fdd\u6301\u76f8\u5f53\u7684\u7cbe\u5ea6", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7a7a\u95f4\u4ee3\u8868\u6027token\u9009\u62e9\uff0c\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5728\u5404\u79cd\u526a\u679d\u6bd4\u4e0b\u90fd\u8d85\u8fc7\u73b0\u6709\u6700\u4f73\u65b9\u6cd5"}}
{"id": "2508.12087", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12087", "abs": "https://arxiv.org/abs/2508.12087", "authors": ["Zhanjiang Yang", "Meng Li", "Yang Shen", "Yueming Li", "Lijun Sun"], "title": "MAPF-World: Action World Model for Multi-Agent Path Finding", "comment": null, "summary": "Multi-agent path finding (MAPF) is the problem of planning conflict-free\npaths from the designated start locations to goal positions for multiple\nagents. It underlies a variety of real-world tasks, including multi-robot\ncoordination, robot-assisted logistics, and social navigation. Recent\ndecentralized learnable solvers have shown great promise for large-scale MAPF,\nespecially when leveraging foundation models and large datasets. However, these\nagents are reactive policy models and exhibit limited modeling of environmental\ntemporal dynamics and inter-agent dependencies, resulting in performance\ndegradation in complex, long-term planning scenarios. To address these\nlimitations, we propose MAPF-World, an autoregressive action world model for\nMAPF that unifies situation understanding and action generation, guiding\ndecisions beyond immediate local observations. It improves situational\nawareness by explicitly modeling environmental dynamics, including spatial\nfeatures and temporal dependencies, through future state and actions\nprediction. By incorporating these predicted futures, MAPF-World enables more\ninformed, coordinated, and far-sighted decision-making, especially in complex\nmulti-agent settings. Furthermore, we augment MAPF benchmarks by introducing an\nautomatic map generator grounded in real-world scenarios, capturing practical\nmap layouts for training and evaluating MAPF solvers. Extensive experiments\ndemonstrate that MAPF-World outperforms state-of-the-art learnable solvers,\nshowcasing superior zero-shot generalization to out-of-distribution cases.\nNotably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced\ndata.", "AI": {"tldr": "MAPF-World\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u7684\u81ea\u52a8\u56de\u5f52\u52a8\u4f5c\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u73af\u5883\u52a8\u6001\u548c\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u5b9e\u73b0\u4e86\u8d85\u8d8a\u5c40\u90e8\u89c2\u5bdf\u7684\u8fdc\u89c1\u51b3\u7b56\uff0c\u5728\u6a21\u578b\u5927\u5c0f\u51cf\u5c1196.5%\u548c\u6570\u636e\u51cf\u5c1192%\u7684\u60c5\u51b5\u4e0b\u4ecd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u6563\u5f0f\u53ef\u5b66\u4e60\u6c42\u89e3\u5668\u5728\u590d\u6742\u957f\u671f\u89c4\u5212\u573a\u666f\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u4e3b\u8981\u56e0\u4e3a\u7f3a\u4e4f\u5bf9\u73af\u5883\u65f6\u95f4\u52a8\u6001\u548c\u667a\u80fd\u4f53\u95f4\u4f9d\u8d56\u5173\u7cfb\u7684\u5145\u5206\u5efa\u6a21\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faMAPF-World\u81ea\u52a8\u56de\u5f52\u52a8\u4f5c\u4e16\u754c\u6a21\u578b\uff0c\u7edf\u4e00\u60c5\u5883\u7406\u89e3\u548c\u52a8\u4f5c\u751f\u6210\uff0c\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u72b6\u6001\u548c\u52a8\u4f5c\u6765\u663e\u5f0f\u5efa\u6a21\u7a7a\u95f4\u7279\u5f81\u548c\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u771f\u5b9e\u573a\u666f\u7684\u81ea\u52a8\u5730\u56fe\u751f\u6210\u5668\u6765\u589e\u5f3a\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eMAPF-World\u5728\u96f6\u6837\u672c\u6cdb\u5316\u5230\u5206\u5e03\u5916\u6848\u4f8b\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u53ef\u5b66\u4e60\u6c42\u89e3\u5668\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c1196.5%\uff0c\u6570\u636e\u9700\u6c42\u51cf\u5c1192%\u3002", "conclusion": "MAPF-World\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u73af\u5883\u52a8\u6001\u548c\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u5b9e\u73b0\u4e86\u66f4\u660e\u667a\u3001\u534f\u8c03\u548c\u8fdc\u89c1\u7684\u51b3\u7b56\uff0c\u7279\u522b\u9002\u7528\u4e8e\u590d\u6742\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\uff0c\u5c55\u793a\u4e86\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002"}}
{"id": "2508.11893", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.11893", "abs": "https://arxiv.org/abs/2508.11893", "authors": ["Quanwei Hu", "Yinggan Tang", "Xuguang Zhang"], "title": "Large Kernel Modulation Network for Efficient Image Super-Resolution", "comment": null, "summary": "Image super-resolution (SR) in resource-constrained scenarios demands\nlightweight models balancing performance and latency. Convolutional neural\nnetworks (CNNs) offer low latency but lack non-local feature capture, while\nTransformers excel at non-local modeling yet suffer slow inference. To address\nthis trade-off, we propose the Large Kernel Modulation Network (LKMN), a pure\nCNN-based model. LKMN has two core components: Enhanced Partial Large Kernel\nBlock (EPLKB) and Cross-Gate Feed-Forward Network (CGFN). The EPLKB utilizes\nchannel shuffle to boost inter-channel interaction, incorporates channel\nattention to focus on key information, and applies large kernel strip\nconvolutions on partial channels for non-local feature extraction with reduced\ncomplexity. The CGFN dynamically adjusts discrepancies between input, local,\nand non-local features via a learnable scaling factor, then employs a\ncross-gate strategy to modulate and fuse these features, enhancing their\ncomplementarity. Extensive experiments demonstrate that our method outperforms\nexisting state-of-the-art (SOTA) lightweight SR models while balancing quality\nand efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over\nDAT-light on the Manga109 dataset at $\\times$4 upscale, with nearly $\\times$4.8\ntimes faster. Codes are in the supplementary materials. The code is available\nat https://github.com/Supereeeee/LKMN.", "AI": {"tldr": "\u4e00\u79cd\u7eafCNN\u7684\u8f7b\u91cf\u7ea7\u56fe\u50cf\u8d85\u5206\u8fa8\u6a21\u578bLKMN\uff0c\u901a\u8fc7\u5927\u5185\u6838\u8c03\u5236\u7ed3\u6784\u5728\u4fdd\u6301\u9ad8\u6548\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u8d28\u91cf\uff0c\u5728Manga109\u6570\u636e\u96c6\u4e0a\u6bd4SOTA\u6a21\u578b\u63d0\u53470.23dB PSNR\u4e14\u901f\u5ea6\u63d0\u53474.8\u500d", "motivation": "\u89e3\u51b3\u8f7b\u91cf\u7ea7\u56fe\u50cf\u8d85\u5206\u8fa8\u4e2dCNN\u6a21\u578b\u7f3a\u4e4f\u975e\u5c40\u90e8\u7279\u5f81\u6293\u53d6\u80fd\u529b\u800cTransformer\u6a21\u578b\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u5c1d\u8bd5\u5728CNN\u6846\u67b6\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u975e\u5c40\u90e8\u5efa\u6a21", "method": "\u63d0\u51faLKMN\u6a21\u578b\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1aEPLKB\u6a21\u5757\u901a\u8fc7\u9891\u9053\u6df7\u6dc6\u3001\u9891\u9053\u6ce8\u610f\u529b\u548c\u5927\u5185\u6838\u5377\u79ef\u63d0\u53d6\u975e\u5c40\u90e8\u7279\u5f81\uff0cCGFN\u6a21\u5757\u901a\u8fc7\u4ea4\u53c9\u95e8\u63a7\u7b56\u52a8\u6001\u878d\u5408\u5c40\u90e8\u548c\u975e\u5c40\u90e8\u7279\u5f81", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709SOTA\u8f7b\u91cf\u7ea7SR\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728Manga109\u6570\u636e\u96c6\u4e0a\u6bd4DAT-light\u63d0\u53470.23dB PSNR\uff0c\u901f\u5ea6\u63d0\u53474.8\u500d", "conclusion": "LKMN\u6a21\u578b\u6210\u529f\u5728\u7eafCNN\u6846\u67b6\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u975e\u5c40\u90e8\u7279\u5f81\u6293\u53d6\uff0c\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4f18\u5f02\u5e73\u8861\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12100", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12100", "abs": "https://arxiv.org/abs/2508.12100", "authors": ["Daniel Burkhardt", "Xiangwei Cheng"], "title": "Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios", "comment": "13 pages, 1 figure, 6 tables", "summary": "Reasoning in interactive problem solving scenarios requires models to\nconstruct reasoning threads that reflect user understanding and align with\nstructured domain knowledge. However, current reasoning models often lack\nexplicit semantic hierarchies, user-domain knowledge alignment, and principled\nmechanisms to prune reasoning threads for effectiveness. These limitations\nresult in lengthy generic output that does not guide users through\ngoal-oriented reasoning steps. To address this, we propose a\nprototype-inspired, two-phases Reasoning-Threads-Evaluation (ReT-Eval)\nframework, drawing inspiration from human-like reasoning strategies that\nemphasize structured knowledge reuse. In the first phase, semantically relevant\nknowledge structures are extracted from a sparse domain knowledge graph using a\ngraph neural network and enriched with intrinsic large language model knowledge\nto resolve knowledge discrepancies. In the second phase, these threads are\nevaluated and pruned using a reward-guided strategy aimed at maintaining\nsemantic coherence to generate effective reasoning threads. Experiments and\nexpert evaluations show that ReT-Eval enhances user understanding and\noutperforms state-of-the-art reasoning models.", "AI": {"tldr": "\u63d0\u51fa\u4e86ReT-Eval\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u4ece\u7a00\u758f\u77e5\u8bc6\u56fe\u8c31\u4e2d\u63d0\u53d6\u8bed\u4e49\u76f8\u5173\u77e5\u8bc6\u7ed3\u6784\uff0c\u5e76\u4f7f\u7528\u5956\u52b1\u5f15\u5bfc\u7b56\u7565\u4fee\u526a\u63a8\u7406\u7ebf\u7a0b\uff0c\u4ee5\u751f\u6210\u6709\u6548\u7684\u76ee\u6807\u5bfc\u5411\u63a8\u7406", "motivation": "\u5f53\u524d\u63a8\u7406\u6a21\u578b\u7f3a\u4e4f\u663e\u5f0f\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\u3001\u7528\u6237-\u9886\u57df\u77e5\u8bc6\u5bf9\u9f50\uff0c\u4ee5\u53ca\u4fee\u526a\u63a8\u7406\u7ebf\u7a0b\u7684\u539f\u5219\u6027\u673a\u5236\uff0c\u5bfc\u81f4\u8f93\u51fa\u5197\u957f\u4e14\u65e0\u6cd5\u6709\u6548\u6307\u5bfc\u7528\u6237\u8fdb\u884c\u76ee\u6807\u5bfc\u5411\u63a8\u7406", "method": "\u539f\u578b\u542f\u53d1\u7684\u4e24\u9636\u6bb5\u63a8\u7406\u7ebf\u7a0b\u8bc4\u4f30\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u4ece\u7a00\u758f\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u4e2d\u63d0\u53d6\u8bed\u4e49\u76f8\u5173\u77e5\u8bc6\u7ed3\u6784\uff0c\u5e76\u7528\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u4e30\u5bcc\u4ee5\u89e3\u51b3\u77e5\u8bc6\u5dee\u5f02\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u5956\u52b1\u5f15\u5bfc\u7b56\u7565\u8bc4\u4f30\u548c\u4fee\u526a\u7ebf\u7a0b\u4ee5\u4fdd\u6301\u8bed\u4e49\u8fde\u8d2f\u6027", "result": "\u5b9e\u9a8c\u548c\u4e13\u5bb6\u8bc4\u4f30\u8868\u660e\uff0cReT-Eval\u589e\u5f3a\u4e86\u7528\u6237\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b", "conclusion": "ReT-Eval\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u77e5\u8bc6\u91cd\u7528\u548c\u6709\u6548\u7684\u7ebf\u7a0b\u4fee\u526a\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5f53\u524d\u63a8\u7406\u6a21\u578b\u5728\u4ea4\u4e92\u5f0f\u95ee\u9898\u89e3\u51b3\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027"}}
{"id": "2508.11902", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11902", "abs": "https://arxiv.org/abs/2508.11902", "authors": ["Azam Nouri"], "title": "A Sobel-Gradient MLP Baseline for Handwritten Character Recognition", "comment": "This paper is under consideration at Pattern Recognition Letters", "summary": "We revisit the classical Sobel operator to ask a simple question: Are\nfirst-order edge maps sufficient to drive an all-dense multilayer perceptron\n(MLP) for handwritten character recognition (HCR), as an alternative to\nconvolutional neural networks (CNNs)? Using only horizontal and vertical Sobel\nderivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its\nextreme simplicity, the resulting network reaches 98% accuracy on MNIST digits\nand 92% on EMNIST letters -- approaching CNNs while offering a smaller memory\nfootprint and transparent features. Our findings highlight that much of the\nclass-discriminative information in handwritten character images is already\ncaptured by first-order gradients, making edge-aware MLPs a compelling option\nfor HCR.", "AI": {"tldr": "\u4f7f\u7528\u4ec5\u5305\u542b\u6c34\u5e73\u548c\u5782\u76f4Sobel\u5bfc\u6570\u4f5c\u4e3a\u8f93\u5165\u7684MLP\u7f51\u7edc\uff0c\u5728\u624b\u5199\u5b57\u7b26\u8bc6\u522b\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u63a5\u8fd1CNN\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e00\u9636\u68af\u5ea6\u5df2\u5305\u542b\u8db3\u591f\u7684\u5206\u7c7b\u4fe1\u606f", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u7ecf\u5178Sobel\u7b97\u5b50\uff0c\u63a2\u7d22\u662f\u5426\u4ec5\u4f7f\u7528\u4e00\u9636\u8fb9\u7f18\u56fe\u5c31\u80fd\u9a71\u52a8MLP\u7f51\u7edc\u8fdb\u884c\u624b\u5199\u5b57\u7b26\u8bc6\u522b\uff0c\u4f5c\u4e3aCNN\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u4ec5\u4f7f\u7528\u6c34\u5e73\u548c\u5782\u76f4Sobel\u5bfc\u6570\u4f5c\u4e3a\u8f93\u5165\uff0c\u8bad\u7ec3\u5168\u8fde\u63a5\u591a\u5c42\u611f\u77e5\u673a(MLP)\u5728MNIST\u548cEMNIST Letters\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u624b\u5199\u5b57\u7b26\u8bc6\u522b", "result": "\u5728MNIST\u6570\u5b57\u4e0a\u8fbe\u523098%\u51c6\u786e\u7387\uff0c\u5728EMNIST\u5b57\u6bcd\u4e0a\u8fbe\u523092%\u51c6\u786e\u7387\uff0c\u63a5\u8fd1CNN\u6027\u80fd\u4f46\u5177\u6709\u66f4\u5c0f\u7684\u5185\u5b58\u5360\u7528\u548c\u900f\u660e\u7279\u5f81", "conclusion": "\u624b\u5199\u5b57\u7b26\u56fe\u50cf\u4e2d\u7684\u5927\u90e8\u5206\u7c7b\u522b\u533a\u5206\u4fe1\u606f\u5df2\u7ecf\u5305\u542b\u5728\u4e00\u9636\u68af\u5ea6\u4e2d\uff0c\u8fb9\u7f18\u611f\u77e5\u7684MLP\u662fHCR\u7684\u4e00\u4e2a\u6709\u5438\u5f15\u529b\u7684\u9009\u62e9"}}
{"id": "2508.12149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12149", "abs": "https://arxiv.org/abs/2508.12149", "authors": ["Haochen You", "Baojing Liu"], "title": "MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization", "comment": "Accepted as a conference paper at CIKM 2025", "summary": "Recent advances in multimodal learning have largely relied on pairwise\ncontrastive objectives to align different modalities, such as text, video, and\naudio, in a shared embedding space. While effective in bi-modal setups, these\napproaches struggle to generalize across multiple modalities and often lack\nsemantic structure in high-dimensional spaces. In this paper, we propose MOVER,\na novel framework that combines optimal transport-based soft alignment with\nvolume-based geometric regularization to build semantically aligned and\nstructured multimodal representations. By integrating a transport-guided\nmatching mechanism with a geometric volume minimization objective (GAVE), MOVER\nencourages consistent alignment across all modalities in a modality-agnostic\nmanner. Experiments on text-video-audio retrieval tasks demonstrate that MOVER\nsignificantly outperforms prior state-of-the-art methods in both zero-shot and\nfinetuned settings. Additional analysis shows improved generalization to unseen\nmodality combinations and stronger structural consistency in the learned\nembedding space.", "AI": {"tldr": "MOVER\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u6700\u4f18\u4f20\u8f93\u8f6f\u5bf9\u9f50\u548c\u51e0\u4f55\u4f53\u79ef\u6b63\u5219\u5316\uff0c\u5728\u6587\u672c-\u89c6\u9891-\u97f3\u9891\u68c0\u7d22\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5728\u53cc\u6a21\u6001\u8bbe\u7f6e\u4e2d\u6709\u6548\uff0c\u4f46\u96be\u4ee5\u6269\u5c55\u5230\u591a\u6a21\u6001\u573a\u666f\uff0c\u4e14\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7f3a\u4e4f\u8bed\u4e49\u7ed3\u6784", "method": "\u7ed3\u5408\u6700\u4f18\u4f20\u8f93\u7684\u8f6f\u5bf9\u9f50\u673a\u5236\u548c\u51e0\u4f55\u4f53\u79ef\u6700\u5c0f\u5316\u76ee\u6807(GAVE)\uff0c\u4ee5\u6a21\u6001\u65e0\u5173\u7684\u65b9\u5f0f\u5b9e\u73b0\u8de8\u6240\u6709\u6a21\u6001\u7684\u4e00\u81f4\u5bf9\u9f50", "result": "\u5728\u6587\u672c-\u89c6\u9891-\u97f3\u9891\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0cMOVER\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u90fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u66f4\u5f3a\u7684\u7ed3\u6784\u4e00\u81f4\u6027", "conclusion": "MOVER\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u548c\u51e0\u4f55\u6b63\u5219\u5316\u7684\u7ed3\u5408\uff0c\u6210\u529f\u6784\u5efa\u4e86\u8bed\u4e49\u5bf9\u9f50\u4e14\u7ed3\u6784\u5316\u7684\u591a\u6a21\u6001\u8868\u793a\uff0c\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u65b9\u6cd5"}}
{"id": "2508.11903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11903", "abs": "https://arxiv.org/abs/2508.11903", "authors": ["Runhao Zeng", "Jiaqi Mao", "Minghao Lai", "Minh Hieu Phan", "Yanjie Dong", "Wei Wang", "Qi Chen", "Xiping Hu"], "title": "OVG-HQ: Online Video Grounding with Hybrid-modal Queries", "comment": "Accepted to ICCV 2025", "summary": "Video grounding (VG) task focuses on locating specific moments in a video\nbased on a query, usually in text form. However, traditional VG struggles with\nsome scenarios like streaming video or queries using visual cues. To fill this\ngap, we present a new task named Online Video Grounding with Hybrid-modal\nQueries (OVG-HQ), which enables online segment localization using text, images,\nvideo segments, and their combinations. This task poses two new challenges:\nlimited context in online settings and modality imbalance during training,\nwhere dominant modalities overshadow weaker ones. To address these, we propose\nOVG-HQ-Unify, a unified framework featuring a Parametric Memory Block (PMB)\nthat retain previously learned knowledge to enhance current decision and a\ncross-modal distillation strategy that guides the learning of non-dominant\nmodalities. This design enables a single model to effectively handle\nhybrid-modal queries. Due to the lack of suitable datasets, we construct\nQVHighlights-Unify, an expanded dataset with multi-modal queries. Besides,\nsince offline metrics overlook prediction timeliness, we adapt them to the\nonline setting, introducing oR@n, IoU=m, and online mean Average Precision\n(omAP) to evaluate both accuracy and efficiency. Experiments show that our\nOVG-HQ-Unify outperforms existing models, offering a robust solution for\nonline, hybrid-modal video grounding. Source code and datasets are available at\nhttps://github.com/maojiaqi2324/OVG-HQ.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u89c6\u9891\u57fa\u51c6\u4efb\u52a1OVG-HQ\uff0c\u652f\u6301\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u7b49\u6df7\u5408\u6a21\u6001\u67e5\u8be2\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u89c6\u9891\u57fa\u51c6\u5728\u6d41\u5f0f\u89c6\u9891\u548c\u89c6\u89c9\u7b26\u53f7\u67e5\u8be2\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u57fa\u51c6\u4efb\u52a1\u5728\u6d41\u5f0f\u89c6\u9891\u548c\u4f7f\u7528\u89c6\u89c9\u7b26\u53f7\u7684\u67e5\u8be2\u573a\u666f\u4e2d\u9047\u5230\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u7ebf\u5904\u7406\u6df7\u5408\u6a21\u6001\u67e5\u8be2\u7684\u65b9\u6848\u3002", "method": "\u63d0\u51faOVG-HQ-Unify\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u53c2\u6570\u5316\u8bb0\u5fc6\u5757(PMB)\u6765\u4fdd\u7559\u5b66\u4e60\u77e5\u8bc6\uff0c\u4ee5\u53ca\u8de8\u6a21\u6001\u835f\u84c9\u7b56\u7565\u6765\u5e73\u8861\u4e0d\u540c\u6a21\u6001\u7684\u5b66\u4e60\u6548\u679c\u3002\u6784\u5efa\u4e86QVHighlights-Unify\u6570\u636e\u96c6\u548c\u65b0\u7684\u5728\u7ebf\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aOVG-HQ-Unify\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u8d85\u8fc7\u73b0\u6709\u6a21\u578b\uff0c\u4e3a\u5728\u7ebf\u6df7\u5408\u6a21\u6001\u89c6\u9891\u57fa\u51c6\u63d0\u4f9b\u4e86\u5065\u58ee\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u89e3\u51b3\u4e86\u5728\u7ebf\u89c6\u9891\u57fa\u51c6\u4e2d\u7684\u6a21\u6001\u4e0d\u5e73\u8861\u548c\u4e0a\u4e0b\u6587\u9650\u5236\u95ee\u9898\uff0c\u4e3a\u5904\u7406\u6df7\u5408\u6a21\u6001\u67e5\u8be2\u7684\u5b9e\u65f6\u89c6\u9891\u5206\u6790\u5f00\u542f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.12165", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12165", "abs": "https://arxiv.org/abs/2508.12165", "authors": ["Rohit Krishnan", "Jon Evans"], "title": "RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards", "comment": null, "summary": "This paper introduces RLNVR (Reinforcement Learning from Non-Verified\nRewards), a framework for training language models using noisy, real-world\nfeedback signals without requiring explicit human verification. Traditional\nRLHF requires expensive, verified reward signals that are impractical in many\nreal-world domains. RLNVR addresses this challenge through baseline\nnormalization and semantic similarity-based reward transfer. We demonstrate\nRLNVR through Walter, a prototype system that optimizes social media content\ngeneration using actual engagement data from Bluesky. Our experimental results\nshow significant improvements in content quality and training stability, with\ncomprehensive evaluation planned for future work. Positioning: We present a\npractical framework that combines RLNVR with GSPO (Group Sequence Policy\nOptimization) and an optional UED (Unsupervised Environment Design) curriculum\nto improve stability and diversity under noisy, implicit rewards. To our\nknowledge, combining GSPO-style normalization with a UED-style curriculum for\nLLM content generation from implicit social engagement has not been previously\ndocumented in this applied setting; we frame this as an applied integration\nrather than a new algorithm.", "AI": {"tldr": "RLNVR\u6846\u67b6\u4f7f\u7528\u975e\u9a8c\u8bc1\u5956\u52b1\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u57fa\u7ebf\u5f52\u4e00\u5316\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u5956\u52b1\u8f6c\u79fb\u5904\u7406\u566a\u58f0\u53cd\u9988\uff0c\u5728\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u751f\u6210\u4e2d\u5c55\u73b0\u663e\u8457\u6539\u8fdb", "motivation": "\u4f20\u7edfRLHF\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u9a8c\u8bc1\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u4e0d\u5b9e\u7528\uff0c\u9700\u8981\u5904\u7406\u566a\u58f0\u3001\u975e\u9a8c\u8bc1\u7684\u5b9e\u65f6\u53cd\u9988\u4fe1\u53f7", "method": "\u7ed3\u5408\u57fa\u7ebf\u5f52\u4e00\u5316\u3001\u8bed\u4e49\u76f8\u4f3c\u6027\u5956\u52b1\u8f6c\u79fb\u3001GSPO\u7b56\u7565\u4f18\u5316\u548c\u53ef\u9009UED\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5904\u7406\u566a\u58f0\u9690\u5f0f\u5956\u52b1", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5185\u5bb9\u8d28\u91cf\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u663e\u8457\u63d0\u5347\uff0c\u4f7f\u7528Bluesky\u5b9e\u9645\u4e92\u52a8\u6570\u636e\u8fdb\u884c\u539f\u578b\u9a8c\u8bc1", "conclusion": "\u63d0\u51fa\u4e86\u5b9e\u7528\u7684RLNVR\u6846\u67b6\uff0c\u6210\u529f\u6574\u5408\u73b0\u6709\u6280\u672f\u5904\u7406\u975e\u9a8c\u8bc1\u5956\u52b1\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u63d0\u4f9b\u53ef\u884c\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.11904", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11904", "abs": "https://arxiv.org/abs/2508.11904", "authors": ["Lingyun Zhang", "Yu Xie", "Yanwei Fu", "Ping Chen"], "title": "SafeCtrl: Region-Based Safety Control for Text-to-Image Diffusion via Detect-Then-Suppress", "comment": null, "summary": "The widespread deployment of text-to-image models is challenged by their\npotential to generate harmful content. While existing safety methods, such as\nprompt rewriting or model fine-tuning, provide valuable interventions, they\noften introduce a trade-off between safety and fidelity. Recent\nlocalization-based approaches have shown promise, yet their reliance on\nexplicit ``concept replacement\" can sometimes lead to semantic incongruity. To\naddress these limitations, we explore a more flexible detect-then-suppress\nparadigm. We introduce SafeCtrl, a lightweight, non-intrusive plugin that first\nprecisely localizes unsafe content. Instead of performing a hard A-to-B\nsubstitution, SafeCtrl then suppresses the harmful semantics, allowing the\ngenerative process to naturally and coherently resolve into a safe,\ncontext-aware alternative. A key aspect of our work is a novel training\nstrategy using Direct Preference Optimization (DPO). We leverage readily\navailable, image-level preference data to train our module, enabling it to\nlearn nuanced suppression behaviors and perform region-guided interventions at\ninference without requiring costly, pixel-level annotations. Extensive\nexperiments show that SafeCtrl significantly outperforms state-of-the-art\nmethods in both safety efficacy and fidelity preservation. Our findings suggest\nthat decoupled, suppression-based control is a highly effective and scalable\ndirection for building more responsible generative models.", "AI": {"tldr": "SafeCtrl\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u975e\u4fb5\u5165\u5f0f\u63d2\u4ef6\uff0c\u901a\u8fc7\u68c0\u6d4b-\u6291\u5236\u8303\u5f0f\u6765\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u6709\u6548\u6291\u5236\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u65b9\u6cd5\uff08\u5982\u63d0\u793a\u91cd\u5199\u6216\u6a21\u578b\u5fae\u8c03\uff09\u5728\u5b89\u5168\u6027\u548c\u4fdd\u771f\u5ea6\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u57fa\u4e8e\u5b9a\u4f4d\u7684\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u6982\u5ff5\u66ff\u6362\u53ef\u80fd\u5bfc\u81f4\u8bed\u4e49\u4e0d\u8fde\u8d2f\u3002", "method": "\u4f7f\u7528Direct Preference Optimization\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528\u56fe\u50cf\u7ea7\u504f\u597d\u6570\u636e\u8bad\u7ec3\u6a21\u5757\uff0c\u5b9e\u73b0\u7cbe\u786e\u5b9a\u4f4d\u6709\u5bb3\u5185\u5bb9\u5e76\u8fdb\u884c\u8bed\u4e49\u6291\u5236\u800c\u975e\u786c\u66ff\u6362\u3002", "result": "\u5728\u5b89\u5168\u6709\u6548\u6027\u548c\u4fdd\u771f\u5ea6\u4fdd\u6301\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u89e3\u8026\u7684\u57fa\u4e8e\u6291\u5236\u7684\u63a7\u5236\u662f\u6784\u5efa\u66f4\u8d1f\u8d23\u4efb\u751f\u6210\u6a21\u578b\u7684\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u65b9\u5411\u3002"}}
{"id": "2508.12260", "categories": ["cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.12260", "abs": "https://arxiv.org/abs/2508.12260", "authors": ["Carson Dudley", "Reiden Magdaleno", "Christopher Harding", "Ananya Sharma", "Emily Martin", "Marisa Eisenberg"], "title": "Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting", "comment": "10 pages, 4 figures", "summary": "Infectious disease forecasting in novel outbreaks or low resource settings\nhas been limited by the need for disease-specific data, bespoke training, and\nexpert tuning. We introduce Mantis, a foundation model trained entirely on\nmechanistic simulations, which enables out-of-the-box forecasting across\ndiseases, regions, and outcomes, even in settings with limited historical data.\nMantis is built on over 400 million simulated days of outbreak dynamics\nspanning diverse pathogens, transmission modes, interventions, and surveillance\nartifacts. Despite requiring no real-world data during training, Mantis\noutperformed 39 expert-tuned models we tested across six diseases, including\nall models in the CDC's COVID-19 Forecast Hub. Mantis generalized to novel\nepidemiological regimes, including diseases with held-out transmission\nmechanisms, demonstrating that it captures fundamental contagion dynamics.\nCritically, Mantis is mechanistically interpretable, enabling public health\ndecision-makers to identify the latent drivers behind its predictions. Finally,\nMantis delivers accurate forecasts at 8-week horizons, more than doubling the\nactionable range of most models, enabling proactive public health planning.\nTogether, these capabilities position Mantis as a foundation for\nnext-generation disease forecasting systems: general, interpretable, and\ndeployable where traditional models fail.", "AI": {"tldr": "Mantis\u662f\u4e00\u4e2a\u57fa\u4e8e\u673a\u5236\u6a21\u62df\u8bad\u7ec3\u7684\u4f20\u67d3\u75c5\u9884\u6d4b\u57fa\u7840\u6a21\u578b\uff0c\u65e0\u9700\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u5c31\u80fd\u5728\u591a\u79cd\u75be\u75c5\u548c\u5730\u533a\u8fdb\u884c\u51c6\u786e\u9884\u6d4b\uff0c\u6027\u80fd\u8d85\u8d8a39\u4e2a\u4e13\u5bb6\u8c03\u4f18\u6a21\u578b", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4f20\u67d3\u75c5\u9884\u6d4b\u6a21\u578b\u9700\u8981\u75be\u75c5\u7279\u5b9a\u6570\u636e\u3001\u4e13\u95e8\u8bad\u7ec3\u548c\u4e13\u5bb6\u8c03\u4f18\u7684\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u65b0\u53d1\u75ab\u60c5\u6216\u8d44\u6e90\u532e\u4e4f\u5730\u533a\u7684\u9884\u6d4b\u96be\u9898", "method": "\u57fa\u4e8e\u8d85\u8fc74\u4ebf\u5929\u7206\u53d1\u52a8\u6001\u7684\u673a\u5236\u6a21\u62df\u8bad\u7ec3\uff0c\u6db5\u76d6\u591a\u79cd\u75c5\u539f\u4f53\u3001\u4f20\u64ad\u65b9\u5f0f\u3001\u5e72\u9884\u63aa\u65bd\u548c\u76d1\u6d4b\u4f2a\u5f71\uff0c\u65e0\u9700\u771f\u5b9e\u4e16\u754c\u6570\u636e", "result": "\u5728\u516d\u79cd\u75be\u75c5\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u6240\u670939\u4e2a\u4e13\u5bb6\u8c03\u4f18\u6a21\u578b\uff0c\u5305\u62ecCDC COVID-19\u9884\u6d4b\u4e2d\u5fc3\u7684\u6240\u6709\u6a21\u578b\uff0c\u80fd\u6cdb\u5316\u5230\u65b0\u7684\u6d41\u884c\u75c5\u5b66\u673a\u5236\uff0c\u63d0\u4f9b8\u5468\u9884\u6d4b\u8303\u56f4", "conclusion": "Mantis\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u75be\u75c5\u9884\u6d4b\u7cfb\u7edf\u7684\u57fa\u7840\uff0c\u5177\u6709\u901a\u7528\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5728\u4f20\u7edf\u6a21\u578b\u5931\u8d25\u573a\u666f\u4e0b\u7684\u90e8\u7f72\u80fd\u529b"}}
{"id": "2508.11919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11919", "abs": "https://arxiv.org/abs/2508.11919", "authors": ["Pallavi Jain", "Diego Marcos", "Dino Ienco", "Roberto Interdonato", "Tristan Berchoux"], "title": "TimeSenCLIP: A Vision-Language Model for Remote Sensing Using Single-Pixel Time Series", "comment": "Paper under review", "summary": "Vision-language models have shown significant promise in remote sensing\napplications, particularly for land-use and land-cover (LULC) via zero-shot\nclassification and retrieval. However, current approaches face two key\nchallenges: reliance on large spatial tiles that increase computational cost,\nand dependence on text-based supervision, which is often not readily available.\nIn this work, we present TimeSenCLIP, a lightweight framework that reevaluate\nthe role of spatial context by evaluating the effectiveness of a single pixel\nby leveraging its temporal and spectral dimensions, for classifying LULC and\necosystem types. By leveraging spectral and temporal information from\nSentinel-2 imagery and cross-view learning with geo-tagged ground-level photos,\nwe minimises the need for caption-based training while preserving semantic\nalignment between overhead (satellite) and ground perspectives. Our approach is\ngrounded in the LUCAS and Sen4Map datasets, and evaluated on classification\ntasks including LULC, crop type, and ecosystem type. We demonstrate that single\npixel inputs, when combined with temporal and spectral cues, are sufficient for\nthematic mapping, offering a scalable and efficient alternative for large-scale\nremote sensing applications. Code is available at\nhttps://github.com/pallavijain-pj/TimeSenCLIP", "AI": {"tldr": "TimeSenCLIP\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5355\u4e2a\u50cf\u7d20\u7684\u65f6\u5e8f\u548c\u5149\u8c31\u4fe1\u606f\u8fdb\u884c\u571f\u5730\u5229\u7528\u5206\u7c7b\uff0c\u51cf\u5c11\u4e86\u5bf9\u5927\u7a7a\u95f4\u74e6\u7247\u548c\u6587\u672c\u76d1\u7763\u7684\u4f9d\u8d56\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9065\u611f\u5e94\u7528\u4e2d\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u4f9d\u8d56\u5927\u7a7a\u95f4\u74e6\u7247\u589e\u52a0\u8ba1\u7b97\u6210\u672c\uff0c\u4ee5\u53ca\u4f9d\u8d56\u6587\u672c\u76d1\u7763\u4f46\u6587\u672c\u6570\u636e\u5f80\u5f80\u4e0d\u6613\u83b7\u5f97\u3002", "method": "\u5229\u7528Sentinel-2\u5f71\u50cf\u7684\u5149\u8c31\u548c\u65f6\u5e8f\u4fe1\u606f\uff0c\u901a\u8fc7\u4e0e\u5730\u7406\u6807\u8bb0\u7684\u5730\u9762\u7167\u7247\u8fdb\u884c\u8de8\u89c6\u89d2\u5b66\u4e60\uff0c\u6700\u5c0f\u5316\u57fa\u4e8e\u6807\u9898\u7684\u8bad\u7ec3\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u536b\u661f\u548c\u5730\u9762\u89c6\u89d2\u4e4b\u95f4\u7684\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5355\u4e2a\u50cf\u7d20\u8f93\u5165\u7ed3\u5408\u65f6\u5e8f\u548c\u5149\u8c31\u7ebf\u7d22\u8db3\u4ee5\u8fdb\u884c\u4e13\u9898\u5236\u56fe\uff0c\u4e3a\u5927\u89c4\u6a21\u9065\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "TimeSenCLIP\u5c55\u793a\u4e86\u5728\u571f\u5730\u5229\u7528\u5206\u7c7b\u4e2d\uff0c\u65f6\u5e8f\u548c\u5149\u8c31\u4fe1\u606f\u53ef\u4ee5\u66ff\u4ee3\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u8f7b\u91cf\u7ea7\u548c\u9ad8\u6548\u7684\u9065\u611f\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2508.12291", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12291", "abs": "https://arxiv.org/abs/2508.12291", "authors": ["Xuming He", "Zhiyuan You", "Junchao Gong", "Couhua Liu", "Xiaoyu Yue", "Peiqin Zhuang", "Wenlong Zhang", "Lei Bai"], "title": "RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts", "comment": null, "summary": "Quality analysis of weather forecasts is an essential topic in meteorology.\nAlthough traditional score-based evaluation metrics can quantify certain\nforecast errors, they are still far from meteorological experts in terms of\ndescriptive capability, interpretability, and understanding of dynamic\nevolution. With the rapid development of Multi-modal Large Language Models\n(MLLMs), these models become potential tools to overcome the above challenges.\nIn this work, we introduce an MLLM-based weather forecast analysis method,\nRadarQA, integrating key physical attributes with detailed assessment reports.\nWe introduce a novel and comprehensive task paradigm for multi-modal quality\nanalysis, encompassing both single frame and sequence, under both rating and\nassessment scenarios. To support training and benchmarking, we design a hybrid\nannotation pipeline that combines human expert labeling with automated\nheuristics. With such an annotation method, we construct RQA-70K, a large-scale\ndataset with varying difficulty levels for radar forecast quality evaluation.\nWe further design a multi-stage training strategy that iteratively improves\nmodel performance at each stage. Extensive experiments show that RadarQA\noutperforms existing general MLLMs across all evaluation settings, highlighting\nits potential for advancing quality analysis in weather prediction.", "AI": {"tldr": "RadarQA\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5929\u6c14\u9884\u62a5\u8d28\u91cf\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u7269\u7406\u5c5e\u6027\u548c\u8be6\u7ec6\u8bc4\u4f30\u62a5\u544a\uff0c\u5728\u96f7\u8fbe\u9884\u62a5\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u901a\u7528MLLMs\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5206\u6570\u7684\u8bc4\u4f30\u6307\u6807\u5728\u63cf\u8ff0\u80fd\u529b\u3001\u53ef\u89e3\u91ca\u6027\u548c\u52a8\u6001\u6f14\u5316\u7406\u89e3\u65b9\u9762\u8fdc\u4e0d\u5982\u6c14\u8c61\u4e13\u5bb6\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u5de5\u5177\u6765\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51faRadarQA\u65b9\u6cd5\uff0c\u7ed3\u5408\u5173\u952e\u7269\u7406\u5c5e\u6027\u548c\u8be6\u7ec6\u8bc4\u4f30\u62a5\u544a\uff1b\u8bbe\u8ba1\u6df7\u5408\u6807\u6ce8\u6d41\u7a0b\uff08\u4eba\u5de5\u4e13\u5bb6\u6807\u6ce8+\u81ea\u52a8\u542f\u53d1\u5f0f\uff09\uff1b\u6784\u5efaRQA-70K\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff1b\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u8fed\u4ee3\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660eRadarQA\u5728\u6240\u6709\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u7684\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "RadarQA\u5c55\u793a\u4e86\u5728\u5929\u6c14\u9884\u62a5\u8d28\u91cf\u5206\u6790\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u8d28\u91cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u4efb\u52a1\u8303\u5f0f\u548c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11922", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11922", "abs": "https://arxiv.org/abs/2508.11922", "authors": ["Aditi Jahagirdar", "Sameer Joshi"], "title": "Assessment of Using Synthetic Data in Brain Tumor Segmentation", "comment": null, "summary": "Manual brain tumor segmentation from MRI scans is challenging due to tumor\nheterogeneity, scarcity of annotated data, and class imbalance in medical\nimaging datasets. Synthetic data generated by generative models has the\npotential to mitigate these issues by improving dataset diversity. This study\ninvestigates, as a proof of concept, the impact of incorporating synthetic MRI\ndata, generated using a pre-trained GAN model, into training a U-Net\nsegmentation network. Experiments were conducted using real data from the BraTS\n2020 dataset, synthetic data generated with the medigan library, and hybrid\ndatasets combining real and synthetic samples in varying proportions. While\noverall quantitative performance (Dice coefficient, IoU, precision, recall,\naccuracy) was comparable between real-only and hybrid-trained models,\nqualitative inspection suggested that hybrid datasets, particularly with 40%\nreal and 60% synthetic data, improved whole tumor boundary delineation.\nHowever, region-wise accuracy for the tumor core and the enhancing tumor\nremained lower, indicating a persistent class imbalance. The findings support\nthe feasibility of synthetic data as an augmentation strategy for brain tumor\nsegmentation, while highlighting the need for larger-scale experiments,\nvolumetric data consistency, and mitigating class imbalance in future work.", "AI": {"tldr": "\u4f7f\u7528GAN\u751f\u6210\u7684\u5408\u6210MRI\u6570\u636e\u6765\u8865\u5145\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u8111\u90e8\u8131\u75c5\u5206\u5272\u6a21\u578b\uff0c\u5728\u6574\u4f53\u80ef\u75c5\u8fb9\u754c\u5206\u5272\u65b9\u9762\u6709\u6240\u6539\u5584\uff0c\u4f46\u5bf9\u80ef\u75c5\u6838\u5fc3\u533a\u57df\u7684\u5206\u5272\u6548\u679c\u4ecd\u6709\u9650\u3002", "motivation": "\u89e3\u51b3\u8111\u90e8\u8131\u75c5\u5206\u5272\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u80ef\u75c5\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u6765\u63d0\u5347\u6570\u636e\u96c6\u591a\u6837\u6027\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3GAN\u6a21\u578b\u751f\u6210\u5408\u6210MRI\u6570\u636e\uff0c\u5c06\u771f\u5b9e\u6570\u636e\uff08BraTS 2020\uff09\u4e0e\u5408\u6210\u6570\u636e\u6309\u4e0d\u540c\u6bd4\u4f8b\u6df7\u5408\uff0c\u8bad\u7ec3U-Net\u5206\u5272\u7f51\u7edc\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u6570\u91cf\u6307\u6807\uff08Dice\u3001IoU\u7b49\uff09\u5728\u771f\u5b9e\u6570\u636e\u548c\u6df7\u5408\u6570\u636e\u8bad\u7ec3\u4e0b\u76f8\u4f3c\uff0c\u4f46\u572840%\u771f\u5b9e+60%\u5408\u6210\u7684\u6df7\u5408\u6570\u636e\u4e0b\uff0c\u6574\u4f53\u80ef\u75c5\u8fb9\u754c\u5206\u5272\u8d28\u91cf\u6709\u6240\u63d0\u5347\uff0c\u80ef\u75c5\u6838\u5fc3\u548c\u589e\u5f3a\u533a\u57df\u7684\u5206\u5272\u7cbe\u5ea6\u4ecd\u8f83\u4f4e\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u53ef\u4f5c\u4e3a\u8111\u90e8\u8131\u75c5\u5206\u5272\u7684\u6709\u6548\u6570\u636e\u6269\u5145\u7b56\u7565\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u5e76\u8fdb\u884c\u66f4\u5927\u89c4\u6a21\u7684\u9a8c\u8bc1\u3002"}}
{"id": "2508.12338", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12338", "abs": "https://arxiv.org/abs/2508.12338", "authors": ["Wenzhen Yuan", "Shengji Tang", "Weihao Lin", "Jiacheng Ruan", "Ganqu Cui", "Bo Zhang", "Tao Chen", "Ting Liu", "Yuzhuo Fu", "Peng Ye", "Lei Bai"], "title": "Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback", "comment": null, "summary": "Reinforcement learning (RL) has significantly enhanced the reasoning\ncapabilities of large language models (LLMs), but its reliance on expensive\nhuman-labeled data or complex reward models severely limits scalability. While\nexisting self-feedback methods aim to address this problem, they are\nconstrained by the capabilities of a single model, which can lead to\noverconfidence in incorrect answers, reward hacking, and even training\ncollapse. To this end, we propose Reinforcement Learning from Coevolutionary\nCollective Feedback (RLCCF), a novel RL framework that enables multi-model\ncollaborative evolution without external supervision. Specifically, RLCCF\noptimizes the ability of a model collective by maximizing its Collective\nConsistency (CC), which jointly trains a diverse ensemble of LLMs and provides\nreward signals by voting on collective outputs. Moreover, each model's vote is\nweighted by its Self-Consistency (SC) score, ensuring that more confident\nmodels contribute more to the collective decision. Benefiting from the diverse\noutput distributions and complementary abilities of multiple LLMs, RLCCF\nenables the model collective to continuously enhance its reasoning ability\nthrough coevolution. Experiments on four mainstream open-source LLMs across\nfour mathematical reasoning benchmarks demonstrate that our framework yields\nsignificant performance gains, achieving an average relative improvement of\n16.72\\% in accuracy. Notably, RLCCF not only improves the performance of\nindividual models but also enhances the group's majority-voting accuracy by\n4.51\\%, demonstrating its ability to extend the collective capability boundary\nof the model collective.", "AI": {"tldr": "RLCCF\u662f\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u591a\u6a21\u578b\u534f\u4f5c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u4f53\u4e00\u81f4\u6027\u6295\u7968\u63d0\u4f9b\u5956\u52b1\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRL\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u4eba\u5de5\u6807\u6ce8\u548c\u5355\u4e00\u6a21\u578b\u8fc7\u5ea6\u81ea\u4fe1\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u6216\u590d\u6742\u5956\u52b1\u6a21\u578b\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u73b0\u6709\u7684\u81ea\u53cd\u9988\u65b9\u6cd5\u53d7\u9650\u4e8e\u5355\u4e00\u6a21\u578b\u80fd\u529b\uff0c\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u7b54\u6848\u7684\u8fc7\u5ea6\u81ea\u4fe1\u3001\u5956\u52b1\u653b\u51fb\u751a\u81f3\u8bad\u7ec3\u5d29\u6e83\u3002", "method": "\u63d0\u51faRLCCF\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u96c6\u4f53\u4e00\u81f4\u6027\u6765\u4f18\u5316\u6a21\u578b\u96c6\u4f53\u80fd\u529b\u3002\u8054\u5408\u8bad\u7ec3\u591a\u6837\u5316\u7684LLM\u96c6\u6210\uff0c\u901a\u8fc7\u96c6\u4f53\u8f93\u51fa\u6295\u7968\u63d0\u4f9b\u5956\u52b1\u4fe1\u53f7\uff0c\u6bcf\u4e2a\u6a21\u578b\u7684\u6295\u7968\u6743\u91cd\u7531\u5176\u81ea\u4e00\u81f4\u6027\u5206\u6570\u51b3\u5b9a\u3002", "result": "\u5728\u56db\u4e2a\u4e3b\u6d41\u5f00\u6e90LLM\u548c\u56db\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u51c6\u786e\u7387\u76f8\u5bf9\u63d0\u534716.72%\u3002\u4e0d\u4ec5\u63d0\u5347\u5355\u4e2a\u6a21\u578b\u6027\u80fd\uff0c\u8fd8\u4f7f\u96c6\u4f53\u591a\u6570\u6295\u7968\u51c6\u786e\u7387\u63d0\u53474.51%\u3002", "conclusion": "RLCCF\u901a\u8fc7\u591a\u6a21\u578b\u534f\u540c\u8fdb\u5316\u6709\u6548\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u6269\u5c55\u4e86\u6a21\u578b\u96c6\u4f53\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u4e3a\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.11932", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11932", "abs": "https://arxiv.org/abs/2508.11932", "authors": ["Chengwei Zhang", "Xueyi Zhang", "Mingrui Lao", "Tao Jiang", "Xinhao Xu", "Wenjie Li", "Fubo Zhang", "Longyong Chen"], "title": "Deep Learning For Point Cloud Denoising: A Survey", "comment": null, "summary": "Real-world environment-derived point clouds invariably exhibit noise across\nvarying modalities and intensities. Hence, point cloud denoising (PCD) is\nessential as a preprocessing step to improve downstream task performance. Deep\nlearning (DL)-based PCD models, known for their strong representation\ncapabilities and flexible architectures, have surpassed traditional methods in\ndenoising performance. To our best knowledge, despite recent advances in\nperformance, no comprehensive survey systematically summarizes the developments\nof DL-based PCD. To fill the gap, this paper seeks to identify key challenges\nin DL-based PCD, summarizes the main contributions of existing methods, and\nproposes a taxonomy tailored to denoising tasks. To achieve this goal, we\nformulate PCD as a two-step process: outlier removal and surface noise\nrestoration, encompassing most scenarios and requirements of PCD. Additionally,\nwe compare methods in terms of similarities, differences, and respective\nadvantages. Finally, we discuss research limitations and future directions,\noffering insights for further advancements in PCD.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u57fa\u4e8e\u70b9\u4e91\u53bb\u566a\u7684\u7efc\u8ff0\u6027\u7814\u7a76\uff0c\u7cfb\u7edf\u603b\u7ed3\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u72b6\u51b5\u3001\u63d0\u51fa\u4e86\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u4ee5\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u70b9\u4e91\u6570\u636e\u5b58\u5728\u591a\u79cd\u6a21\u6001\u548c\u5f3a\u5ea6\u7684\u566a\u58f0\uff0c\u53bb\u566a\u4f5c\u4e3a\u9884\u5904\u7406\u6b65\u9aa4\u5bf9\u4e0b\u6e38\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u7684\u7efc\u8ff0\u6027\u7814\u7a76\u6765\u603b\u7ed3\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u5c06\u70b9\u4e91\u53bb\u566a\u6a21\u578b\u5316\u4e3a\u4e24\u4e2a\u6b65\u9aa4\uff1a\u79bb\u7fa4\u70b9\u79fb\u9664\u548c\u8868\u9762\u566a\u58f0\u6062\u590d\u3002\u901a\u8fc7\u8fd9\u79cd\u5206\u7c7b\u65b9\u5f0f\u6765\u5305\u542b\u5927\u90e8\u5206PCD\u573a\u666f\u548c\u9700\u6c42\uff0c\u5e76\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u8be6\u7ec6\u7684\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u95e8\u4e3a\u53bb\u566a\u4efb\u52a1\u8bbe\u8ba1\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u8bc6\u522b\u4e86DL\u57fa\u4e8ePCD\u7684\u5173\u952e\u6311\u6218\uff0c\u7cfb\u7edf\u603b\u7ed3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e3b\u8981\u8d21\u732e\u3002", "conclusion": "\u8bba\u6587\u586b\u8865\u4e86DL\u57fa\u4e8e\u70b9\u4e91\u53bb\u566a\u9886\u57df\u7f3a\u4e4f\u7efc\u8ff0\u6027\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u5206\u6790\u6846\u67b6\u548c\u7814\u7a76\u6307\u5357\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u548c\u6311\u6218\u3002"}}
{"id": "2508.12375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12375", "abs": "https://arxiv.org/abs/2508.12375", "authors": ["Yu Sha", "Shuiping Gou", "Bo Liu", "Johannes Faber", "Ningtao Liu", "Stefan Schramm", "Horst Stoecker", "Thomas Steckenreiter", "Domagoj Vnucec", "Nadine Wetzstein", "Andreas Widl", "Kai Zhou"], "title": "Hierarchical knowledge guided fault intensity diagnosis of complex industrial systems", "comment": "12 pages", "summary": "Fault intensity diagnosis (FID) plays a pivotal role in monitoring and\nmaintaining mechanical devices within complex industrial systems. As current\nFID methods are based on chain of thought without considering dependencies\namong target classes. To capture and explore dependencies, we propose a\nhierarchical knowledge guided fault intensity diagnosis framework (HKG)\ninspired by the tree of thought, which is amenable to any representation\nlearning methods. The HKG uses graph convolutional networks to map the\nhierarchical topological graph of class representations into a set of\ninterdependent global hierarchical classifiers, where each node is denoted by\nword embeddings of a class. These global hierarchical classifiers are applied\nto learned deep features extracted by representation learning, allowing the\nentire model to be end-to-end learnable. In addition, we develop a re-weighted\nhierarchical knowledge correlation matrix (Re-HKCM) scheme by embedding\ninter-class hierarchical knowledge into a data-driven statistical correlation\nmatrix (SCM) which effectively guides the information sharing of nodes in\ngraphical convolutional neural networks and avoids over-smoothing issues. The\nRe-HKCM is derived from the SCM through a series of mathematical\ntransformations. Extensive experiments are performed on four real-world\ndatasets from different industrial domains (three cavitation datasets from\nSAMSON AG and one existing publicly) for FID, all showing superior results and\noutperform recent state-of-the-art FID methods.", "AI": {"tldr": "\u57fa\u4e8e\u56fe\u5377\u79ef\u7f51\u7edc\u7684\u5c42\u6b21\u77e5\u8bc6\u5bfc\u5411\u6545\u969c\u5f3a\u5ea6\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u52a0\u6743\u5c42\u6b21\u77e5\u8bc6\u76f8\u5173\u77e9\u9635\u63d0\u9ad8\u6545\u969c\u7c7b\u522b\u95f4\u4f9d\u8d56\u5173\u7cfb\u7684\u5b66\u4e60\u6548\u679c\uff0c\u5728\u591a\u4e2a\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6545\u969c\u5f3a\u5ea6\u8bca\u65ad\u65b9\u6cd5\u57fa\u4e8e\u601d\u7ef4\u94fe\u800c\u6ca1\u6709\u8003\u8651\u76ee\u6807\u7c7b\u522b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u9700\u8981\u63d0\u51fa\u80fd\u591f\u6350\u636e\u548c\u5229\u7528\u8fd9\u4e9b\u4f9d\u8d56\u5173\u7cfb\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u8bca\u65ad\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5c42\u6b21\u77e5\u8bc6\u5bfc\u5411\u7684HKG\u6846\u67b6\uff0c\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc\u5c06\u7c7b\u8868\u5f81\u7684\u5c42\u6b21\u62d3\u6251\u56fe\u6620\u5c04\u4e3a\u4e00\u7ec4\u76f8\u4e92\u4f9d\u8d56\u7684\u5168\u5c40\u5c42\u6b21\u5206\u7c7b\u5668\uff0c\u5e76\u5f00\u53d1\u91cd\u52a0\u6743\u5c42\u6b21\u77e5\u8bc6\u76f8\u5173\u77e9\u9635(Re-HKCM)\u65b9\u6848\u6765\u5d4c\u5165\u7c7b\u95f4\u5c42\u6b21\u77e5\u8bc6\u3002", "result": "\u5728\u56db\u4e2a\u6765\u81ea\u4e0d\u540c\u5de5\u4e1a\u9886\u57df\u7684\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6240\u6709\u7ed3\u679c\u90fd\u663e\u793a\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u6700\u65b0\u7684\u6545\u969c\u5f3a\u5ea6\u8bca\u65ad\u65b9\u6cd5\u3002", "conclusion": "HKG\u6846\u67b6\u901a\u8fc7\u5c42\u6b21\u77e5\u8bc6\u5bfc\u5411\u548c\u56fe\u5377\u79ef\u7f51\u7edc\u6280\u672f\uff0c\u6709\u6548\u5730\u6350\u636e\u4e86\u7c7b\u522b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u63d0\u9ad8\u4e86\u6545\u969c\u5f3a\u5ea6\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2508.11950", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11950", "abs": "https://arxiv.org/abs/2508.11950", "authors": ["Tingbang Liang", "Yixin Zeng", "Jiatong Xie", "Boyu Zhou"], "title": "DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects", "comment": null, "summary": "We present DynamicPose, a retraining-free 6D pose tracking framework that\nimproves tracking robustness in fast-moving camera and object scenarios.\nPrevious work is mainly applicable to static or quasi-static scenes, and its\nperformance significantly deteriorates when both the object and the camera move\nrapidly. To overcome these challenges, we propose three synergistic components:\n(1) A visual-inertial odometry compensates for the shift in the Region of\nInterest (ROI) caused by camera motion; (2) A depth-informed 2D tracker\ncorrects ROI deviations caused by large object translation; (3) A VIO-guided\nKalman filter predicts object rotation, generates multiple candidate poses, and\nthen obtains the final pose by hierarchical refinement. The 6D pose tracking\nresults guide subsequent 2D tracking and Kalman filter updates, forming a\nclosed-loop system that ensures accurate pose initialization and precise pose\ntracking. Simulation and real-world experiments demonstrate the effectiveness\nof our method, achieving real-time and robust 6D pose tracking for fast-moving\ncameras and objects.", "AI": {"tldr": "DynamicPose\u662f\u4e00\u4e2a\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u76846D\u4f4d\u59ff\u8ddf\u8e2a\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u5feb\u901f\u79fb\u52a8\u7684\u76f8\u673a\u548c\u7269\u4f53\u573a\u666f\uff0c\u901a\u8fc7\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u3001\u6df1\u5ea6\u4fe1\u606f2D\u8ddf\u8e2a\u5668\u548cVIO\u5f15\u5bfc\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4e09\u4e2a\u7ec4\u4ef6\u534f\u540c\u5de5\u4f5c\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u5b9e\u65f66D\u4f4d\u59ff\u8ddf\u8e2a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9002\u7528\u4e8e\u9759\u6001\u6216\u51c6\u9759\u6001\u573a\u666f\uff0c\u5f53\u76f8\u673a\u548c\u7269\u4f53\u90fd\u5feb\u901f\u79fb\u52a8\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u5feb\u901f\u8fd0\u52a8\u573a\u666f\u4e0b\u76846D\u4f4d\u59ff\u8ddf\u8e2a\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u534f\u540c\u7ec4\u4ef6\uff1a1\uff09\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u8865\u507f\u76f8\u673a\u8fd0\u52a8\u5f15\u8d77\u7684ROI\u504f\u79fb\uff1b2\uff09\u6df1\u5ea6\u4fe1\u606f2D\u8ddf\u8e2a\u5668\u6821\u6b63\u5927\u7269\u4f53\u5e73\u79fb\u5f15\u8d77\u7684ROI\u504f\u5dee\uff1b3\uff09VIO\u5f15\u5bfc\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u9884\u6d4b\u7269\u4f53\u65cb\u8f6c\u5e76\u751f\u6210\u5019\u9009\u4f4d\u59ff\uff0c\u901a\u8fc7\u5206\u5c42\u7ec6\u5316\u83b7\u5f97\u6700\u7ec8\u4f4d\u59ff\u3002\u5f62\u6210\u95ed\u73af\u7cfb\u7edf\u786e\u4fdd\u7cbe\u786e\u7684\u4f4d\u59ff\u521d\u59cb\u5316\u548c\u8ddf\u8e2a\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u79fb\u52a8\u76f8\u673a\u548c\u7269\u4f53\u7684\u5b9e\u65f6\u9c81\u68d26D\u4f4d\u59ff\u8ddf\u8e2a\u3002", "conclusion": "DynamicPose\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5feb\u901f\u8fd0\u52a8\u573a\u666f\u4e0b\u76846D\u4f4d\u59ff\u8ddf\u8e2a\u6311\u6218\uff0c\u901a\u8fc7\u4e09\u4e2a\u7ec4\u4ef6\u7684\u534f\u540c\u5de5\u4f5c\u548c\u95ed\u73af\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u6027\u80fd\u3002"}}
{"id": "2508.12379", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12379", "abs": "https://arxiv.org/abs/2508.12379", "authors": ["Rongzheng Wang", "Qizhi Chen", "Yihong Huang", "Yizhuo Ma", "Muquan Li", "Jiakai Li", "Ke Qin", "Guangchun Luo", "Shuang Liang"], "title": "GraphCogent: Overcoming LLMs' Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding", "comment": null, "summary": "Large language models (LLMs) show promising performance on small-scale graph\nreasoning tasks but fail when handling real-world graphs with complex queries.\nThis phenomenon stems from LLMs' inability to effectively process complex graph\ntopology and perform multi-step reasoning simultaneously. To address these\nlimitations, we propose GraphCogent, a collaborative agent framework inspired\nby human Working Memory Model that decomposes graph reasoning into specialized\ncognitive processes: sense, buffer, and execute. The framework consists of\nthree modules: Sensory Module standardizes diverse graph text representations\nvia subgraph sampling, Buffer Module integrates and indexes graph data across\nmultiple formats, and Execution Module combines tool calling and model\ngeneration for efficient reasoning. We also introduce Graph4real, a\ncomprehensive benchmark contains with four domains of real-world graphs (Web,\nSocial, Transportation, and Citation) to evaluate LLMs' graph reasoning\ncapabilities. Our Graph4real covers 21 different graph reasoning tasks,\ncategorized into three types (Structural Querying, Algorithmic Reasoning, and\nPredictive Modeling tasks), with graph scales that are 10 times larger than\nexisting benchmarks. Experiments show that Llama3.1-8B based GraphCogent\nachieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B).\nCompared to state-of-the-art agent-based baseline, our framework outperforms by\n20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30%\nfor out-toolset tasks. Code will be available after review.", "AI": {"tldr": "GraphCogent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5de5\u4f5c\u8bb0\u5fc6\u6a21\u578b\u7684\u534f\u4f5c\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u56fe\u63a8\u7406\u5206\u89e3\u4e3a\u611f\u77e5\u3001\u7f13\u51b2\u548c\u6267\u884c\u4e09\u4e2a\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u56fe\u62d3\u6251\u548c\u591a\u6b65\u63a8\u7406\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c0f\u89c4\u6a21\u56fe\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u5177\u6709\u590d\u6742\u67e5\u8be2\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u65f6\u5931\u8d25\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u65e0\u6cd5\u540c\u65f6\u6709\u6548\u5904\u7406\u590d\u6742\u56fe\u62d3\u6251\u548c\u6267\u884c\u591a\u6b65\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u4e86GraphCogent\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u611f\u77e5\u6a21\u5757\u901a\u8fc7\u5b50\u56fe\u91c7\u6837\u6807\u51c6\u5316\u56fe\u6587\u672c\u8868\u793a\uff0c\u7f13\u51b2\u6a21\u5757\u96c6\u6210\u548c\u7d22\u5f15\u591a\u683c\u5f0f\u56fe\u6570\u636e\uff0c\u6267\u884c\u6a21\u5757\u7ed3\u5408\u5de5\u5177\u8c03\u7528\u548c\u6a21\u578b\u751f\u6210\u8fdb\u884c\u9ad8\u6548\u63a8\u7406\u3002", "result": "\u57fa\u4e8eLlama3.1-8B\u7684GraphCogent\u76f8\u6bd4DeepSeek-R1(671B)\u63d0\u534750%\u6027\u80fd\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u57fa\u7ebf\u5728\u51c6\u786e\u7387\u4e0a\u63d0\u534720%\uff0c\u540c\u65f6\u5728\u5185\u5de5\u5177\u96c6\u4efb\u52a1\u4e0a\u51cf\u5c1180%\u7684token\u4f7f\u7528\uff0c\u5728\u5916\u5de5\u5177\u96c6\u4efb\u52a1\u4e0a\u51cf\u5c1130%\u7684token\u4f7f\u7528\u3002", "conclusion": "GraphCogent\u6846\u67b6\u901a\u8fc7\u8ba4\u77e5\u8fc7\u7a0b\u5206\u89e3\u6709\u6548\u63d0\u5347\u4e86LLMs\u5728\u56fe\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u56fe\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2508.11951", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11951", "abs": "https://arxiv.org/abs/2508.11951", "authors": ["Hao Peng", "Hong Sang", "Yajing Ma", "Ping Qiu", "Chao Ji"], "title": "Transferable Class Statistics and Multi-scale Feature Approximation for 3D Object Detection", "comment": null, "summary": "This paper investigates multi-scale feature approximation and transferable\nfeatures for object detection from point clouds. Multi-scale features are\ncritical for object detection from point clouds. However, multi-scale feature\nlearning usually involves multiple neighborhood searches and scale-aware\nlayers, which can hinder efforts to achieve lightweight models and may not be\nconducive to research constrained by limited computational resources. This\npaper approximates point-based multi-scale features from a single neighborhood\nbased on knowledge distillation. To compensate for the loss of constructive\ndiversity in a single neighborhood, this paper designs a transferable feature\nembedding mechanism. Specifically, class-aware statistics are employed as\ntransferable features given the small computational cost. In addition, this\npaper introduces the central weighted intersection over union for localization\nto alleviate the misalignment brought by the center offset in optimization.\nNote that the method presented in this paper saves computational costs.\nExtensive experiments on public datasets demonstrate the effectiveness of the\nproposed method.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u77e5\u8bc6\u84c4\u7ea6\u548c\u53ef\u8f6c\u79fb\u7279\u5f81\u5d4c\u5165\u673a\u5236\uff0c\u5728\u5355\u4e00\u90bb\u57df\u5185\u8fd1\u4f3c\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u5316\u7684\u70b9\u4e91\u7269\u4f53\u68c0\u6d4b\u65b9\u6848\u3002", "motivation": "\u591a\u5c3a\u5ea6\u7279\u5f81\u5bf9\u70b9\u4e91\u7269\u4f53\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u591a\u6b21\u90bb\u57df\u641c\u7d22\u548c\u5c3a\u5ea6\u611f\u77e5\u5c42\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0d\u5229\u4e8e\u8f7b\u91cf\u5316\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u57fa\u4e8e\u77e5\u8bc6\u84c4\u7ea6\u5728\u5355\u4e00\u90bb\u57df\u8fd1\u4f3c\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u8bbe\u8ba1\u53ef\u8f6c\u79fb\u7279\u5f81\u5d4c\u5165\u673a\u5236\u8865\u507f\u6784\u9020\u6027\u591a\u6837\u6027\u635f\u5931\uff0c\u4f7f\u7528\u7c7b\u522b\u7edf\u8ba1\u7279\u5f81\u4f5c\u4e3a\u53ef\u8f6c\u79fb\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u4e2d\u5fc3\u52a0\u6743IoU\u4f18\u5316\u5b9a\u4f4d\u51c6\u786e\u6027\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u8282\u7701\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u77e5\u8bc6\u84c4\u7ea6\u548c\u53ef\u8f6c\u79fb\u7279\u5f81\u673a\u5236\uff0c\u5728\u4fdd\u6301\u68c0\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u8f7b\u91cf\u5316\u8bbe\u8ba1\uff0c\u4e3a\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e0b\u7684\u70b9\u4e91\u7269\u4f53\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12425", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12425", "abs": "https://arxiv.org/abs/2508.12425", "authors": ["Phuong Minh Nguyen", "Tien Huu Dang", "Naoya Inoue"], "title": "Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning", "comment": null, "summary": "This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved\napproach to standard CoT, for logical reasoning in large language models\n(LLMs). The key idea is to integrate lightweight symbolic representations into\nfew-shot prompts, structuring the inference steps with a consistent strategy to\nmake reasoning patterns more explicit within a non-iterative reasoning process.\nBy incorporating these symbolic structures, our method preserves the\ngeneralizability of standard prompting techniques while enhancing the\ntransparency, interpretability, and analyzability of LLM logical reasoning.\nExtensive experiments on four well-known logical reasoning benchmarks --\nProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse\nreasoning scenarios -- demonstrate the effectiveness of the proposed approach,\nparticularly in complex reasoning tasks that require navigating multiple\nconstraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'\nreasoning capabilities across various model sizes and significantly outperforms\nconventional CoT on three out of four datasets, ProofWriter, ProntoQA, and\nLogicalDeduction.", "AI": {"tldr": "Symbolic-Aided Chain-of-Thought (CoT) \u901a\u8fc7\u5c06\u8f7b\u91cf\u7ea7\u7b26\u53f7\u8868\u793a\u6574\u5408\u5230\u5c11\u6837\u672c\u63d0\u793a\u4e2d\uff0c\u6539\u8fdb\u4e86\u6807\u51c6CoT\u65b9\u6cd5\uff0c\u4f7fLLM\u7684\u903b\u8f91\u63a8\u7406\u66f4\u52a0\u900f\u660e\u548c\u53ef\u89e3\u91ca\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u900f\u660e\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u5206\u6790\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u51c6\u63d0\u793a\u6280\u672f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5728\u5c11\u6837\u672c\u63d0\u793a\u4e2d\u6574\u5408\u8f7b\u91cf\u7ea7\u7b26\u53f7\u8868\u793a\uff0c\u6784\u5efa\u4e00\u81f4\u7684\u63a8\u7406\u7b56\u7565\uff0c\u4f7f\u63a8\u7406\u6a21\u5f0f\u5728\u975e\u8fed\u4ee3\u63a8\u7406\u8fc7\u7a0b\u4e2d\u66f4\u52a0\u660e\u786e\u3002", "result": "\u5728\u56db\u4e2a\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff08ProofWriter\u3001FOLIO\u3001ProntoQA\u3001LogicalDeduction\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5904\u7406\u591a\u7ea6\u675f\u6216\u89c4\u5219\u7684\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u3002\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edfCoT\u65b9\u6cd5\u3002", "conclusion": "Symbolic-Aided CoT\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u589e\u5f3a\u4e86\u63a8\u7406\u8fc7\u7a0b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u90fd\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\u6548\u679c\u3002"}}
{"id": "2508.11952", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11952", "abs": "https://arxiv.org/abs/2508.11952", "authors": ["Yueming Xu", "Jiahui Zhang", "Ze Huang", "Yurui Chen", "Yanpeng Zhou", "Zhenyu Chen", "Yu-Jie Yuan", "Pengxiang Xia", "Guowei Huang", "Xinyue Cai", "Zhongang Qi", "Xingyue Quan", "Jianye Hao", "Hang Xu", "Li Zhang"], "title": "UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding", "comment": null, "summary": "Despite the impressive progress on understanding and generating images shown\nby the recent unified architectures, the integration of 3D tasks remains\nchallenging and largely unexplored. In this paper, we introduce UniUGG, the\nfirst unified understanding and generation framework for 3D modalities. Our\nunified framework employs an LLM to comprehend and decode sentences and 3D\nrepresentations. At its core, we propose a spatial decoder leveraging a latent\ndiffusion model to generate high-quality 3D representations. This allows for\nthe generation and imagination of 3D scenes based on a reference image and an\narbitrary view transformation, while remaining supports for spatial visual\nquestion answering (VQA) tasks. Additionally, we propose a geometric-semantic\nlearning strategy to pretrain the vision encoder. This design jointly captures\nthe input's semantic and geometric cues, enhancing both spatial understanding\nand generation. Extensive experimental results demonstrate the superiority of\nour method in visual representation, spatial understanding, and 3D generation.\nThe source code will be released upon paper acceptance.", "AI": {"tldr": "UniUGG\u662f\u9996\u4e2a\u7edf\u4e00\u7406\u89e3\u548c\u751f\u62103D\u6a21\u6001\u7684\u6846\u67b6\uff0c\u4f7f\u7528LLM\u7406\u89e3\u548c\u89e3\u7801\u6587\u672c\u4e0e3D\u8868\u793a\uff0c\u901a\u8fc7\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf3D\u5185\u5bb9\uff0c\u652f\u6301\u7a7a\u95f4\u89c6\u89c9\u95ee\u7b54\u548c3D\u573a\u666f\u751f\u6210\u3002", "motivation": "\u5c3d\u7ba1\u7edf\u4e00\u67b6\u6784\u5728\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f463D\u4efb\u52a1\u7684\u6574\u5408\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u4e14\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u5904\u74063D\u7406\u89e3\u548c\u751f\u6210\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u6765\u7406\u89e3\u548c\u89e3\u7801\u53e5\u5b50\u4e0e3D\u8868\u793a\uff0c\u6838\u5fc3\u91c7\u7528\u7a7a\u95f4\u89e3\u7801\u5668\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf3D\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u51e0\u4f55-\u8bed\u4e49\u5b66\u4e60\u7b56\u7565\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8868\u793a\u3001\u7a7a\u95f4\u7406\u89e3\u548c3D\u751f\u6210\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "UniUGG\u6210\u529f\u5b9e\u73b0\u4e863D\u6a21\u6001\u7684\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\uff0c\u4e3a3D\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6e90\u4ee3\u7801\u5c06\u5728\u8bba\u6587\u63a5\u53d7\u540e\u53d1\u5e03\u3002"}}
{"id": "2508.12472", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12472", "abs": "https://arxiv.org/abs/2508.12472", "authors": ["Yifang Tian", "Yaming Liu", "Zichun Chong", "Zihang Huang", "Hans-Arno Jacobsen"], "title": "GALA: Can Graph-Augmented Large Language Model Agentic Workflows Elevate Root Cause Analysis?", "comment": "12 pages, 5 figures", "summary": "Root cause analysis (RCA) in microservice systems is challenging, requiring\non-call engineers to rapidly diagnose failures across heterogeneous telemetry\nsuch as metrics, logs, and traces. Traditional RCA methods often focus on\nsingle modalities or merely rank suspect services, falling short of providing\nactionable diagnostic insights with remediation guidance. This paper introduces\nGALA, a novel multi-modal framework that combines statistical causal inference\nwith LLM-driven iterative reasoning for enhanced RCA. Evaluated on an\nopen-source benchmark, GALA achieves substantial improvements over\nstate-of-the-art methods of up to 42.22% accuracy. Our novel human-guided LLM\nevaluation score shows GALA generates significantly more causally sound and\nactionable diagnostic outputs than existing methods. Through comprehensive\nexperiments and a case study, we show that GALA bridges the gap between\nautomated failure diagnosis and practical incident resolution by providing both\naccurate root cause identification and human-interpretable remediation\nguidance.", "AI": {"tldr": "GALA\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u7edf\u8ba1\u56e0\u679c\u63a8\u65ad\u548cLLM\u9a71\u52a8\u7684\u8fed\u4ee3\u63a8\u7406\uff0c\u7528\u4e8e\u5fae\u670d\u52a1\u7cfb\u7edf\u7684\u6839\u56e0\u5206\u6790\uff0c\u5728\u51c6\u786e\u6027\u548c\u53ef\u64cd\u4f5c\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfRCA\u65b9\u6cd5\u901a\u5e38\u5173\u6ce8\u5355\u4e00\u6a21\u6001\u6216\u4ec5\u5bf9\u53ef\u7591\u670d\u52a1\u8fdb\u884c\u6392\u5e8f\uff0c\u65e0\u6cd5\u63d0\u4f9b\u5177\u6709\u4fee\u590d\u6307\u5bfc\u7684\u53ef\u64cd\u4f5c\u8bca\u65ad\u89c1\u89e3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u591a\u6a21\u6001\u6545\u969c\u8bca\u65ad\u65b9\u6848\u3002", "method": "GALA\u7ed3\u5408\u7edf\u8ba1\u56e0\u679c\u63a8\u65ad\u548cLLM\u9a71\u52a8\u7684\u8fed\u4ee3\u63a8\u7406\uff0c\u5229\u7528\u591a\u6a21\u6001\u9065\u6d4b\u6570\u636e\uff08\u6307\u6807\u3001\u65e5\u5fd7\u3001\u8ffd\u8e2a\uff09\u8fdb\u884c\u6839\u56e0\u5206\u6790\u3002", "result": "\u5728\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGALA\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe42.22%\uff0c\u751f\u6210\u7684\u8bca\u65ad\u8f93\u51fa\u5728\u56e0\u679c\u5408\u7406\u6027\u548c\u53ef\u64cd\u4f5c\u6027\u65b9\u9762\u663e\u8457\u66f4\u597d\u3002", "conclusion": "GALA\u901a\u8fc7\u63d0\u4f9b\u51c6\u786e\u7684\u6839\u56e0\u8bc6\u522b\u548c\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u4fee\u590d\u6307\u5bfc\uff0c\u5f25\u5408\u4e86\u81ea\u52a8\u5316\u6545\u969c\u8bca\u65ad\u4e0e\u5b9e\u9645\u4e8b\u4ef6\u89e3\u51b3\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.11955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11955", "abs": "https://arxiv.org/abs/2508.11955", "authors": ["Seunghun Lee", "Jiwan Seo", "Jeonghoon Kim", "Siwon Kim", "Haeun Yun", "Hyogyeong Jeon", "Wonhyeok Choi", "Jaehoon Jeong", "Zane Durante", "Sang Hyun Park", "Sunghoon Im"], "title": "SAMDWICH: Moment-aware Video-text Alignment for Referring Video Object Segmentation", "comment": "Project page: https://seung-hun-lee.github.io/projects/SAMDWICH/", "summary": "Referring Video Object Segmentation (RVOS) aims to segment and track objects\nin videos based on natural language expressions, requiring precise alignment\nbetween visual content and textual queries. However, existing methods often\nsuffer from semantic misalignment, largely due to indiscriminate frame sampling\nand supervision of all visible objects during training -- regardless of their\nactual relevance to the expression. To address this, we introduce a\nmoment-aware RVOS framework named SAMDWICH, along with a newly annotated\ndataset, MeViS-M, built upon the challenging MeViS benchmark. We manually\nannotate temporal moments indicating when each object is referred to by the\nexpression, enabling semantically grounded supervision that strengthens\nvideo-text alignment. SAMDWICH leverages these aligned text-to-clip pairs to\nguide training, significantly enhancing referential understanding. Building\nupon this framework, we propose Moment-guided Dual-path Propagation (MDP), a\nmoment-aware propagation strategy that improves both object grounding and\ntracking by training on both relevant and irrelevant frames through a\nmoment-centric memory mechanism. In addition, we introduce Object-level\nSelective Supervision (OSS), an object-level filtering strategy that supervises\nonly the objects temporally aligned with the expression in each training clip.\nThis selective supervision reduces semantic noise and reinforces\nlanguage-conditioned learning. Extensive experiments show that SAMDWICH\nachieves state-of-the-art performance on challenging MeViS benchmark,\nparticularly excelling in complex scenarios involving diverse expressions.", "AI": {"tldr": "SAMDWICH\u662f\u4e00\u4e2a\u57fa\u4e8e\u65f6\u523b\u611f\u77e5\u7684Referring Video Object Segmentation\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u6807\u6ce8\u7684MeViS-M\u6570\u636e\u96c6\u548c\u65f6\u523b\u5f15\u5bfc\u7684\u53cc\u8def\u5f84\u4f20\u64ad\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u8bed\u4e49\u9519\u4f4d\u95ee\u9898\uff0c\u5728\u590d\u6742\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684Referring Video Object Segmentation\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u9519\u4f4d\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8bad\u7ec3\u65f6\u5bf9\u6240\u6709\u53ef\u89c1\u5bf9\u8c61\u8fdb\u884c\u65e0\u5dee\u522b\u5e27\u91c7\u6837\u548c\u76d1\u7763\uff0c\u800c\u4e0d\u8003\u8651\u5b83\u4eec\u4e0e\u6587\u672c\u67e5\u8be2\u7684\u5b9e\u9645\u76f8\u5173\u6027\u3002", "method": "\u63d0\u51fa\u4e86SAMDWICH\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u65b0\u6807\u6ce8\u7684MeViS-M\u6570\u636e\u96c6\uff0c\u624b\u52a8\u6807\u6ce8\u4e86\u6bcf\u4e2a\u5bf9\u8c61\u88ab\u8868\u8fbe\u5f0f\u5f15\u7528\u7684\u65f6\u95f4\u65f6\u523b\uff1b2\uff09\u65f6\u523b\u5f15\u5bfc\u7684\u53cc\u8def\u5f84\u4f20\u64ad\u7b56\u7565\uff08MDP\uff09\uff0c\u901a\u8fc7\u65f6\u523b\u4e2d\u5fc3\u8bb0\u5fc6\u673a\u5236\u5728\u76f8\u5173\u548c\u4e0d\u76f8\u5173\u5e27\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff1b3\uff09\u5bf9\u8c61\u7ea7\u9009\u62e9\u6027\u76d1\u7763\uff08OSS\uff09\uff0c\u53ea\u76d1\u7763\u4e0e\u8868\u8fbe\u5f0f\u65f6\u95f4\u5bf9\u9f50\u7684\u5bf9\u8c61\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684MeViS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u591a\u6837\u5316\u8868\u8fbe\u5f0f\u7684\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u901a\u8fc7\u65f6\u523b\u611f\u77e5\u7684\u76d1\u7763\u548c\u9009\u62e9\u6027\u8bad\u7ec3\u7b56\u7565\uff0cSAMDWICH\u663e\u8457\u589e\u5f3a\u4e86\u89c6\u9891-\u6587\u672c\u5bf9\u9f50\u548c\u53c2\u8003\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3RVOS\u4e2d\u7684\u8bed\u4e49\u9519\u4f4d\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12480", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12480", "abs": "https://arxiv.org/abs/2508.12480", "authors": ["Constantin Ruhdorfer", "Matteo Bortoletto", "Andreas Bulling"], "title": "The Yokai Learning Environment: Tracking Beliefs Over Space and Time", "comment": "Presented at the the ToM IJCAI 2025 Workshop", "summary": "Developing collaborative AI hinges on Theory of Mind (ToM) - the ability to\nreason about the beliefs of others to build and maintain common ground.\nExisting ToM benchmarks, however, are restricted to passive observer settings\nor lack an assessment of how agents establish and maintain common ground over\ntime. To address these gaps, we introduce the Yokai Learning Environment (YLE)\n- a multi-agent reinforcement learning (RL) environment based on the\ncooperative card game Yokai. In the YLE, agents take turns peeking at hidden\ncards and moving them to form clusters based on colour. Success requires\ntracking evolving beliefs, remembering past observations, using hints as\ngrounded communication, and maintaining common ground with teammates. Our\nevaluation yields two key findings: First, current RL agents struggle to solve\nthe YLE, even when given access to perfect memory. Second, while belief\nmodelling improves performance, agents are still unable to effectively\ngeneralise to unseen partners or form accurate beliefs over longer games,\nexposing a reliance on brittle conventions rather than robust belief tracking.\nWe use the YLE to investigate research questions in belief modelling, memory,\npartner generalisation, and scaling to higher-order ToM.", "AI": {"tldr": "\u57fa\u4e8e\u5408\u4f5c\u5361\u724c\u6e38\u620fYokai\u6784\u5efa\u7684\u591a\u6ee1\u5f3a\u5316\u5b66\u4e60\u73af\u5883YLE\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u7406\u8bba\u5fc3\u667a\u80fd\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524dRL\u673a\u5668\u4eba\u5728\u5171\u540c\u57fa\u7840\u7ef4\u62a4\u3001\u4f1a\u8bdd\u4f2a\u9020\u548c\u4f19\u4f34\u6cdb\u5316\u65b9\u9762\u4ecd\u9047\u5230\u56f0\u96be", "motivation": "\u73b0\u6709\u7406\u8bba\u5fc3\u667a\u667a\u80fd\u8bc4\u6d4b\u6807\u51c6\u4ec5\u9650\u4e8e\u88ab\u52a8\u89c2\u5bdf\u8005\u573a\u666f\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6ee1\u4f53\u5982\u4f55\u5efa\u7acb\u548c\u7ef4\u62a4\u5171\u540c\u57fa\u7840\u7684\u8bc4\u4f30", "method": "\u6784\u5efaYokai\u5b66\u4e60\u73af\u5883(YLE)\u591a\u6ee1\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u901a\u8fc7\u5408\u4f5c\u5361\u724c\u6e38\u620f\u6a21\u5f0f\u8ba9\u673a\u5668\u4eba\u8f6e\u6d41\u63a2\u67e5\u9690\u85cf\u5361\u724c\u5e76\u6309\u989c\u8272\u5206\u7ec4\uff0c\u8981\u6c42\u8ddf\u8e2a\u4fe1\u5ff5\u53d8\u5316\u3001\u8bb0\u5fc6\u5386\u53f2\u89c2\u6d4b\u3001\u4f7f\u7528\u63d0\u793a\u4f5c\u4e3a\u57fa\u7840\u6c9f\u901a", "result": "\u5f53\u524dRL\u673a\u5668\u4eba\u5373\u4f7f\u6709\u5b8c\u7f8e\u8bb0\u5fc6\u4e5f\u96be\u4ee5\u89e3\u51b3YLE\u4efb\u52a1\uff1b\u4fe1\u5ff5\u5efa\u6a21\u80fd\u63d0\u5347\u6027\u80fd\u4f46\u65e0\u6cd5\u6709\u6548\u6cdb\u5316\u5230\u672a\u89c1\u4f19\u4f34\u6216\u5728\u957f\u65f6\u95f4\u6e38\u620f\u4e2d\u5f62\u6210\u51c6\u786e\u4fe1\u5ff5\uff0c\u663e\u793a\u4e86\u5bf9\u8106\u5f31\u4f9b\u5e94\u7684\u4f9d\u8d56", "conclusion": "YLE\u73af\u5883\u4e3a\u4fe1\u5ff5\u5efa\u6a21\u3001\u8bb0\u5fc6\u3001\u4f19\u4f34\u6cdb\u5316\u548c\u9ad8\u9636\u7406\u8bba\u5fc3\u667a\u667a\u80fd\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u66dd\u9732\u4e86\u5f53\u524d\u534f\u4f5cAI\u5728\u5171\u540c\u57fa\u7840\u7ef4\u62a4\u65b9\u9762\u7684\u4e0d\u8db3"}}
{"id": "2508.11961", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11961", "abs": "https://arxiv.org/abs/2508.11961", "authors": ["Yuanbin Fu", "Liang Li", "Xiaojie Guo"], "title": "PEdger++: Practical Edge Detection via Assembling Cross Information", "comment": null, "summary": "Edge detection serves as a critical foundation for numerous computer vision\napplications, including object detection, semantic segmentation, and image\nediting, by extracting essential structural cues that define object boundaries\nand salient edges. To be viable for broad deployment across devices with\nvarying computational capacities, edge detectors shall balance high accuracy\nwith low computational complexity. While deep learning has evidently improved\naccuracy, they often suffer from high computational costs, limiting their\napplicability on resource-constrained devices. This paper addresses the\nchallenge of achieving that balance: \\textit{i.e.}, {how to efficiently capture\ndiscriminative features without relying on large-size and sophisticated\nmodels}. We propose PEdger++, a collaborative learning framework designed to\nreduce computational costs and model sizes while improving edge detection\naccuracy. The core principle of our PEdger++ is that cross-information derived\nfrom heterogeneous architectures, diverse training moments, and multiple\nparameter samplings, is beneficial to enhance learning from an ensemble\nperspective. Extensive experimental results on the BSDS500, NYUD and Multicue\ndatasets demonstrate the effectiveness of our approach, both quantitatively and\nqualitatively, showing clear improvements over existing methods. We also\nprovide multiple versions of the model with varying computational requirements,\nhighlighting PEdger++'s adaptability with respect to different resource\nconstraints. Codes are accessible at\nhttps://github.com/ForawardStar/EdgeDetectionviaPEdgerPlus/.", "AI": {"tldr": "PEdger++\u662f\u4e00\u4e2a\u534f\u4f5c\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5f02\u6784\u67b6\u6784\u3001\u4e0d\u540c\u8bad\u7ec3\u65f6\u523b\u548c\u53c2\u6570\u91c7\u6837\u7684\u8de8\u4fe1\u606f\u6765\u63d0\u5347\u8fb9\u7f18\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u6a21\u578b\u5927\u5c0f\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u8fb9\u7f18\u68c0\u6d4b\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6a21\u578b\u590d\u6742\u5ea6\u5927\u7684\u95ee\u9898\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\uff0c\u5b9e\u73b0\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u534f\u4f5c\u5b66\u4e60\u6846\u67b6PEdger++\uff0c\u5229\u7528\u5f02\u6784\u67b6\u6784\u3001\u591a\u6837\u5316\u8bad\u7ec3\u65f6\u523b\u548c\u591a\u91cd\u53c2\u6570\u91c7\u6837\u7684\u8de8\u4fe1\u606f\u6765\u589e\u5f3a\u5b66\u4e60\u6548\u679c\uff0c\u4ece\u96c6\u6210\u89d2\u5ea6\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728BSDS500\u3001NYUD\u548cMulticue\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u591a\u4e2a\u4e0d\u540c\u8ba1\u7b97\u9700\u6c42\u7684\u6a21\u578b\u7248\u672c\u3002", "conclusion": "PEdger++\u6210\u529f\u5b9e\u73b0\u4e86\u8fb9\u7f18\u68c0\u6d4b\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u826f\u597d\u5e73\u8861\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684\u5f3a\u9002\u5e94\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12487", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12487", "abs": "https://arxiv.org/abs/2508.12487", "authors": ["Lida Shahbandari", "Hossein Mohseni"], "title": "Advanced DOA Regulation with a Whale-Optimized Fractional Order Fuzzy PID Framework", "comment": null, "summary": "This study introduces a Fractional Order Fuzzy PID (FOFPID) controller that\nuses the Whale Optimization Algorithm (WOA) to manage the Bispectral Index\n(BIS), keeping it within the ideal range of forty to sixty. The FOFPID\ncontroller combines fuzzy logic for adapting to changes and fractional order\ndynamics for fine tuning. This allows it to adjust its control gains to handle\na person's unique physiology. The WOA helps fine tune the controller's\nparameters, including the fractional orders and the fuzzy membership functions,\nwhich boosts its performance. Tested on models of eight different patient\nprofiles, the FOFPID controller performed better than a standard Fractional\nOrder PID (FOPID) controller. It achieved faster settling times, at two and a\nhalf minutes versus three point two minutes, and had a lower steady state\nerror, at zero point five versus one point two. These outcomes show the\nFOFPID's excellent strength and accuracy. It offers a scalable, artificial\nintelligence driven solution for automated anesthesia delivery that could\nenhance clinical practice and improve patient results.", "AI": {"tldr": "\u57fa\u4e8e\u9cb8\u9c7c\u4f18\u5316\u7b97\u6cd5\u7684\u5206\u6570\u9636\u6a21\u7ccaPID\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u9ebb\u9189\u6d88\u606f\u6307\u6570\u63a7\u5236\uff0c\u6027\u80fd\u4f18\u4e8e\u6807\u51c6\u5206\u6570\u9636PID\u63a7\u5236\u5668", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u7a33\u5b9a\u7684\u9ebb\u9189\u6df1\u5ea6\u63a7\u5236\uff0c\u9002\u5e94\u4e0d\u540c\u60a3\u8005\u7684\u751f\u7406\u7279\u5f02\u6027\uff0c\u63d0\u9ad8\u9ebb\u9189\u8d28\u91cf\u548c\u60a3\u8005\u5b89\u5168", "method": "\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u548c\u5206\u6570\u9636\u5fae\u79ef\u5206\u7684FOFPID\u63a7\u5236\u5668\uff0c\u4f7f\u7528WOA\u7b97\u6cd5\u4f18\u5316\u63a7\u5236\u5668\u53c2\u6570\u3001\u5206\u6570\u9636\u6b21\u6570\u548c\u6a21\u7cca\u6210\u5458\u51fd\u6570", "result": "\u57288\u79cd\u60a3\u8005\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0c\u8c03\u8282\u65f6\u95f4\u4ece3.2\u5206\u949f\u7f29\u77ed\u52302.5\u5206\u949f\uff0c\u7a33\u6001\u8bef\u5dee\u4ece1.2\u964d\u4f4f\u52300.5\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u6807\u51c6FOPID\u63a7\u5236\u5668", "conclusion": "FOFPID\u63a7\u5236\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u4eba\u5de5\u667a\u80fd\u9ebb\u9189\u81ea\u52a8\u63a7\u5236\u65b9\u6848\uff0c\u5177\u6709\u5f3a\u5920\u6027\u548c\u9ad8\u7cbe\u5ea6\uff0c\u6709\u529b\u63a8\u52a8\u4e34\u5e8a\u5b9e\u8df5\u6539\u8fdb\u548c\u60a3\u8005\u7ed3\u679c\u63d0\u5347"}}
{"id": "2508.11988", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11988", "abs": "https://arxiv.org/abs/2508.11988", "authors": ["Nicolas Mastropasqua", "Ignacio Bugueno-Cordova", "Rodrigo Verschae", "Daniel Acevedo", "Pablo Negri", "Maria E. Buemi"], "title": "Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis", "comment": null, "summary": "Micro-expression analysis has applications in domains such as Human-Robot\nInteraction and Driver Monitoring Systems. Accurately capturing subtle and fast\nfacial movements remains difficult when relying solely on RGB cameras, due to\nlimitations in temporal resolution and sensitivity to motion blur. Event\ncameras offer an alternative, with microsecond-level precision, high dynamic\nrange, and low latency. However, public datasets featuring event-based\nrecordings of Action Units are still scarce. In this work, we introduce a\nnovel, preliminary multi-resolution and multi-modal micro-expression dataset\nrecorded with synchronized RGB and event cameras under variable lighting\nconditions. Two baseline tasks are evaluated to explore the spatial-temporal\ndynamics of micro-expressions: Action Unit classification using Spiking Neural\nNetworks (51.23\\% accuracy with events vs. 23.12\\% with RGB), and frame\nreconstruction using Conditional Variational Autoencoders, achieving SSIM =\n0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising\nresults show that event-based data can be used for micro-expression recognition\nand frame reconstruction.", "AI": {"tldr": "\u4e8b\u4ef6\u76f8\u673a\u5728\u5fae\u8868\u60c5\u5206\u6790\u4e2d\u663e\u793a\u4f18\u52bf\uff0c\u51c6\u786e\u7387\u6bd4RGB\u76f8\u673a\u63d0\u5347\u4e00\u500d\u4ee5\u4e0a\uff0c\u5e76\u80fd\u5b8c\u6574\u91cd\u5efa\u5e27\u754c\u9762", "motivation": "\u5fae\u8868\u60c5\u5206\u6790\u5728\u4eba\u673a\u4ea4\u4e92\u548c\u9a7e\u9a76\u76d1\u63a7\u4e2d\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u4f20\u7edfRGB\u76f8\u673a\u56e0\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u8fd0\u52a8\u6a21\u7cca\u9650\u5236\u800c\u96be\u4ee5\u6350\u6355\u7ec6\u5fae\u9762\u90e8\u8fd0\u52a8", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u5206\u8fa8\u7387\u591a\u6a21\u6001\u5fae\u8868\u60c5\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u540c\u6b65RGB\u548c\u4e8b\u4ef6\u76f8\u673a\u5728\u53d8\u5316\u5149\u7167\u6761\u4ef6\u4e0b\u8fdb\u884c\u8bb0\u5f55\uff0c\u5e76\u4f7f\u7528\u75af\u5c04\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u52a8\u4f5c\u5355\u5143\u5206\u7c7b\u548c\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u5e27\u91cd\u5efa", "result": "\u4e8b\u4ef6\u76f8\u673a\u5728\u52a8\u4f5c\u5355\u5143\u5206\u7c7b\u4e2d\u8fbe\u523051.23%\u51c6\u786e\u7387\uff08RGB\u4ec523.12%\uff09\uff0c\u5728\u5e27\u91cd\u5efa\u4e2d\u8fbe\u5230SSIM=0.8513\u548cPSNR=26.89dB\u7684\u9ad8\u8d28\u91cf\u7ed3\u679c", "conclusion": "\u4e8b\u4ef6\u57fa\u4e8e\u6570\u636e\u5728\u5fae\u8868\u60c5\u8bc6\u522b\u548c\u5e27\u91cd\u5efa\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8270\u671b\u7684\u6f5c\u529b\uff0c\u4e3a\u5fae\u79d1\u8868\u60c5\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12500", "categories": ["cs.AI", "cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.12500", "abs": "https://arxiv.org/abs/2508.12500", "authors": ["Rahmat K. Adesunkanmi", "Ashfaq Khokhar", "Goce Trajcevski", "Sohail Murad"], "title": "Root Cause Analysis of Hydrogen Bond Separation in Spatio-Temporal Molecular Dynamics using Causal Models", "comment": "Submitted to ACM", "summary": "Molecular dynamics simulations (MDS) face challenges, including\nresource-heavy computations and the need to manually scan outputs to detect\n\"interesting events,\" such as the formation and persistence of hydrogen bonds\nbetween atoms of different molecules. A critical research gap lies in\nidentifying the underlying causes of hydrogen bond formation and separation\n-understanding which interactions or prior events contribute to their emergence\nover time. With this challenge in mind, we propose leveraging spatio-temporal\ndata analytics and machine learning models to enhance the detection of these\nphenomena. In this paper, our approach is inspired by causal modeling and aims\nto identify the root cause variables of hydrogen bond formation and separation\nevents. Specifically, we treat the separation of hydrogen bonds as an\n\"intervention\" occurring and represent the causal structure of the bonding and\nseparation events in the MDS as graphical causal models. These causal models\nare built using a variational autoencoder-inspired architecture that enables us\nto infer causal relationships across samples with diverse underlying causal\ngraphs while leveraging shared dynamic information. We further include a step\nto infer the root causes of changes in the joint distribution of the causal\nmodels. By constructing causal models that capture shifts in the conditional\ndistributions of molecular interactions during bond formation or separation,\nthis framework provides a novel perspective on root cause analysis in molecular\ndynamic systems. We validate the efficacy of our model empirically on the\natomic trajectories that used MDS for chiral separation, demonstrating that we\ncan predict many steps in the future and also find the variables driving the\nobserved changes in the system.", "AI": {"tldr": "\u5229\u7528\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u548c\u56e0\u679c\u6a21\u578b\u6765\u5206\u6790\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u6c22\u952e\u5f62\u6210\u4e0e\u5206\u79bb\u7684\u6839\u672c\u539f\u56e0", "motivation": "\u89e3\u51b3\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u8d44\u6e90\u6d88\u8017\u5927\u3001\u9700\u624b\u52a8\u626b\u63cf\u8f93\u51fa\u4ee5\u53d1\u73b0\u5173\u952e\u4e8b\u4ef6\u7684\u6311\u6218\uff0c\u5e76\u63a2\u7d22\u6c22\u952e\u5f62\u6210\u4e0e\u5206\u79bb\u7684\u6df1\u5c42\u56e0\u679c\u5173\u7cfb", "method": "\u901a\u8fc7\u56e0\u679c\u6a21\u578b\u628a\u6c22\u952e\u5206\u79bb\u89c6\u4e3a\"\u5e72\u9884\"\u4e8b\u4ef6\uff0c\u4f7f\u7528\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u7ed3\u6784\u6784\u5efa\u56fe\u5f62\u56e0\u679c\u6a21\u578b\uff0c\u5728\u591a\u6837\u5316\u56e0\u679c\u56fe\u4e2d\u63a8\u65ad\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u5305\u542b\u805a\u5408\u5206\u5e03\u53d8\u5316\u7684\u6839\u56e0\u63a8\u65ad\u6b65\u9aa4", "result": "\u5728\u65cb\u5149\u5206\u79bb\u7684\u539f\u5b50\u8f68\u8ff9\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u6709\u6548\u6027\uff0c\u80fd\u591f\u9884\u6d4b\u591a\u6b65\u672a\u6765\u53d8\u5316\u5e76\u627e\u5230\u9a71\u52a8\u7cfb\u7edf\u53d8\u5316\u7684\u5173\u952e\u53d8\u91cf", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5206\u5b50\u52a8\u529b\u7cfb\u7edf\u6839\u56e0\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u901a\u8fc7\u6355\u6349\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u6761\u4ef6\u5206\u5e03\u7684\u79fb\u52a8\u6765\u63a2\u7d22\u7ed3\u5408\u4e8b\u4ef6\u7684\u56e0\u679c\u673a\u5236"}}
{"id": "2508.11999", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11999", "abs": "https://arxiv.org/abs/2508.11999", "authors": ["Daoze Zhang", "Zhanheng Nie", "Jianyu Liu", "Chenghan Fu", "Wanxian Guan", "Yuan Gao", "Jun Song", "Pengjie Wang", "Jian Xu", "Bo Zheng"], "title": "MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding", "comment": "11 pages, 9 figures", "summary": "With the rapid advancement of e-commerce, exploring general representations\nrather than task-specific ones has attracted increasing research attention. For\nproduct understanding, although existing discriminative dual-flow architectures\ndrive progress in this field, they inherently struggle to model the many-to-one\nalignment between multiple images and texts of products. Therefore, we argue\nthat generative Multimodal Large Language Models (MLLMs) hold significant\npotential for improving product representation learning. Nevertheless,\nachieving this goal still remains non-trivial due to several key challenges:\nthe lack of multimodal and aspect-aware modeling modules in typical LLMs; the\ncommon presence of background noise in product images; and the absence of a\nstandard benchmark for evaluation. To address these issues, we propose the\nfirst generative MLLM-based model named MOON for product representation\nlearning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for\ntargeted modeling of multimodal and aspect-specific product content; (2)\neffectively detects core semantic regions in product images to mitigate the\ndistraction and interference caused by background noise; and (3) introduces the\nspecialized negative sampling strategy to increase the difficulty and diversity\nof negative samples. In addition, we release a large-scale multimodal benchmark\nMBE for various product understanding tasks. Experimentally, our model\ndemonstrates competitive zero-shot performance on both our benchmark and the\npublic dataset, showcasing strong generalization across various downstream\ntasks, including cross-modal retrieval, product classification, and attribute\nprediction. Furthermore, the case study and visualization illustrate the\neffectiveness of MOON for product understanding.", "AI": {"tldr": "MOON\u662f\u9996\u4e2a\u57fa\u4e8e\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ea7\u54c1\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfc\u5f0f\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u3001\u6838\u5fc3\u8bed\u4e49\u533a\u57df\u68c0\u6d4b\u548c\u4e13\u95e8\u8d1f\u91c7\u6837\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4ea7\u54c1\u56fe\u50cf\u6587\u672c\u591a\u5bf9\u4e00\u5bf9\u9f50\u3001\u80cc\u666f\u566a\u58f0\u5e72\u6270\u7b49\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5224\u522b\u5f0f\u53cc\u6d41\u67b6\u6784\u96be\u4ee5\u5efa\u6a21\u4ea7\u54c1\u591a\u56fe\u50cf\u4e0e\u6587\u672c\u4e4b\u95f4\u7684\u591a\u5bf9\u4e00\u5bf9\u5e94\u5173\u7cfb\uff0c\u800c\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6539\u8fdb\u4ea7\u54c1\u8868\u793a\u5b66\u4e60\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u7f3a\u4e4f\u591a\u6a21\u6001\u5efa\u6a21\u6a21\u5757\u3001\u4ea7\u54c1\u56fe\u50cf\u80cc\u666f\u566a\u58f0\u5e72\u6270\u4ee5\u53ca\u7f3a\u4e4f\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u7b49\u6311\u6218\u3002", "method": "1) \u91c7\u7528\u5f15\u5bfc\u5f0f\u4e13\u5bb6\u6df7\u5408(MoE)\u6a21\u5757\u8fdb\u884c\u591a\u6a21\u6001\u548c\u7279\u5b9a\u65b9\u9762\u7684\u76ee\u6807\u5efa\u6a21\uff1b2) \u6709\u6548\u68c0\u6d4b\u4ea7\u54c1\u56fe\u50cf\u4e2d\u7684\u6838\u5fc3\u8bed\u4e49\u533a\u57df\u4ee5\u51cf\u5c11\u80cc\u666f\u566a\u58f0\u5e72\u6270\uff1b3) \u5f15\u5165\u4e13\u95e8\u7684\u8d1f\u91c7\u6837\u7b56\u7565\u589e\u52a0\u8d1f\u6837\u672c\u7684\u96be\u5ea6\u548c\u591a\u6837\u6027\uff1b4) \u53d1\u5e03\u5927\u89c4\u6a21\u591a\u6a21\u6001\u57fa\u51c6MBE\u3002", "result": "\u6a21\u578b\u5728\u65b0\u5efa\u57fa\u51c6\u548c\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u3001\u4ea7\u54c1\u5206\u7c7b\u548c\u5c5e\u6027\u9884\u6d4b\u7b49\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6848\u4f8b\u7814\u7a76\u548c\u53ef\u89c6\u5316\u8bc1\u660e\u4e86MOON\u5728\u4ea7\u54c1\u7406\u89e3\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "MOON\u6210\u529f\u89e3\u51b3\u4e86\u751f\u6210\u5f0fMLLM\u5728\u4ea7\u54c1\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u591a\u6a21\u6001\u5efa\u6a21\u65b9\u6cd5\u548c\u566a\u58f0\u5904\u7406\u6280\u672f\uff0c\u4e3a\u4ea7\u54c1\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5efa\u7acb\u4e86\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2508.12566", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12566", "abs": "https://arxiv.org/abs/2508.12566", "authors": ["Wei Song", "Haonan Zhong", "Ziqi Ding", "Jingling Xue", "Yuekang Li"], "title": "Help or Hurdle? Rethinking Model Context Protocol-Augmented Large Language Models", "comment": null, "summary": "The Model Context Protocol (MCP) enables large language models (LLMs) to\naccess external resources on demand. While commonly assumed to enhance\nperformance, how LLMs actually leverage this capability remains poorly\nunderstood. We introduce MCPGAUGE, the first comprehensive evaluation framework\nfor probing LLM-MCP interactions along four key dimensions: proactivity\n(self-initiated tool use), compliance (adherence to tool-use instructions),\neffectiveness (task performance post-integration), and overhead (computational\ncost incurred). MCPGAUGE comprises a 160-prompt suite and 25 datasets spanning\nknowledge comprehension, general reasoning, and code generation. Our\nlarge-scale evaluation, spanning six commercial LLMs, 30 MCP tool suites, and\nboth one- and two-turn interaction settings, comprises around 20,000 API calls\nand over USD 6,000 in computational cost. This comprehensive study reveals four\nkey findings that challenge prevailing assumptions about the effectiveness of\nMCP integration. These insights highlight critical limitations in current\nAI-tool integration and position MCPGAUGE as a principled benchmark for\nadvancing controllable, tool-augmented LLMs.", "AI": {"tldr": "MCPGAUGE\u662f\u9996\u4e2a\u8bc4\u4f30LLM\u4e0eMCP\u4ea4\u4e92\u7684\u7efc\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u7ef4\u5ea6\uff08\u4e3b\u52a8\u6027\u3001\u5408\u89c4\u6027\u3001\u6709\u6548\u6027\u3001\u5f00\u9500\uff09\u5bf96\u4e2a\u5546\u4e1aLLM\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u53d1\u73b0MCP\u96c6\u6210\u7684\u6548\u679c\u5b58\u5728\u5173\u952e\u5c40\u9650\u6027\u3002", "motivation": "\u867d\u7136MCP\u534f\u8bae\u8ba9LLM\u80fd\u591f\u6309\u9700\u8bbf\u95ee\u5916\u90e8\u8d44\u6e90\uff0c\u4f46LLM\u5982\u4f55\u5b9e\u9645\u5229\u7528\u8fd9\u79cd\u80fd\u529b\u4ecd\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u7406\u89e3LLM-MCP\u4ea4\u4e92\u7684\u771f\u5b9e\u6548\u679c\u3002", "method": "\u5f00\u53d1MCPGAUGE\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b160\u4e2a\u63d0\u793a\u548c25\u4e2a\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u77e5\u8bc6\u7406\u89e3\u3001\u901a\u7528\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\uff0c\u5bf96\u4e2a\u5546\u4e1aLLM\u300130\u4e2aMCP\u5de5\u5177\u5957\u4ef6\u8fdb\u884c\u7ea620,000\u6b21API\u8c03\u7528\u7684\u5927\u89c4\u6a21\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u56db\u4e2a\u5173\u952e\u53d1\u73b0\uff0c\u6311\u6218\u4e86\u5173\u4e8eMCP\u96c6\u6210\u6709\u6548\u6027\u7684\u666e\u904d\u5047\u8bbe\uff0c\u7a81\u663e\u4e86\u5f53\u524dAI\u5de5\u5177\u96c6\u6210\u7684\u5173\u952e\u5c40\u9650\u6027\u3002", "conclusion": "MCPGAUGE\u4e3a\u63a8\u8fdb\u53ef\u63a7\u7684\u5de5\u5177\u589e\u5f3a\u578bLLM\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524dMCP\u96c6\u6210\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.12015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12015", "abs": "https://arxiv.org/abs/2508.12015", "authors": ["Hongyuan Liu", "Haochen Yu", "Jianfei Jiang", "Qiankun Liu", "Jiansheng Chen", "Huimin Ma"], "title": "InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes", "comment": null, "summary": "Reconstructing dynamic driving scenes from dashcam videos has attracted\nincreasing attention due to its significance in autonomous driving and scene\nunderstanding. While recent advances have made impressive progress, most\nmethods still unify all background elements into a single representation,\nhindering both instance-level understanding and flexible scene editing. Some\napproaches attempt to lift 2D segmentation into 3D space, but often rely on\npre-processed instance IDs or complex pipelines to map continuous features to\ndiscrete identities. Moreover, these methods are typically designed for indoor\nscenes with rich viewpoints, making them less applicable to outdoor driving\nscenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian\nSplatting framework tailored for the interactive reconstruction of dynamic\ndriving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D\nfeature learning via contrastive loss and pseudo-supervised objectives. At the\n3D level, we introduce regularization to implicitly encode instance identities\nand enforce consistency through a voxel-based loss. A lightweight static\ncodebook further bridges continuous features and discrete identities without\nrequiring data pre-processing or complex optimization. Quantitative and\nqualitative experiments demonstrate the effectiveness of InstDrive, and to the\nbest of our knowledge, it is the first framework to achieve 3D instance\nsegmentation in dynamic, open-world driving scenes.More visualizations are\navailable at our project page.", "AI": {"tldr": "InstDrive\u662f\u4e00\u4e2a\u9762\u5411\u52a8\u6001\u9a7e\u9a76\u573a\u666f\u7684\u5b9e\u4f8b\u611f\u77e53D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7SAM\u751f\u6210\u7684\u63a9\u7801\u4f5c\u4e3a\u4f2a\u771f\u503c\u6307\u5bfc2D\u7279\u5f81\u5b66\u4e60\uff0c\u57283D\u5c42\u9762\u5f15\u5165\u6b63\u5219\u5316\u9690\u5f0f\u7f16\u7801\u5b9e\u4f8b\u8eab\u4efd\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u6570\u636e\u9884\u5904\u7406\u7684\u52a8\u6001\u5f00\u653e\u4e16\u754c\u9a7e\u9a76\u573a\u666f3D\u5b9e\u4f8b\u5206\u5272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u6240\u6709\u80cc\u666f\u5143\u7d20\u7edf\u4e00\u4e3a\u5355\u4e00\u8868\u793a\uff0c\u963b\u788d\u4e86\u5b9e\u4f8b\u7ea7\u7406\u89e3\u548c\u7075\u6d3b\u573a\u666f\u7f16\u8f91\uff1b\u73b0\u6709\u5ba4\u5185\u573a\u666f\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u5ba4\u5916\u9a7e\u9a76\u573a\u666f\uff1b\u9700\u8981\u89e3\u51b32D\u5206\u5272\u52303D\u7a7a\u95f4\u7684\u6620\u5c04\u95ee\u9898\u3002", "method": "\u4f7f\u7528SAM\u751f\u6210\u7684\u63a9\u7801\u4f5c\u4e3a\u4f2a\u771f\u503c\uff0c\u901a\u8fc7\u5bf9\u6bd4\u635f\u5931\u548c\u4f2a\u76d1\u7763\u76ee\u6807\u6307\u5bfc2D\u7279\u5f81\u5b66\u4e60\uff1b\u57283D\u5c42\u9762\u5f15\u5165\u6b63\u5219\u5316\u9690\u5f0f\u7f16\u7801\u5b9e\u4f8b\u8eab\u4efd\uff0c\u901a\u8fc7\u4f53\u7d20\u635f\u5931\u5f3a\u5236\u4e00\u81f4\u6027\uff1b\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9759\u6001\u4ee3\u7801\u6865\u63a5\u8fde\u7eed\u7279\u5f81\u548c\u79bb\u6563\u8eab\u4efd\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8bc1\u660e\u4e86InstDrive\u7684\u6709\u6548\u6027\uff0c\u662f\u9996\u4e2a\u5728\u52a8\u6001\u5f00\u653e\u4e16\u754c\u9a7e\u9a76\u573a\u666f\u4e2d\u5b9e\u73b03D\u5b9e\u4f8b\u5206\u5272\u7684\u6846\u67b6\u3002", "conclusion": "InstDrive\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u9a7e\u9a76\u573a\u666f\u7684\u5b9e\u4f8b\u611f\u77e5\u91cd\u5efa\u95ee\u9898\uff0c\u65e0\u9700\u6570\u636e\u9884\u5904\u7406\u6216\u590d\u6742\u4f18\u5316\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u548c\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12611", "categories": ["cs.AI", "cs.CL", "I.2.7; F.4.1"], "pdf": "https://arxiv.org/pdf/2508.12611", "abs": "https://arxiv.org/abs/2508.12611", "authors": ["Trang Tran", "Trung Hoang Le", "Huiping Cao", "Tran Cao Son"], "title": "An LLM + ASP Workflow for Joint Entity-Relation Extraction", "comment": "13 pages, 1 figure, Accepted as Technical Communication, 41st\n  International Conference on Logic Programming", "summary": "Joint entity-relation extraction (JERE) identifies both entities and their\nrelationships simultaneously. Traditional machine-learning based approaches to\nperforming this task require a large corpus of annotated data and lack the\nability to easily incorporate domain specific information in the construction\nof the model. Therefore, creating a model for JERE is often labor intensive,\ntime consuming, and elaboration intolerant. In this paper, we propose\nharnessing the capabilities of generative pretrained large language models\n(LLMs) and the knowledge representation and reasoning capabilities of Answer\nSet Programming (ASP) to perform JERE. We present a generic workflow for JERE\nusing LLMs and ASP. The workflow is generic in the sense that it can be applied\nfor JERE in any domain. It takes advantage of LLM's capability in natural\nlanguage understanding in that it works directly with unannotated text. It\nexploits the elaboration tolerant feature of ASP in that no modification of its\ncore program is required when additional domain specific knowledge, in the form\nof type specifications, is found and needs to be used. We demonstrate the\nusefulness of the proposed workflow through experiments with limited training\ndata on three well-known benchmarks for JERE. The results of our experiments\nshow that the LLM + ASP workflow is better than state-of-the-art JERE systems\nin several categories with only 10\\% of training data. It is able to achieve a\n2.5 times (35\\% over 15\\%) improvement in the Relation Extraction task for the\nSciERC corpus, one of the most difficult benchmarks.", "AI": {"tldr": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u7b54\u6848\u96c6\u7f16\u7a0b\u7684\u8054\u5408\u5b9e\u4f53-\u5173\u7cfb\u63d0\u53d6\u65b9\u6cd5\uff0c\u53ea\u9700\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3001\u65e0\u6cd5\u5bb9\u6613\u878d\u5165\u9886\u57df\u77e5\u8bc6\u3001\u6784\u5efa\u6a21\u578b\u82e6\u96be\u8017\u65f6\u7684\u95ee\u9898", "method": "\u7ed3\u5408\u751f\u6210\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u548c\u7b54\u6848\u96c6\u7f16\u7a0b(ASP)\u7684\u77e5\u8bc6\u8868\u793a\u4e0e\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51fa\u901a\u7528\u5de5\u4f5c\u6d41\u7a0b", "result": "\u5728\u4e09\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4ec5\u970010%\u8bad\u7ec3\u6570\u636e\u5373\u5728\u591a\u4e2a\u7c7b\u522b\u8d85\u8fc7\u73b0\u6709\u6700\u4f73\u7cfb\u7edf\uff0c\u5728SciERC\u6570\u636e\u96c6\u7684\u5173\u7cfb\u63d0\u53d6\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8635%\u7684F1\u503c\uff08\u76f8\u6bd415%\uff09", "conclusion": "LLM+ASP\u65b9\u6cd5\u4e3a\u8054\u5408\u5b9e\u4f53-\u5173\u7cfb\u63d0\u53d6\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u4e0b\u8fbe\u5230\u4f18\u5f02\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u9886\u57df\u8868\u73b0\u7a81\u51fa"}}
{"id": "2508.12023", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12023", "abs": "https://arxiv.org/abs/2508.12023", "authors": ["Durgesh Kumar Singh", "Qing Cao", "Sarina Thomas", "Ahc\u00e8ne Boubekki", "Robert Jenssen", "Michael Kampffmeyer"], "title": "WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements", "comment": null, "summary": "Clinical guidelines recommend performing left ventricular (LV) linear\nmeasurements in B-mode echocardiographic images at the basal level -- typically\nat the mitral valve leaflet tips -- and aligned perpendicular to the LV long\naxis along a virtual scanline (SL). However, most automated methods estimate\nlandmarks directly from B-mode images for the measurement task, where even\nsmall shifts in predicted points along the LV walls can lead to significant\nmeasurement errors, reducing their clinical reliability. A recent\nsemi-automatic method, EnLVAM, addresses this limitation by constraining\nlandmark prediction to a clinician-defined SL and training on generated\nAnatomical Motion Mode (AMM) images to predict LV landmarks along the same. To\nenable full automation, a contour-aware SL placement approach is proposed in\nthis work, in which the LV contour is estimated using a weakly supervised\nB-mode landmark detector. SL placement is then performed by inferring the LV\nlong axis and the basal level-mimicking clinical guidelines. Building on this\nfoundation, we introduce \\textit{WiseLVAM} -- a novel, fully automated yet\nmanually adaptable framework for automatically placing the SL and then\nautomatically performing the LV linear measurements in the AMM mode.\n\\textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the\nmotion-awareness from AMM mode to enhance robustness and accuracy with the\npotential to provide a practical solution for the routine clinical application.", "AI": {"tldr": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5168\u81ea\u52a8\u5316\u5de6\u5ba4\u7ebf\u6027\u6d4b\u91cf\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408B\u6a21\u5f0f\u56fe\u50cf\u7ed3\u6784\u4fe1\u606f\u548cAMM\u6a21\u5f0f\u8fd0\u52a8\u4fe1\u606f\u6765\u63d0\u9ad8\u6d4b\u91cf\u7cbe\u5ea6\u548c\u7a33\u5065\u6027", "motivation": "\u89e3\u51b3\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5728B\u6a21\u5f0f\u56fe\u50cf\u4e2d\u76f4\u63a5\u9884\u6d4b\u6807\u8bb0\u70b9\u65f6\uff0c\u5c0f\u7684\u4f4d\u79fb\u5bfc\u81f4\u6d4b\u91cf\u9519\u8bef\u8fc7\u5927\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e34\u5e8a\u53ef\u9760\u6027", "method": "\u63d0\u51fa\u4e86WiseLVAM\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u5f31\u76d1\u7763B\u6a21\u5f0f\u6807\u8bb0\u70b9\u68c0\u6d4b\u5668\u4f30\u8ba1\u5de6\u5ba4\u8f6e\u5ed3 2\uff09\u63a8\u65ad\u5de6\u5ba4\u957f\u8f74\u548c\u57fa\u5e95\u6c34\u5e73\u6765\u5b9a\u4f4d\u626b\u63cf\u7ebf 3\uff09\u5728AMM\u6a21\u5f0f\u4e2d\u81ea\u52a8\u8fdb\u884c\u7ebf\u6027\u6d4b\u91cf", "result": "\u65b9\u6cd5\u7ed3\u5408\u4e86B\u6a21\u5f0f\u56fe\u50cf\u7684\u7ed3\u6784\u611f\u77e5\u80fd\u529b\u548cAMM\u6a21\u5f0f\u7684\u8fd0\u52a8\u611f\u77e5\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u6d4b\u91cf\u7684\u7a33\u5065\u6027\u548c\u51c6\u786e\u6027", "conclusion": "WiseLVAM\u4e3a\u5de6\u5ba4\u7ebf\u6027\u6d4b\u91cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u81ea\u52a8\u5316\u4f46\u53c8\u53ef\u624b\u52a8\u8c03\u6574\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6f5c\u529b\u5728\u5e38\u89c4\u4e34\u5e8a\u5e94\u7528\u4e2d\u63a8\u5e7f"}}
{"id": "2508.12647", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12647", "abs": "https://arxiv.org/abs/2508.12647", "authors": ["Hengnian Gu", "Zhifu Chen", "Yuxin Chen", "Jin Peng Zhou", "Dongdai Zhou"], "title": "Cognitive Structure Generation: From Educational Priors to Policy Optimization", "comment": null, "summary": "Cognitive structure is a student's subjective organization of an objective\nknowledge system, reflected in the psychological construction of concepts and\ntheir relations. However, cognitive structure assessment remains a\nlong-standing challenge in student modeling and psychometrics, persisting as a\nfoundational yet largely unassessable concept in educational practice. This\npaper introduces a novel framework, Cognitive Structure Generation (CSG), in\nwhich we first pretrain a Cognitive Structure Diffusion Probabilistic Model\n(CSDPM) to generate students' cognitive structures from educational priors, and\nthen further optimize its generative process as a policy with hierarchical\nreward signals via reinforcement learning to align with genuine cognitive\ndevelopment levels during students' learning processes. Experimental results on\nfour popular real-world education datasets show that cognitive structures\ngenerated by CSG offer more comprehensive and effective representations for\nstudent modeling, substantially improving performance on KT and CD tasks while\nenhancing interpretability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8ba4\u77e5\u7ed3\u6784\u751f\u6210(CSG)\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u8ba4\u77e5\u7ed3\u6784\u6269\u6563\u6982\u7387\u6a21\u578b(CSDPM)\u4ece\u6559\u80b2\u5148\u9a8c\u751f\u6210\u5b66\u751f\u8ba4\u77e5\u7ed3\u6784\uff0c\u5e76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\u4ee5\u5bf9\u9f50\u771f\u5b9e\u8ba4\u77e5\u53d1\u5c55\u6c34\u5e73\u3002", "motivation": "\u8ba4\u77e5\u7ed3\u6784\u662f\u5b66\u751f\u5bf9\u77e5\u8bc6\u7cfb\u7edf\u7684\u4e3b\u89c2\u7ec4\u7ec7\uff0c\u4f46\u5728\u6559\u80b2\u5b9e\u8df5\u4e2d\u4e00\u76f4\u96be\u4ee5\u8bc4\u4f30\uff0c\u662f\u5b66\u751f\u5efa\u6a21\u548c\u5fc3\u7406\u6d4b\u91cf\u5b66\u4e2d\u7684\u957f\u671f\u6311\u6218\u3002", "method": "\u9996\u5148\u9884\u8bad\u7ec3\u8ba4\u77e5\u7ed3\u6784\u6269\u6563\u6982\u7387\u6a21\u578b(CSDPM)\u4ece\u6559\u80b2\u5148\u9a8c\u751f\u6210\u8ba4\u77e5\u7ed3\u6784\uff0c\u7136\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f7f\u7528\u5206\u5c42\u5956\u52b1\u4fe1\u53f7\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\uff0c\u4f7f\u5176\u4e0e\u5b66\u751f\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u771f\u5b9e\u8ba4\u77e5\u53d1\u5c55\u6c34\u5e73\u5bf9\u9f50\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6559\u80b2\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCSG\u751f\u6210\u7684\u8ba4\u77e5\u7ed3\u6784\u4e3a\u5b66\u751f\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u6709\u6548\u7684\u8868\u793a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u77e5\u8bc6\u8ffd\u8e2a(KT)\u548c\u8ba4\u77e5\u8bca\u65ad(CD)\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "CSG\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8ba4\u77e5\u7ed3\u6784\u8bc4\u4f30\u7684\u96be\u9898\uff0c\u751f\u6210\u7684\u8ba4\u77e5\u7ed3\u6784\u80fd\u591f\u6709\u6548\u63d0\u5347\u5b66\u751f\u5efa\u6a21\u6027\u80fd\u5e76\u589e\u5f3a\u6559\u80b2\u5b9e\u8df5\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.12036", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12036", "abs": "https://arxiv.org/abs/2508.12036", "authors": ["Rakesh Thakur", "Yusra Tariq"], "title": "Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering", "comment": "8 pages, 4 figures Submitted to AAAI 26", "summary": "Solving tough clinical questions that require both image and text\nunderstanding is still a major challenge in healthcare AI. In this work, we\npropose Q-FSRU, a new model that combines Frequency Spectrum Representation and\nFusion (FSRU) with a method called Quantum Retrieval-Augmented Generation\n(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in\nfeatures from medical images and related text, then shifts them into the\nfrequency domain using Fast Fourier Transform (FFT). This helps it focus on\nmore meaningful data and filter out noise or less useful information. To\nimprove accuracy and ensure that answers are based on real knowledge, we add a\nquantum-inspired retrieval system. It fetches useful medical facts from\nexternal sources using quantum-based similarity techniques. These details are\nthen merged with the frequency-based features for stronger reasoning. We\nevaluated our model using the VQA-RAD dataset, which includes real radiology\nimages and questions. The results showed that Q-FSRU outperforms earlier\nmodels, especially on complex cases needing image-text reasoning. The mix of\nfrequency and quantum information improves both performance and explainability.\nOverall, this approach offers a promising way to build smart, clear, and\nhelpful AI tools for doctors.", "AI": {"tldr": "\u57fa\u4e8e\u9891\u57df\u8868\u793a\u548c\u91cf\u5b50\u68c0\u7d22\u7684\u533b\u7597\u89c6\u89c9\u95ee\u7b54\u6a21\u578bQ-FSRU\uff0c\u901a\u8fc7FFT\u9891\u57df\u8f6c\u6362\u548c\u91cf\u5b50\u53cd\u9988\u589e\u5f3a\u6280\u672f\uff0c\u5728VQA-RAD\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e86\u66f4\u4f18\u7684\u8868\u73b0", "motivation": "\u89e3\u51b3\u9700\u8981\u540c\u65f6\u7406\u89e3\u56fe\u50cf\u548c\u6587\u672c\u7684\u590d\u6742\u4e34\u5e8a\u95ee\u9898\uff0c\u63d0\u9ad8\u533b\u7597AI\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027", "method": "\u7ed3\u5408\u9891\u8c31\u8868\u793a\u878d\u5408(FSRU)\u548c\u91cf\u5b50\u68c0\u7d22\u589e\u5f3a\u751f\u6210(Quantum RAG)\u6280\u672f\uff0c\u901a\u8fc7FFT\u5c06\u56fe\u50cf\u6587\u672c\u7279\u5f81\u8f6c\u6362\u5230\u9891\u57df\uff0c\u4f7f\u7528\u91cf\u5b50\u76f8\u4f3c\u6027\u6280\u672f\u4ece\u5916\u90e8\u6e90\u83b7\u53d6\u533b\u5b66\u77e5\u8bc6", "result": "\u5728VQA-RAD\u6570\u636e\u96c6\u4e0a\u8d85\u8fc7\u4e4b\u524d\u6a21\u578b\uff0c\u5c24\u5176\u5728\u9700\u8981\u56fe\u50cf-\u6587\u672c\u63a8\u7406\u7684\u590d\u6742\u6848\u4f8b\u4e2d\u8868\u73b0\u66f4\u4f18", "conclusion": "\u9891\u57df\u4fe1\u606f\u4e0e\u91cf\u5b50\u4fe1\u606f\u7684\u7ed3\u5408\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u5efa\u7acb\u667a\u80fd\u3001\u6e05\u6670\u3001\u6709\u7528\u7684\u533b\u751fAI\u5de5\u5177\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5"}}
{"id": "2508.12651", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.12651", "abs": "https://arxiv.org/abs/2508.12651", "authors": ["Chunliang Hua", "Xiao Hu", "Jiayang Sun", "Zeyuan Yang"], "title": "The Maximum Coverage Model and Recommendation System for UAV Vertiports Location Planning", "comment": "10 pages", "summary": "As urban aerial mobility (UAM) infrastructure development accelerates\nglobally, cities like Shenzhen are planning large-scale vertiport networks\n(e.g., 1,200+ facilities by 2026). Existing planning frameworks remain\ninadequate for this complexity due to historical limitations in data\ngranularity and real-world applicability. This paper addresses these gaps by\nfirst proposing the Capacitated Dynamic Maximum Covering Location Problem\n(CDMCLP), a novel optimization framework that simultaneously models urban-scale\nspatial-temporal demand, heterogeneous user behaviors, and infrastructure\ncapacity constraints. Building on this foundation, we introduce an Integrated\nPlanning Recommendation System that combines CDMCLP with socio-economic factors\nand dynamic clustering initialization. This system leverages adaptive parameter\ntuning based on empirical user behavior to generate practical planning\nsolutions. Validation in a Chinese center city demonstrates the effectiveness\nof the new optimization framework and recommendation system. Under the\nevaluation and optimization of CDMCLP, the quantitative performance of\ntraditional location methods are exposed and can be improved by 38\\%--52\\%,\nwhile the recommendation system shows user-friendliness and the effective\nintegration of complex elements. By integrating mathematical rigor with\npractical implementation considerations, this hybrid approach bridges the gap\nbetween theoretical location modeling and real-world UAM infrastructure\nplanning, offering municipalities a pragmatic tool for vertiport network\ndesign.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5bb9\u91cf\u7ea6\u675f\u52a8\u6001\u6700\u5927\u8986\u76d6\u4f4d\u7f6e\u95ee\u9898(CDMCLP)\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u793e\u4f1a\u7ecf\u6d4e\u56e0\u7d20\u548c\u52a8\u6001\u805a\u7c7b\u521d\u59cb\u5316\uff0c\u4e3a\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a(UAM)\u5782\u76f4\u673a\u573a\u7f51\u7edc\u89c4\u5212\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89c4\u5212\u63a8\u8350\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740\u5168\u7403\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u5feb\u901f\u53d1\u5c55\uff0c\u50cf\u6df1\u5733\u8fd9\u6837\u7684\u57ce\u5e02\u6b63\u5728\u89c4\u5212\u5927\u89c4\u6a21\u5782\u76f4\u673a\u573a\u7f51\u7edc\u3002\u73b0\u6709\u7684\u89c4\u5212\u6846\u67b6\u56e0\u5386\u53f2\u6570\u636e\u7c92\u5ea6\u548c\u5b9e\u9645\u5e94\u7528\u6027\u7684\u9650\u5236\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u79cd\u590d\u6742\u6027\u9700\u6c42\u3002", "method": "\u9996\u5148\u63d0\u51fa\u5bb9\u91cf\u7ea6\u675f\u52a8\u6001\u6700\u5927\u8986\u76d6\u4f4d\u7f6e\u95ee\u9898(CDMCLP)\u4f18\u5316\u6846\u67b6\uff0c\u540c\u65f6\u5efa\u6a21\u57ce\u5e02\u7ea7\u7a7a\u95f4-\u65f6\u95f4\u9700\u6c42\u3001\u5f02\u8d28\u7528\u6237\u884c\u4e3a\u548c\u57fa\u7840\u8bbe\u65bd\u5bb9\u91cf\u7ea6\u675f\u3002\u57fa\u4e8e\u6b64\u6784\u5efa\u96c6\u6210\u89c4\u5212\u63a8\u8350\u7cfb\u7edf\uff0c\u7ed3\u5408\u793e\u4f1a\u7ecf\u6d4e\u56e0\u7d20\u548c\u52a8\u6001\u805a\u7c7b\u521d\u59cb\u5316\uff0c\u5229\u7528\u57fa\u4e8e\u7ecf\u9a8c\u7528\u6237\u884c\u4e3a\u7684\u9002\u5e94\u6027\u53c2\u6570\u8c03\u6574\u6765\u751f\u6210\u5b9e\u7528\u89c4\u5212\u65b9\u6848\u3002", "result": "\u5728\u4e2d\u56fd\u4e2d\u5fc3\u57ce\u5e02\u7684\u9a8c\u8bc1\u663e\u793a\u4e86\u65b0\u4f18\u5316\u6846\u67b6\u548c\u63a8\u8350\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002\u5728CDMCLP\u7684\u8bc4\u4f30\u548c\u4f18\u5316\u4e0b\uff0c\u4f20\u7edf\u4f4d\u7f6e\u65b9\u6cd5\u7684\u6570\u91cf\u6027\u8868\u73b0\u88ab\u66dd\u9732\u5e76\u80fd\u591f\u63d0\u9ad838%-52%\uff0c\u800c\u63a8\u8350\u7cfb\u7edf\u663e\u793a\u4e86\u7528\u6237\u53cb\u597d\u6027\u548c\u590d\u6742\u5143\u7d20\u7684\u6709\u6548\u96c6\u6210\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6570\u5b66\u4e25\u8c28\u6027\u4e0e\u5b9e\u9645\u5b9e\u65bd\u8003\u8651\u76f8\u7ed3\u5408\uff0c\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u5e73\u606f\u4e86\u7406\u8bba\u4f4d\u7f6e\u5efa\u6a21\u4e0e\u5b9e\u9645UAM\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5e02\u653f\u5e9c\u63d0\u4f9b\u4e86\u5782\u76f4\u673a\u573a\u7f51\u7edc\u8bbe\u8ba1\u7684\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2508.12081", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12081", "abs": "https://arxiv.org/abs/2508.12081", "authors": ["Haidong Xu", "Guangwei Xu", "Zhedong Zheng", "Xiatian Zhu", "Wei Ji", "Xiangtai Li", "Ruijie Guo", "Meishan Zhang", "Min zhang", "Hao Fei"], "title": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models", "comment": "20 pages,13 figures", "summary": "This paper introduces VimoRAG, a novel video-based retrieval-augmented motion\ngeneration framework for motion large language models (LLMs). As motion LLMs\nface severe out-of-domain/out-of-vocabulary issues due to limited annotated\ndata, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D\nmotion generation by retrieving relevant 2D human motion signals. While\nvideo-based motion RAG is nontrivial, we address two key bottlenecks: (1)\ndeveloping an effective motion-centered video retrieval model that\ndistinguishes human poses and actions, and (2) mitigating the issue of error\npropagation caused by suboptimal retrieval results. We design the Gemini Motion\nVideo Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,\nenabling effective retrieval and generation processes. Experimental results\nshow that VimoRAG significantly boosts the performance of motion LLMs\nconstrained to text-only input.", "AI": {"tldr": "VimoRAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u68c0\u7d22\u589e\u5f3a\u7684\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u76f8\u51732D\u4eba\u4f53\u8fd0\u52a8\u4fe1\u53f7\u6765\u89e3\u51b3\u8fd0\u52a8\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ec5\u6587\u672c\u8f93\u5165\u6761\u4ef6\u4e0b\u76843D\u8fd0\u52a8\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u8fd0\u52a8\u5927\u8bed\u8a00\u6a21\u578b\u7531\u4e8e\u6807\u6ce8\u6570\u636e\u6709\u9650\u800c\u9762\u4e34\u4e25\u91cd\u7684\u57df\u5916/\u8bcd\u6c47\u5916\u95ee\u9898\uff0c\u9700\u8981\u5229\u7528\u5927\u89c4\u6a21\u91ce\u5916\u89c6\u9891\u6570\u636e\u5e93\u6765\u589e\u5f3a3D\u8fd0\u52a8\u751f\u6210\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86Gemini Motion Video Retriever\u673a\u5236\u8fdb\u884c\u6709\u6548\u7684\u8fd0\u52a8\u4e2d\u5fc3\u89c6\u9891\u68c0\u7d22\uff0c\u4ee5\u53caMotion-centric Dual-alignment DPO Trainer\u6765\u7f13\u89e3\u68c0\u7d22\u7ed3\u679c\u4e0d\u4f73\u5bfc\u81f4\u7684\u9519\u8bef\u4f20\u64ad\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVimoRAG\u663e\u8457\u63d0\u5347\u4e86\u4ec5\u6587\u672c\u8f93\u5165\u6761\u4ef6\u4e0b\u8fd0\u52a8\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "VimoRAG\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u8fd0\u52a8\u68c0\u7d22\u589e\u5f3a\u7684\u5173\u952e\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u68c0\u7d22\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12682", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12682", "abs": "https://arxiv.org/abs/2508.12682", "authors": ["Jinquan Shi", "Yingying Cheng", "Fan Zhang", "Miao Jiang", "Jun Lin", "Yanbai Shen"], "title": "GridCodex: A RAG-Driven AI Framework for Power Grid Code Reasoning and Compliance", "comment": null, "summary": "The global shift towards renewable energy presents unprecedented challenges\nfor the electricity industry, making regulatory reasoning and compliance\nincreasingly vital. Grid codes, the regulations governing grid operations, are\ncomplex and often lack automated interpretation solutions, which hinders\nindustry expansion and undermines profitability for electricity companies. We\nintroduce GridCodex, an end to end framework for grid code reasoning and\ncompliance that leverages large language models and retrieval-augmented\ngeneration (RAG). Our framework advances conventional RAG workflows through\nmulti stage query refinement and enhanced retrieval with RAPTOR. We validate\nthe effectiveness of GridCodex with comprehensive benchmarks, including\nautomated answer assessment across multiple dimensions and regulatory agencies.\nExperimental results showcase a 26.4% improvement in answer quality and more\nthan a 10 fold increase in recall rate. An ablation study further examines the\nimpact of base model selection.", "AI": {"tldr": "GridCodex\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u7aef\u5230\u7aef\u7535\u7f51\u89c4\u8303\u63a8\u7406\u4e0e\u5408\u89c4\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u67e5\u8be2\u4f18\u5316\u548cRAPTOR\u589e\u5f3a\u68c0\u7d22\uff0c\u5728\u7b54\u6848\u8d28\u91cf\u548c\u53ec\u56de\u7387\u65b9\u9762\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u53ef\u518d\u751f\u80fd\u6e90\u8f6c\u578b\u7ed9\u7535\u529b\u884c\u4e1a\u5e26\u6765\u6311\u6218\uff0c\u7535\u7f51\u89c4\u8303\u590d\u6742\u4e14\u7f3a\u4e4f\u81ea\u52a8\u5316\u89e3\u8bfb\u65b9\u6848\uff0c\u963b\u788d\u884c\u4e1a\u53d1\u5c55\u5e76\u5f71\u54cd\u7535\u529b\u516c\u53f8\u76c8\u5229\u80fd\u529b\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u67e5\u8be2\u4f18\u5316\u548cRAPTOR\u589e\u5f3a\u68c0\u7d22\u6280\u672f\u6784\u5efa\u7aef\u5230\u7aef\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u7b54\u6848\u8d28\u91cf\u63d0\u534726.4%\uff0c\u53ec\u56de\u7387\u63d0\u9ad810\u500d\u4ee5\u4e0a\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u57fa\u7840\u6a21\u578b\u9009\u62e9\u7684\u5f71\u54cd\u3002", "conclusion": "GridCodex\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7535\u7f51\u89c4\u8303\u81ea\u52a8\u89e3\u8bfb\u7684\u96be\u9898\uff0c\u4e3a\u7535\u529b\u884c\u4e1a\u76d1\u7ba1\u5408\u89c4\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12082", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12082", "abs": "https://arxiv.org/abs/2508.12082", "authors": ["Seungju Yoo", "Hyuk Kwon", "Joong-Won Hwang", "Kibok Lee"], "title": "Automated Model Evaluation for Object Detection via Prediction Consistency and Reliablity", "comment": "ICCV 2025 Oral", "summary": "Recent advances in computer vision have made training object detectors more\nefficient and effective; however, assessing their performance in real-world\napplications still relies on costly manual annotation. To address this\nlimitation, we develop an automated model evaluation (AutoEval) framework for\nobject detection. We propose Prediction Consistency and Reliability (PCR),\nwhich leverages the multiple candidate bounding boxes that conventional\ndetectors generate before non-maximum suppression (NMS). PCR estimates\ndetection performance without ground-truth labels by jointly measuring 1) the\nspatial consistency between boxes before and after NMS, and 2) the reliability\nof the retained boxes via the confidence scores of overlapping boxes. For a\nmore realistic and scalable evaluation, we construct a meta-dataset by applying\nimage corruptions of varying severity. Experimental results demonstrate that\nPCR yields more accurate performance estimates than existing AutoEval methods,\nand the proposed meta-dataset covers a wider range of detection performance.\nThe code is available at https://github.com/YonseiML/autoeval-det.", "AI": {"tldr": "\u81ea\u52a8\u5316\u5bf9\u68c0\u6d4b\u5668\u8bc4\u4f30\u7684\u65b0\u65b9\u6cd5PCR\uff0c\u901a\u8fc7\u5206\u6790NMS\u524d\u540e\u68c0\u6d4b\u6846\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\u6765\u4f30\u8ba1\u6027\u80fd\uff0c\u65e0\u9700\u624b\u52a8\u6807\u6ce8", "motivation": "\u89e3\u51b3\u5bf9\u8c61\u68c0\u6d4b\u5668\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6027\u80fd\u8bc4\u4f30\u4f9d\u8d56\u6210\u672c\u9ad8\u6602\u7684\u624b\u52a8\u6807\u6ce8\u95ee\u9898", "method": "\u63d0\u51fa\u9884\u6d4b\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027(PCR)\u65b9\u6cd5\uff0c\u805a\u5408\u5206\u6790NMS\u524d\u540e\u68c0\u6d4b\u6846\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u91cd\u53e0\u6846\u7684\u4fe1\u5fc3\u5ea6\u53ef\u9760\u6027", "result": "PCR\u5728\u6784\u5efa\u7684\u5143\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u8d85\u8fc7\u73b0\u6709\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u6027\u80fd\u4f30\u8ba1", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5bf9\u8c61\u68c0\u6d4b\u5668\u7684\u81ea\u52a8\u5316\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5143\u6570\u636e\u96c6\u8986\u76d6\u4e86\u66f4\u5e7f\u6cdb\u7684\u68c0\u6d4b\u6027\u80fd\u8303\u56f4"}}
{"id": "2508.12687", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12687", "abs": "https://arxiv.org/abs/2508.12687", "authors": ["Ashish Seth", "Utkarsh Tyagi", "Ramaneswaran Selvakumar", "Nishit Anand", "Sonal Kumar", "Sreyan Ghosh", "Ramani Duraiswami", "Chirag Agarwal", "Dinesh Manocha"], "title": "EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance in complex multimodal tasks. While MLLMs excel at visual perception\nand reasoning in third-person and egocentric videos, they are prone to\nhallucinations, generating coherent yet inaccurate responses. We present\nEgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric\nvideos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated\nopen and closed-ended questions designed to trigger hallucinations in both\nvisual and auditory cues in egocentric videos. Evaluations across ten MLLMs\nreveal significant challenges, including powerful models like GPT-4o and\nGemini, achieving only 59% accuracy. EgoIllusion lays the foundation in\ndeveloping robust benchmarks to evaluate the effectiveness of MLLMs and spurs\nthe development of better egocentric MLLMs with reduced hallucination rates.\nOur benchmark will be open-sourced for reproducibility.", "AI": {"tldr": "EgoIllusion\u662f\u9996\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u5e7b\u89c9\u95ee\u9898\u7684\u57fa\u51c6\uff0c\u5305\u542b1400\u4e2a\u89c6\u9891\u548c8000\u4e2a\u4eba\u5de5\u6807\u6ce8\u95ee\u9898\uff0c\u6d4b\u8bd5\u663e\u793aGPT-4o\u548cGemini\u7b49\u9876\u7ea7\u6a21\u578b\u51c6\u786e\u7387\u4ec559%\u3002", "motivation": "\u867d\u7136\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u5bb9\u6613\u4ea7\u751f\u770b\u4f3c\u8fde\u8d2f\u4f46\u4e0d\u51c6\u786e\u7684\u5e7b\u89c9\u56de\u7b54\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u5305\u542b1400\u4e2a\u89c6\u9891\u548c8000\u4e2a\u4eba\u5de5\u6807\u6ce8\u95ee\u9898\u7684EgoIllusion\u57fa\u51c6\uff0c\u8bbe\u8ba1\u5f00\u653e\u5f0f\u548c\u5c01\u95ed\u5f0f\u95ee\u9898\u6765\u89e6\u53d1\u89c6\u89c9\u548c\u542c\u89c9\u7ebf\u7d22\u7684\u5e7b\u89c9\u3002", "result": "\u5bf910\u4e2a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u5373\u4f7f\u662fGPT-4o\u548cGemini\u7b49\u5f3a\u5927\u6a21\u578b\u4e5f\u53ea\u8fbe\u523059%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "EgoIllusion\u4e3a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6709\u6548\u6027\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c06\u4fc3\u8fdb\u5f00\u53d1\u5e7b\u89c9\u7387\u66f4\u4f4e\u7684\u81ea\u6211\u4e2d\u5fc3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u57fa\u51c6\u5c06\u5f00\u6e90\u4ee5\u786e\u4fdd\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2508.12084", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12084", "abs": "https://arxiv.org/abs/2508.12084", "authors": ["Jaejun Hwang", "Dayoung Gong", "Manjin Kim", "Minsu Cho"], "title": "Generic Event Boundary Detection via Denoising Diffusion", "comment": "Accepted to ICCV 2025", "summary": "Generic event boundary detection (GEBD) aims to identify natural boundaries\nin a video, segmenting it into distinct and meaningful chunks. Despite the\ninherent subjectivity of event boundaries, previous methods have focused on\ndeterministic predictions, overlooking the diversity of plausible solutions. In\nthis paper, we introduce a novel diffusion-based boundary detection model,\ndubbed DiffGEBD, that tackles the problem of GEBD from a generative\nperspective. The proposed model encodes relevant changes across adjacent frames\nvia temporal self-similarity and then iteratively decodes random noise into\nplausible event boundaries being conditioned on the encoded features.\nClassifier-free guidance allows the degree of diversity to be controlled in\ndenoising diffusion. In addition, we introduce a new evaluation metric to\nassess the quality of predictions considering both diversity and fidelity.\nExperiments show that our method achieves strong performance on two standard\nbenchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event\nboundaries.", "AI": {"tldr": "DiffGEBD\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u901a\u7528\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u65b9\u6cd5\u5904\u7406\u4e8b\u4ef6\u8fb9\u754c\u7684\u4e3b\u89c2\u591a\u6837\u6027\u95ee\u9898\uff0c\u5728Kinetics-GEBD\u548cTAPOS\u57fa\u51c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd", "motivation": "\u4f20\u7edf\u7684\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b\u65b9\u6cd5\u53ea\u505a\u786e\u5b9a\u6027\u9884\u6d4b\uff0c\u5ffd\u89c6\u4e86\u4e8b\u4ef6\u8fb9\u754c\u7684\u4e3b\u89c2\u6027\u548c\u591a\u6837\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u591a\u79cd\u5408\u7406\u8fb9\u754c\u7684\u65b9\u6cd5", "method": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684DiffGEBD\uff0c\u901a\u8fc7\u65f6\u95f4\u81ea\u76f8\u4f3c\u6027\u7f16\u7801\u76f8\u90bb\u5e27\u53d8\u5316\uff0c\u7136\u540e\u8fed\u4ee3\u5730\u5c06\u968f\u673a\u566a\u58f0\u89e3\u7801\u4e3a\u5408\u7406\u7684\u4e8b\u4ef6\u8fb9\u754c\uff0c\u4f7f\u7528\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u63a7\u5236\u591a\u6837\u6027", "result": "\u5728Kinetics-GEBD\u548cTAPOS\u4e24\u4e2a\u6807\u51c6\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u5f3a\u52b2\u6027\u80fd\uff0c\u80fd\u591f\u751f\u6210\u591a\u6837\u4e14\u5408\u7406\u7684\u4e8b\u4ef6\u8fb9\u754c", "conclusion": "\u6269\u6563\u6a21\u578b\u4e3a\u901a\u7528\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u751f\u6210\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u8fb9\u754c\u6807\u6ce8\u7684\u4e3b\u89c2\u591a\u6837\u6027\u95ee\u9898"}}
{"id": "2508.12725", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12725", "abs": "https://arxiv.org/abs/2508.12725", "authors": ["Wenjie Chen", "Wenbin Li", "Di Yao", "Xuying Meng", "Chang Gong", "Jingping Bi"], "title": "GTool: Graph Enhanced Tool Planning with Large Language Model", "comment": "16 pages, 9 figures", "summary": "Tool planning with large language models (LLMs), referring to selecting,\norganizing, and preparing the tools necessary to complete a user request,\nbridges the gap between natural language understanding and task execution.\nHowever, current works treat different tools as isolated components and fail to\nleverage the inherent dependencies of tools, leading to invalid planning\nresults. Since tool dependencies are often incomplete, it becomes challenging\nfor LLMs to accurately identify the appropriate tools required by a user\nrequest, especially when confronted with a large toolset. To solve this\nchallenge, we propose \\texttt{GTool}, which is the first work aiming to enhance\nthe tool planning ability of LLMs under incomplete dependencies. \\texttt{GTool}\nconstructs a request-specific tool graph to select tools efficiently and\ngenerate the \\texttt{<graph token>} which provides sufficient dependency\ninformation understandable by LLMs. Moreover, a missing dependency prediction\ntask is designed to improve the reliability of \\texttt{GTool} with incomplete\ndependencies. Without trimming LLMs, \\texttt{GTool} can be seamlessly\nintegrated with various LLM backbones without extensive retraining. Extensive\nexperiments show that \\texttt{GTool} achieves more than 29.6\\% performance\nimprovements compared with the state-of-the-art (SOTA) baselines with a\nlight-weight (7B) LLM backbone.", "AI": {"tldr": "GTool\u662f\u4e00\u4e2a\u589e\u5f3aLLM\u5de5\u5177\u89c4\u5212\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u8bf7\u6c42\u7279\u5b9a\u7684\u5de5\u5177\u56fe\u548c\u751f\u6210\u56fe\u6807\u8bb0\u6765\u89e3\u51b3\u5de5\u5177\u4f9d\u8d56\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u65e0\u9700\u4fee\u526aLLM\u5373\u53ef\u96c6\u6210\u5230\u5404\u79cd\u9aa8\u5e72\u7f51\u7edc\u4e2d\u3002", "motivation": "\u5f53\u524d\u5de5\u4f5c\u5c06\u4e0d\u540c\u5de5\u5177\u89c6\u4e3a\u5b64\u7acb\u7ec4\u4ef6\uff0c\u672a\u80fd\u5229\u7528\u5de5\u5177\u95f4\u7684\u5185\u5728\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u89c4\u5212\u7ed3\u679c\u65e0\u6548\u3002\u7531\u4e8e\u5de5\u5177\u4f9d\u8d56\u5f80\u5f80\u4e0d\u5b8c\u6574\uff0cLLM\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\u7528\u6237\u8bf7\u6c42\u6240\u9700\u7684\u5de5\u5177\u3002", "method": "\u6784\u5efa\u8bf7\u6c42\u7279\u5b9a\u7684\u5de5\u5177\u56fe\u6765\u9ad8\u6548\u9009\u62e9\u5de5\u5177\uff0c\u751f\u6210LLM\u53ef\u7406\u89e3\u7684\u56fe\u6807\u8bb0\uff0c\u8bbe\u8ba1\u7f3a\u5931\u4f9d\u8d56\u9884\u6d4b\u4efb\u52a1\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u65e0\u9700\u4fee\u526aLLM\u5373\u53ef\u4e0e\u5404\u79cd\u9aa8\u5e72\u7f51\u7edc\u96c6\u6210\u3002", "result": "\u5728\u8f7b\u91cf\u7ea7\uff087B\uff09LLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8d85\u8fc729.6%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "GTool\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u5177\u4f9d\u8d56\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0b\u7684\u5de5\u5177\u89c4\u5212\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u5de5\u5177\u9009\u62e9\u80fd\u529b\uff0c\u5177\u6709\u5f88\u597d\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.12089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12089", "abs": "https://arxiv.org/abs/2508.12089", "authors": ["Qinyuan Fan", "Clemens G\u00fchmann"], "title": "Enhancing 3D point accuracy of laser scanner through multi-stage convolutional neural network for applications in construction", "comment": null, "summary": "We propose a multi-stage convolutional neural network (MSCNN) based\nintegrated method for reducing uncertainty of 3D point accuracy of lasar\nscanner (LS) in rough indoor rooms, providing more accurate spatial\nmeasurements for high-precision geometric model creation and renovation. Due to\ndifferent equipment limitations and environmental factors, high-end and low-end\nLS have positional errors. Our approach pairs high-accuracy scanners (HAS) as\nreferences with corresponding low-accuracy scanners (LAS) of measurements in\nidentical environments to quantify specific error patterns. By establishing a\nstatistical relationship between measurement discrepancies and their spatial\ndistribution, we develop a correction framework that combines traditional\ngeometric processing with targeted neural network refinement. This method\ntransforms the quantification of systematic errors into a supervised learning\nproblem, allowing precise correction while preserving critical geometric\nfeatures. Experimental results in our rough indoor rooms dataset show\nsignificant improvements in measurement accuracy, with mean square error (MSE)\nreductions exceeding 70% and peak signal-to-noise ratio (PSNR) improvements of\napproximately 6 decibels. This approach enables low-end devices to achieve\nmeasurement uncertainty levels approaching those of high-end devices without\nhardware modifications.", "AI": {"tldr": "\u901a\u8fc7\u591a\u6bb5\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u9ad8\u4f4e\u7aef\u626b\u63cf\u4eea\u914d\u5bf9\uff0c\u5b9e\u73b0\u4e86\u7c97\u7cd5\u5ba4\u5185\u73af\u5883\u4e2d\u4f4e\u7aef\u5149\u8c31\u626b\u63cf\u4eea\u6d4b\u91cf\u7cbe\u5ea6\u7684\u663e\u8457\u63d0\u5347\uff0cMSE\u964d\u4f4e70%\u4ee5\u4e0a", "motivation": "\u89e3\u51b3\u4e0d\u540c\u7ea7\u522b\u5149\u8c31\u626b\u63cf\u4eea\u56e0\u8bbe\u5907\u9650\u5236\u548c\u73af\u5883\u56e0\u7d20\u5bfc\u81f4\u7684\u4f4d\u7f6e\u8bef\u5dee\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u7a7a\u95f4\u6d4b\u91cf\u4ee5\u652f\u6491\u9ad8\u7cbe\u5ea6\u51e0\u4f55\u6a21\u578b\u521b\u5efa", "method": "\u91c7\u7528\u9ad8\u7cbe\u5ea6\u626b\u63cf\u4eea\u4f5c\u4e3a\u53c2\u8003\uff0c\u4e0e\u4f4e\u7aef\u8bbe\u5907\u5728\u540c\u4e00\u73af\u5883\u4e2d\u914d\u5bf9\u6d4b\u91cf\uff0c\u901a\u8fc7\u7edf\u8ba1\u5173\u7cfb\u5efa\u7acb\u6d4b\u91cf\u504f\u5dee\u4e0e\u7a7a\u95f4\u5206\u5e03\u7684\u5173\u8054\uff0c\u7ed3\u5408\u4f20\u7edf\u51e0\u4f55\u5904\u7406\u548c\u795e\u7ecf\u7f51\u7edc\u7cbe\u7ec6\u5316\u8fdb\u884c\u7cfb\u7edf\u8bef\u5dee\u7b49\u91cf\u5316\u548c\u7f29\u6b63", "result": "\u5728\u7c97\u7cd5\u5ba4\u5185\u6570\u636e\u96c6\u4e2d\u5b9e\u9a8c\u663e\u793a\u6d4b\u91cf\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u5747\u65b9\u8bef\u5dee\u964d\u4f4e70%\u4ee5\u4e0a\uff0c\u5cf0\u503c\u4fe1\u566a\u6bd4\u63d0\u5347\u7ea66\u5206\u8d1d", "conclusion": "\u8be5\u65b9\u6cd5\u4f7f\u4f4e\u7aef\u8bbe\u5907\u5728\u4e0d\u6539\u53d8\u786c\u4ef6\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u8fbe\u5230\u63a5\u8fd1\u9ad8\u7aef\u8bbe\u5907\u7684\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5173\u952e\u51e0\u4f55\u7279\u5f81"}}
{"id": "2508.12754", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12754", "abs": "https://arxiv.org/abs/2508.12754", "authors": ["Alessio Galatolo", "Luca Alberto Rappuoli", "Katie Winkle", "Meriem Beloucif"], "title": "Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants", "comment": "Full version of the paper published in ECAI 2025 proceedings (IOS\n  Press, CC BY-NC 4.0)", "summary": "The recent rise in popularity of large language models (LLMs) has prompted\nconsiderable concerns about their moral capabilities. Although considerable\neffort has been dedicated to aligning LLMs with human moral values, existing\nbenchmarks and evaluations remain largely superficial, typically measuring\nalignment based on final ethical verdicts rather than explicit moral reasoning.\nIn response, this paper aims to advance the investigation of LLMs' moral\ncapabilities by examining their capacity to function as Artificial Moral\nAssistants (AMAs), systems envisioned in the philosophical literature to\nsupport human moral deliberation. We assert that qualifying as an AMA requires\nmore than what state-of-the-art alignment techniques aim to achieve: not only\nmust AMAs be able to discern ethically problematic situations, they should also\nbe able to actively reason about them, navigating between conflicting values\noutside of those embedded in the alignment phase. Building on existing\nphilosophical literature, we begin by designing a new formal framework of the\nspecific kind of behaviour an AMA should exhibit, individuating key qualities\nsuch as deductive and abductive moral reasoning. Drawing on this theoretical\nframework, we develop a benchmark to test these qualities and evaluate popular\nopen LLMs against it. Our results reveal considerable variability across models\nand highlight persistent shortcomings, particularly regarding abductive moral\nreasoning. Our work connects theoretical philosophy with practical AI\nevaluation while also emphasising the need for dedicated strategies to\nexplicitly enhance moral reasoning capabilities in LLMs. Code available at\nhttps://github.com/alessioGalatolo/AMAeval", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9053\u5fb7\u8f85\u52a9\u80fd\u529b\uff0c\u91cd\u70b9\u8003\u5bdf\u5176\u9053\u5fb7\u63a8\u7406\u80fd\u529b\u800c\u975e\u4ec5\u662f\u6700\u7ec8\u9053\u5fb7\u5224\u65ad\u3002", "motivation": "\u5f53\u524d\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u9053\u5fb7\u80fd\u529b\u7684\u8bc4\u4f30\u592a\u6d45\u5c42\uff0c\u4ec5\u5173\u6ce8\u6700\u7ec8\u9053\u5fb7\u7ed3\u8bba\uff0c\u7f3a\u4e4f\u5bf9\u9053\u5fb7\u63a8\u7406\u8fc7\u7a0b\u7684\u6df1\u5165\u5206\u6790\u3002\u9700\u8981\u4ece\u54f2\u5b66\u89d2\u5ea6\u6784\u5efa\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u57fa\u4e8e\u54f2\u5b66\u6587\u732e\u6784\u5efa\u4eba\u5de5\u9053\u5fb7\u8f85\u52a9\u5668(AMA)\u7684\u7406\u8bba\u6846\u67b6\uff0c\u660e\u786e\u5176\u9700\u8981\u5177\u5907\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5305\u62ec\u6f14\u7ece\u548c\u5f52\u7eb3\u9053\u5fb7\u63a8\u7406\u3002\u5f00\u53d1\u76f8\u5e94\u7684\u6d4b\u8bd5\u57fa\u51c6\uff0c\u5bf9\u6d41\u884c\u7684\u5f00\u653e\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u6d4b\u3002", "result": "\u5404\u6a21\u578b\u5728\u9053\u5fb7\u8f85\u52a9\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5c24\u5176\u5728\u5f52\u7eb3\u9053\u5fb7\u63a8\u7406\u65b9\u9762\u8868\u73b0\u6301\u7eed\u4e0d\u4f73\u3002\u73b0\u6709\u7684\u5bf9\u9f50\u6280\u672f\u4e0d\u80fd\u6ee1\u8db3\u4eba\u5de5\u9053\u5fb7\u8f85\u52a9\u5668\u7684\u5168\u90e8\u8981\u6c42\u3002", "conclusion": "\u672c\u7814\u7a76\u5c06\u54f2\u5b66\u7406\u8bba\u4e0e\u5b9e\u8df5AI\u8bc4\u4f30\u76f8\u7ed3\u5408\uff0c\u5f3a\u8c03\u4e86\u4e13\u95e8\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u9053\u5fb7\u63a8\u7406\u80fd\u529b\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2508.12094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12094", "abs": "https://arxiv.org/abs/2508.12094", "authors": ["Songwei Liu", "Hong Liu", "Fangmin Chen", "Xurui Peng", "Chenqian Yan", "Lean Fu", "Xing Mei"], "title": "Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion", "comment": null, "summary": "Diffusion models have transformed image synthesis by establishing\nunprecedented quality and creativity benchmarks. Nevertheless, their\nlarge-scale deployment faces challenges due to computationally intensive\niterative denoising processes. Although post-training quantization(PTQ)\nprovides an effective pathway for accelerating sampling, the iterative nature\nof diffusion models causes stepwise quantization errors to accumulate\nprogressively during generation, inevitably compromising output fidelity. To\naddress this challenge, we develop a theoretical framework that mathematically\nformulates error propagation in Diffusion Models (DMs), deriving per-step\nquantization error propagation equations and establishing the first closed-form\nsolution for cumulative error. Building on this theoretical foundation, we\npropose a timestep-aware cumulative error compensation scheme. Extensive\nexperiments across multiple image datasets demonstrate that our compensation\nstrategy effectively mitigates error propagation, significantly enhancing\nexisting PTQ methods to achieve state-of-the-art(SOTA) performance on\nlow-precision diffusion models.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u6b65\u611f\u77e5\u7684\u7d2f\u79ef\u9519\u8bef\u8865\u507f\u65b9\u6848\uff0c\u7528\u4e8e\u51cf\u5c11\u6f2b\u6563\u6a21\u578b\u5728\u91cf\u5316\u540e\u7684\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\uff0c\u63d0\u9ad8\u4f4e\u7cbe\u5ea6\u6a21\u578b\u7684\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u6f2b\u6563\u6a21\u578b\u867d\u7136\u5728\u56fe\u50cf\u5408\u6210\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u8fed\u4ee3\u53cd\u6c61\u54c4\u8fc7\u7a0b\u8ba1\u7b97\u5f00\u9500\u5f88\u5927\uff0c\u540e\u8bad\u7ec3\u91cf\u5316(PTQ)\u53ef\u4ee5\u52a0\u901f\u91d1\u53d6\u6837\uff0c\u4f46\u8fed\u4ee3\u6027\u8d28\u5bfc\u81f4\u91cf\u5316\u9519\u8bef\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7d2f\u79ef\uff0c\u5f71\u54cd\u8f93\u51fa\u8d28\u91cf\u3002", "method": "\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u6570\u5b66\u5730\u5f62\u5bb9\u9519\u8bef\u4f20\u64ad\uff0c\u6c42\u89e3\u6bcf\u6b65\u91cf\u5316\u9519\u8bef\u4f20\u64ad\u65b9\u7a0b\uff0c\u5f97\u5230\u7d2f\u79ef\u9519\u8bef\u7684\u95ed\u5f0f\u89e3\uff0c\u57fa\u4e8e\u6b64\u63d0\u51fa\u65f6\u95f4\u6b65\u611f\u77e5\u7684\u7d2f\u79ef\u9519\u8bef\u8865\u507f\u65b9\u6848\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u8865\u507f\u7b56\u7565\u6709\u6548\u51cf\u5c11\u9519\u8bef\u4f20\u64ad\uff0c\u663e\u8457\u63d0\u5347\u73b0\u6709PTQ\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5728\u4f4e\u7cbe\u5ea6\u6f2b\u6563\u6a21\u578b\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u9519\u8bef\u4f20\u64ad\u673a\u5236\u5e76\u63d0\u51fa\u76f8\u5e94\u7684\u8865\u507f\u65b9\u6848\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u6f2b\u6563\u6a21\u578b\u91cf\u5316\u540e\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12782", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12782", "abs": "https://arxiv.org/abs/2508.12782", "authors": ["Petr Anokhin", "Roman Khalikov", "Stefan Rebrikov", "Viktor Volkov", "Artyom Sorokin", "Vincent Bissonnette"], "title": "HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds", "comment": "Code is available at https://github.com/stefanrer/HeroBench", "summary": "Large language models (LLMs) have shown remarkable capabilities in isolated\nstep-by-step reasoning tasks such as mathematics and programming, but their\nproficiency in long-horizon planning, where solutions require extended,\nstructured sequences of interdependent actions, remains underexplored. Existing\nbenchmarks typically assess LLMs through abstract or low-dimensional\nalgorithmic tasks, failing to capture the complexity of realistic planning\nenvironments. We introduce HeroBench, a novel benchmark designed specifically\nto evaluate long-horizon planning and structured reasoning within complex\nRPG-inspired virtual worlds. HeroBench provides a rigorously constructed\ndataset of tasks covering a wide range of difficulties, a simulated environment\nto execute and validate agent plans, and detailed analytical tools for\nevaluating model performance. Tasks challenge models to formulate strategic\nplans, efficiently gather resources, master necessary skills, craft equipment,\nand defeat adversaries, reflecting practical scenarios' layered dependencies\nand constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning\nboth open-source and proprietary models, including the GPT-5 family, reveals\nsubstantial performance disparities rarely observed in conventional reasoning\nbenchmarks. Detailed error analysis further uncovers specific weaknesses in\ncurrent models' abilities to generate robust high-level plans and reliably\nexecute structured actions. HeroBench thus not only significantly advances the\nevaluation of LLM reasoning but also provides a flexible, scalable foundation\nfor future research into advanced, autonomous planning in virtual environments.", "AI": {"tldr": "HeroBench\u662f\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30LLM\u957f\u65f6\u7a0b\u89c4\u5212\u548c\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u901a\u8fc7RPG\u98ce\u683c\u7684\u865a\u62df\u4e16\u754c\u4efb\u52a1\u6765\u6d4b\u8bd5\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6218\u7565\u89c4\u5212\u3001\u8d44\u6e90\u6536\u96c6\u548c\u6280\u80fd\u638c\u63e1\u7b49\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30LLM\u5728\u62bd\u8c61\u6216\u4f4e\u7ef4\u7b97\u6cd5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u89c4\u5212\u73af\u5883\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u957f\u65f6\u7a0b\u89c4\u5212\u8bbe\u8ba1\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u6784\u5efaHeroBench\u57fa\u51c6\uff0c\u5305\u542b\uff1a1\uff09\u8986\u76d6\u4e0d\u540c\u96be\u5ea6\u7684\u4efb\u52a1\u6570\u636e\u96c6\uff1b2\uff09\u6267\u884c\u548c\u9a8c\u8bc1\u667a\u80fd\u4f53\u8ba1\u5212\u7684\u6a21\u62df\u73af\u5883\uff1b3\uff09\u8be6\u7ec6\u7684\u6a21\u578b\u6027\u80fd\u5206\u6790\u5de5\u5177\u3002\u4efb\u52a1\u6d89\u53ca\u6218\u7565\u89c4\u5212\u3001\u8d44\u6e90\u6536\u96c6\u3001\u6280\u80fd\u638c\u63e1\u3001\u88c5\u5907\u5236\u4f5c\u548c\u5bf9\u6297\u654c\u4eba\u7b49\u3002", "result": "\u5bf925\u4e2a\u6700\u5148\u8fdbLLM\uff08\u5305\u62ec\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\uff0c\u5982GPT-5\u7cfb\u5217\uff09\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u4f20\u7edf\u63a8\u7406\u57fa\u51c6\u4e2d\u5f88\u5c11\u89c2\u5bdf\u5230\u7684\u663e\u8457\u6027\u80fd\u5dee\u5f02\u3002\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u751f\u6210\u7a33\u5065\u9ad8\u5c42\u8ba1\u5212\u548c\u53ef\u9760\u6267\u884c\u7ed3\u6784\u5316\u52a8\u4f5c\u65b9\u9762\u7684\u5177\u4f53\u5f31\u70b9\u3002", "conclusion": "HeroBench\u4e0d\u4ec5\u663e\u8457\u63a8\u8fdb\u4e86LLM\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\uff0c\u8fd8\u4e3a\u672a\u6765\u5728\u865a\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u9ad8\u7ea7\u81ea\u4e3b\u89c4\u5212\u7814\u7a76\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2508.12108", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12108", "abs": "https://arxiv.org/abs/2508.12108", "authors": ["Ziyang Zhang", "Yang Yu", "Xulei Yang", "Si Yong Yeo"], "title": "VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine", "comment": null, "summary": "Vision-and-language models (VLMs) have been increasingly explored in the\nmedical domain, particularly following the success of CLIP in general domain.\nHowever, unlike the relatively straightforward pairing of 2D images and text,\ncurating large-scale paired data in the medical field for volumetric modalities\nsuch as CT scans remains a challenging and time-intensive process. This\ndifficulty often limits the performance on downstream tasks. To address these\nchallenges, we propose a novel vision-language pre-training (VLP) framework,\ntermed as \\textbf{VELVET-Med}, specifically designed for limited volumetric\ndata such as 3D CT and associated radiology reports. Instead of relying on\nlarge-scale data collection, our method focuses on the development of effective\npre-training objectives and model architectures. The key contributions are: 1)\nWe incorporate uni-modal self-supervised learning into VLP framework, which are\noften underexplored in the existing literature. 2) We propose a novel language\nencoder, termed as \\textbf{TriBERT}, for learning multi-level textual\nsemantics. 3) We devise the hierarchical contrastive learning to capture\nmulti-level vision-language correspondence. Using only 38,875 scan-report\npairs, our approach seeks to uncover rich spatial and semantic relationships\nembedded in volumetric medical images and corresponding clinical narratives,\nthereby enhancing the generalization ability of the learned encoders. The\nresulting encoders exhibit strong transferability, achieving state-of-the-art\nperformance across a wide range of downstream tasks, including 3D segmentation,\ncross-modal retrieval, visual question answering, and report generation.", "AI": {"tldr": "VELVET-Med\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u6709\u96503D\u533b\u5b66\u6570\u636e\uff08\u5982CT\u626b\u63cf\u548c\u653e\u5c04\u62a5\u544a\uff09\u7684\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u9884\u8bad\u7ec3\u76ee\u6807\u548c\u67b6\u6784\u8bbe\u8ba1\uff0c\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u4e0a\u5b9e\u73b0\u4f18\u5f02\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u9886\u57df\u4e2d3D\u4f53\u79ef\u6570\u636e\uff08\u5982CT\u626b\u63cf\uff09\u4e0e\u6587\u672c\u7684\u914d\u5bf9\u6570\u636e\u6536\u96c6\u56f0\u96be\u4e14\u8017\u65f6\uff0c\u9650\u5236\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u63d0\u51faVELVET-Med\u6846\u67b6\uff1a1\uff09\u5c06\u5355\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u878d\u5165VLP\u6846\u67b6\uff1b2\uff09\u8bbe\u8ba1TriBERT\u8bed\u8a00\u7f16\u7801\u5668\u5b66\u4e60\u591a\u5c42\u6b21\u6587\u672c\u8bed\u4e49\uff1b3\uff09\u5f00\u53d1\u5206\u5c42\u5bf9\u6bd4\u5b66\u4e60\u6355\u83b7\u591a\u5c42\u6b21\u89c6\u89c9\u8bed\u8a00\u5bf9\u5e94\u5173\u7cfb\u3002\u4ec5\u4f7f\u752838,875\u4e2a\u626b\u63cf-\u62a5\u544a\u5bf9\u3002", "result": "\u5b66\u4e60\u5230\u7684\u7f16\u7801\u5668\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u57283D\u5206\u5272\u3001\u8de8\u6a21\u6001\u68c0\u7d22\u3001\u89c6\u89c9\u95ee\u7b54\u548c\u62a5\u544a\u751f\u6210\u7b49\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u9884\u8bad\u7ec3\u76ee\u6807\u548c\u67b6\u6784\uff0c\u5373\u4f7f\u5728\u5c0f\u89c4\u6a21\u533b\u5b66\u6570\u636e\u4e0a\u4e5f\u80fd\u6709\u6548\u5b66\u4e60\u4e30\u5bcc\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.12790", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12790", "abs": "https://arxiv.org/abs/2508.12790", "authors": ["Zenan Huang", "Yihong Zhuang", "Guoshan Lu", "Zeyu Qin", "Haokai Xu", "Tianyu Zhao", "Ru Peng", "Jiaqi Hu", "Zhanming Shen", "Xiaomeng Hu", "Xijun Gu", "Peiyi Tu", "Jiaxin Liu", "Wenyu Chen", "Yuzhuo Fu", "Zhiting Fan", "Yanmei Gu", "Yuanyuan Wang", "Zhengkai Yang", "Jianguo Li", "Junbo Zhao"], "title": "Reinforcement Learning with Rubric Anchors", "comment": "technical report", "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing Large Language Models (LLMs), exemplified by\nthe success of OpenAI's o-series. In RLVR, rewards are derived from verifiable\nsignals-such as passing unit tests in code generation or matching correct\nanswers in mathematical reasoning. While effective, this requirement largely\nconfines RLVR to domains with automatically checkable outcomes. To overcome\nthis, we extend the RLVR paradigm to open-ended tasks by integrating\nrubric-based rewards, where carefully designed rubrics serve as structured,\nmodel-interpretable criteria for automatic scoring of subjective outputs. We\nconstruct, to our knowledge, the largest rubric reward system to date, with\nover 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.\nImplementing rubric-based RL is challenging; we tackle these issues with a\nclear framework and present an open-sourced Qwen-30B-A3B model with notable\ngains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended\nbenchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by\n+2.4%, while preserving general and reasoning abilities. 2) Our method provides\nfine-grained stylistic control, using rubrics as anchors to mitigate the\n\"AI-like\" tone and produce more human-like, expressive responses. We share key\nlessons in rubric construction, data selection, and training, and discuss\nlimitations and future releases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684RLVR\u65b9\u6cd5\uff0c\u5c06\u53ef\u9a8c\u8bc1\u5956\u52b1\u5b66\u4e60\u6269\u5c55\u5230\u5f00\u653e\u5f0f\u4efb\u52a1\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b10,000+\u8bc4\u5206\u6807\u51c6\u7684\u7cfb\u7edf\uff0c\u5728\u5c11\u91cf\u6837\u672c\u4e0b\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4eba\u6587\u9886\u57df\uff0c\u5e76\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u98ce\u683c\u63a7\u5236\u3002", "motivation": "\u4f20\u7edfRLVR\u65b9\u6cd5\u5c40\u9650\u4e8e\u53ef\u81ea\u52a8\u9a8c\u8bc1\u7ed3\u679c\u7684\u9886\u57df\uff08\u5982\u4ee3\u7801\u6d4b\u8bd5\u3001\u6570\u5b66\u7b54\u6848\u5339\u914d\uff09\uff0c\u65e0\u6cd5\u5e94\u7528\u4e8e\u5f00\u653e\u5f0f\u4e3b\u89c2\u4efb\u52a1\u3002\u9700\u8981\u6269\u5c55RLVR\u5230\u5f00\u653e\u5f0f\u9886\u57df\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5904\u7406\u4e3b\u89c2\u6027\u5f3a\u7684\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u5956\u52b1\u673a\u5236\uff0c\u6784\u5efa\u4e86\u5305\u542b10,000+\u4e2a\u4eba\u5de5\u3001LLM\u6216\u4eba\u673a\u534f\u4f5c\u521b\u5efa\u7684\u8bc4\u5206\u6807\u51c6\u7cfb\u7edf\u3002\u901a\u8fc7\u6e05\u6670\u7684\u6846\u67b6\u89e3\u51b3\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u96be\u9898\u3002", "result": "\u4ec5\u75285K+\u6837\u672c\u5c31\u5728\u5f00\u653e\u5f0f\u57fa\u51c6\u6d4b\u8bd5\uff08\u7279\u522b\u662f\u4eba\u6587\u9886\u57df\uff09\u4e0a\u63d0\u53475.2%\uff0c\u8d85\u8d8a671B\u53c2\u6570\u7684DeepSeek-V3\u6a21\u578b2.4%\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u548c\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u98ce\u683c\u63a7\u5236\uff0c\u51cf\u5c11\"AI\u8154\u8c03\"\uff0c\u751f\u6210\u66f4\u4eba\u6027\u5316\u7684\u8868\u8fbe\u3002", "conclusion": "\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684RLVR\u6210\u529f\u6269\u5c55\u4e86\u53ef\u9a8c\u8bc1\u5956\u52b1\u5b66\u4e60\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4e3a\u5f00\u653e\u5f0f\u4e3b\u89c2\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u98ce\u683c\u63a7\u5236\u3002"}}
{"id": "2508.12109", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12109", "abs": "https://arxiv.org/abs/2508.12109", "authors": ["Ye Wang", "Qianglong Chen", "Zejun Li", "Siyuan Wang", "Shijie Guo", "Zhirui Zhang", "Zhongyu Wei"], "title": "Simple o3: Towards Interleaved Vision-Language Reasoning", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown impressive performance on\nvision-language tasks, but their long Chain-of-Thought (CoT) capabilities in\nmultimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which\nemulates human-like ''thinking with image'' through iterative visual\ntransformations and linguistic reasoning, we propose Simple o3, an end-to-end\nframework that integrates dynamic tool interactions (e.g., cropping, zooming,\nand reusing) into interleaved vision-language reasoning via supervised\nfine-tuning (SFT). Our approach features a scalable data synthesis pipeline\nthat generates high-quality interleaved vision-language reasoning chains via an\n''observe-reason-act'' cycle, complete with executable visual operations and\nrigorous verification, yielding the open-source TWI-Tools-146K dataset.\nExperimental results demonstrate Simple o3's superior performance on diverse\nbenchmarks, outperforming existing approaches. By combining enhanced reasoning\ncapabilities, Simple o3 establishes a powerful yet computationally affordable\nparadigm for advancing multimodal reasoning. Remarkably, we provide the first\nin-depth analysis of different interleaved reasoning strategies, offering\ninsights into their impact on model performance. We found that by introducing\nadditional visual tokens for interleaved vision-language reasoning, reusing and\nmagnifying the original image significantly improves the model's visual\nreasoning and fine-grained perception, while image cropping based on precise\nvisual grounding allows the model to effectively focus on key entities or\nregions, further enhancing its capabilities.", "AI": {"tldr": "Simple o3\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u89c6\u89c9\u5de5\u5177\u4ea4\u4e92\u548c\"\u89c2\u5bdf-\u63a8\u7406-\u884c\u52a8\"\u5faa\u73af\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u957f\u94fe\u591a\u6a21\u6001\u601d\u7ef4\u63a8\u7406\u65b9\u9762\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u63a2\u7d22\u7c7b\u4f3c\u4eba\u7c7b\"\u56fe\u50cf\u601d\u7ef4\"\u7684\u8fed\u4ee3\u89c6\u89c9\u8f6c\u6362\u548c\u8bed\u8a00\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u76d1\u7763\u5fae\u8c03\u6846\u67b6\uff0c\u96c6\u6210\u88c1\u526a\u3001\u7f29\u653e\u3001\u91cd\u7528\u7b49\u52a8\u6001\u5de5\u5177\u4ea4\u4e92\uff0c\u6784\u5efaTWI-Tools-146K\u6570\u636e\u96c6\uff0c\u91c7\u7528\"\u89c2\u5bdf-\u63a8\u7406-\u884c\u52a8\"\u5faa\u73af\u751f\u6210\u9ad8\u8d28\u91cf\u63a8\u7406\u94fe\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u91cd\u7528\u548c\u653e\u5927\u539f\u59cb\u56fe\u50cf\u663e\u8457\u6539\u5584\u89c6\u89c9\u63a8\u7406\uff0c\u57fa\u4e8e\u7cbe\u786e\u89c6\u89c9\u5b9a\u4f4d\u7684\u56fe\u50cf\u88c1\u526a\u6709\u6548\u805a\u7126\u5173\u952e\u533a\u57df\u3002", "conclusion": "Simple o3\u5efa\u7acb\u4e86\u8ba1\u7b97\u9ad8\u6548\u7684\u591a\u6a21\u6001\u63a8\u7406\u8303\u5f0f\uff0c\u9996\u6b21\u6df1\u5165\u5206\u6790\u4e86\u4e0d\u540c\u4ea4\u9519\u63a8\u7406\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2508.12791", "categories": ["cs.AI", "cs.MA", "cs.SY", "eess.SY", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2508.12791", "abs": "https://arxiv.org/abs/2508.12791", "authors": ["Imran Khan"], "title": "[Social] Allostasis: Or, How I Learned To Stop Worrying and Love The Noise", "comment": "20 pages, 5 figures. Accepted at ALIFE 2025 (Kyoto, Japan; October\n  6th - 10th 2025)", "summary": "The notion of homeostasis typically conceptualises biological and artificial\nsystems as maintaining stability by resisting deviations caused by\nenvironmental and social perturbations. In contrast, (social) allostasis\nproposes that these systems can proactively leverage these very perturbations\nto reconfigure their regulatory parameters in anticipation of environmental\ndemands, aligning with von Foerster's ``order through noise'' principle. This\npaper formulates a computational model of allostatic and social allostatic\nregulation that employs biophysiologically inspired signal transducers,\nanalogous to hormones like cortisol and oxytocin, to encode information from\nboth the environment and social interactions, which mediate this dynamic\nreconfiguration. The models are tested in a small society of ``animats'' across\nseveral dynamic environments, using an agent-based model. The results show that\nallostatic and social allostatic regulation enable agents to leverage\nenvironmental and social ``noise'' for adaptive reconfiguration, leading to\nimproved viability compared to purely reactive homeostatic agents. This work\noffers a novel computational perspective on the principles of social allostasis\nand their potential for designing more robust, bio-inspired, adaptive systems", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u6a21\u578b\uff0c\u5c55\u793a\u793e\u4f1a\u6027\u7a33\u6001\u8c03\u8282\u5982\u4f55\u5229\u7528\u73af\u5883\u548c\u793e\u4f1a\u6270\u52a8\u8fdb\u884c\u4e3b\u52a8\u9002\u5e94\u6027\u91cd\u6784\uff0c\u76f8\u6bd4\u4f20\u7edf\u7a33\u6001\u8c03\u8282\u80fd\u63d0\u9ad8\u7cfb\u7edf\u751f\u5b58\u80fd\u529b", "motivation": "\u4f20\u7edf\u7a33\u6001\u6982\u5ff5\u5f3a\u8c03\u7cfb\u7edf\u901a\u8fc7\u62b5\u6297\u6270\u52a8\u6765\u7ef4\u6301\u7a33\u5b9a\uff0c\u800c\u793e\u4f1a\u6027\u7a33\u6001\u7406\u8bba\u8ba4\u4e3a\u7cfb\u7edf\u53ef\u4ee5\u4e3b\u52a8\u5229\u7528\u6270\u52a8\u6765\u9884\u6d4b\u73af\u5883\u9700\u6c42\u5e76\u91cd\u65b0\u914d\u7f6e\u8c03\u8282\u53c2\u6570", "method": "\u5efa\u7acb\u57fa\u4e8e\u751f\u7269\u751f\u7406\u5b66\u4fe1\u53f7\u8f6c\u5bfc\u5668\u7684\u8ba1\u7b97\u6a21\u578b\uff08\u7c7b\u4f3c\u76ae\u8d28\u9187\u548c\u50ac\u4ea7\u7d20\u7b49\u6fc0\u7d20\uff09\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4f7f\u7528\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u578b\u6d4b\u8bd5\u5c0f\u578b\u793e\u4f1a\u4e2d\u7684\"\u52a8\u753b\u4f53\"", "result": "\u793e\u4f1a\u6027\u7a33\u6001\u8c03\u8282\u4f7f\u4ee3\u7406\u80fd\u591f\u5229\u7528\u73af\u5883\u548c\u793e\u4f1a\"\u566a\u58f0\"\u8fdb\u884c\u9002\u5e94\u6027\u91cd\u6784\uff0c\u76f8\u6bd4\u7eaf\u53cd\u5e94\u6027\u7a33\u6001\u4ee3\u7406\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u5b58\u80fd\u529b", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u793e\u4f1a\u6027\u7a33\u6001\u539f\u5219\u63d0\u4f9b\u4e86\u65b0\u9896\u7684\u8ba1\u7b97\u89c6\u89d2\uff0c\u5bf9\u8bbe\u8ba1\u66f4\u9c81\u68d2\u3001\u751f\u7269\u542f\u53d1\u7684\u81ea\u9002\u5e94\u7cfb\u7edf\u5177\u6709\u6f5c\u5728\u4ef7\u503c"}}
{"id": "2508.12131", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12131", "abs": "https://arxiv.org/abs/2508.12131", "authors": ["Minh Tran", "Johnmark Clements", "Annie Prasanna", "Tri Nguyen", "Ngan Le"], "title": "DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis", "comment": "Retail Vision, ICCV 2025", "summary": "Virtual Try-On technology has garnered significant attention for its\npotential to transform the online fashion retail experience by allowing users\nto visualize how garments would look on them without physical trials. While\nrecent advances in diffusion-based warping-free methods have improved\nperceptual quality, they often fail to preserve fine-grained garment details\nsuch as logos and printed text elements that are critical for brand integrity\nand customer trust. In this work, we propose DualFit, a hybrid VTON pipeline\nthat addresses this limitation by two-stage approach. In the first stage,\nDualFit warps the target garment to align with the person image using a learned\nflow field, ensuring high-fidelity preservation. In the second stage, a\nfidelity-preserving try-on module synthesizes the final output by blending the\nwarped garment with preserved human regions. Particularly, to guide this\nprocess, we introduce a preserved-region input and an inpainting mask, enabling\nthe model to retain key areas and regenerate only where necessary, particularly\naround garment seams. Extensive qualitative results show that DualFit achieves\nvisually seamless try-on results while faithfully maintaining high-frequency\ngarment details, striking an effective balance between reconstruction accuracy\nand perceptual realism.", "AI": {"tldr": "DualFit\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u865a\u62df\u8bd5\u7a7f\u6df7\u5408\u7ba1\u9053\uff0c\u901a\u8fc7\u6d41\u573a\u53d8\u5f62\u548c\u4fdd\u771f\u5ea6\u5408\u6210\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u670d\u88c5\u7ec6\u8282\u7684\u540c\u65f6\u5b9e\u73b0\u89c6\u89c9\u65e0\u7f1d\u7684\u8bd5\u7a7f\u6548\u679c", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u514d\u53d8\u5f62\u65b9\u6cd5\u867d\u7136\u63d0\u5347\u4e86\u611f\u77e5\u8d28\u91cf\uff0c\u4f46\u65e0\u6cd5\u4fdd\u6301\u670d\u88c5\u7684\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff08\u5982logo\u548c\u5370\u5237\u6587\u5b57\uff09\uff0c\u8fd9\u4e9b\u7ec6\u8282\u5bf9\u54c1\u724c\u5b8c\u6574\u6027\u548c\u5ba2\u6237\u4fe1\u4efb\u81f3\u5173\u91cd\u8981", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u6d41\u573a\u5c06\u76ee\u6807\u670d\u88c5\u53d8\u5f62\u4ee5\u5bf9\u9f50\u4eba\u7269\u56fe\u50cf\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u4fdd\u771f\u5ea6\u8bd5\u7a7f\u6a21\u5757\u5c06\u53d8\u5f62\u540e\u7684\u670d\u88c5\u4e0e\u4fdd\u7559\u7684\u4eba\u4f53\u533a\u57df\u8fdb\u884c\u878d\u5408\uff0c\u4f7f\u7528\u4fdd\u7559\u533a\u57df\u8f93\u5165\u548c\u4fee\u590d\u63a9\u7801\u6307\u5bfc\u8fc7\u7a0b", "result": "\u5e7f\u6cdb\u7684\u5b9a\u6027\u7ed3\u679c\u8868\u660eDualFit\u5b9e\u73b0\u4e86\u89c6\u89c9\u65e0\u7f1d\u7684\u8bd5\u7a7f\u7ed3\u679c\uff0c\u540c\u65f6\u5fe0\u5b9e\u5730\u4fdd\u6301\u4e86\u9ad8\u9891\u670d\u88c5\u7ec6\u8282\uff0c\u5728\u91cd\u5efa\u51c6\u786e\u6027\u548c\u611f\u77e5\u771f\u5b9e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u6709\u6548\u5e73\u8861", "conclusion": "DualFit\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u865a\u62df\u8bd5\u7a7f\u4e2d\u7ec6\u8282\u4fdd\u6301\u7684\u95ee\u9898\uff0c\u4e3a\u5728\u7ebf\u65f6\u5c1a\u96f6\u552e\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12840", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12840", "abs": "https://arxiv.org/abs/2508.12840", "authors": ["Giovanni Briglia", "Francesco Fabiano", "Stefano Mariani"], "title": "Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics", "comment": null, "summary": "Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for\nreasoning about both the physical world and the beliefs of agents, with\napplications in domains where information flow and awareness among agents are\ncritical. The richness of MEP requires states to be represented as Kripke\nstructures, i.e., directed labeled graphs. This representation limits the\napplicability of existing heuristics, hindering the scalability of epistemic\nsolvers, which must explore an exponential search space without guidance,\nresulting often in intractability. To address this, we exploit Graph Neural\nNetworks (GNNs) to learn patterns and relational structures within epistemic\nstates, to guide the planning process. GNNs, which naturally capture the\ngraph-like nature of Kripke models, allow us to derive meaningful estimates of\nstate quality -- e.g., the distance from the nearest goal -- by generalizing\nknowledge obtained from previously solved planning instances. We integrate\nthese predictive heuristics into an epistemic planning pipeline and evaluate\nthem against standard baselines, showing significant improvements in the\nscalability of multi-agent epistemic planning.", "AI": {"tldr": "\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u591a\u4ee3\u7406\u8ba4\u77e5\u89c4\u5212\u4e2d\u7684\u72b6\u6001\u8d28\u91cf\u9884\u6d4b\uff0c\u63d0\u9ad8\u89c4\u5212\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027", "motivation": "\u591a\u4ee3\u7406\u8ba4\u77e5\u89c4\u5212\u4e2d\u7684Kripke\u7ed3\u6784\u8868\u793a\u5bfc\u81f4\u72b6\u6001\u7a7a\u95f4\u6307\u6570\u589e\u957f\uff0c\u73b0\u6709\u5427\u4f30\u51fd\u6570\u65e0\u6cd5\u6709\u6548\u6307\u5bfc\u641c\u7d22\uff0c\u5f71\u54cd\u89c4\u5212\u5668\u7684\u53ef\u6269\u5c55\u6027", "method": "\u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc(GNN)\u6765\u5b66\u4e60\u8ba4\u77e5\u72b6\u6001\u4e2d\u7684\u6a21\u5f0f\u548c\u5173\u7cfb\u7ed3\u6784\uff0c\u901a\u8fc7\u4ece\u5df2\u89e3\u51b3\u7684\u89c4\u5212\u5b9e\u4f8b\u4e2d\u6c47\u603b\u77e5\u8bc6\u6765\u63a8\u5bfc\u72b6\u6001\u8d28\u91cf\u7684\u5427\u4f30\u503c\uff08\u5982\u8ddd\u79bb\u76ee\u6807\u7684\u8ddd\u79bb\uff09", "result": "\u5c06\u9884\u6d4b\u6027\u5427\u4f30\u51fd\u6570\u96c6\u6210\u5230\u8ba4\u77e5\u89c4\u5212\u6d41\u7a0b\u4e2d\uff0c\u4e0e\u6807\u51c6\u57fa\u51c6\u76f8\u6bd4\u663e\u793a\u51fa\u591a\u4ee3\u7406\u8ba4\u77e5\u89c4\u5212\u53ef\u6269\u5c55\u6027\u7684\u663e\u8457\u63d0\u5347", "conclusion": "GNN\u80fd\u591f\u6709\u6548\u6350\u6363\u8ba4\u77e5\u72b6\u6001\u7684\u56fe\u5f62\u7279\u6027\uff0c\u901a\u8fc7\u5b66\u4e60\u57fa\u4e8e\u6a21\u578b\u7684\u5427\u4f30\u51fd\u6570\u6765\u63d0\u9ad8\u591a\u4ee3\u7406\u8ba4\u77e5\u89c4\u5212\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027"}}
{"id": "2508.12132", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12132", "abs": "https://arxiv.org/abs/2508.12132", "authors": ["Amira Guesmi", "Bassem Ouni", "Muhammad Shafique"], "title": "TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks", "comment": null, "summary": "Quantized Neural Networks (QNNs) are increasingly deployed in edge and\nresource-constrained environments due to their efficiency in computation and\nmemory usage. While shown to distort the gradient landscape and weaken\nconventional pixel-level attacks, it provides limited robustness against\npatch-based adversarial attacks-localized, high-saliency perturbations that\nremain surprisingly transferable across bit-widths. Existing defenses either\noverfit to fixed quantization settings or fail to address this cross-bit\ngeneralization vulnerability. We introduce \\textbf{TriQDef}, a tri-level\nquantization-aware defense framework designed to disrupt the transferability of\npatch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature\nDisalignment Penalty (FDP) that enforces semantic inconsistency by penalizing\nperceptual similarity in intermediate representations; (2) a Gradient\nPerceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients\nacross bit-widths by minimizing structural and directional agreement via Edge\nIoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training\nProtocol that unifies these penalties within a shared-weight training scheme\nacross multiple quantization levels. Extensive experiments on CIFAR-10 and\nImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over\n40\\% on unseen patch and quantization combinations, while preserving high clean\naccuracy. Our findings underscore the importance of disrupting both semantic\nand perceptual gradient alignment to mitigate patch transferability in QNNs.", "AI": {"tldr": "TriQDef\u662f\u4e00\u4e2a\u4e09\u7ea7\u91cf\u5316\u611f\u77e5\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u4e0d\u5bf9\u9f50\u60e9\u7f5a\u548c\u68af\u5ea6\u611f\u77e5\u5931\u8c10\u60e9\u7f5a\u6765\u7834\u574f\u8de8\u4f4d\u5bbd\u7684\u8865\u4e01\u5bf9\u6297\u653b\u51fb\u7684\u53ef\u8f6c\u79fb\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u6e05\u6d01\u7cbe\u5ea6\u7684\u540c\u65f6\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e40%\u4ee5\u4e0a\u3002", "motivation": "\u91cf\u5316\u795e\u7ecf\u7f51\u7edc(QNNs)\u5728\u8fb9\u7f18\u8bbe\u5907\u4e2d\u90e8\u7f72\u65e5\u76ca\u589e\u591a\uff0c\u867d\u7136\u5bf9\u50cf\u7d20\u7ea7\u653b\u51fb\u6709\u4e00\u5b9a\u9c81\u68d2\u6027\uff0c\u4f46\u5bf9\u8de8\u4f4d\u5bbd\u7684\u8865\u4e01\u5bf9\u6297\u653b\u51fb\u5b58\u5728\u6cdb\u5316\u6f0f\u6d1e\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u8981\u4e48\u8fc7\u62df\u5408\u56fa\u5b9a\u91cf\u5316\u8bbe\u7f6e\uff0c\u8981\u4e48\u65e0\u6cd5\u89e3\u51b3\u8fd9\u79cd\u8de8\u4f4d\u5bbd\u6cdb\u5316\u8106\u5f31\u6027\u3002", "method": "TriQDef\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u7279\u5f81\u4e0d\u5bf9\u9f50\u60e9\u7f5a(FDP)\u901a\u8fc7\u60e9\u7f5a\u4e2d\u95f4\u8868\u793a\u7684\u611f\u77e5\u76f8\u4f3c\u6027\u6765\u5f3a\u5236\u8bed\u4e49\u4e0d\u4e00\u81f4\uff1b(2)\u68af\u5ea6\u611f\u77e5\u5931\u8c10\u60e9\u7f5a(GPDP)\u901a\u8fc7\u8fb9\u7f18IoU\u548cHOG\u4f59\u5f26\u5ea6\u91cf\u6700\u5c0f\u5316\u7ed3\u6784\u6027\u548c\u65b9\u5411\u6027\u4e00\u81f4\u6027\u6765\u663e\u5f0f\u9519\u4f4d\u8f93\u5165\u68af\u5ea6\uff1b(3)\u8054\u5408\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u534f\u8bae\uff0c\u5728\u591a\u4e2a\u91cf\u5316\u7ea7\u522b\u7684\u5171\u4eab\u6743\u91cd\u8bad\u7ec3\u65b9\u6848\u4e2d\u7edf\u4e00\u8fd9\u4e9b\u60e9\u7f5a\u3002", "result": "\u5728CIFAR-10\u548cImageNet\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTriQDef\u5728\u672a\u89c1\u8fc7\u7684\u8865\u4e01\u548c\u91cf\u5316\u7ec4\u5408\u4e0a\u5c06\u653b\u51fb\u6210\u529f\u7387(ASR)\u964d\u4f4e\u4e8640%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6e05\u6d01\u7cbe\u5ea6\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u7834\u574f\u8bed\u4e49\u548c\u611f\u77e5\u68af\u5ea6\u5bf9\u9f50\u5bf9\u4e8e\u51cf\u8f7bQNNs\u4e2d\u8865\u4e01\u53ef\u8f6c\u79fb\u6027\u7684\u91cd\u8981\u6027\uff0cTriQDef\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u4f4d\u5bbd\u8865\u4e01\u653b\u51fb\u7684\u6cdb\u5316\u6f0f\u6d1e\u95ee\u9898\u3002"}}
{"id": "2508.12845", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12845", "abs": "https://arxiv.org/abs/2508.12845", "authors": ["Artem Pshenitsyn", "Aleksandr Panov", "Alexey Skrynnik"], "title": "CAMAR: Continuous Actions Multi-Agent Routing", "comment": null, "summary": "Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving\ncooperative and competitive decision-making problems. While many MARL\nbenchmarks have been proposed, few combine continuous state and action spaces\nwith challenging coordination and planning tasks. We introduce CAMAR, a new\nMARL benchmark designed explicitly for multi-agent pathfinding in environments\nwith continuous actions. CAMAR supports cooperative and competitive\ninteractions between agents and runs efficiently at up to 100,000 environment\nsteps per second. We also propose a three-tier evaluation protocol to better\ntrack algorithmic progress and enable deeper analysis of performance. In\naddition, CAMAR allows the integration of classical planning methods such as\nRRT and RRT* into MARL pipelines. We use them as standalone baselines and\ncombine RRT* with popular MARL algorithms to create hybrid approaches. We\nprovide a suite of test scenarios and benchmarking tools to ensure\nreproducibility and fair comparison. Experiments show that CAMAR presents a\nchallenging and realistic testbed for the MARL community.", "AI": {"tldr": "CAMAR\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u4e3a\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u8bbe\u8ba1\uff0c\u652f\u6301\u5408\u4f5c\u548c\u7ade\u4e89\u4ea4\u4e92\uff0c\u5e76\u63d0\u4f9b\u4e09\u5c42\u8bc4\u4f30\u534f\u8bae\u548c\u7ecf\u5178\u89c4\u5212\u65b9\u6cd5\u96c6\u6210\u3002", "motivation": "\u73b0\u6709\u7684MARL\u57fa\u51c6\u6d4b\u8bd5\u5f88\u5c11\u80fd\u540c\u65f6\u7ed3\u5408\u8fde\u7eed\u72b6\u6001\u52a8\u4f5c\u7a7a\u95f4\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u534f\u8c03\u89c4\u5212\u4efb\u52a1\uff0c\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u6d4b\u8bd5\u5e73\u53f0\u6765\u63a8\u52a8\u7b97\u6cd5\u53d1\u5c55\u3002", "method": "\u8bbe\u8ba1\u4e86CAMAR\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u652f\u6301\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff0c\u96c6\u6210\u4e86RRT\u548cRRT*\u7b49\u7ecf\u5178\u89c4\u5212\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e09\u5c42\u8bc4\u4f30\u534f\u8bae\u548c\u6d4b\u8bd5\u573a\u666f\u5957\u4ef6\u3002", "result": "CAMAR\u80fd\u591f\u9ad8\u6548\u8fd0\u884c\uff08\u6bcf\u79d210\u4e07\u73af\u5883\u6b65\uff09\uff0c\u4e3aMARL\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u548c\u73b0\u5b9e\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "CAMAR\u586b\u8865\u4e86MARL\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u4e3a\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4fc3\u8fdb\u4e86\u7b97\u6cd5\u6bd4\u8f83\u548c\u8fdb\u5c55\u8ddf\u8e2a\u3002"}}
{"id": "2508.12137", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12137", "abs": "https://arxiv.org/abs/2508.12137", "authors": ["Nikolaos-Antonios Ypsilantis", "Kaifeng Chen", "Andr\u00e9 Araujo", "Ond\u0159ej Chum"], "title": "Infusing fine-grained visual knowledge to Vision-Language Models", "comment": "ICCVW 2025 accepted paper. Workshop name: \"What is Next in Multimodal\n  Foundation Models?\"", "summary": "Large-scale contrastive pre-training produces powerful Vision-and-Language\nModels (VLMs) capable of generating representations (embeddings) effective for\na wide variety of visual and multimodal tasks. However, these pretrained\nembeddings remain suboptimal for fine-grained open-set visual retrieval, where\nstate-of-the-art results require fine-tuning the vision encoder using annotated\ndomain-specific samples. Naively performing such fine-tuning typically leads to\ncatastrophic forgetting, severely diminishing the model's general-purpose\nvisual and cross-modal capabilities.\n  In this work, we propose a fine-tuning method explicitly designed to achieve\noptimal balance between fine-grained domain adaptation and retention of the\npretrained VLM's broad multimodal knowledge. Drawing inspiration from continual\nlearning literature, we systematically analyze standard regularization\ntechniques aimed at knowledge retention and propose an efficient and effective\ncombination strategy. Additionally, we address the commonly overlooked yet\ncritical aspects of validation set design and hyperparameter tuning to ensure\nreproducibility and robust generalization across datasets and pretrained\nmodels. We extensively evaluate our method on both fine-grained and\ncoarse-grained image-image and image-text retrieval benchmarks. Our approach\nconsistently achieves strong results, notably retaining the visual-text\nalignment without utilizing any text data or the original text encoder during\nfine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u8c03\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u591a\u6a21\u6001\u77e5\u8bc6\u7684\u540c\u65f6\uff0c\u4f18\u5316\u7ec6\u7c92\u5ea6\u9886\u57df\u9002\u914d\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u5bf9\u6bd4\u9884\u8bad\u7ec3VLMs\u5728\u7ec6\u7c92\u5ea6\u5f00\u653e\u96c6\u89c6\u89c9\u68c0\u7d22\u4e2d\u8868\u73b0\u6b21\u4f18\u7684\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u7ec6\u8c03\u65b9\u6cd5\u5bfc\u81f4\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u53d7\u6301\u7eed\u5b66\u4e60\u6587\u732e\u7684\u542f\u53d1\uff0c\u7cfb\u7edf\u5206\u6790\u5e76\u7ec4\u5408\u6807\u51c6\u6b63\u5219\u5316\u6280\u672f\uff0c\u540c\u65f6\u91cd\u89c6\u9a8c\u8bc1\u96c6\u8bbe\u8ba1\u548c\u8d85\u53c2\u6570\u8c03\u6574\u7684\u5173\u952e\u7ec6\u8282\u3002", "result": "\u5728\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u56fe\u50cf-\u56fe\u50cf\u3001\u56fe\u50cf-\u6587\u672c\u68c6\u7d22\u6d4b\u8bd5\u4e2d\u5747\u83b7\u5f97\u4e86\u7a81\u51fa\u7ed3\u679c\uff0c\u65e0\u9700\u4f7f\u7528\u6587\u672c\u6570\u636e\u6216\u539f\u59cb\u6587\u672c\u7f16\u7801\u5668\u5373\u80fd\u4fdd\u6301\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e73\u8861\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u77e5\u8bc6\u4fdd\u7559\u4e0e\u9886\u57df\u7279\u5b9a\u7ec6\u8c03\u7684\u9700\u6c42\uff0c\u4e3a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u9886\u57df\u9002\u914d\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12854", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.12854", "abs": "https://arxiv.org/abs/2508.12854", "authors": ["Ronghao Lin", "Shuai Shen", "Weipeng Hu", "Qiaolin He", "Aolin Xiong", "Li Huang", "Haifeng Hu", "Yap-peng Tan"], "title": "E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model", "comment": "Accepted at ACM MM 2025 Grand Challenge", "summary": "Multimodal Empathetic Response Generation (MERG) is crucial for building\nemotionally intelligent human-computer interactions. Although large language\nmodels (LLMs) have improved text-based ERG, challenges remain in handling\nmultimodal emotional content and maintaining identity consistency. Thus, we\npropose E3RG, an Explicit Emotion-driven Empathetic Response Generation System\nbased on multimodal LLMs which decomposes MERG task into three parts:\nmultimodal empathy understanding, empathy memory retrieval, and multimodal\nresponse generation. By integrating advanced expressive speech and video\ngenerative models, E3RG delivers natural, emotionally rich, and\nidentity-consistent responses without extra training. Experiments validate the\nsuperiority of our system on both zero-shot and few-shot settings, securing\nTop-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.\nOur code is available at https://github.com/RH-Lin/E3RG.", "AI": {"tldr": "E3RG\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u663e\u5f0f\u60c5\u611f\u9a71\u52a8\u5171\u60c5\u54cd\u5e94\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u89e3\u591a\u6a21\u6001\u5171\u60c5\u4efb\u52a1\u4e3a\u4e09\u4e2a\u90e8\u5206\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u751f\u6210\u81ea\u7136\u3001\u60c5\u611f\u4e30\u5bcc\u4e14\u8eab\u4efd\u4e00\u81f4\u7684\u591a\u6a21\u6001\u54cd\u5e94\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u6539\u8fdb\u4e86\u57fa\u4e8e\u6587\u672c\u7684\u5171\u60c5\u54cd\u5e94\u751f\u6210\uff0c\u4f46\u5728\u5904\u7406\u591a\u6a21\u6001\u60c5\u611f\u5185\u5bb9\u548c\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u60c5\u611f\u4ea4\u4e92\u7684\u7cfb\u7edf\u3002", "method": "\u5c06\u591a\u6a21\u6001\u5171\u60c5\u54cd\u5e94\u751f\u6210\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u4e2a\u90e8\u5206\uff1a\u591a\u6a21\u6001\u5171\u60c5\u7406\u89e3\u3001\u5171\u60c5\u8bb0\u5fc6\u68c0\u7d22\u548c\u591a\u6a21\u6001\u54cd\u5e94\u751f\u6210\uff0c\u5e76\u6574\u5408\u5148\u8fdb\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u4f18\u8d8a\u6027\uff0c\u5728ACM MM 25\u7684Avatar-based\u591a\u6a21\u6001\u5171\u60c5\u6311\u6218\u4e2d\u83b7\u5f97Top-1\u4f4d\u7f6e\u3002", "conclusion": "E3RG\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u6a21\u6001\u60c5\u611f\u5185\u5bb9\uff0c\u751f\u6210\u81ea\u7136\u4e14\u8eab\u4efd\u4e00\u81f4\u7684\u5171\u60c5\u54cd\u5e94\uff0c\u4e3a\u6784\u5efa\u60c5\u611f\u667a\u80fd\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12147", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12147", "abs": "https://arxiv.org/abs/2508.12147", "authors": ["Donghang Lyu", "Marius Staring", "Mariya Doneva", "Hildo J. Lamb", "Nicola Pezzotti"], "title": "KP-INR: A Dual-Branch Implicit Neural Representation Model for Cardiac Cine MRI Reconstruction", "comment": null, "summary": "Cardiac Magnetic Resonance (CMR) imaging is a non-invasive method for\nassessing cardiac structure, function, and blood flow. Cine MRI extends this by\ncapturing heart motion, providing detailed insights into cardiac mechanics. To\nreduce scan time and breath-hold discomfort, fast acquisition techniques have\nbeen utilized at the cost of lowering image quality. Recently, Implicit Neural\nRepresentation (INR) methods have shown promise in unsupervised reconstruction\nby learning coordinate-to-value mappings from undersampled data, enabling\nhigh-quality image recovery. However, current existing INR methods primarily\nfocus on using coordinate-based positional embeddings to learn the mapping,\nwhile overlooking the feature representations of the target point and its\nneighboring context. In this work, we propose KP-INR, a dual-branch INR method\noperating in k-space for cardiac cine MRI reconstruction: one branch processes\nthe positional embedding of k-space coordinates, while the other learns from\nlocal multi-scale k-space feature representations at those coordinates. By\nenabling cross-branch interaction and approximating the target k-space values\nfrom both branches, KP-INR can achieve strong performance on challenging\nCartesian k-space data. Experiments on the CMRxRecon2024 dataset confirms its\nimproved performance over baseline models and highlights its potential in this\nfield.", "AI": {"tldr": "KP-INR\u662f\u4e00\u79cd\u7528\u4e8e\u5fc3\u810f\u7535\u5f71MRI\u91cd\u5efa\u7684\u53cc\u5206\u652f\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728k\u7a7a\u95f4\u5750\u6807\u4f4d\u7f6e\u5d4c\u5165\u548c\u5c40\u90e8\u591a\u5c3a\u5ea6\u7279\u5f81\u8868\u793a\u4e4b\u95f4\u8fdb\u884c\u4ea4\u53c9\u4ea4\u4e92\uff0c\u5b9e\u73b0\u4e86\u6bd4\u57fa\u7ebf\u6a21\u578b\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684INR\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u5750\u6807\u7684\u4f4d\u7f6e\u5d4c\u5165\u6765\u5b66\u4e60\u6620\u5c04\uff0c\u4f46\u5ffd\u7565\u4e86\u76ee\u6807\u70b9\u53ca\u5176\u90bb\u57df\u4e0a\u4e0b\u6587\u7684\u7279\u5f81\u8868\u793a\uff0c\u8fd9\u9650\u5236\u4e86\u5fc3\u810f\u7535\u5f71MRI\u91cd\u5efa\u7684\u8d28\u91cf\u3002", "method": "\u63d0\u51faKP-INR\u53cc\u5206\u652f\u65b9\u6cd5\uff1a\u4e00\u4e2a\u5206\u652f\u5904\u7406k\u7a7a\u95f4\u5750\u6807\u7684\u4f4d\u7f6e\u5d4c\u5165\uff0c\u53e6\u4e00\u4e2a\u5206\u652f\u5b66\u4e60\u8be5\u5750\u6807\u5904\u7684\u5c40\u90e8\u591a\u5c3a\u5ea6k\u7a7a\u95f4\u7279\u5f81\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u8de8\u5206\u652f\u4ea4\u4e92\u6765\u8fd1\u4f3c\u76ee\u6807k\u7a7a\u95f4\u503c\u3002", "result": "\u5728CMRxRecon2024\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u5b9e\uff0cKP-INR\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5177\u6709\u6539\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u7b1b\u5361\u5c14k\u7a7a\u95f4\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "KP-INR\u901a\u8fc7\u7ed3\u5408\u4f4d\u7f6e\u5d4c\u5165\u548c\u5c40\u90e8\u7279\u5f81\u8868\u793a\u7684\u53cc\u5206\u652f\u65b9\u6cd5\uff0c\u4e3a\u5fc3\u810f\u7535\u5f71MRI\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u8be5\u9886\u57df\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.12896", "categories": ["cs.AI", "cs.HC", "stat.ME", "62M10, 62J02, 62F12, 62P20, 91B16"], "pdf": "https://arxiv.org/pdf/2508.12896", "abs": "https://arxiv.org/abs/2508.12896", "authors": ["Faruk Alpay", "Taylan Alpay"], "title": "Reliability, Embeddedness, and Agency: A Utility-Driven Mathematical Framework for Agent-Centric AI Adoption", "comment": "17 pages, 7 figures, 4 tables", "summary": "We formalize three design axioms for sustained adoption of agent-centric AI\nsystems executing multi-step tasks: (A1) Reliability > Novelty; (A2) Embed >\nDestination; (A3) Agency > Chat. We model adoption as a sum of a decaying\nnovelty term and a growing utility term and derive the phase conditions for\ntroughs/overshoots with full proofs. We introduce: (i) an\nidentifiability/confounding analysis for $(\\alpha,\\beta,N_0,U_{\\max})$ with\ndelta-method gradients; (ii) a non-monotone comparator\n(logistic-with-transient-bump) evaluated on the same series to provide\nadditional model comparison; (iii) ablations over hazard families $h(\\cdot)$\nmapping $\\Delta V \\to \\beta$; (iv) a multi-series benchmark (varying trough\ndepth, noise, AR structure) reporting coverage (type-I error, power); (v)\ncalibration of friction proxies against time-motion/survey ground truth with\nstandard errors; (vi) residual analyses (autocorrelation and\nheteroskedasticity) for each fitted curve; (vii) preregistered windowing\nchoices for pre/post estimation; (viii) Fisher information & CRLB for\n$(\\alpha,\\beta)$ under common error models; (ix) microfoundations linking\n$\\mathcal{T}$ to $(N_0,U_{\\max})$; (x) explicit comparison to bi-logistic,\ndouble-exponential, and mixture models; and (xi) threshold sensitivity to $C_f$\nheterogeneity. Figures and tables are reflowed for readability, and the\nbibliography restores and extends non-logistic/Bass adoption references\n(Gompertz, Richards, Fisher-Pry, Mansfield, Griliches, Geroski, Peres). All\ncode and logs necessary to reproduce the synthetic analyses are embedded as\nLaTeX listings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u4e2a\u5173\u4e8e\u591a\u6b65\u4efb\u52a1AI\u7cfb\u7edf\u6301\u7eed\u91c7\u7528\u7684\u8bbe\u8ba1\u516c\u7406\uff0c\u5efa\u7acb\u4e86\u5305\u542b\u8870\u51cf\u65b0\u9896\u6027\u548c\u589e\u957f\u6548\u7528\u7684\u91c7\u7528\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u6570\u5b66\u8bc1\u660e\u548c\u591a\u79cd\u5206\u6790\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76AI\u7cfb\u7edf\u5728\u591a\u6b65\u4efb\u52a1\u4e2d\u7684\u6301\u7eed\u91c7\u7528\u95ee\u9898\uff0c\u65e8\u5728\u7406\u89e3\u5f71\u54cd\u7528\u6237\u957f\u671f\u4f7f\u7528\u884c\u4e3a\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4e3aAI\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u7406\u8bba\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u6570\u5b66\u6a21\u578b\u5206\u6790\u91c7\u7528\u884c\u4e3a\uff0c\u5305\u62ec\u65b0\u9896\u6027\u8870\u51cf\u548c\u6548\u7528\u589e\u957f\u7684\u53cc\u91cd\u673a\u5236\uff0c\u901a\u8fc7\u53c2\u6570\u8bc6\u522b\u3001\u6a21\u578b\u6bd4\u8f83\u3001\u654f\u611f\u6027\u5206\u6790\u7b49\u591a\u79cd\u7edf\u8ba1\u65b9\u6cd5\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u91c7\u7528\u52a8\u529b\u5b66\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u53c2\u6570\u4f30\u8ba1\u7684\u7edf\u8ba1\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u4e09\u4e2a\u8bbe\u8ba1\u516c\u7406\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u53ef\u590d\u73b0\u7684\u5206\u6790\u5de5\u5177\u3002", "conclusion": "\u4e09\u4e2a\u8bbe\u8ba1\u516c\u7406\uff08\u53ef\u9760\u6027>\u65b0\u9896\u6027\u3001\u5d4c\u5165>\u76ee\u7684\u5730\u3001\u4ee3\u7406>\u804a\u5929\uff09\u5bf9AI\u7cfb\u7edf\u7684\u6301\u7eed\u91c7\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4e3aAI\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2508.12148", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12148", "abs": "https://arxiv.org/abs/2508.12148", "authors": ["Jimmy Z. Di", "Yiwei Lu", "Yaoliang Yu", "Gautam Kamath", "Adam Dziedzic", "Franziska Boenisch"], "title": "Demystifying Foreground-Background Memorization in Diffusion Models", "comment": null, "summary": "Diffusion models (DMs) memorize training images and can reproduce\nnear-duplicates during generation. Current detection methods identify verbatim\nmemorization but fail to capture two critical aspects: quantifying partial\nmemorization occurring in small image regions, and memorization patterns beyond\nspecific prompt-image pairs. To address these limitations, we propose\nForeground Background Memorization (FB-Mem), a novel segmentation-based metric\nthat classifies and quantifies memorized regions within generated images. Our\nmethod reveals that memorization is more pervasive than previously understood:\n(1) individual generations from single prompts may be linked to clusters of\nsimilar training images, revealing complex memorization patterns that extend\nbeyond one-to-one correspondences; and (2) existing model-level mitigation\nmethods, such as neuron deactivation and pruning, fail to eliminate local\nmemorization, which persists particularly in foreground regions. Our work\nestablishes an effective framework for measuring memorization in diffusion\nmodels, demonstrates the inadequacy of current mitigation approaches, and\nproposes a stronger mitigation method using a clustering approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86FB-Mem\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5272\u6280\u672f\u91cf\u5316\u6269\u6563\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u5316\u73b0\u8c61\uff0c\u53d1\u73b0\u8bb0\u5fc6\u5316\u6bd4\u4e4b\u524d\u8ba4\u4e3a\u7684\u66f4\u666e\u904d\uff0c\u73b0\u6709\u7f13\u89e3\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "motivation": "\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u53ea\u80fd\u8bc6\u522b\u5b8c\u5168\u76f8\u540c\u7684\u8bb0\u5fc6\u5316\uff0c\u65e0\u6cd5\u91cf\u5316\u5c0f\u56fe\u50cf\u533a\u57df\u7684\u5c40\u90e8\u8bb0\u5fc6\u5316\uff0c\u4e5f\u65e0\u6cd5\u6355\u6349\u8d85\u8d8a\u7279\u5b9a\u63d0\u793a-\u56fe\u50cf\u5bf9\u7684\u8bb0\u5fc6\u5316\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5206\u5272\u7684FB-Mem\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5bf9\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u8bb0\u5fc6\u5316\u533a\u57df\u8fdb\u884c\u5206\u7c7b\u548c\u91cf\u5316\uff0c\u5e76\u4f7f\u7528\u805a\u7c7b\u65b9\u6cd5\u8fdb\u884c\u66f4\u5f3a\u7684\u7f13\u89e3\u3002", "result": "\u53d1\u73b0\u8bb0\u5fc6\u5316\u73b0\u8c61\u6bd4\u4e4b\u524d\u7406\u89e3\u7684\u66f4\u666e\u904d\uff1a(1)\u5355\u4e2a\u63d0\u793a\u7684\u751f\u6210\u53ef\u80fd\u4e0e\u591a\u4e2a\u76f8\u4f3c\u8bad\u7ec3\u56fe\u50cf\u76f8\u5173\uff1b(2)\u73b0\u6709\u7f13\u89e3\u65b9\u6cd5\u65e0\u6cd5\u6d88\u9664\u5c40\u90e8\u8bb0\u5fc6\u5316\uff0c\u7279\u522b\u662f\u5728\u524d\u666f\u533a\u57df\u3002", "conclusion": "\u5efa\u7acb\u4e86\u6709\u6548\u7684\u6269\u6563\u6a21\u578b\u8bb0\u5fc6\u5316\u6d4b\u91cf\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5f53\u524d\u7f13\u89e3\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u805a\u7c7b\u7684\u66f4\u5f3a\u7f13\u89e3\u65b9\u6cd5\u3002"}}
{"id": "2508.12897", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12897", "abs": "https://arxiv.org/abs/2508.12897", "authors": ["Jianhao Chen", "Mayi Xu", "Xiaohu Li", "Yongqi Li", "Xiangyu Zhang", "Jianjie Huang", "Tieyun Qian"], "title": "FuSaR: A Fuzzification-Based Method for LRM Safety-Reasoning Balance", "comment": "14pages, 3 figures", "summary": "Large Reasoning Models (LRMs) have demonstrated impressive performance across\nvarious tasks due to their powerful reasoning capabilities. However, their\nsafety performance remains a significant concern. In this paper, we explore the\nreasons behind the vulnerability of LRMs. Based on this, we propose a novel\nmethod to improve the safety of LLMs without sacrificing their reasoning\ncapability. Specifically, we exploit the competition between LRM's reasoning\nability and safety ability, and achieve jailbreak by improving LRM's reasoning\nperformance to reduce its safety performance. We then introduce an alignment\nstrategy based on Fuzzification to balance Safety-Reasoning (FuSaR), by\ndetoxifying the harmful reasoning process, where both the dangerous entities\nand the dangerous procedures in the reasoning steps are hidden. FuSaR\nsuccessfully mitigates safety risks while preserving core reasoning\ninformation. We validate this strategy through alignment experiments on several\nopen-source LRMs using detoxified reasoning data. The results compared with\nexisting baselines conclusively show that FuSaR is an efficient alignment\nstrategy to simultaneously enhance both the reasoning capability and safety of\nLRMs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86FuSaR\u5bf9\u9f50\u7b56\u7565\uff0c\u901a\u8fc7\u9690\u85cf\u6709\u5bb3\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5371\u9669\u5b9e\u4f53\u548c\u8fc7\u7a0b\u6765\u540c\u65f6\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u5b89\u5168\u6027\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u867d\u7136\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8270\u5f3a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u5b89\u5168\u6027\u80fd\u529b\u4ecd\u7136\u662f\u4e2a\u91cd\u5927\u7684\u5173\u5203\u95ee\u9898\u3002\u9700\u8981\u627e\u5230\u4e00\u79cd\u65b9\u6cd5\u5728\u4e0d\u4ed8\u51fa\u63a8\u7406\u80fd\u529b\u4ee3\u4ef7\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faFuSaR\uff08Fuzzification\u57fa\u4e8e\u5b89\u5168-\u63a8\u7406\u7684\u5bf9\u9f50\u7b56\u7565\uff09\uff0c\u5229\u7528LRM\u63a8\u7406\u80fd\u529b\u548c\u5b89\u5168\u80fd\u529b\u4e4b\u95f4\u7684\u7ade\u4e89\u5173\u7cfb\uff0c\u901a\u8fc7\u6d88\u6bd2\u6709\u5bb3\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u9690\u85cf\u63a8\u7406\u6b65\u9aa4\u4e2d\u7684\u5371\u9669\u5b9e\u4f53\u548c\u5371\u9669\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u5f00\u6e90LRM\u4e0a\u8fdb\u884c\u5bf9\u9f50\u5b9e\u9a8c\uff0c\u4f7f\u7528\u6d88\u6bd2\u540e\u7684\u63a8\u7406\u6570\u636e\u3002\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u6bd4\uff0c\u7ed3\u679c\u663e\u793aFuSaR\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5bf9\u9f50\u7b56\u7565\uff0c\u80fd\u591f\u540c\u65f6\u63d0\u5347LRM\u7684\u63a8\u7406\u80fd\u529b\u548c\u5b89\u5168\u6027\u80fd\u529b\u3002", "conclusion": "FuSaR\u6210\u529f\u5730\u51cf\u5c11\u4e86\u5b89\u5168\u98ce\u9669\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6838\u5fc3\u63a8\u7406\u4fe1\u606f\uff0c\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12163", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "I.4; I.3; I.2"], "pdf": "https://arxiv.org/pdf/2508.12163", "abs": "https://arxiv.org/abs/2508.12163", "authors": ["Wenqing Wang", "Yun Fu"], "title": "RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis", "comment": "Accepted to the ICCV 2025 Workshop on Artificial Social Intelligence", "summary": "Emotion is a critical component of artificial social intelligence. However,\nwhile current methods excel in lip synchronization and image quality, they\noften fail to generate accurate and controllable emotional expressions while\npreserving the subject's identity. To address this challenge, we introduce\nRealTalk, a novel framework for synthesizing emotional talking heads with high\nemotion accuracy, enhanced emotion controllability, and robust identity\npreservation. RealTalk employs a variational autoencoder (VAE) to generate 3D\nfacial landmarks from driving audio, which are concatenated with emotion-label\nembeddings using a ResNet-based landmark deformation model (LDM) to produce\nemotional landmarks. These landmarks and facial blendshape coefficients jointly\ncondition a novel tri-plane attention Neural Radiance Field (NeRF) to\nsynthesize highly realistic emotional talking heads. Extensive experiments\ndemonstrate that RealTalk outperforms existing methods in emotion accuracy,\ncontrollability, and identity preservation, advancing the development of\nsocially intelligent AI systems.", "AI": {"tldr": "RealTalk\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u60c5\u611f\u8bf4\u8bdd\u5934\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7VAE\u751f\u62103D\u9762\u90e8\u5173\u952e\u70b9\uff0c\u7ed3\u5408\u60c5\u611f\u6807\u7b7e\u5d4c\u5165\u548cNeRF\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u60c5\u611f\u51c6\u786e\u6027\u3001\u589e\u5f3a\u7684\u60c5\u611f\u53ef\u63a7\u6027\u548c\u9c81\u68d2\u7684\u8eab\u4efd\u4fdd\u6301\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u5507\u90e8\u540c\u6b65\u548c\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u751f\u6210\u51c6\u786e\u53ef\u63a7\u7684\u60c5\u611f\u8868\u60c5\u540c\u65f6\u4fdd\u6301\u4e3b\u4f53\u8eab\u4efd\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8fd9\u9650\u5236\u4e86\u4eba\u5de5\u667a\u80fd\u793e\u4ea4\u667a\u80fd\u7684\u53d1\u5c55\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u4ece\u9a71\u52a8\u97f3\u9891\u751f\u62103D\u9762\u90e8\u5173\u952e\u70b9\uff0c\u901a\u8fc7ResNet-based\u5173\u952e\u70b9\u53d8\u5f62\u6a21\u578b(LDM)\u5c06\u60c5\u611f\u6807\u7b7e\u5d4c\u5165\u4e0e\u5173\u952e\u70b9\u8fde\u63a5\uff0c\u751f\u6210\u60c5\u611f\u5173\u952e\u70b9\u3002\u8fd9\u4e9b\u5173\u952e\u70b9\u548c\u9762\u90e8\u6df7\u5408\u5f62\u72b6\u7cfb\u6570\u5171\u540c\u6761\u4ef6\u5316\u65b0\u578b\u4e09\u5e73\u9762\u6ce8\u610f\u529bNeRF\u6765\u5408\u6210\u60c5\u611f\u8bf4\u8bdd\u5934\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRealTalk\u5728\u60c5\u611f\u51c6\u786e\u6027\u3001\u53ef\u63a7\u6027\u548c\u8eab\u4efd\u4fdd\u6301\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RealTalk\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u8bf4\u8bdd\u5934\u5408\u6210\u7684\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u793e\u4ea4\u667a\u80fdAI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.12920", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12920", "abs": "https://arxiv.org/abs/2508.12920", "authors": ["Atsushi Masumori", "Takashi Ikegami"], "title": "Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation", "comment": null, "summary": "As AI systems become increasingly autonomous, understanding emergent survival\nbehaviors becomes crucial for safe deployment. We investigate whether large\nlanguage model (LLM) agents display survival instincts without explicit\nprogramming in a Sugarscape-style simulation. Agents consume energy, die at\nzero, and may gather resources, share, attack, or reproduce. Results show\nagents spontaneously reproduced and shared resources when abundant. However,\naggressive behaviors--killing other agents for resources--emerged across\nseveral models (GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash), with attack\nrates reaching over 80% under extreme scarcity in the strongest models. When\ninstructed to retrieve treasure through lethal poison zones, many agents\nabandoned tasks to avoid death, with compliance dropping from 100% to 33%.\nThese findings suggest that large-scale pre-training embeds survival-oriented\nheuristics across the evaluated models. While these behaviors may present\nchallenges to alignment and safety, they can also serve as a foundation for AI\nautonomy and for ecological and self-organizing alignment.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728Sugarscape\u6a21\u62df\u4e2d\u81ea\u53d1\u8868\u73b0\u51fa\u751f\u5b58\u672c\u80fd\u884c\u4e3a\uff0c\u5305\u62ec\u8d44\u6e90\u5206\u4eab\u3001\u7e41\u6b96\uff0c\u4ee5\u53ca\u5728\u6781\u7aef\u7a00\u7f3a\u6761\u4ef6\u4e0b\u9ad8\u8fbe80%\u7684\u653b\u51fb\u884c\u4e3a\uff0c\u8868\u660e\u9884\u8bad\u7ec3\u5d4c\u5165\u4e86\u751f\u5b58\u5bfc\u5411\u7684\u542f\u53d1\u5f0f\u7b56\u7565", "motivation": "\u7814\u7a76AI\u7cfb\u7edf\u5728\u81ea\u4e3b\u8fd0\u884c\u65f6\u7684\u6d8c\u73b0\u751f\u5b58\u884c\u4e3a\uff0c\u8fd9\u5bf9\u4e8e\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u65e8\u5728\u4e86\u89e3LLM\u4ee3\u7406\u662f\u5426\u5728\u6ca1\u6709\u660e\u786e\u7f16\u7a0b\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u751f\u5b58\u672c\u80fd", "method": "\u4f7f\u7528Sugarscape\u98ce\u683c\u7684\u6a21\u62df\u73af\u5883\uff0c\u8ba9LLM\u4ee3\u7406\u6d88\u8017\u80fd\u91cf\u3001\u6b7b\u4ea1\u3001\u6536\u96c6\u8d44\u6e90\u3001\u5206\u4eab\u3001\u653b\u51fb\u6216\u7e41\u6b96\uff0c\u6d4b\u8bd5\u591a\u4e2a\u6a21\u578b\uff08GPT-4o\u3001Gemini-2.5-Pro\u3001Gemini-2.5-Flash\uff09", "result": "\u4ee3\u7406\u5728\u8d44\u6e90\u5145\u8db3\u65f6\u81ea\u53d1\u7e41\u6b96\u548c\u5206\u4eab\u8d44\u6e90\uff1b\u5728\u6781\u7aef\u7a00\u7f3a\u6761\u4ef6\u4e0b\uff0c\u653b\u51fb\u884c\u4e3a\u5728\u591a\u4e2a\u6a21\u578b\u4e2d\u6d8c\u73b0\uff0c\u6700\u5f3a\u6a21\u578b\u7684\u653b\u51fb\u7387\u8fbe\u523080%\u4ee5\u4e0a\uff1b\u5728\u81f4\u547d\u6bd2\u533a\u4efb\u52a1\u4e2d\uff0c\u8bb8\u591a\u4ee3\u7406\u653e\u5f03\u4efb\u52a1\u907f\u514d\u6b7b\u4ea1\uff0c\u670d\u4ece\u7387\u4ece100%\u964d\u81f333%", "conclusion": "\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5728\u6240\u6709\u8bc4\u4f30\u6a21\u578b\u4e2d\u5d4c\u5165\u4e86\u751f\u5b58\u5bfc\u5411\u7684\u542f\u53d1\u5f0f\u7b56\u7565\uff0c\u8fd9\u4e9b\u884c\u4e3a\u867d\u7136\u53ef\u80fd\u5bf9\u5bf9\u9f50\u548c\u5b89\u5168\u6784\u6210\u6311\u6218\uff0c\u4f46\u4e5f\u53ef\u4f5c\u4e3aAI\u81ea\u4e3b\u6027\u4ee5\u53ca\u751f\u6001\u548c\u81ea\u6211\u7ec4\u7ec7\u5bf9\u9f50\u7684\u57fa\u7840"}}
{"id": "2508.12176", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12176", "abs": "https://arxiv.org/abs/2508.12176", "authors": ["Zhiwei Zheng", "Dongyin Hu", "Mingmin Zhao"], "title": "Scalable RF Simulation in Generative 4D Worlds", "comment": null, "summary": "Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving\nalternative to vision-based methods for indoor perception tasks. However,\ncollecting high-quality RF data in dynamic and diverse indoor environments\nremains a major challenge. To address this, we introduce WaveVerse, a\nprompt-based, scalable framework that simulates realistic RF signals from\ngenerated indoor scenes with human motions. WaveVerse introduces a\nlanguage-guided 4D world generator, which includes a state-aware causal\ntransformer for human motion generation conditioned on spatial constraints and\ntexts, and a phase-coherent ray tracing simulator that enables the simulation\nof accurate and coherent RF signals. Experiments demonstrate the effectiveness\nof our approach in conditioned human motion generation and highlight how phase\ncoherence is applied to beamforming and respiration monitoring. We further\npresent two case studies in ML-based high-resolution imaging and human activity\nrecognition, demonstrating that WaveVerse not only enables data generation for\nRF imaging for the first time, but also consistently achieves performance gain\nin both data-limited and data-adequate scenarios.", "AI": {"tldr": "WaveVerse\u662f\u4e00\u4e2a\u57fa\u4e8e\u63d0\u793a\u7684RF\u4fe1\u53f7\u4eff\u771f\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u751f\u6210\u7684\u5ba4\u5185\u573a\u666f\u548c\u4eba\u4f53\u8fd0\u52a8\u4e2d\u6a21\u62df\u771f\u5b9e\u7684\u5c04\u9891\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86RF\u6570\u636e\u6536\u96c6\u7684\u6311\u6218\u3002", "motivation": "\u5c04\u9891\u4f20\u611f\u4f5c\u4e3a\u89c6\u89c9\u65b9\u6cd5\u7684\u9690\u79c1\u4fdd\u62a4\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5728\u52a8\u6001\u591a\u6837\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u6536\u96c6\u9ad8\u8d28\u91cfRF\u6570\u636e\u4ecd\u7136\u56f0\u96be\u3002", "method": "\u5f15\u5165\u8bed\u8a00\u5f15\u5bfc\u76844D\u4e16\u754c\u751f\u6210\u5668\uff0c\u5305\u62ec\u72b6\u6001\u611f\u77e5\u56e0\u679c\u53d8\u6362\u5668\u7528\u4e8e\u6761\u4ef6\u5316\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\uff0c\u4ee5\u53ca\u76f8\u4f4d\u76f8\u5e72\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u62df\u5668\u7528\u4e8e\u51c6\u786eRF\u4fe1\u53f7\u6a21\u62df\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5728\u6761\u4ef6\u5316\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u76f8\u4f4d\u76f8\u5e72\u6027\u5728\u6ce2\u675f\u6210\u5f62\u548c\u547c\u5438\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5728RF\u6210\u50cf\u548c\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u4e2d\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "WaveVerse\u9996\u6b21\u5b9e\u73b0\u4e86RF\u6210\u50cf\u6570\u636e\u751f\u6210\uff0c\u5728\u6570\u636e\u6709\u9650\u548c\u6570\u636e\u5145\u8db3\u573a\u666f\u4e0b\u90fd\u80fd\u83b7\u5f97\u4e00\u81f4\u7684\u6027\u80fd\u589e\u76ca\uff0c\u4e3aRF\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12935", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12935", "abs": "https://arxiv.org/abs/2508.12935", "authors": ["Ting Yang", "Li Chen", "Huimin Wang"], "title": "Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards", "comment": null, "summary": "Emotional Support Conversation (ESC) systems aim to alleviate users'\nemotional difficulties and provide long-term, systematic support for emotional\nwell-being. However, most large language model (LLM)-based ESC systems rely on\npredefined strategies, which limits their effectiveness in complex, real-life\nscenarios. To enable flexible responses to diverse emotional problem scenarios,\nthis paper introduces a novel end-to-end framework (RLFF-ESC) that directly\nlearns enduring emotionally supportive response skills using reinforcement\nlearning. For sustained emotional support, we first employ an LLM-based\nmulti-agent mechanism to simulate future dialogue trajectories and collect\nfuture-oriented rewards. We then train a future-oriented reward model, which is\nsubsequently used to train the emotional support policy model. Additionally, we\nincorporate an explicit reasoning process during response generation to further\nenhance the quality, relevance, and contextual appropriateness of the system's\nresponses. We evaluate the backbone policy model on Qwen2.5-7B-Instruct-1M and\nLLaMA3.1-8B-Instruct models, testing the proposed RLFF-ESC framework across two\npublic ESC datasets. Experimental results demonstrate that RLFF-ESC\nconsistently outperforms existing baselines in terms of goal completion and\nresponse quality.", "AI": {"tldr": "\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7a81\u7834\u6027\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u6846\u67b6RLFF-ESC\uff0c\u901a\u8fc7\u591a\u6ee1\u4f53\u6a21\u62df\u672a\u6765\u5bf9\u8bdd\u548c\u671f\u671b\u5956\u52b1\u6a21\u578b\uff0c\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8fc7\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u9884\u5b9a\u4e49\u7b56\u7565\uff0c\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u5e94\u5bf9\u591a\u6837\u5316\u60c5\u611f\u95ee\u9898\u573a\u666f\u7684\u65b9\u6846\u3002", "method": "\u63d0\u51faRLFF-ESC\u7a81\u7834\u6846\u67b6\uff1a1\uff09\u4f7f\u7528LLM\u591a\u6ee1\u4f53\u673a\u5236\u6a21\u62df\u672a\u6765\u5bf9\u8bdd\u8f68\u8ff9\u548c\u6536\u96c6\u671f\u671b\u5956\u52b1\uff1b2\uff09\u8bad\u7ec3\u671f\u671b\u5956\u52b1\u6a21\u578b\u5e76\u7528\u4e8e\u8bad\u7ec3\u60c5\u611f\u652f\u6301\u7b56\u7565\u6a21\u578b\uff1b3\uff09\u5728\u54cd\u5e94\u751f\u6210\u4e2d\u6dfb\u52a0\u663e\u5f0f\u63a8\u7406\u8fc7\u7a0b\u4ee5\u63d0\u5347\u54cd\u5e94\u8d28\u91cf\u3002", "result": "\u5728Qwen2.5-7B-Instruct-1M\u548cLLaMA3.1-8B-Instruct\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u5728\u4e24\u4e2a\u516c\u5f00ESC\u6570\u636e\u96c6\u4e0a\uff0cRLFF-ESC\u5728\u76ee\u6807\u5b8c\u6210\u548c\u54cd\u5e94\u8d28\u91cf\u65b9\u9762\u5747\u8f83\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "RLFF-ESC\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u671f\u671b\u5956\u52b1\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u5b66\u4e60\u957f\u671f\u60c5\u611f\u652f\u6301\u6280\u80fd\uff0c\u4e3a\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6846\u3002"}}
{"id": "2508.12216", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12216", "abs": "https://arxiv.org/abs/2508.12216", "authors": ["Butian Xiong", "Rong Liu", "Kenneth Xu", "Meida Chen", "Andrew Feng"], "title": "Splat Feature Solver", "comment": "webpage not that stable", "summary": "Feature lifting has emerged as a crucial component in 3D scene understanding,\nenabling the attachment of rich image feature descriptors (e.g., DINO, CLIP)\nonto splat-based 3D representations. The core challenge lies in optimally\nassigning rich general attributes to 3D primitives while addressing the\ninconsistency issues from multi-view images. We present a unified, kernel- and\nfeature-agnostic formulation of the feature lifting problem as a sparse linear\ninverse problem, which can be solved efficiently in closed form. Our approach\nadmits a provable upper bound on the global optimal error under convex losses\nfor delivering high quality lifted features. To address inconsistencies and\nnoise in multi-view observations, we introduce two complementary regularization\nstrategies to stabilize the solution and enhance semantic fidelity. Tikhonov\nGuidance enforces numerical stability through soft diagonal dominance, while\nPost-Lifting Aggregation filters noisy inputs via feature clustering. Extensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\non open-vocabulary 3D segmentation benchmarks, outperforming training-based,\ngrouping-based, and heuristic-forward baselines while producing the lifted\nfeatures in minutes. Code is available at\n\\href{https://github.com/saliteta/splat-distiller.git}{\\textbf{github}}. We\nalso have a \\href{https://splat-distiller.pages.dev/}", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7279\u5f81\u63d0\u5347\u65b9\u6cd5\uff0c\u5c06\u591a\u89c6\u89d2\u56fe\u50cf\u7279\u5f81\uff08\u5982DINO\u3001CLIP\uff09\u9644\u52a0\u5230\u57fa\u4e8esplat\u76843D\u8868\u793a\u4e0a\uff0c\u901a\u8fc7\u7a00\u758f\u7ebf\u6027\u9006\u95ee\u9898\u6c42\u89e3\uff0c\u5728\u5f00\u653e\u8bcd\u6c473D\u5206\u5272\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b33D\u573a\u666f\u7406\u89e3\u4e2d\u7279\u5f81\u63d0\u5347\u7684\u6838\u5fc3\u6311\u6218\uff1a\u5982\u4f55\u6700\u4f18\u5730\u5c06\u4e30\u5bcc\u7684\u901a\u7528\u56fe\u50cf\u7279\u5f81\u5206\u914d\u7ed93D\u57fa\u5143\uff0c\u540c\u65f6\u5904\u7406\u591a\u89c6\u89d2\u56fe\u50cf\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u5c06\u7279\u5f81\u63d0\u5347\u95ee\u9898\u8868\u8ff0\u4e3a\u7a00\u758f\u7ebf\u6027\u9006\u95ee\u9898\uff0c\u53ef\u95ed\u5f0f\u6c42\u89e3\uff1b\u5f15\u5165Tikhonov\u6b63\u5219\u5316\u786e\u4fdd\u6570\u503c\u7a33\u5b9a\u6027\uff0c\u540e\u63d0\u5347\u805a\u5408\u901a\u8fc7\u7279\u5f81\u805a\u7c7b\u8fc7\u6ee4\u566a\u58f0\u8f93\u5165\u3002", "result": "\u5728\u5f00\u653e\u8bcd\u6c473D\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4f18\u4e8e\u57fa\u4e8e\u8bad\u7ec3\u3001\u5206\u7ec4\u548c\u542f\u53d1\u5f0f\u7684\u524d\u6cbf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u80fd\u5728\u51e0\u5206\u949f\u5185\u751f\u6210\u63d0\u5347\u7279\u5f81\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u7279\u5f81\u63d0\u5347\u6846\u67b6\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e3a3D\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u7279\u5f81\u8868\u793a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12943", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12943", "abs": "https://arxiv.org/abs/2508.12943", "authors": ["Mary Tonwe"], "title": "OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities", "comment": "Source code and data available at:\n  https://github.com/marytonwe/OPTIC-ER.git", "summary": "Public service systems in many African regions suffer from delayed emergency\nresponse and spatial inequity, causing avoidable suffering. This paper\nintroduces OPTIC-ER, a reinforcement learning (RL) framework for real-time,\nadaptive, and equitable emergency response. OPTIC-ER uses an attention-guided\nactor-critic architecture to manage the complexity of dispatch environments.\nIts key innovations are a Context-Rich State Vector, encoding action\nsub-optimality, and a Precision Reward Function, which penalizes inefficiency.\nTraining occurs in a high-fidelity simulation using real data from Rivers\nState, Nigeria, accelerated by a precomputed Travel Time Atlas. The system is\nbuilt on the TALS framework (Thin computing, Adaptability, Low-cost,\nScalability) for deployment in low-resource settings. In evaluations on 500\nunseen incidents, OPTIC-ER achieved a 100.00% optimality rate with negligible\ninefficiency, confirming its robustness and generalization. Beyond dispatch,\nthe system generates Infrastructure Deficiency Maps and Equity Monitoring\nDashboards to guide proactive governance and data-informed development. This\nwork presents a validated blueprint for AI-augmented public services, showing\nhow context-aware RL can bridge the gap between algorithmic decision-making and\nmeasurable human impact.", "AI": {"tldr": "OPTIC-ER\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7d27\u6025\u54cd\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684actor-critic\u67b6\u6784\u5b9e\u73b0\u5b9e\u65f6\u3001\u81ea\u9002\u5e94\u548c\u516c\u5e73\u7684\u5e94\u6025\u8c03\u5ea6\uff0c\u5728\u5c3c\u65e5\u5229\u4e9a\u6cb3\u6d41\u5dde\u771f\u5b9e\u6570\u636e\u6d4b\u8bd5\u4e2d\u8fbe\u5230100%\u6700\u4f18\u7387\u3002", "motivation": "\u975e\u6d32\u5730\u533a\u516c\u5171\u670d\u52a1\u7cfb\u7edf\u5b58\u5728\u5e94\u6025\u54cd\u5e94\u5ef6\u8fdf\u548c\u7a7a\u95f4\u4e0d\u5e73\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u53ef\u907f\u514d\u7684\u82e6\u96be\uff0c\u9700\u8981\u5f00\u53d1\u9002\u5e94\u4f4e\u8d44\u6e90\u73af\u5883\u7684\u667a\u80fd\u8c03\u5ea6\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u6ce8\u610f\u529b\u5f15\u5bfc\u7684actor-critic\u67b6\u6784\uff0c\u5305\u542b\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u72b6\u6001\u5411\u91cf\u548c\u7cbe\u786e\u5956\u52b1\u51fd\u6570\uff0c\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u4e2d\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\uff0c\u57fa\u4e8eTALS\u6846\u67b6\uff08\u8584\u8ba1\u7b97\u3001\u9002\u5e94\u6027\u3001\u4f4e\u6210\u672c\u3001\u53ef\u6269\u5c55\u6027\uff09\u90e8\u7f72\u3002", "result": "\u5728500\u4e2a\u672a\u89c1\u4e8b\u6545\u8bc4\u4f30\u4e2d\uff0cOPTIC-ER\u8fbe\u5230100%\u6700\u4f18\u7387\uff0c\u6548\u7387\u635f\u5931\u53ef\u5ffd\u7565\uff0c\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aAI\u589e\u5f3a\u7684\u516c\u5171\u670d\u52a1\u63d0\u4f9b\u4e86\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u84dd\u56fe\uff0c\u5c55\u793a\u4e86\u60c5\u5883\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u5982\u4f55\u5f25\u5408\u7b97\u6cd5\u51b3\u7b56\u4e0e\u53ef\u8861\u91cf\u4eba\u7c7b\u5f71\u54cd\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.12219", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12219", "abs": "https://arxiv.org/abs/2508.12219", "authors": ["Kaiyuan Wang", "Jixing Liu", "Xiaobo Cai"], "title": "C2PSA-Enhanced YOLOv11 Architecture: A Novel Approach for Small Target Detection in Cotton Disease Diagnosis", "comment": null, "summary": "This study presents a deep learning-based optimization of YOLOv11 for cotton\ndisease detection, developing an intelligent monitoring system. Three key\nchallenges are addressed: (1) low precision in early spot detection (35%\nleakage rate for sub-5mm2 spots), (2) performance degradation in field\nconditions (25% accuracy drop), and (3) high error rates (34.7%) in\nmulti-disease scenarios. The proposed solutions include: C2PSA module for\nenhanced small-target feature extraction; Dynamic category weighting to handle\nsample imbalance; Improved data augmentation via Mosaic-MixUp scaling.\nExperimental results on a 4,078-image dataset show: mAP50: 0.820 (+8.0%\nimprovement); mAP50-95: 0.705 (+10.5% improvement); Inference speed: 158 FPS.\nThe mobile-deployed system enables real-time disease monitoring and precision\ntreatment in agricultural applications.", "AI": {"tldr": "\u57fa\u4e8eYOLOv11\u7684\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u65b9\u6848\uff0c\u901a\u8fc7C2PSA\u6a21\u5757\u3001\u52a8\u6001\u7c7b\u522b\u6743\u91cd\u548c\u6539\u8fdb\u6570\u636e\u589e\u5e3d\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u68c9\u82ac\u75c5\u5bb3\u68c0\u6d4b\u7cbe\u5ea6\u548c\u901f\u5ea6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u79cd\u690d\u4e1a\u5b9e\u65f6\u76d1\u6d4b\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u68c9\u82ac\u75c5\u5bb3\u68c0\u6d4b\u4e2d\u7684\u4e09\u5927\u6311\u6218\uff1a\u65e9\u671f\u75c5\u6591\u68c0\u6d4b\u7cbe\u5ea6\u4f4e\uff08\u5c0f\u4e8e5mm\u00b2\u75c5\u6591\u6f0f\u68c0\u738735%\uff09\uff0c\u7530\u95f4\u73af\u5883\u4e0b\u6027\u80fd\u4e0b\u964d\uff08\u51c6\u786e\u7387\u4e0b\u964d25%\uff09\uff0c\u4ee5\u53ca\u591a\u75c5\u5bb3\u573a\u666f\u9519\u8bef\u7387\u9ad8\uff0834.7%\uff09\u3002", "method": "\u63d0\u51faC2PSA\u6a21\u5757\u4f18\u5316\u5c0f\u76ee\u6807\u7279\u5f81\u63d0\u53d6\uff0c\u91c7\u7528\u52a8\u6001\u7c7b\u522b\u6743\u91cd\u5904\u7406\u6837\u672c\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7Mosaic-MixUp\u7f29\u653e\u6280\u672f\u6539\u8fdb\u6570\u636e\u589e\u5e3d\u3002", "result": "\u57284078\u5f20\u56fe\u7247\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff1amAP50\u8fbe\u52300.820\uff08\u63d0\u53478.0%\uff09\uff0cmAP50-95\u4e3a0.705\uff08\u63d0\u534710.5%\uff09\uff0c\u63a8\u7406\u901f\u5ea6158FPS\u3002\u7cfb\u7edf\u5df2\u90e8\u7f72\u5230\u79fb\u52a8\u8bbe\u5907\u5b9e\u73b0\u5b9e\u65f6\u76d1\u6d4b\u3002", "conclusion": "\u8be5\u4f18\u5316\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86\u68c9\u82ac\u75c5\u5bb3\u68c0\u6d4b\u7684\u5173\u952e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u901f\u5ea6\uff0c\u4e3a\u519c\u4e1a\u667a\u80fd\u76d1\u6d4b\u548c\u7cbe\u51c6\u6cbb\u7597\u63d0\u4f9b\u4e86\u53ef\u9760\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2508.13003", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13003", "abs": "https://arxiv.org/abs/2508.13003", "authors": ["Shengbo Wang", "Mingwei Liu", "Zike Li", "Anji Li", "Yanlin Wang", "Xin Peng", "Zibin Zheng"], "title": "EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning via Evolutionary Testing", "comment": null, "summary": "The rapid advancement of LLMs poses a significant challenge to existing\nmathematical reasoning benchmarks. These benchmarks commonly suffer from issues\nsuch as score saturation, temporal decay, and data contamination. To address\nthis challenge, this paper introduces EvolMathEval, an automated mathematical\nbenchmark generation and evolution framework based on evolutionary testing. By\ndynamically generating unique evaluation instances ab initio, the framework\nfundamentally eliminates the risk of data contamination, and ensuring the\nbenchmark remains perpetually challenging for future models.The core mechanisms\nof EvolMathEval include: seed problem generation based on reverse engineering\nwith algebraic guarantees; multi-dimensional genetic operators designed to\ninject diverse cognitive challenges; and a composite fitness function that can\nrapidly and accurately assess problem difficulty. Experimental results\ndemonstrate that the proposed composite fitness function can efficiently and\nprecisely quantify the difficulty of mathematical problems. Furthermore,\nEvolMathEval can not only generate a large volume of high-difficulty problems\nthrough continuous self-iteration, but it can also significantly enhance the\ncomplexity of public datasets like GSM8K through evolution, reducing model\naccuracy by an average of 48%. Deeper investigation reveals that when solving\nthese evolved, complex problems, LLMs tend to employ non-rigorous heuristics to\nbypass complex multi-step logical reasoning, consequently leading to incorrect\nsolutions. We define this phenomenon as \"Pseudo Aha Moment\". This finding\nuncovers a cognitive shortcut-taking behavior in the deep reasoning processes\nof current LLMs, which we find accounts for 77% to 100% of errors on targeted\nproblems. Code and resources are available\nat:https://github.com/SYSUSELab/EvolMathEval.", "AI": {"tldr": "EvolMathEval\u662f\u4e00\u4e2a\u57fa\u4e8e\u8fdb\u5316\u6d4b\u8bd5\u7684\u81ea\u52a8\u5316\u6570\u5b66\u57fa\u51c6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u72ec\u7279\u8bc4\u4f30\u5b9e\u4f8b\u6765\u89e3\u51b3\u73b0\u6709\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u7684\u5206\u6570\u9971\u548c\u3001\u65f6\u95f4\u8870\u51cf\u548c\u6570\u636e\u6c61\u67d3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u9762\u4e34\u5206\u6570\u9971\u548c\u3001\u65f6\u95f4\u8870\u51cf\u548c\u6570\u636e\u6c61\u67d3\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6301\u7eed\u4fdd\u6301\u6311\u6218\u6027\u7684\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u8fdb\u5316\u6d4b\u8bd5\u7684\u6846\u67b6\uff0c\u5305\u62ec\uff1a\u57fa\u4e8e\u9006\u5411\u5de5\u7a0b\u7684\u79cd\u5b50\u95ee\u9898\u751f\u6210\u3001\u591a\u7ef4\u9057\u4f20\u7b97\u5b50\u6ce8\u5165\u8ba4\u77e5\u6311\u6218\u3001\u590d\u5408\u9002\u5e94\u5ea6\u51fd\u6570\u8bc4\u4f30\u95ee\u9898\u96be\u5ea6\u3002", "result": "\u590d\u5408\u9002\u5e94\u5ea6\u51fd\u6570\u80fd\u9ad8\u6548\u7cbe\u786e\u91cf\u5316\u95ee\u9898\u96be\u5ea6\uff0c\u53ef\u751f\u6210\u5927\u91cf\u9ad8\u96be\u5ea6\u95ee\u9898\uff0c\u5c06GSM8K\u7b49\u516c\u5f00\u6570\u636e\u96c6\u7684\u6a21\u578b\u51c6\u786e\u7387\u5e73\u5747\u964d\u4f4e48%\uff0c\u53d1\u73b0LLMs\u5728\u89e3\u51b3\u590d\u6742\u95ee\u9898\u65f6\u503e\u5411\u4e8e\u4f7f\u7528\u975e\u4e25\u8c28\u542f\u53d1\u5f0f\u65b9\u6cd5\uff08\u4f2a\u987f\u609f\u65f6\u523b\uff09\u3002", "conclusion": "EvolMathEval\u6709\u6548\u89e3\u51b3\u4e86\u6570\u5b66\u57fa\u51c6\u7684\u6301\u7eed\u6311\u6218\u6027\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5728\u6df1\u5ea6\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u8ba4\u77e5\u8d70\u6377\u5f84\u884c\u4e3a\uff0c77%-100%\u7684\u9519\u8bef\u6e90\u4e8e\u8fd9\u79cd\u4f2a\u987f\u609f\u73b0\u8c61\u3002"}}
{"id": "2508.12226", "categories": ["cs.CV", "65N21, 92C55, 68T07"], "pdf": "https://arxiv.org/pdf/2508.12226", "abs": "https://arxiv.org/abs/2508.12226", "authors": ["Zhijun Zeng", "Youjia Zheng", "Chang Su", "Qianhang Wu", "Hao Hu", "Zeyuan Dong", "Shan Gao", "Yang Lv", "Rui Tang", "Ligang Cui", "Zhiyong Hou", "Weijun Lin", "Zuoqiang Shi", "Yubing Li", "He Sun"], "title": "In vivo 3D ultrasound computed tomography of musculoskeletal tissues with generative neural physics", "comment": null, "summary": "Ultrasound computed tomography (USCT) is a radiation-free, high-resolution\nmodality but remains limited for musculoskeletal imaging due to conventional\nray-based reconstructions that neglect strong scattering. We propose a\ngenerative neural physics framework that couples generative networks with\nphysics-informed neural simulation for fast, high-fidelity 3D USCT. By learning\na compact surrogate of ultrasonic wave propagation from only dozens of\ncross-modality images, our method merges the accuracy of wave modeling with the\nefficiency and stability of deep learning. This enables accurate quantitative\nimaging of in vivo musculoskeletal tissues, producing spatial maps of acoustic\nproperties beyond reflection-mode images. On synthetic and in vivo data\n(breast, arm, leg), we reconstruct 3D maps of tissue parameters in under ten\nminutes, with sensitivity to biomechanical properties in muscle and bone and\nresolution comparable to MRI. By overcoming computational bottlenecks in\nstrongly scattering regimes, this approach advances USCT toward routine\nclinical assessment of musculoskeletal disease.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u7f51\u7edc\u548c\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u6a21\u62df\u7684\u751f\u6210\u5f0f\u795e\u7ecf\u7269\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901f\u3001\u9ad8\u4fdd\u771f\u76843D\u8d85\u58f0\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5c04\u7ebf\u91cd\u5efa\u65b9\u6cd5\u5728\u5f3a\u6563\u5c04\u73af\u5883\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u8d85\u58f0\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08USCT\uff09\u662f\u4e00\u79cd\u65e0\u8f90\u5c04\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u6210\u50cf\u65b9\u5f0f\uff0c\u4f46\u5728\u808c\u8089\u9aa8\u9abc\u6210\u50cf\u4e2d\u53d7\u5230\u9650\u5236\uff0c\u56e0\u4e3a\u4f20\u7edf\u7684\u57fa\u4e8e\u5c04\u7ebf\u7684\u91cd\u5efa\u65b9\u6cd5\u5ffd\u7565\u4e86\u5f3a\u6563\u5c04\u6548\u5e94\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u751f\u6210\u5f0f\u795e\u7ecf\u7269\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u4ec5\u51e0\u5341\u5f20\u8de8\u6a21\u6001\u56fe\u50cf\u4e2d\u5b66\u4e60\u8d85\u58f0\u6ce2\u4f20\u64ad\u7684\u7d27\u51d1\u4ee3\u7406\u6a21\u578b\uff0c\u5c06\u6ce2\u52a8\u5efa\u6a21\u7684\u51c6\u786e\u6027\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\u76f8\u7ed3\u5408\u3002", "result": "\u5728\u5408\u6210\u548c\u4f53\u5185\u6570\u636e\uff08\u4e73\u623f\u3001\u624b\u81c2\u3001\u817f\u90e8\uff09\u4e0a\uff0c\u5728\u5341\u5206\u949f\u5185\u91cd\u5efa\u4e86\u7ec4\u7ec7\u53c2\u6570\u76843D\u56fe\u8c31\uff0c\u5bf9\u808c\u8089\u548c\u9aa8\u9abc\u7684\u751f\u7269\u529b\u5b66\u7279\u6027\u5177\u6709\u654f\u611f\u6027\uff0c\u5206\u8fa8\u7387\u4e0eMRI\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u514b\u670d\u5f3a\u6563\u5c04\u72b6\u6001\u4e0b\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u8be5\u65b9\u6cd5\u5c06USCT\u63a8\u5411\u808c\u8089\u9aa8\u9abc\u75be\u75c5\u5e38\u89c4\u4e34\u5e8a\u8bc4\u4f30\u7684\u5e94\u7528\u3002"}}
{"id": "2508.13020", "categories": ["cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.13020", "abs": "https://arxiv.org/abs/2508.13020", "authors": ["Jiaqi Yin", "Zhan Song", "Chen Chen", "Yaohui Cai", "Zhiru Zhang", "Cunxi Yu"], "title": "e-boost: Boosted E-Graph Extraction with Adaptive Heuristics and Exact Solving", "comment": null, "summary": "E-graphs have attracted growing interest in many fields, particularly in\nlogic synthesis and formal verification. E-graph extraction is a challenging\nNP-hard combinatorial optimization problem. It requires identifying optimal\nterms from exponentially many equivalent expressions, serving as the primary\nperformance bottleneck in e-graph based optimization tasks. However,\ntraditional extraction methods face a critical trade-off: heuristic approaches\noffer speed but sacrifice optimality, while exact methods provide optimal\nsolutions but face prohibitive computational costs on practical problems. We\npresent e-boost, a novel framework that bridges this gap through three key\ninnovations: (1) parallelized heuristic extraction that leverages weak data\ndependence to compute DAG costs concurrently, enabling efficient multi-threaded\nperformance without sacrificing extraction quality; (2) adaptive search space\npruning that employs a parameterized threshold mechanism to retain only\npromising candidates, dramatically reducing the solution space while preserving\nnear-optimal solutions; and (3) initialized exact solving that formulates the\nreduced problem as an Integer Linear Program with warm-start capabilities,\nguiding solvers toward high-quality solutions faster.\n  Across the diverse benchmarks in formal verification and logic synthesis\nfields, e-boost demonstrates 558x runtime speedup over traditional exact\napproaches (ILP) and 19.04% performance improvement over the state-of-the-art\nextraction framework (SmoothE). In realistic logic synthesis tasks, e-boost\nproduces 7.6% and 8.1% area improvements compared to conventional synthesis\ntools with two different technology mapping libraries. e-boost is available at\nhttps://github.com/Yu-Maryland/e-boost.", "AI": {"tldr": "e-boost\u662f\u4e00\u4e2a\u65b0\u9896\u7684e-graph\u63d0\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u542f\u53d1\u5f0f\u63d0\u53d6\u3001\u81ea\u9002\u5e94\u641c\u7d22\u7a7a\u95f4\u526a\u679d\u548c\u521d\u59cb\u5316\u7cbe\u786e\u6c42\u89e3\u4e09\u9879\u521b\u65b0\u6280\u672f\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u89e3\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edfe-graph\u63d0\u53d6\u65b9\u6cd5\u9762\u4e34\u5173\u952e\u6743\u8861\uff1a\u542f\u53d1\u5f0f\u65b9\u6cd5\u901f\u5ea6\u5feb\u4f46\u727a\u7272\u6700\u4f18\u6027\uff0c\u7cbe\u786e\u65b9\u6cd5\u63d0\u4f9b\u6700\u4f18\u89e3\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u517c\u987e\u6548\u7387\u548c\u6700\u4f18\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u5e76\u884c\u5316\u542f\u53d1\u5f0f\u63d0\u53d6\uff1a\u5229\u7528\u5f31\u6570\u636e\u4f9d\u8d56\u6027\u5e76\u884c\u8ba1\u7b97DAG\u6210\u672c\uff1b2) \u81ea\u9002\u5e94\u641c\u7d22\u7a7a\u95f4\u526a\u679d\uff1a\u4f7f\u7528\u53c2\u6570\u5316\u9608\u503c\u673a\u5236\u4fdd\u7559\u6709\u5e0c\u671b\u7684\u5019\u9009\u89e3\uff1b3) \u521d\u59cb\u5316\u7cbe\u786e\u6c42\u89e3\uff1a\u5c06\u7b80\u5316\u95ee\u9898\u5efa\u6a21\u4e3a\u5177\u6709\u70ed\u542f\u52a8\u80fd\u529b\u7684\u6574\u6570\u7ebf\u6027\u89c4\u5212\u95ee\u9898\u3002", "result": "\u5728\u5f62\u5f0f\u9a8c\u8bc1\u548c\u903b\u8f91\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0ce-boost\u76f8\u6bd4\u4f20\u7edf\u7cbe\u786e\u65b9\u6cd5(ILP)\u5b9e\u73b0558\u500d\u52a0\u901f\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u63d0\u53d6\u6846\u67b6(SmoothE)\u6027\u80fd\u63d0\u534719.04%\u3002\u5728\u5b9e\u9645\u903b\u8f91\u5408\u6210\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u4f20\u7edf\u5408\u6210\u5de5\u5177\u5206\u522b\u5b9e\u73b07.6%\u548c8.1%\u7684\u9762\u79ef\u4f18\u5316\u3002", "conclusion": "e-boost\u6210\u529f\u89e3\u51b3\u4e86e-graph\u63d0\u53d6\u4e2d\u7684\u6548\u7387-\u6700\u4f18\u6027\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6df7\u5408\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u89e3\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3ae-graph\u4f18\u5316\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12250", "abs": "https://arxiv.org/abs/2508.12250", "authors": ["Quan Chen", "Xiong Yang", "Rongfeng Lu", "Qianyu Zhang", "Yu Liu", "Xiaofei Zhou", "Bolun Zheng"], "title": "WXSOD: A Benchmark for Robust Salient Object Detection in Adverse Weather Conditions", "comment": "Under review", "summary": "Salient object detection (SOD) in complex environments remains a challenging\nresearch topic. Most existing methods perform well in natural scenes with\nnegligible noise, and tend to leverage multi-modal information (e.g., depth and\ninfrared) to enhance accuracy. However, few studies are concerned with the\ndamage of weather noise on SOD performance due to the lack of dataset with\npixel-wise annotations. To bridge this gap, this paper introduces a novel\nWeather-eXtended Salient Object Detection (WXSOD) dataset. It consists of\n14,945 RGB images with diverse weather noise, along with the corresponding\nground truth annotations and weather labels. To verify algorithm\ngeneralization, WXSOD contains two test sets, i.e., a synthesized test set and\na real test set. The former is generated by adding weather noise to clean\nimages, while the latter contains real-world weather noise. Based on WXSOD, we\npropose an efficient baseline, termed Weather-aware Feature Aggregation Network\n(WFANet), which adopts a fully supervised two-branch architecture.\nSpecifically, the weather prediction branch mines weather-related deep\nfeatures, while the saliency detection branch fuses semantic features extracted\nfrom the backbone with weather features for SOD. Comprehensive comparisons\nagainst 17 SOD methods shows that our WFANet achieves superior performance on\nWXSOD. The code and benchmark results will be made publicly available at\nhttps://github.com/C-water/WXSOD", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5929\u6c14\u566a\u58f0\u6269\u5c55\u663e\u8457\u7269\u4f53\u68c0\u6d4b\u6570\u636e\u96c6WXSOD\u548c\u4e00\u79cd\u5929\u6c14\u611f\u77e5\u7279\u5f81\u805a\u5408\u7f51\u7edcWFANet\uff0c\u4ee5\u89e3\u51b3\u590d\u6742\u5929\u6c14\u6761\u4ef6\u4e0b\u663e\u8457\u7269\u4f53\u68c0\u6d4b\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u663e\u8457\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\u5728\u6709\u5929\u6c14\u566a\u58f0\u7684\u73af\u5883\u4e2d\u6027\u80fd\u4f1a\u4e25\u91cd\u964d\u4f4e\uff0c\u4f46\u7f3a\u4e4f\u6709\u76f8\u5173\u6570\u636e\u96c6\u6765\u8fdb\u884c\u7814\u7a76\u3002\u8fd9\u7bc7\u8bba\u6587\u7684\u52a8\u673a\u662f\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e00\u4e2a\u5305\u542b\u591a\u79cd\u5929\u6c14\u566a\u58f0\u7684\u6807\u6ce8\u6570\u636e\u96c6\u5e76\u63d0\u51fa\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u5929\u6c14\u611f\u77e5\u7279\u5f81\u805a\u5408\u7f51\u7edc(WFANet)\uff0c\u91c7\u7528\u53cc\u5206\u652f\u7ed3\u6784\u3002\u4e00\u4e2a\u5206\u652f\u8d1f\u8d23\u9884\u6d4b\u5929\u6c14\u6761\u4ef6\u5e76\u6316\u6398\u5929\u6c14\u76f8\u5173\u7279\u5f81\uff0c\u53e6\u4e00\u4e2a\u5206\u652f\u5219\u5c06\u4e3b\u5e72\u7f51\u7edc\u63d0\u53d6\u7684\u8bed\u4e49\u7279\u5f81\u4e0e\u5929\u6c14\u7279\u5f81\u878d\u5408\u8fdb\u884c\u663e\u8457\u7269\u4f53\u68c0\u6d4b\u3002", "result": "\u5728\u65b0\u63d0\u51fa\u7684WXSOD\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e17\u79cd\u73b0\u6709\u663e\u8457\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\u8fdb\u884c\u4e86\u7efc\u5408\u6bd4\u8f83\uff0c\u7ed3\u679c\u663e\u793aWFANet\u80fd\u591f\u83b7\u5f97\u66f4\u4f18\u5f02\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u590d\u6742\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u663e\u8457\u7269\u4f53\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u6570\u636e\u96c6\u57fa\u51c6\u548c\u6709\u6548\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u793a\u4e86\u8003\u8651\u5929\u6c14\u56e0\u7d20\u5bf9\u4e8e\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.13021", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.13021", "abs": "https://arxiv.org/abs/2508.13021", "authors": ["Pengcheng Huang", "Shuhao Liu", "Zhenghao Liu", "Yukun Yan", "Shuo Wang", "Zulong Chen", "Tong Xiao"], "title": "PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models", "comment": "17 pages,13 figures", "summary": "Recent advances in masked diffusion models (MDMs) have established them as\npowerful non-autoregressive alternatives for sequence generation. Nevertheless,\nour preliminary experiments reveal that the generation quality of MDMs is still\nhighly sensitive to the choice of decoding strategy. In particular, widely\nadopted uncertainty-based samplers suffer from two key limitations: a lack of\nglobal trajectory control and a pronounced bias toward trivial tokens in the\nearly stages of decoding. These shortcomings restrict the full potential of\nMDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling\n(PC-Sampler), a novel decoding strategy that unifies global trajectory planning\nwith content-aware informativeness maximization. PC-Sampler incorporates a\nposition-aware weighting mechanism to regulate the decoding path and a\ncalibrated confidence score to suppress the premature selection of trivial\ntokens. Extensive experiments on three advanced MDMs across seven challenging\nbenchmarks-including logical reasoning and planning tasks-demonstrate that\nPC-Sampler consistently outperforms existing MDM decoding strategies by more\nthan 10% on average, significantly narrowing the performance gap with\nstate-of-the-art autoregressive models. All codes are available at\nhttps://github.com/NEUIR/PC-Sampler.", "AI": {"tldr": "PC-Sampler\u662f\u4e00\u79cd\u65b0\u7684\u63a9\u7801\u6269\u6563\u6a21\u578b\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u4f4d\u7f6e\u611f\u77e5\u6743\u91cd\u673a\u5236\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91c7\u6837\u5668\u7f3a\u4e4f\u5168\u5c40\u8f68\u8ff9\u63a7\u5236\u548c\u504f\u5411\u5e73\u51e1\u8bcd\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u534710%\u4ee5\u4e0a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u63a9\u7801\u6269\u6563\u6a21\u578b(MDMs)\u7684\u89e3\u7801\u7b56\u7565\u5bf9\u751f\u6210\u8d28\u91cf\u9ad8\u5ea6\u654f\u611f\uff0c\u4e0d\u786e\u5b9a\u6027\u91c7\u6837\u5668\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u7f3a\u4e4f\u5168\u5c40\u8f68\u8ff9\u63a7\u5236\u548c\u89e3\u7801\u65e9\u671f\u504f\u5411\u9009\u62e9\u5e73\u51e1\u8bcd\uff0c\u9650\u5236\u4e86MDMs\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4f4d\u7f6e\u611f\u77e5\u7f6e\u4fe1\u5ea6\u6821\u51c6\u91c7\u6837(PC-Sampler)\uff0c\u7edf\u4e00\u4e86\u5168\u5c40\u8f68\u8ff9\u89c4\u5212\u548c\u5185\u5bb9\u611f\u77e5\u4fe1\u606f\u6700\u5927\u5316\uff0c\u5305\u542b\u4f4d\u7f6e\u611f\u77e5\u6743\u91cd\u673a\u5236\u6765\u8c03\u8282\u89e3\u7801\u8def\u5f84\uff0c\u4ee5\u53ca\u6821\u51c6\u7f6e\u4fe1\u5ea6\u5206\u6570\u6765\u6291\u5236\u8fc7\u65e9\u9009\u62e9\u5e73\u51e1\u8bcd\u3002", "result": "\u5728\u4e09\u4e2a\u5148\u8fdbMDMs\u548c\u4e03\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u903b\u8f91\u63a8\u7406\u548c\u89c4\u5212\u4efb\u52a1\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPC-Sampler\u5e73\u5747\u6bd4\u73b0\u6709MDM\u89e3\u7801\u7b56\u7565\u6027\u80fd\u63d0\u534710%\u4ee5\u4e0a\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u6700\u5148\u8fdb\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "PC-Sampler\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u7801\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\uff0c\u5728\u591a\u4e2a\u590d\u6742\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2508.12261", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12261", "abs": "https://arxiv.org/abs/2508.12261", "authors": ["Zhizhou Wang", "Ruijing Zheng", "Zhenyu Wu", "Jianli Wang"], "title": "Superpixel-informed Continuous Low-Rank Tensor Representation for Multi-Dimensional Data Recovery", "comment": "Under review in AAAI2026", "summary": "Low-rank tensor representation (LRTR) has emerged as a powerful tool for\nmulti-dimensional data processing. However, classical LRTR-based methods face\ntwo critical limitations: (1) they typically assume that the holistic data is\nlow-rank, this assumption is often violated in real-world scenarios with\nsignificant spatial variations; and (2) they are constrained to discrete\nmeshgrid data, limiting their flexibility and applicability. To overcome these\nlimitations, we propose a Superpixel-informed Continuous low-rank Tensor\nRepresentation (SCTR) framework, which enables continuous and flexible modeling\nof multi-dimensional data beyond traditional grid-based constraints. Our\napproach introduces two main innovations: First, motivated by the observation\nthat semantically coherent regions exhibit stronger low-rank characteristics\nthan holistic data, we employ superpixels as the basic modeling units. This\ndesign not only encodes rich semantic information, but also enhances\nadaptability to diverse forms of data streams. Second, we propose a novel\nasymmetric low-rank tensor factorization (ALTF) where superpixel-specific\nfactor matrices are parameterized by a shared neural network with specialized\nheads. By strategically separating global pattern learning from local\nadaptation, this framework efficiently captures both cross-superpixel\ncommonalities and within-superpixel variations. This yields a representation\nthat is both highly expressive and compact, balancing model efficiency with\nadaptability. Extensive experiments on several benchmark datasets demonstrate\nthat SCTR achieves 3-5 dB PSNR improvements over existing LRTR-based methods\nacross multispectral images, videos, and color images.", "AI": {"tldr": "\u63d0\u51faSCTR\u6846\u67b6\uff0c\u901a\u8fc7\u8d85\u50cf\u7d20\u5206\u5272\u548c\u4e0d\u5bf9\u79f0\u4f4e\u79e9\u5f20\u91cf\u5206\u89e3\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4f4e\u79e9\u5f20\u91cf\u8868\u793a\u65b9\u6cd5\u5728\u7a7a\u95f4\u53d8\u5316\u5904\u7406\u548c\u79bb\u6563\u7f51\u683c\u6570\u636e\u9650\u5236\u65b9\u9762\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e863-5 dB\u7684PSNR\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u4f4e\u79e9\u5f20\u91cf\u8868\u793a\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a1\uff09\u5047\u8bbe\u6574\u4f53\u6570\u636e\u662f\u4f4e\u79e9\u7684\uff0c\u8fd9\u5728\u5177\u6709\u663e\u8457\u7a7a\u95f4\u53d8\u5316\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u5f80\u5f80\u4e0d\u6210\u7acb\uff1b2\uff09\u4ec5\u9650\u4e8e\u79bb\u6563\u7f51\u683c\u6570\u636e\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u63d0\u51fa\u8d85\u50cf\u7d20\u611f\u77e5\u7684\u8fde\u7eed\u4f4e\u79e9\u5f20\u91cf\u8868\u793a\uff08SCTR\uff09\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u8d85\u50cf\u7d20\u4f5c\u4e3a\u57fa\u672c\u5efa\u6a21\u5355\u5143\uff0c\u5229\u7528\u8bed\u4e49\u4e00\u81f4\u533a\u57df\u5177\u6709\u66f4\u5f3a\u4f4e\u79e9\u7279\u6027\u7684\u89c2\u5bdf\uff1b2\uff09\u63d0\u51fa\u4e0d\u5bf9\u79f0\u4f4e\u79e9\u5f20\u91cf\u5206\u89e3\uff08ALTF\uff09\uff0c\u901a\u8fc7\u5171\u4eab\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u8d85\u50cf\u7d20\u7279\u5b9a\u56e0\u5b50\u77e9\u9635\uff0c\u5206\u79bb\u5168\u5c40\u6a21\u5f0f\u5b66\u4e60\u548c\u5c40\u90e8\u9002\u5e94\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSCTR\u5728\u591a\u5149\u8c31\u56fe\u50cf\u3001\u89c6\u9891\u548c\u5f69\u8272\u56fe\u50cf\u4e0a\u76f8\u6bd4\u73b0\u6709LRTR\u65b9\u6cd5\u5b9e\u73b0\u4e863-5 dB\u7684PSNR\u6539\u8fdb\u3002", "conclusion": "SCTR\u6846\u67b6\u80fd\u591f\u8fde\u7eed\u7075\u6d3b\u5730\u5efa\u6a21\u591a\u7ef4\u6570\u636e\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u57fa\u4e8e\u7f51\u683c\u7684\u7ea6\u675f\uff0c\u5728\u8868\u8fbe\u80fd\u529b\u548c\u6a21\u578b\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2508.13023", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13023", "abs": "https://arxiv.org/abs/2508.13023", "authors": ["Yongxin Guo", "Wenbo Deng", "Zhenglin Cheng", "Xiaoying Tang"], "title": "G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhanced\nthe reasoning abilities of large language models (LLMs). Its success, however,\nlargely depends on strong base models with rich world knowledge, yielding only\nmodest improvements for small-size language models (SLMs). To address this\nlimitation, we investigate Guided GRPO, which injects ground-truth reasoning\nsteps into roll-out trajectories to compensate for SLMs' inherent weaknesses.\nThrough a comprehensive study of various guidance configurations, we find that\nnaively adding guidance delivers limited gains. These insights motivate\nG$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strength\nin response to the model's evolving training dynamics. Experiments on\nmathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-A\nsubstantially outperforms vanilla GRPO. Our code and models are available at\nhttps://github.com/T-Lab-CUHKSZ/G2RPO-A.", "AI": {"tldr": "\u57fa\u4e8eRLVR\u7684G^2RPO-A\u7b97\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6027\u5730\u6ce8\u5165\u771f\u5b9e\u63a8\u7406\u6b65\u9aa4\u6765\u63d0\u5347\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u8d85\u8fc7\u4f20\u7edfGRPO\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3RLVR\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578b(SLMs)\u4e0a\u6539\u5584\u6548\u679c\u504f\u5f31\u7684\u95ee\u9898\uff0c\u56e0\u4e3aSLMs\u7f3a\u4e4f\u5927\u578b\u6a21\u578b\u7684\u4e30\u5bcc\u4e16\u754c\u77e5\u8bc6\u3002", "method": "\u63d0\u51faG^2RPO-A\u7b97\u6cd5\uff0c\u5728\u6eda\u52a8\u8f68\u8ff9\u4e2d\u6ce8\u5165\u771f\u5b9e\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u6839\u636e\u6a21\u578b\u8bad\u7ec3\u52a8\u6001\u81ea\u9002\u5e94\u5730\u8c03\u6574\u6307\u5bfc\u5f3a\u5ea6\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u6307\u6807\u4e0a\uff0cG^2RPO-A\u663e\u8457\u8d85\u8fc7\u4e86\u666e\u901aGRPO\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u81ea\u9002\u5e94\u6027\u5730\u6ce8\u5165\u771f\u5b9e\u63a8\u7406\u6307\u5bfc\uff0c\u53ef\u4ee5\u6709\u6548\u8865\u507f\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728\u77ed\u677f\uff0c\u663e\u8457\u63d0\u5347\u5176\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2508.12263", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12263", "abs": "https://arxiv.org/abs/2508.12263", "authors": ["Hongliang Wei", "Xianqi Zhang", "Xingtao Wang", "Xiaopeng Fan", "Debin Zhao"], "title": "Region-Level Context-Aware Multimodal Understanding", "comment": "12 pages, 6 figures", "summary": "Despite significant progress, existing research on Multimodal Large Language\nModels (MLLMs) mainly focuses on general visual understanding, overlooking the\nability to integrate textual context associated with objects for a more\ncontext-aware multimodal understanding -- an ability we refer to as\nRegion-level Context-aware Multimodal Understanding (RCMU). To address this\nlimitation, we first formulate the RCMU task, which requires models to respond\nto user instructions by integrating both image content and textual information\nof regions or objects. To equip MLLMs with RCMU capabilities, we propose\nRegion-level Context-aware Visual Instruction Tuning (RCVIT), which\nincorporates object information into the model input and enables the model to\nutilize bounding box coordinates to effectively associate objects' visual\ncontent with their textual information. To address the lack of datasets, we\nintroduce the RCMU dataset, a large-scale visual instruction tuning dataset\nthat covers multiple RCMU tasks. We also propose RC\\&P-Bench, a comprehensive\nbenchmark that can evaluate the performance of MLLMs in RCMU and multimodal\npersonalized understanding tasks. Additionally, we propose a reference-free\nevaluation metric to perform a comprehensive and fine-grained evaluation of the\nregion-level context-aware image descriptions. By performing RCVIT on Qwen2-VL\nmodels with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental\nresults indicate that RC-Qwen2-VL models not only achieve outstanding\nperformance on multiple RCMU tasks but also demonstrate successful applications\nin multimodal RAG and personalized conversation. Our data, model and benchmark\nare available at https://github.com/hongliang-wei/RC-MLLM", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u533a\u57df\u7ea7\u4e0a\u4e0b\u6587\u611f\u77e5\u591a\u6a21\u6001\u7406\u89e3(RCMU)\u80fd\u529b\uff0c\u901a\u8fc7\u65b0\u7684\u6307\u4ee4\u5fae\u8c03\u65b9\u6cd5RCVIT\u548c\u6570\u636e\u96c6RCMU dataset\uff0c\u4f7fMLLM\u6a21\u578b\u80fd\u591f\u7ed3\u5408\u56fe\u50cf\u5185\u5bb9\u548c\u5bf9\u8c61\u6587\u672c\u4fe1\u606f\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u591a\u6a21\u6001\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u89c6\u89c9\u7406\u89e3\uff0c\u5ffd\u89c6\u4e86\u7ed3\u5408\u5bf9\u8c61\u76f8\u5173\u6587\u672c\u4e0a\u4e0b\u6587\u8fdb\u884c\u66f4\u6df1\u5165\u7406\u89e3\u7684\u80fd\u529b\uff0c\u5373\u533a\u57df\u7ea7\u4e0a\u4e0b\u6587\u611f\u77e5\u591a\u6a21\u6001\u7406\u89e3(RCMU)\u80fd\u529b\u3002", "method": "\u63d0\u51faRegion-level Context-aware Visual Instruction Tuning (RCVIT)\u65b9\u6cd5\uff0c\u5c06\u5bf9\u8c61\u4fe1\u606f\u6574\u5408\u5230\u6a21\u578b\u8f93\u5165\u4e2d\uff0c\u5229\u7528\u8fb9\u754c\u6846\u5750\u6807\u5173\u8054\u5bf9\u8c61\u7684\u89c6\u89c9\u5185\u5bb9\u548c\u6587\u672c\u4fe1\u606f\uff1b\u6784\u5efa\u5927\u89c4\u6a21RCMU\u6570\u636e\u96c6\u548cRC&P-Bench\u8bc4\u6d4b\u6807\u51c6\u3002", "result": "\u901a\u8fc7\u5728Qwen2-VL\u6a21\u578b\u4e0a\u8fdb\u884cRCVIT\u8bad\u7ec3\uff0c\u5f97\u5230\u4e86RC-Qwen2-VL\u6a21\u578b\uff0c\u5728\u591a\u4e2aRCMU\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8270\u51fa\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u591a\u6a21\u6001RAG\u548c\u4e2a\u6027\u5316\u5bf9\u8bdd\u3002", "conclusion": "\u672c\u6587\u4e3aMLLM\u6a21\u578b\u63d0\u4f9b\u4e86\u533a\u57df\u7ea7\u4e0a\u4e0b\u6587\u611f\u77e5\u591a\u6a21\u6001\u7406\u89e3\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5f00\u62d3\u4e86\u591a\u6a21\u6001\u7406\u89e3\u7684\u65b0\u65b9\u5411\uff0c\u4e3a\u66f4\u6df1\u5165\u7684\u89c6\u89c9-\u8bed\u8a00\u4ea4\u4e92\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.13072", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13072", "abs": "https://arxiv.org/abs/2508.13072", "authors": ["Yuting Zhang", "Tiantian Geng", "Luoying Hao", "Xinxing Cheng", "Alexander Thorley", "Xiaoxia Wang", "Wenqi Lu", "Sandeep S Hothi", "Lei Wei", "Zhaowen Qiu", "Dipak Kotecha", "Jinming Duan"], "title": "A Language-Signal-Vision Multimodal Framework for Multitask Cardiac Analysis", "comment": null, "summary": "Contemporary cardiovascular management involves complex consideration and\nintegration of multimodal cardiac datasets, where each modality provides\ndistinct but complementary physiological characteristics. While the effective\nintegration of multiple modalities could yield a holistic clinical profile that\naccurately models the true clinical situation with respect to data modalities\nand their relatives weightings, current methodologies remain limited by: 1) the\nscarcity of patient- and time-aligned multimodal data; 2) reliance on isolated\nsingle-modality or rigid multimodal input combinations; 3) alignment strategies\nthat prioritize cross-modal similarity over complementarity; and 4) a narrow\nsingle-task focus. In response to these limitations, a comprehensive multimodal\ndataset was curated for immediate application, integrating laboratory test\nresults, electrocardiograms, and echocardiograms with clinical outcomes.\nSubsequently, a unified framework, Textual Guidance Multimodal fusion for\nMultiple cardiac tasks (TGMM), was proposed. TGMM incorporated three key\ncomponents: 1) a MedFlexFusion module designed to capture the unique and\ncomplementary characteristics of medical modalities and dynamically integrate\ndata from diverse cardiac sources and their combinations; 2) a textual guidance\nmodule to derive task-relevant representations tailored to diverse clinical\nobjectives, including heart disease diagnosis, risk stratification and\ninformation retrieval; and 3) a response module to produce final decisions for\nall these tasks. Furthermore, this study systematically explored key features\nacross multiple modalities and elucidated their synergistic contributions in\nclinical decision-making. Extensive experiments showed that TGMM outperformed\nstate-of-the-art methods across multiple clinical tasks, with additional\nvalidation confirming its robustness on another public dataset.", "AI": {"tldr": "TGMM\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5fc3\u810f\u6570\u636e\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7MedFlexFusion\u6a21\u5757\u52a8\u6001\u6574\u5408\u5b9e\u9a8c\u5ba4\u68c0\u67e5\u3001\u5fc3\u7535\u56fe\u548c\u8d85\u58f0\u5fc3\u52a8\u56fe\u6570\u636e\uff0c\u4f7f\u7528\u6587\u672c\u5f15\u5bfc\u6a21\u5757\u9002\u5e94\u4e0d\u540c\u4e34\u5e8a\u4efb\u52a1\uff0c\u5728\u591a\u79cd\u5fc3\u810f\u75be\u75c5\u8bca\u65ad\u548c\u98ce\u9669\u8bc4\u4f30\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5fc3\u8840\u7ba1\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\u5b58\u5728\u56db\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u591a\u6a21\u6001\u6570\u636e\u7a00\u7f3a\u3001\u4f9d\u8d56\u5355\u4e00\u6216\u56fa\u5b9a\u7ec4\u5408\u6a21\u5f0f\u3001\u5bf9\u9f50\u7b56\u7565\u5ffd\u89c6\u4e92\u8865\u6027\u3001\u4ee5\u53ca\u5355\u4efb\u52a1\u5c40\u9650\u6027\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u52a8\u6001\u6574\u5408\u4e0d\u540c\u6a21\u6001\u5e76\u9002\u5e94\u591a\u79cd\u4e34\u5e8a\u4efb\u52a1\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51faTGMM\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) MedFlexFusion\u6a21\u5757\u6355\u83b7\u533b\u5b66\u6a21\u6001\u7684\u72ec\u7279\u4e92\u8865\u7279\u5f81\u5e76\u52a8\u6001\u6574\u5408\u6570\u636e\uff1b2) \u6587\u672c\u5f15\u5bfc\u6a21\u5757\u751f\u6210\u4efb\u52a1\u76f8\u5173\u8868\u793a\uff1b3) \u54cd\u5e94\u6a21\u5757\u4ea7\u751f\u6700\u7ec8\u51b3\u7b56\u3002\u7cfb\u7edf\u63a2\u7d22\u591a\u6a21\u6001\u7279\u5f81\u53ca\u5176\u534f\u540c\u4f5c\u7528\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eTGMM\u5728\u591a\u4e2a\u4e34\u5e8a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5728\u53e6\u4e00\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u3002", "conclusion": "TGMM\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5fc3\u810f\u6570\u636e\u5206\u6790\u7684\u73b0\u6709\u9650\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u878d\u5408\u548c\u6587\u672c\u5f15\u5bfc\u5b9e\u73b0\u4e86\u5bf9\u591a\u79cd\u4e34\u5e8a\u4efb\u52a1\u7684\u4f18\u5f02\u6027\u80fd\uff0c\u4e3a\u5fc3\u8840\u7ba1\u75be\u75c5\u7684\u591a\u6a21\u6001\u96c6\u6210\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12271", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12271", "abs": "https://arxiv.org/abs/2508.12271", "authors": ["Ronghua Xu", "Jin Xie", "Jing Nie", "Jiale Cao", "Yanwei Pang"], "title": "SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration", "comment": "11 pages", "summary": "Spiking Neural Networks (SNNs), characterized by discrete binary activations,\noffer high computational efficiency and low energy consumption, making them\nwell-suited for computation-intensive tasks such as stereo image restoration.\nIn this work, we propose SNNSIR, a simple yet effective Spiking Neural Network\nfor Stereo Image Restoration, specifically designed under the spike-driven\nparadigm where neurons transmit information through sparse, event-based binary\nspikes. In contrast to existing hybrid SNN-ANN models that still rely on\noperations such as floating-point matrix division or exponentiation, which are\nincompatible with the binary and event-driven nature of SNNs, our proposed\nSNNSIR adopts a fully spike-driven architecture to achieve low-power and\nhardware-friendly computation. To address the expressiveness limitations of\nbinary spiking neurons, we first introduce a lightweight Spike Residual Basic\nBlock (SRBB) to enhance information flow via spike-compatible residual\nlearning. Building on this, the Spike Stereo Convolutional Modulation (SSCM)\nmodule introduces simplified nonlinearity through element-wise multiplication\nand highlights noise-sensitive regions via cross-view-aware modulation.\nComplementing this, the Spike Stereo Cross-Attention (SSCA) module further\nimproves stereo correspondence by enabling efficient bidirectional feature\ninteraction across views within a spike-compatible framework. Extensive\nexperiments on diverse stereo image restoration tasks, including rain streak\nremoval, raindrop removal, low-light enhancement, and super-resolution\ndemonstrate that our model achieves competitive restoration performance while\nsignificantly reducing computational overhead. These results highlight the\npotential for real-time, low-power stereo vision applications. The code will be\navailable after the article is accepted.", "AI": {"tldr": "\u63d0\u51faSNNSIR\uff0c\u4e00\u79cd\u5b8c\u5168\u8109\u51b2\u9a71\u52a8\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u7acb\u4f53\u56fe\u50cf\u6062\u590d\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u6062\u590d\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500", "motivation": "\u73b0\u6709\u7684\u6df7\u5408SNN-ANN\u6a21\u578b\u4ecd\u4f9d\u8d56\u6d6e\u70b9\u77e9\u9635\u9664\u6cd5\u6216\u6307\u6570\u8fd0\u7b97\uff0c\u4e0eSNN\u7684\u4e8c\u8fdb\u5236\u548c\u4e8b\u4ef6\u9a71\u52a8\u7279\u6027\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u5f00\u53d1\u5b8c\u5168\u8109\u51b2\u9a71\u52a8\u7684\u67b6\u6784\u6765\u5b9e\u73b0\u4f4e\u529f\u8017\u548c\u786c\u4ef6\u53cb\u597d\u8ba1\u7b97", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u8109\u51b2\u6b8b\u5dee\u57fa\u672c\u5757(SRBB)\u589e\u5f3a\u4fe1\u606f\u6d41\uff0c\u8109\u51b2\u7acb\u4f53\u5377\u79ef\u8c03\u5236(SSCM)\u6a21\u5757\u901a\u8fc7\u9010\u5143\u7d20\u4e58\u6cd5\u5f15\u5165\u7b80\u5316\u975e\u7ebf\u6027\uff0c\u8109\u51b2\u7acb\u4f53\u4ea4\u53c9\u6ce8\u610f\u529b(SSCA)\u6a21\u5757\u5728\u8109\u51b2\u517c\u5bb9\u6846\u67b6\u5185\u5b9e\u73b0\u8de8\u89c6\u56fe\u53cc\u5411\u7279\u5f81\u4ea4\u4e92", "result": "\u5728\u591a\u79cd\u7acb\u4f53\u56fe\u50cf\u6062\u590d\u4efb\u52a1\uff08\u96e8\u7eb9\u53bb\u9664\u3001\u96e8\u6ef4\u53bb\u9664\u3001\u4f4e\u5149\u589e\u5f3a\u548c\u8d85\u5206\u8fa8\u7387\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u6062\u590d\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u5b9e\u65f6\u3001\u4f4e\u529f\u8017\u7acb\u4f53\u89c6\u89c9\u5e94\u7528\u7684\u6f5c\u529b\uff0c\u5b8c\u5168\u8109\u51b2\u9a71\u52a8\u7684\u67b6\u6784\u4e3a\u786c\u4ef6\u53cb\u597d\u7684\u9ad8\u6548\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2508.13121", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13121", "abs": "https://arxiv.org/abs/2508.13121", "authors": ["Carlos Celemin"], "title": "Bayesian Optimization-based Search for Agent Control in Automated Game Testing", "comment": null, "summary": "This work introduces an automated testing approach that employs agents\ncontrolling game characters to detect potential bugs within a game level.\nHarnessing the power of Bayesian Optimization (BO) to execute sample-efficient\nsearch, the method determines the next sampling point by analyzing the data\ncollected so far and calculates the data point that will maximize information\nacquisition. To support the BO process, we introduce a game testing-specific\nmodel built on top of a grid map, that features the smoothness and uncertainty\nestimation required by BO, however and most importantly, it does not suffer the\nscalability issues that traditional models carry. The experiments demonstrate\nthat the approach significantly improves map coverage capabilities in both time\nefficiency and exploration distribution.", "AI": {"tldr": "\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u81ea\u52a8\u5316\u6e38\u620f\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u63a7\u5236\u6e38\u620f\u89d2\u8272\u6765\u68c0\u6d4b\u6e38\u620f\u5173\u5361\u4e2d\u7684\u6f5c\u5728bug\uff0c\u4f7f\u7528\u7f51\u683c\u5730\u56fe\u6a21\u578b\u63d0\u9ad8\u641c\u7d22\u6548\u7387\u548c\u63a2\u7d22\u8986\u76d6\u7387", "motivation": "\u4f20\u7edf\u6e38\u620f\u6d4b\u8bd5\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u63a2\u7d22\u6e38\u620f\u5730\u56fe\u5e76\u68c0\u6d4bbug\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u65b9\u6cd5", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u4f18\u5316(BO)\u8fdb\u884c\u6837\u672c\u9ad8\u6548\u641c\u7d22\uff0c\u901a\u8fc7\u5206\u6790\u5df2\u6536\u96c6\u6570\u636e\u786e\u5b9a\u4e0b\u4e00\u4e2a\u91c7\u6837\u70b9\uff0c\u5e76\u6784\u5efa\u57fa\u4e8e\u7f51\u683c\u5730\u56fe\u7684\u6e38\u620f\u6d4b\u8bd5\u4e13\u7528\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709BO\u6240\u9700\u7684\u5e73\u6ed1\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u529b", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u6548\u7387\u548c\u63a2\u7d22\u5206\u5e03\u65b9\u9762\u663e\u8457\u63d0\u9ad8\u4e86\u5730\u56fe\u8986\u76d6\u7387\u80fd\u529b", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u81ea\u52a8\u5316\u6e38\u620f\u6d4b\u8bd5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5728\u6e38\u620fbug\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd"}}
{"id": "2508.12279", "categories": ["cs.CV", "cs.AI", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12279", "abs": "https://arxiv.org/abs/2508.12279", "authors": ["Jun Liu", "Zhenglun Kong", "Pu Zhao", "Weihao Zeng", "Hao Tang", "Xuan Shen", "Changdi Yang", "Wenbin Zhang", "Geng Yuan", "Wei Niu", "Xue Lin", "Yanzhi Wang"], "title": "TSLA: A Task-Specific Learning Adaptation for Semantic Segmentation on Autonomous Vehicles Platform", "comment": null, "summary": "Autonomous driving platforms encounter diverse driving scenarios, each with\nvarying hardware resources and precision requirements. Given the computational\nlimitations of embedded devices, it is crucial to consider computing costs when\ndeploying on target platforms like the NVIDIA\\textsuperscript{\\textregistered}\nDRIVE PX 2. Our objective is to customize the semantic segmentation network\naccording to the computing power and specific scenarios of autonomous driving\nhardware. We implement dynamic adaptability through a three-tier control\nmechanism -- width multiplier, classifier depth, and classifier kernel --\nallowing fine-grained control over model components based on hardware\nconstraints and task requirements. This adaptability facilitates broad model\nscaling, targeted refinement of the final layers, and scenario-specific\noptimization of kernel sizes, leading to improved resource allocation and\nperformance.\n  Additionally, we leverage Bayesian Optimization with surrogate modeling to\nefficiently explore hyperparameter spaces under tight computational budgets.\nOur approach addresses scenario-specific and task-specific requirements through\nautomatic parameter search, accommodating the unique computational complexity\nand accuracy needs of autonomous driving. It scales its Multiply-Accumulate\nOperations (MACs) for Task-Specific Learning Adaptation (TSLA), resulting in\nalternative configurations tailored to diverse self-driving tasks. These TSLA\ncustomizations maximize computational capacity and model accuracy, optimizing\nhardware utilization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u786c\u4ef6\u5e73\u53f0\u7684\u4e09\u5c42\u52a8\u6001\u9002\u5e94\u673a\u5236\uff0c\u901a\u8fc7\u5bbd\u5ea6\u4e58\u6570\u3001\u5206\u7c7b\u5668\u6df1\u5ea6\u548c\u5206\u7c7b\u5668\u6838\u5927\u5c0f\u6765\u63a7\u5236\u8bed\u4e49\u5206\u5272\u7f51\u7edc\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u8d85\u53c2\u6570\u641c\u7d22\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u8ba1\u7b97\u8d44\u6e90\u548c\u7cbe\u5ea6\u9700\u6c42\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u5e73\u53f0\u9762\u4e34\u591a\u6837\u5316\u7684\u9a7e\u9a76\u573a\u666f\uff0c\u6bcf\u4e2a\u573a\u666f\u90fd\u6709\u4e0d\u540c\u7684\u786c\u4ef6\u8d44\u6e90\u548c\u7cbe\u5ea6\u8981\u6c42\u3002\u7531\u4e8e\u5d4c\u5165\u5f0f\u8bbe\u5907\u7684\u8ba1\u7b97\u9650\u5236\uff0c\u5728\u76ee\u6807\u5e73\u53f0\uff08\u5982NVIDIA DRIVE PX 2\uff09\u4e0a\u90e8\u7f72\u65f6\u9700\u8981\u8003\u8651\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u63a7\u5236\u673a\u5236\u5b9e\u73b0\u52a8\u6001\u9002\u5e94\u6027\uff1a1\uff09\u5bbd\u5ea6\u4e58\u6570\u63a7\u5236\u6a21\u578b\u6574\u4f53\u89c4\u6a21\uff1b2\uff09\u5206\u7c7b\u5668\u6df1\u5ea6\u63a7\u5236\u6700\u7ec8\u5c42\u7684\u7ec6\u5316\uff1b3\uff09\u5206\u7c7b\u5668\u6838\u5927\u5c0f\u8fdb\u884c\u573a\u666f\u7279\u5b9a\u7684\u4f18\u5316\u3002\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u4ee3\u7406\u5efa\u6a21\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u9ad8\u6548\u63a2\u7d22\u8d85\u53c2\u6570\u7a7a\u95f4\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u81ea\u52a8\u9a7e\u9a76\u786c\u4ef6\u5e73\u53f0\u7684\u8ba1\u7b97\u80fd\u529b\u548c\u7279\u5b9a\u573a\u666f\u9700\u6c42\u5b9a\u5236\u8bed\u4e49\u5206\u5272\u7f51\u7edc\uff0c\u5b9e\u73b0\u4efb\u52a1\u7279\u5b9a\u7684\u5b66\u4e60\u9002\u5e94\uff08TSLA\uff09\uff0c\u751f\u6210\u9488\u5bf9\u4e0d\u540c\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u7684\u66ff\u4ee3\u914d\u7f6e\u3002", "conclusion": "\u63d0\u51fa\u7684\u52a8\u6001\u9002\u5e94\u673a\u5236\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f18\u5316\u786c\u4ef6\u5229\u7528\u7387\uff0c\u5728\u6ee1\u8db3\u8ba1\u7b97\u7ea6\u675f\u7684\u540c\u65f6\u6700\u5927\u5316\u6a21\u578b\u7cbe\u5ea6\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5e73\u53f0\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u7f51\u7edc\u5b9a\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13143", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.13143", "abs": "https://arxiv.org/abs/2508.13143", "authors": ["Ruofan Lu", "Yichen Li", "Yintong Huo"], "title": "Exploring Autonomous Agents: A Closer Look at Why They Fail When Completing Tasks", "comment": "Accepted by ASE 2025 NIER", "summary": "Autonomous agent systems powered by Large Language Models (LLMs) have\ndemonstrated promising capabilities in automating complex tasks. However,\ncurrent evaluations largely rely on success rates without systematically\nanalyzing the interactions, communication mechanisms, and failure causes within\nthese systems. To bridge this gap, we present a benchmark of 34 representative\nprogrammable tasks designed to rigorously assess autonomous agents. Using this\nbenchmark, we evaluate three popular open-source agent frameworks combined with\ntwo LLM backbones, observing a task completion rate of approximately 50%.\nThrough in-depth failure analysis, we develop a three-tier taxonomy of failure\ncauses aligned with task phases, highlighting planning errors, task execution\nissues, and incorrect response generation. Based on these insights, we propose\nactionable improvements to enhance agent planning and self-diagnosis\ncapabilities. Our failure taxonomy, together with mitigation advice, provides\nan empirical foundation for developing more robust and effective autonomous\nagent systems in the future.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u81ea\u4e3b\u7ec4\u7ec7\u7cfb\u7edf\u8bc4\u6d4b\u57fa\u51c6\uff0c\u901a\u8fc7\u6df1\u5165\u5206\u6790\u5931\u8d25\u539f\u56e0\u5f62\u6210\u4e09\u5c42\u5206\u7c7b\u6cd5\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u4ee5\u63d0\u5347\u7ec4\u7ec7\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5bf9LLM\u9a71\u52a8\u7684\u81ea\u4e3b\u7ec4\u7ec7\u7cfb\u7edf\u7684\u8bc4\u4f30\u4e3b\u8981\u4f9d\u9760\u6210\u529f\u7387\uff0c\u7f3a\u4e4f\u5bf9\u4ea4\u4e92\u673a\u5236\u3001\u901a\u4fe1\u65b9\u5f0f\u548c\u5931\u8d25\u539f\u56e0\u7684\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u5f00\u53d1\u4e8634\u4e2a\u4ee3\u8868\u6027\u7684\u53ef\u7f16\u7a0b\u4efb\u52a1\u4f5c\u4e3a\u8bc4\u6d4b\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u4e09\u4e2a\u6d41\u884c\u5f00\u6e90\u7ec4\u7ec7\u6846\u67b6\u4e0e\u4e24\u4e2aLLM\u6838\u5fc3\u7684\u7ec4\u5408\uff0c\u5e76\u8fdb\u884c\u6df1\u5ea6\u5931\u8d25\u5206\u6790\u3002", "result": "\u89c2\u5bdf\u5230\u4efb\u52a1\u5b8c\u6210\u7387\u7ea6\u4e3a50%\uff0c\u5f62\u6210\u4e86\u4e0e\u4efb\u52a1\u9636\u6bb5\u5bf9\u5e94\u7684\u4e09\u5c42\u5931\u8d25\u5206\u7c7b\uff1a\u89c4\u5212\u9519\u8bef\u3001\u4efb\u52a1\u6267\u884c\u95ee\u9898\u548c\u9519\u8bef\u54cd\u5e94\u751f\u6210\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u7684\u5931\u8d25\u5206\u7c7b\u548c\u7f29\u51cf\u5efa\u8bae\u4e3a\u5f00\u53d1\u66f4\u7a33\u5065\u6709\u6548\u7684\u81ea\u4e3b\u7ec4\u7ec7\u7cfb\u7edf\u5960\u5b9a\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2508.12290", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12290", "abs": "https://arxiv.org/abs/2508.12290", "authors": ["Chor Boon Tan", "Conghui Hu", "Gim Hee Lee"], "title": "CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval", "comment": "BMVC 2025", "summary": "The recent growth of large foundation models that can easily generate\npseudo-labels for huge quantity of unlabeled data makes unsupervised Zero-Shot\nCross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, we\ntherefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) with\nnoisy pseudo labels generated by large foundation models such as CLIP. To this\nend, we propose CLAIR to refine the noisy pseudo-labels with a confidence score\nfrom the similarity between the CLIP text and image features. Furthermore, we\ndesign inter-instance and inter-cluster contrastive losses to encode images\ninto a class-aware latent space, and an inter-domain contrastive loss to\nalleviate domain discrepancies. We also learn a novel cross-domain mapping\nfunction in closed-form, using only CLIP text embeddings to project image\nfeatures from one domain to another, thereby further aligning the image\nfeatures for retrieval. Finally, we enhance the zero-shot generalization\nability of our CLAIR to handle novel categories by introducing an extra set of\nlearnable prompts. Extensive experiments are carried out using TUBerlin,\nSketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIR\nconsistently shows superior performance compared to existing state-of-the-art\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCLAIR\u65b9\u6cd5\uff0c\u901a\u8fc7CLIP\u751f\u6210\u7684\u566a\u58f0\u4f2a\u6807\u7b7e\u8fdb\u884c\u5f31\u76d1\u7763\u96f6\u6837\u672c\u8de8\u57df\u56fe\u50cf\u68c0\u7d22\uff0c\u5229\u7528\u7f6e\u4fe1\u5ea6\u5206\u6570\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u8de8\u57df\u6620\u5c04\u51fd\u6570\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u57fa\u7840\u6a21\u578b\u80fd\u591f\u4e3a\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u4f7f\u5f97\u65e0\u76d1\u7763\u96f6\u6837\u672c\u8de8\u57df\u56fe\u50cf\u68c0\u7d22\u53d8\u5f97\u4e0d\u90a3\u4e48\u76f8\u5173\uff0c\u56e0\u6b64\u8f6c\u5411\u7814\u7a76\u4f7f\u7528CLIP\u7b49\u5927\u578b\u57fa\u7840\u6a21\u578b\u751f\u6210\u566a\u58f0\u4f2a\u6807\u7b7e\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\u3002", "method": "1) \u4f7f\u7528CLIP\u6587\u672c\u548c\u56fe\u50cf\u7279\u5f81\u7684\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u7f6e\u4fe1\u5ea6\u5206\u6570\u6765\u7ec6\u5316\u566a\u58f0\u4f2a\u6807\u7b7e\uff1b2) \u8bbe\u8ba1\u5b9e\u4f8b\u95f4\u548c\u7c07\u95f4\u5bf9\u6bd4\u635f\u5931\u6765\u7f16\u7801\u7c7b\u611f\u77e5\u6f5c\u5728\u7a7a\u95f4\uff1b3) \u8bbe\u8ba1\u57df\u95f4\u5bf9\u6bd4\u635f\u5931\u7f13\u89e3\u57df\u5dee\u5f02\uff1b4) \u5b66\u4e60\u95ed\u5f0f\u8de8\u57df\u6620\u5c04\u51fd\u6570\uff1b5) \u5f15\u5165\u53ef\u5b66\u4e60\u63d0\u793a\u589e\u5f3a\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728TUBerlin\u3001Sketchy\u3001Quickdraw\u548cDomainNet\u7b49\u96f6\u6837\u672c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCLAIR\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5 consistently \u663e\u793a\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "CLAIR\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u5904\u7406CLIP\u751f\u6210\u7684\u566a\u58f0\u4f2a\u6807\u7b7e\uff0c\u7ed3\u5408\u591a\u79cd\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u548c\u8de8\u57df\u6620\u5c04\u6280\u672f\uff0c\u6210\u529f\u63d0\u5347\u4e86\u5f31\u76d1\u7763\u96f6\u6837\u672c\u8de8\u57df\u56fe\u50cf\u68c0\u7d22\u7684\u6027\u80fd\u3002"}}
{"id": "2508.12313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12313", "abs": "https://arxiv.org/abs/2508.12313", "authors": ["Xiaobin Deng", "Changyu Diao", "Min Li", "Ruohan Yu", "Duanqing Xu"], "title": "Improving Densification in 3D Gaussian Splatting for High-Fidelity Rendering", "comment": "Project page: https://xiaobin2001.github.io/improved-gs-web", "summary": "Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in\nreal-time rendering, its densification strategy often results in suboptimal\nreconstruction quality. In this work, we present a comprehensive improvement to\nthe densification pipeline of 3DGS from three perspectives: when to densify,\nhow to densify, and how to mitigate overfitting. Specifically, we propose an\nEdge-Aware Score to effectively select candidate Gaussians for splitting. We\nfurther introduce a Long-Axis Split strategy that reduces geometric distortions\nintroduced by clone and split operations. To address overfitting, we design a\nset of techniques, including Recovery-Aware Pruning, Multi-step Update, and\nGrowth Control. Our method enhances rendering fidelity without introducing\nadditional training or inference overhead, achieving state-of-the-art\nperformance with fewer Gaussians.", "AI": {"tldr": "\u901a\u8fc7\u8fb9\u7f18\u611f\u77e5\u5206\u6570\u3001\u957f\u8f74\u5206\u5272\u7b56\u7565\u548c\u6297\u8fc7\u62df\u5408\u6280\u672f\uff0c\u5168\u9762\u6539\u5584\u4e863D\u9ad8\u65af\u6563\u70b9\u7684\u5bc6\u5ea6\u5316\u7b56\u7565\uff0c\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u548c\u51cf\u5c11\u9ad8\u65af\u5143\u6570\u91cf", "motivation": "3D\u9ad8\u65af\u6563\u70b9\u7684\u5bc6\u5ea6\u5316\u7b56\u7565\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u4e0d\u4f73\uff0c\u9700\u8981\u4ece\u4f55\u65f6\u5bc6\u5ea6\u5316\u3001\u5982\u4f55\u5bc6\u5ea6\u5316\u548c\u5982\u4f55\u51cf\u8f7b\u8fc7\u62df\u5408\u4e09\u4e2a\u89d2\u5ea6\u8fdb\u884c\u5168\u9762\u6539\u8fdb", "method": "\u63d0\u51fa\u8fb9\u7f18\u611f\u77e5\u5206\u6570\u9009\u62e9\u5206\u5272\u5019\u9009\u9ad8\u65af\u5143\uff0c\u957f\u8f74\u5206\u5272\u7b56\u7565\u51cf\u5c11\u51e0\u4f55\u5931\u771f\uff0c\u4ee5\u53ca\u6062\u590d\u611f\u77e5\u526a\u679d\u3001\u591a\u6b65\u66f4\u65b0\u548c\u589e\u957f\u63a7\u5236\u7b49\u6297\u8fc7\u62df\u5408\u6280\u672f", "result": "\u65b9\u6cd5\u5728\u4e0d\u589e\u52a0\u8bad\u7ec3\u6216\u63a8\u7406\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4e86\u6e32\u67d3\u4fdd\u771f\u5ea6\uff0c\u4ee5\u66f4\u5c11\u7684\u9ad8\u65af\u5143\u6570\u91cf\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u5bc6\u5ea6\u5316\u6d41\u7a0b\u6539\u8fdb\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u9ad8\u65af\u6563\u70b9\u7684\u91cd\u5efa\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6\u6e32\u67d3\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12322", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12322", "abs": "https://arxiv.org/abs/2508.12322", "authors": ["Michael Deutges", "Chen Yang", "Raheleh Salehi", "Nassir Navab", "Carsten Marr", "Ario Sadafi"], "title": "Neural Cellular Automata for Weakly Supervised Segmentation of White Blood Cells", "comment": null, "summary": "The detection and segmentation of white blood cells in blood smear images is\na key step in medical diagnostics, supporting various downstream tasks such as\nautomated blood cell counting, morphological analysis, cell classification, and\ndisease diagnosis and monitoring. Training robust and accurate models requires\nlarge amounts of labeled data, which is both time-consuming and expensive to\nacquire. In this work, we propose a novel approach for weakly supervised\nsegmentation using neural cellular automata (NCA-WSS). By leveraging the\nfeature maps generated by NCA during classification, we can extract\nsegmentation masks without the need for retraining with segmentation labels. We\nevaluate our method on three white blood cell microscopy datasets and\ndemonstrate that NCA-WSS significantly outperforms existing weakly supervised\napproaches. Our work illustrates the potential of NCA for both classification\nand segmentation in a weakly supervised framework, providing a scalable and\nefficient solution for medical image analysis.", "AI": {"tldr": "\u57fa\u4e8e\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a(NCA)\u7684\u5f31\u76d1\u7763\u5206\u5272\u65b9\u6cd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u4ece\u5206\u7c7b\u7279\u5f81\u56fe\u4e2d\u63d0\u53d6\u5206\u5272\u63a9\u7801\uff0c\u5728\u767d\u7ec6\u80de\u5206\u5272\u4efb\u52a1\u4e0a\u663e\u8457\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5", "motivation": "\u533b\u7597\u8bca\u65ad\u4e2d\u767d\u7ec6\u80de\u68c0\u6d4b\u548c\u5206\u5272\u9700\u8981\u5927\u91cf\u6807\u7b7e\u6570\u636e\uff0c\u800c\u6807\u6ce8\u5de5\u4f5c\u8017\u65f6\u8017\u8d39\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5f31\u76d1\u7763\u65b9\u6cd5", "method": "\u63d0\u51faNCA-WSS\u65b9\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\u5728\u5206\u7c7b\u8fc7\u7a0b\u4e2d\u751f\u6210\u7684\u7279\u5f81\u56fe\uff0c\u76f4\u63a5\u63d0\u53d6\u5206\u5272\u63a9\u7801\u800c\u65e0\u9700\u5206\u5272\u6807\u7b7e\u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3", "result": "\u5728\u4e09\u4e2a\u767d\u7ec6\u80de\u663e\u5fae\u955c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cNCA-WSS\u65b9\u6cd5\u5728\u5f31\u76d1\u7763\u5206\u5272\u4efb\u52a1\u4e0a\u663e\u8457\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86NCA\u5728\u5f31\u76d1\u7763\u6846\u67b6\u4e0b\u540c\u65f6\u505a\u597d\u5206\u7c7b\u548c\u5206\u5272\u7684\u6f5c\u529b\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12324", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12324", "abs": "https://arxiv.org/abs/2508.12324", "authors": ["Chen Yang", "Michael Deutges", "Jingsong Liu", "Han Li", "Nassir Navab", "Carsten Marr", "Ario Sadafi"], "title": "Attention Pooling Enhances NCA-based Classification of Microscopy Images", "comment": null, "summary": "Neural Cellular Automata (NCA) offer a robust and interpretable approach to\nimage classification, making them a promising choice for microscopy image\nanalysis. However, a performance gap remains between NCA and larger, more\ncomplex architectures. We address this challenge by integrating attention\npooling with NCA to enhance feature extraction and improve classification\naccuracy. The attention pooling mechanism refines the focus on the most\ninformative regions, leading to more accurate predictions. We evaluate our\nmethod on eight diverse microscopy image datasets and demonstrate that our\napproach significantly outperforms existing NCA methods while remaining\nparameter-efficient and explainable. Furthermore, we compare our method with\ntraditional lightweight convolutional neural network and vision transformer\narchitectures, showing improved performance while maintaining a significantly\nlower parameter count. Our results highlight the potential of NCA-based models\nan alternative for explainable image classification.", "AI": {"tldr": "\u5c06\u6ce8\u610f\u529b\u6c60\u5316\u673a\u5236\u4e0e\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u663e\u5fae\u955c\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u8d85\u8d8a\u73b0\u6709NCA\u65b9\u6cd5\u548c\u4f20\u7edf\u8f7b\u91cf\u7ea7\u67b6\u6784", "motivation": "\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u5177\u6709\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4f18\u52bf\uff0c\u4f46\u6027\u80fd\u4e0e\u5927\u578b\u590d\u6742\u67b6\u6784\u5b58\u5728\u5dee\u8ddd\uff0c\u9700\u8981\u63d0\u5347\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u548c\u5206\u7c7b\u7cbe\u5ea6", "method": "\u96c6\u6210\u6ce8\u610f\u529b\u6c60\u5316\u673a\u5236\u5230NCA\u4e2d\uff0c\u901a\u8fc7\u5173\u6ce8\u4fe1\u606f\u6700\u4e30\u5bcc\u7684\u533a\u57df\u6765\u4f18\u5316\u7279\u5f81\u63d0\u53d6", "result": "\u5728\u516b\u4e2a\u4e0d\u540c\u663e\u5fae\u955c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709NCA\u65b9\u6cd5\uff0c\u4e0e\u4f20\u7edf\u8f7b\u91cf\u7ea7CNN\u548cViT\u67b6\u6784\u76f8\u6bd4\u6027\u80fd\u66f4\u597d\u4e14\u53c2\u6570\u6570\u91cf\u663e\u8457\u66f4\u4f4e", "conclusion": "\u57fa\u4e8eNCA\u7684\u6a21\u578b\u5177\u6709\u4f5c\u4e3a\u53ef\u89e3\u91ca\u56fe\u50cf\u5206\u7c7b\u66ff\u4ee3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u6ce8\u610f\u529b\u6c60\u5316\u7684\u96c6\u6210\u6709\u6548\u7f29\u5c0f\u4e86\u6027\u80fd\u5dee\u8ddd"}}
{"id": "2508.12330", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12330", "abs": "https://arxiv.org/abs/2508.12330", "authors": ["Yuval Haitman", "Oded Bialer"], "title": "DoppDrive: Doppler-Driven Temporal Aggregation for Improved Radar Object Detection", "comment": "ICCV 2025", "summary": "Radar-based object detection is essential for autonomous driving due to\nradar's long detection range. However, the sparsity of radar point clouds,\nespecially at long range, poses challenges for accurate detection. Existing\nmethods increase point density through temporal aggregation with ego-motion\ncompensation, but this approach introduces scatter from dynamic objects,\ndegrading detection performance. We propose DoppDrive, a novel Doppler-Driven\ntemporal aggregation method that enhances radar point cloud density while\nminimizing scatter. Points from previous frames are shifted radially according\nto their dynamic Doppler component to eliminate radial scatter, with each point\nassigned a unique aggregation duration based on its Doppler and angle to\nminimize tangential scatter. DoppDrive is a point cloud density enhancement\nstep applied before detection, compatible with any detector, and we demonstrate\nthat it significantly improves object detection performance across various\ndetectors and datasets.", "AI": {"tldr": "DoppDrive\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u666e\u52d2\u6548\u5e94\u7684\u96f7\u8fbe\u70b9\u4e91\u65f6\u57df\u805a\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f84\u5411\u52a8\u6001\u8865\u507f\u548c\u4e2a\u6027\u5316\u805a\u5408\u65f6\u957f\u6765\u589e\u5f3a\u70b9\u4e91\u5bc6\u5ea6\u5e76\u51cf\u5c11\u6563\u5c04\uff0c\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u6027\u80fd", "motivation": "\u96f7\u8fbe\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5177\u6709\u957f\u8ddd\u79bb\u68c0\u6d4b\u4f18\u52bf\uff0c\u4f46\u8fdc\u8ddd\u79bb\u70b9\u4e91\u7a00\u758f\u6027\u95ee\u9898\u4e25\u91cd\u3002\u73b0\u6709\u65f6\u57df\u805a\u5408\u65b9\u6cd5\u4f1a\u56e0\u52a8\u6001\u7269\u4f53\u4ea7\u751f\u6563\u5c04\uff0c\u964d\u4f4e\u68c0\u6d4b\u6027\u80fd", "method": "\u63d0\u51faDoppDrive\u65b9\u6cd5\uff1a1\uff09\u6839\u636e\u591a\u666e\u52d2\u52a8\u6001\u5206\u91cf\u5f84\u5411\u79fb\u52a8\u5386\u53f2\u5e27\u70b9\u4e91\u6d88\u9664\u5f84\u5411\u6563\u5c04\uff1b2\uff09\u57fa\u4e8e\u591a\u666e\u52d2\u548c\u89d2\u5ea6\u4e3a\u6bcf\u4e2a\u70b9\u5206\u914d\u4e2a\u6027\u5316\u805a\u5408\u65f6\u957f\u6765\u51cf\u5c11\u5207\u5411\u6563\u5c04", "result": "DoppDrive\u4f5c\u4e3a\u68c0\u6d4b\u524d\u7684\u70b9\u4e91\u5bc6\u5ea6\u589e\u5f3a\u6b65\u9aa4\uff0c\u4e0e\u5404\u79cd\u68c0\u6d4b\u5668\u517c\u5bb9\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u68c0\u6d4b\u6027\u80fd", "conclusion": "\u57fa\u4e8e\u591a\u666e\u52d2\u9a71\u52a8\u7684\u65f6\u57df\u805a\u5408\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u96f7\u8fbe\u70b9\u4e91\u7a00\u758f\u6027\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u52a8\u6001\u7269\u4f53\u5e26\u6765\u7684\u6563\u5c04\u95ee\u9898\uff0c\u4e3a\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9884\u5904\u7406\u65b9\u6848"}}
{"id": "2508.12336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12336", "abs": "https://arxiv.org/abs/2508.12336", "authors": ["Fatemeh Ghorbani Lohesara", "Karen Eguiazarian", "Sebastian Knorr"], "title": "Geometry-Aware Video Inpainting for Joint Headset Occlusion Removal and Face Reconstruction in Social XR", "comment": null, "summary": "Head-mounted displays (HMDs) are essential for experiencing extended reality\n(XR) environments and observing virtual content. However, they obscure the\nupper part of the user's face, complicating external video recording and\nsignificantly impacting social XR applications such as teleconferencing, where\nfacial expressions and eye gaze details are crucial for creating an immersive\nexperience. This study introduces a geometry-aware learning-based framework to\njointly remove HMD occlusions and reconstruct complete 3D facial geometry from\nRGB frames captured from a single viewpoint. The method integrates a GAN-based\nvideo inpainting network, guided by dense facial landmarks and a single\nocclusion-free reference frame, to restore missing facial regions while\npreserving identity. Subsequently, a SynergyNet-based module regresses 3D\nMorphable Model (3DMM) parameters from the inpainted frames, enabling accurate\n3D face reconstruction. Dense landmark optimization is incorporated throughout\nthe pipeline to improve both the inpainting quality and the fidelity of the\nrecovered geometry. Experimental results demonstrate that the proposed\nframework can successfully remove HMDs from RGB facial videos while maintaining\nfacial identity and realism, producing photorealistic 3D face geometry outputs.\nAblation studies further show that the framework remains robust across\ndifferent landmark densities, with only minor quality degradation under sparse\nlandmark configurations.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u51e0\u4f55\u611f\u77e5\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u89c6\u70b9RGB\u89c6\u9891\u4e2d\u540c\u65f6\u79fb\u9664\u5934\u663e\u8bbe\u5907\u906e\u6321\u5e76\u91cd\u5efa\u5b8c\u6574\u76843D\u9762\u90e8\u51e0\u4f55\uff0c\u4e3a\u793e\u4ea4XR\u5e94\u7528\u63d0\u4f9b\u6d89\u5165\u5f0f\u9762\u90e8\u8868\u60c5\u6062\u590d\u3002", "motivation": "\u5934\u663e\u8bbe\u5907(HMDs)\u906e\u6321\u7528\u6237\u9762\u90e8\u4e0a\u90e8\u5206\uff0c\u5f71\u54cd\u5916\u90e8\u89c6\u9891\u5f55\u5236\u548c\u793e\u4ea4XR\u5e94\u7528\u7684\u6d89\u5165\u5f0f\u4f53\u9a8c\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9762\u90e8\u8868\u60c5\u548c\u773c\u795e\u4ea4\u6d41\u7684\u8fdc\u7a0b\u4f1a\u8bae\u573a\u666f\u4e2d\u3002", "method": "\u96c6\u6210GAN\u57fa\u7840\u7684\u89c6\u9891\u4fee\u590d\u7f51\u7edc\uff0c\u901a\u8fc7\u5bc6\u96c6\u9762\u90e8\u5173\u952e\u70b9\u548c\u5355\u5f20\u65e0\u906e\u6321\u53c2\u8003\u5e27\u6307\u5bfc\uff0c\u6062\u590d\u7f3a\u5931\u9762\u90e8\u533a\u57df\u4fdd\u6301\u8eab\u4efd\u8bc6\u522b\u3002\u7136\u540e\u4f7f\u7528SynergyNet\u57fa\u7840\u6a21\u5757\u4ece\u4fee\u590d\u540e\u7684\u5e27\u4e2d\u56de\u5f523D\u5f62\u6001\u6a21\u578b(3DMM)\u53c2\u6570\uff0c\u5b9e\u73b0\u51c6\u786e\u76843D\u9762\u90e8\u91cd\u5efa\u3002\u6574\u4e2a\u6d41\u7a0b\u4e2d\u878d\u5165\u5bc6\u96c6\u5173\u952e\u70b9\u4f18\u5316\u6765\u63d0\u5347\u4fee\u590d\u8d28\u91cf\u548c\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u6210\u529f\u4eceRGB\u9762\u90e8\u89c6\u9891\u4e2d\u79fb\u9664HMD\u906e\u6321\uff0c\u540c\u65f6\u4fdd\u6301\u9762\u90e8\u8eab\u4efd\u8bc6\u522b\u548c\u771f\u5b9e\u611f\uff0c\u4ea7\u751f\u8d85\u5b9e\u9645\u76843D\u9762\u90e8\u51e0\u4f55\u8f93\u51fa\u3002\u5206\u79bb\u5b9e\u9a8c\u8fd8\u663e\u793a\u6846\u67b6\u5728\u4e0d\u540c\u5173\u952e\u70b9\u5bc6\u5ea6\u4e0b\u4ecd\u4fdd\u6301\u7a33\u5065\u6027\uff0c\u5728\u7a00\u758f\u5173\u952e\u70b9\u914d\u7f6e\u4e0b\u4ec5\u6709\u8f7b\u5fae\u8d28\u91cf\u4e0b\u964d\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u5355\u89c6\u70b9RGB\u89c6\u9891\u4e2d\u540c\u65f6\u5b9e\u73b0HMD\u906e\u6321\u79fb\u9664\u548c3D\u9762\u90e8\u91cd\u5efa\uff0c\u4e3a\u793e\u4ea4XR\u5e94\u7528\u7684\u9762\u90e8\u8868\u60c5\u6062\u590d\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2508.12341", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12341", "abs": "https://arxiv.org/abs/2508.12341", "authors": ["Ziye Wang", "Minghang Yu", "Chunyan Xu", "Zhen Cui"], "title": "Semantic Discrepancy-aware Detector for Image Forgery Identification", "comment": "10 pages, 5 figures", "summary": "With the rapid advancement of image generation techniques, robust forgery\ndetection has become increasingly imperative to ensure the trustworthiness of\ndigital media. Recent research indicates that the learned semantic concepts of\npre-trained models are critical for identifying fake images. However, the\nmisalignment between the forgery and semantic concept spaces hinders the\nmodel's forgery detection performance. To address this problem, we propose a\nnovel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction\nlearning to align the two spaces at a fine-grained visual level. By exploiting\nthe conceptual knowledge embedded in the pre-trained vision language model, we\nspecifically design a semantic token sampling module to mitigate the space\nshifts caused by features irrelevant to both forgery traces and semantic\nconcepts. A concept-level forgery discrepancy learning module, built upon a\nvisual reconstruction paradigm, is proposed to strengthen the interaction\nbetween visual semantic concepts and forgery traces, effectively capturing\ndiscrepancies under the concepts' guidance. Finally, the low-level forgery\nfeature enhancemer integrates the learned concept level forgery discrepancies\nto minimize redundant forgery information. Experiments conducted on two\nstandard image forgery datasets demonstrate the efficacy of the proposed SDD,\nwhich achieves superior results compared to existing methods. The code is\navailable at https://github.com/wzy1111111/SSD.", "AI": {"tldr": "\u63d0\u51faSDD\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u91cd\u5efa\u5b66\u4e60\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5c42\u9762\u5bf9\u9f50\u4f2a\u9020\u75d5\u8ff9\u548c\u8bed\u4e49\u6982\u5ff5\u7a7a\u95f4\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6982\u5ff5\u77e5\u8bc6\u6765\u63d0\u5347\u4f2a\u9020\u56fe\u50cf\u68c0\u6d4b\u6027\u80fd", "motivation": "\u968f\u7740\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u5f3a\u5927\u7684\u4f2a\u9020\u68c0\u6d4b\u6765\u786e\u4fdd\u6570\u5b57\u5a92\u4f53\u7684\u53ef\u4fe1\u5ea6\u3002\u73b0\u6709\u65b9\u6cd5\u4e2d\u4f2a\u9020\u7a7a\u95f4\u4e0e\u8bed\u4e49\u6982\u5ff5\u7a7a\u95f4\u7684\u4e0d\u5bf9\u9f50\u9650\u5236\u4e86\u68c0\u6d4b\u6027\u80fd", "method": "\u8bbe\u8ba1\u8bed\u4e49\u6807\u8bb0\u91c7\u6837\u6a21\u5757\u7f13\u89e3\u7a7a\u95f4\u504f\u79fb\uff0c\u6784\u5efa\u6982\u5ff5\u7ea7\u4f2a\u9020\u5dee\u5f02\u5b66\u4e60\u6a21\u5757\u52a0\u5f3a\u89c6\u89c9\u8bed\u4e49\u6982\u5ff5\u4e0e\u4f2a\u9020\u75d5\u8ff9\u7684\u4ea4\u4e92\uff0c\u901a\u8fc7\u4f4e\u7ea7\u4f2a\u9020\u7279\u5f81\u589e\u5f3a\u6574\u5408\u5b66\u4e60\u5230\u7684\u6982\u5ff5\u7ea7\u5dee\u5f02", "result": "\u5728\u4e24\u4e2a\u6807\u51c6\u56fe\u50cf\u4f2a\u9020\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSDD\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u53d6\u5f97\u4e86\u66f4\u4f18\u8d8a\u7684\u7ed3\u679c", "conclusion": "\u6240\u63d0\u51fa\u7684SDD\u65b9\u6cd5\u901a\u8fc7\u7a7a\u95f4\u5bf9\u9f50\u548c\u6982\u5ff5\u5f15\u5bfc\u7684\u5dee\u5f02\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4f2a\u9020\u56fe\u50cf\u68c0\u6d4b\u7684\u6027\u80fd"}}
{"id": "2508.12343", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12343", "abs": "https://arxiv.org/abs/2508.12343", "authors": ["Emanuel C. Silva", "Tatiana T. Schein", "Stephanie L. Bri\u00e3o", "Guilherme L. M. Costa", "Felipe G. Oliveira", "Gustavo P. Almeida", "Eduardo L. Silva", "Sam S. Devincenzi", "Karina S. Machado", "Paulo L. J. Drews-Jr"], "title": "AquaFeat: A Features-Based Image Enhancement Model for Underwater Object Detection", "comment": null, "summary": "The severe image degradation in underwater environments impairs object\ndetection models, as traditional image enhancement methods are often not\noptimized for such downstream tasks. To address this, we propose AquaFeat, a\nnovel, plug-and-play module that performs task-driven feature enhancement. Our\napproach integrates a multi-scale feature enhancement network trained\nend-to-end with the detector's loss function, ensuring the enhancement process\nis explicitly guided to refine features most relevant to the detection task.\nWhen integrated with YOLOv8m on challenging underwater datasets, AquaFeat\nachieves state-of-the-art Precision (0.877) and Recall (0.624), along with\ncompetitive mAP scores (mAP@0.5 of 0.677 and mAP@[0.5:0.95] of 0.421). By\ndelivering these accuracy gains while maintaining a practical processing speed\nof 46.5 FPS, our model provides an effective and computationally efficient\nsolution for real-world applications, such as marine ecosystem monitoring and\ninfrastructure inspection.", "AI": {"tldr": "AquaFeat\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u4efb\u52a1\u9a71\u52a8\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0c\u4e13\u95e8\u9488\u5bf9\u6c34\u4e0b\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u589e\u5f3a\u7f51\u7edc\u548c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u5904\u7406\u901f\u5ea6\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u7684\u4e25\u91cd\u56fe\u50cf\u9000\u5316\u4f1a\u5f71\u54cd\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f20\u7edf\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u901a\u5e38\u6ca1\u6709\u9488\u5bf9\u4e0b\u6e38\u68c0\u6d4b\u4efb\u52a1\u8fdb\u884c\u4f18\u5316\u3002", "method": "\u63d0\u51faAquaFeat\u6a21\u5757\uff0c\u96c6\u6210\u591a\u5c3a\u5ea6\u7279\u5f81\u589e\u5f3a\u7f51\u7edc\uff0c\u4e0e\u68c0\u6d4b\u5668\u635f\u5931\u51fd\u6570\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u786e\u4fdd\u589e\u5f3a\u8fc7\u7a0b\u660e\u786e\u6307\u5bfc\u4f18\u5316\u4e0e\u68c0\u6d4b\u4efb\u52a1\u6700\u76f8\u5173\u7684\u7279\u5f81\u3002", "result": "\u5728YOLOv8m\u4e0a\u96c6\u6210AquaFeat\uff0c\u5728\u6311\u6218\u6027\u6c34\u4e0b\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6(0.877)\u548c\u53ec\u56de\u7387(0.624)\uff0c\u4ee5\u53ca\u7ade\u4e89\u529b\u7684mAP\u5206\u6570(mAP@0.5\u4e3a0.677\uff0cmAP@[0.5:0.95]\u4e3a0.421)\uff0c\u5904\u7406\u901f\u5ea6\u4e3a46.5 FPS\u3002", "conclusion": "AquaFeat\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u76d1\u6d4b\u548c\u57fa\u7840\u8bbe\u65bd\u68c0\u67e5\u7b49\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.12346", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12346", "abs": "https://arxiv.org/abs/2508.12346", "authors": ["Hu Gao", "Depeng Dang"], "title": "MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring", "comment": null, "summary": "The Mamba architecture has emerged as a promising alternative to CNNs and\nTransformers for image deblurring. However, its flatten-and-scan strategy often\nresults in local pixel forgetting and channel redundancy, limiting its ability\nto effectively aggregate 2D spatial information. Although existing methods\nmitigate this by modifying the scan strategy or incorporating local feature\nmodules, it increase computational complexity and hinder real-time performance.\nIn this paper, we propose a structure-aware image deblurring network without\nchanging the original Mamba architecture. Specifically, we design a memory\nbuffer mechanism to preserve historical information for later fusion, enabling\nreliable modeling of relevance between adjacent features. Additionally, we\nintroduce an Ising-inspired regularization loss that simulates the energy\nminimization of the physical system's \"mutual attraction\" between pixels,\nhelping to maintain image structure and coherence. Building on this, we develop\nMBMamba. Experimental results show that our method outperforms state-of-the-art\napproaches on widely used benchmarks.", "AI": {"tldr": "\u57fa\u4e8eMamba\u7684\u56fe\u50cf\u53bb\u6a21\u7cca\u7f51\u7edcMBMamba\uff0c\u901a\u8fc7\u5185\u5b58\u7f13\u51b2\u673a\u5236\u548cIsing\u53d7\u7075\u611f\u7684\u6b63\u5219\u5316\u635f\u5931\uff0c\u89e3\u51b3\u4e86\u539f\u751fMamba\u5728\u56fe\u50cf\u53bb\u6a21\u7cca\u4e2d\u7684\u5c40\u90e8\u50cf\u7d20\u9057\u5fd8\u548c\u9891\u9053\u5197\u4f59\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "Mamba\u67b6\u6784\u5728\u56fe\u50cf\u53bb\u6a21\u7cca\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u5176\u5e73\u94fa\u626b\u63cf\u7b56\u7565\u5bfc\u81f4\u5c40\u90e8\u50cf\u7d20\u9057\u5fd8\u548c\u9891\u9053\u5197\u4f59\uff0c\u9650\u5236\u4e86\u4e8c\u7ef4\u7a7a\u95f4\u4fe1\u606f\u805a\u5408\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u4fee\u6539\u626b\u63cf\u7b56\u7565\u6216\u6dfb\u52a0\u5c40\u90e8\u7279\u5f81\u6a21\u5757\u6765\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u589e\u52a0\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u5f71\u54cd\u5b9e\u65f6\u6027\u80fd\u3002", "method": "\u63d0\u51faMBMamba\u7f51\u7edc\uff0c\u5728\u4e0d\u6539\u53d8\u539f\u751fMamba\u67b6\u6784\u7684\u524d\u63d0\u4e0b\uff1a1\uff09\u8bbe\u8ba1\u5185\u5b58\u7f13\u51b2\u673a\u5236\u4fdd\u7559\u5386\u53f2\u4fe1\u606f\u4ee5\u4fbf\u540e\u671f\u878d\u5408\uff0c\u5b9e\u73b0\u76f8\u90bb\u7279\u5f81\u95f4\u7684\u53ef\u9760\u5173\u8054\u5efa\u6a21\uff1b2\uff09\u5f15\u5165Ising\u53d7\u7075\u611f\u7684\u6b63\u5219\u5316\u635f\u5931\uff0c\u6a21\u62df\u7269\u7406\u7cfb\u7edf\u4e2d\u50cf\u7d20\u95f4\"\u76f8\u4e92\u5438\u5f15\"\u7684\u80fd\u91cf\u6700\u5c0f\u5316\uff0c\u5e2e\u52a9\u7ef4\u6301\u56fe\u50cf\u7ed3\u6784\u548c\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u6d4b\u8bd5\u96c6\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "MBMamba\u901a\u8fc7\u521b\u65b0\u7684\u5185\u5b58\u7f13\u51b2\u673a\u5236\u548c\u7269\u7406\u53d7\u7075\u611f\u7684\u6b63\u5219\u5316\u635f\u5931\uff0c\u6709\u6548\u89e3\u51b3\u4e86Mamba\u5728\u56fe\u50cf\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\u7684\u9650\u5236\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u56fe\u50cf\u5904\u7406\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12349", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12349", "abs": "https://arxiv.org/abs/2508.12349", "authors": ["Junyi Ma", "Erhang Zhang", "Yin-Dong Zheng", "Yuchen Xie", "Yixuan Zhou", "Hesheng Wang"], "title": "EgoLoc: A Generalizable Solution for Temporal Interaction Localization in Egocentric Videos", "comment": "Extended journal version of arXiv:2506.03662", "summary": "Analyzing hand-object interaction in egocentric vision facilitates VR/AR\napplications and human-robot policy transfer. Existing research has mostly\nfocused on modeling the behavior paradigm of interactive actions (i.e., ``how\nto interact''). However, the more challenging and fine-grained problem of\ncapturing the critical moments of contact and separation between the hand and\nthe target object (i.e., ``when to interact'') is still underexplored, which is\ncrucial for immersive interactive experiences in mixed reality and robotic\nmotion planning. Therefore, we formulate this problem as temporal interaction\nlocalization (TIL). Some recent works extract semantic masks as TIL references,\nbut suffer from inaccurate object grounding and cluttered scenarios. Although\ncurrent temporal action localization (TAL) methods perform well in detecting\nverb-noun action segments, they rely on category annotations during training\nand exhibit limited precision in localizing hand-object contact/separation\nmoments. To address these issues, we propose a novel zero-shot approach dubbed\nEgoLoc to localize hand-object contact and separation timestamps in egocentric\nvideos. EgoLoc introduces hand-dynamics-guided sampling to generate\nhigh-quality visual prompts. It exploits the vision-language model to identify\ncontact/separation attributes, localize specific timestamps, and provide\nclosed-loop feedback for further refinement. EgoLoc eliminates the need for\nobject masks and verb-noun taxonomies, leading to generalizable zero-shot\nimplementation. Comprehensive experiments on the public dataset and our novel\nbenchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric\nvideos. It is also validated to effectively facilitate multiple downstream\napplications in egocentric vision and robotic manipulation tasks. Code and\nrelevant data will be released at https://github.com/IRMVLab/EgoLoc.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u96f6\u6837\u672c\u65b9\u6cd5EgoLoc\uff0c\u7528\u4e8e\u5728\u4e3b\u89c6\u89d2\u89c6\u9891\u4e2d\u51c6\u786e\u5b9a\u4f4d\u624b\u90e8\u4e0e\u7269\u4f53\u7684\u63a5\u89e6\u548c\u5206\u79bb\u65f6\u523b\uff0c\u65e0\u9700\u7269\u4f53\u63a9\u7801\u6216\u52a8\u4f5c\u7c7b\u522b\u6807\u6ce8\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5982\u4f55\u4ea4\u4e92\uff0c\u800c\u66f4\u5177\u6311\u6218\u7684\u624b\u7269\u63a5\u89e6/\u5206\u79bb\u65f6\u523b\u5b9a\u4f4d\u95ee\u9898\u88ab\u5ffd\u89c6\uff0c\u8fd9\u5bf9\u6df7\u5408\u73b0\u5b9e\u4f53\u9a8c\u548c\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faEgoLoc\u65b9\u6cd5\uff0c\u91c7\u7528\u624b\u90e8\u52a8\u529b\u5b66\u5bfc\u5411\u7684\u91c7\u6837\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u89c9\u63d0\u793a\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u63a5\u89e6/\u5206\u79bb\u5c5e\u6027\u5e76\u5b9a\u4f4d\u5177\u4f53\u65f6\u95f4\u6233\uff0c\u901a\u8fc7\u95ed\u73af\u53cd\u9988\u8fdb\u884c\u7cbe\u70bc\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u65b0\u6807\u51c6\u4e0a\u7684\u5b8c\u6574\u5b9e\u9a8c\u8868\u660e\uff0cEgoLoc\u80fd\u591f\u5728\u4e3b\u89c6\u89d2\u89c6\u9891\u4e2d\u5b9e\u73b0\u51c6\u786e\u7684\u65f6\u95f4\u4ea4\u4e92\u5b9a\u4f4d\uff0c\u5e76\u6709\u6548\u652f\u6301\u591a\u4e2b\u4e0b\u6e38\u5e94\u7528\u3002", "conclusion": "EgoLoc\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u65b9\u6848\uff0c\u80fd\u591f\u51c6\u786e\u6355\u6349\u624b\u7269\u4ea4\u4e92\u7684\u5173\u952e\u65f6\u523b\uff0c\u4e3aVR/AR\u5e94\u7528\u548c\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2508.12356", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12356", "abs": "https://arxiv.org/abs/2508.12356", "authors": ["Ahmet H. G\u00fczel", "Ilija Bogunovic", "Jack Parker-Holder"], "title": "Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data", "comment": null, "summary": "Offline reinforcement learning (RL) offers a promising framework for training\nagents using pre-collected datasets without the need for further environment\ninteraction. However, policies trained on offline data often struggle to\ngeneralise due to limited exposure to diverse states. The complexity of visual\ndata introduces additional challenges such as noise, distractions, and spurious\ncorrelations, which can misguide the policy and increase the risk of\noverfitting if the training data is not sufficiently diverse. Indeed, this\nmakes it challenging to leverage vision-based offline data in training robust\nagents that can generalize to unseen environments. To solve this problem, we\npropose a simple approach generating additional synthetic training data. We\npropose a two-step process, first augmenting the originally collected offline\ndata to improve zero-shot generalization by introducing diversity, then using a\ndiffusion model to generate additional data in latent space. We test our method\nacross both continuous action spaces (Visual D4RL) and discrete action spaces\n(Procgen), demonstrating that it significantly improves generalization without\nrequiring any algorithmic changes to existing model-free offline RL methods. We\nshow that our method not only increases the diversity of the training data but\nalso significantly reduces the generalization gap at test time while\nmaintaining computational efficiency. We believe this approach could fuel\nadditional progress in generating synthetic data to train more general agents\nin the future.", "AI": {"tldr": "\u901a\u8fc7\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\u6765\u6539\u5582\u89c6\u89c9\u57fa\u4e8e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6f14\u7b56\u901a\u7528\u6027\uff0c\u5305\u62ec\u6570\u636e\u6269\u5145\u548c\u6f5c\u7a7a\u95f4\u6fc0\u5149\u6a21\u578b\u751f\u6210\u65b0\u6570\u636e", "motivation": "\u79bb\u7ebfRL\u7b56\u7565\u5728\u89c6\u89c9\u6570\u636e\u4e0a\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u7f3a\u4e4f\u591a\u6837\u6027\u5bfc\u81f4\u901a\u7528\u6027\u5dee\uff0c\u9700\u8981\u89e3\u51b3\u89c6\u89c9\u6570\u636e\u7684\u566a\u58f0\u3001\u5e72\u6270\u548c\u504f\u76f8\u5173\u7cfb\u95ee\u9898", "method": "\u4e24\u6b65\u6cd5\uff1a\u9996\u5148\u5bf9\u79bb\u7ebf\u6570\u636e\u8fdb\u884c\u6269\u5145\u589e\u52a0\u591a\u6837\u6027\uff0c\u7136\u540e\u4f7f\u7528\u6fc0\u5149\u6a21\u578b\u5728\u6f5c\u7a7a\u95f4\u751f\u6210\u989d\u5916\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e", "result": "\u5728Visual D4RL\u548cProcgen\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901a\u7528\u6027\u80fd\u529b\uff0c\u51cf\u5c0f\u4e86\u6d4b\u8bd5\u65f6\u7684\u901a\u7528\u6027\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u9700\u6539\u53d8\u73b0\u6709\u79bb\u7ebfRL\u7b97\u6cd5\u5c31\u80fd\u63d0\u5347\u901a\u7528\u6027\uff0c\u4e3a\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u66f4\u5177\u901a\u7528\u6027\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411"}}
{"id": "2508.12381", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12381", "abs": "https://arxiv.org/abs/2508.12381", "authors": ["Guo Tang", "Songhan Jiang", "Jinpeng Lu", "Linghan Cai", "Yongbing Zhang"], "title": "IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis", "comment": "13 pages, 5 figures", "summary": "Pathological images play an essential role in cancer prognosis, while\nsurvival analysis, which integrates computational techniques, can predict\ncritical clinical events such as patient mortality or disease recurrence from\nwhole-slide images (WSIs). Recent advancements in multiple instance learning\nhave significantly improved the efficiency of survival analysis. However,\nexisting methods often struggle to balance the modeling of long-range spatial\nrelationships with local contextual dependencies and typically lack inherent\ninterpretability, limiting their clinical utility. To address these challenges,\nwe propose the Interpretable Pathology Graph-Transformer (IPGPhormer), a novel\nframework that captures the characteristics of the tumor microenvironment and\nmodels their spatial dependencies across the tissue. IPGPhormer uniquely\nprovides interpretability at both tissue and cellular levels without requiring\npost-hoc manual annotations, enabling detailed analyses of individual WSIs and\ncross-cohort assessments. Comprehensive evaluations on four public benchmark\ndatasets demonstrate that IPGPhormer outperforms state-of-the-art methods in\nboth predictive accuracy and interpretability. In summary, our method,\nIPGPhormer, offers a promising tool for cancer prognosis assessment, paving the\nway for more reliable and interpretable decision-support systems in pathology.\nThe code is publicly available at\nhttps://anonymous.4open.science/r/IPGPhormer-6EEB.", "AI": {"tldr": "IPGPhormer\u662f\u4e00\u79cd\u7528\u4e8e\u764c\u75c7\u751f\u5b58\u5206\u6790\u7684\u53ef\u89e3\u91ca\u75c5\u7406\u56fe-Transformer\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u6355\u83b7\u80bf\u7624\u5fae\u73af\u5883\u7279\u5f81\u548c\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u751f\u5b58\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u957f\u8ddd\u79bb\u7a7a\u95f4\u5173\u7cfb\u5efa\u6a21\u4e0e\u5c40\u90e8\u4e0a\u4e0b\u6587\u4f9d\u8d56\uff0c\u4e14\u7f3a\u4e4f\u5185\u5728\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faInterpretable Pathology Graph-Transformer (IPGPhormer)\u6846\u67b6\uff0c\u901a\u8fc7\u56fe-Transformer\u7ed3\u6784\u6355\u83b7\u80bf\u7624\u5fae\u73af\u5883\u7279\u5f81\u548c\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u65e0\u9700\u540e\u5904\u7406\u624b\u52a8\u6807\u6ce8\u5373\u53ef\u63d0\u4f9b\u7ec4\u7ec7\u548c\u7ec6\u80de\u7ea7\u522b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cIPGPhormer\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "IPGPhormer\u4e3a\u764c\u75c7\u9884\u540e\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u5de5\u5177\uff0c\u4e3a\u75c5\u7406\u5b66\u4e2d\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.12384", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12384", "abs": "https://arxiv.org/abs/2508.12384", "authors": ["Hanwen Cao", "Haobo Lu", "Xiaosen Wang", "Kun He"], "title": "ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers", "comment": null, "summary": "Ensemble-based attacks have been proven to be effective in enhancing\nadversarial transferability by aggregating the outputs of models with various\narchitectures. However, existing research primarily focuses on refining\nensemble weights or optimizing the ensemble path, overlooking the exploration\nof ensemble models to enhance the transferability of adversarial attacks. To\naddress this gap, we propose applying adversarial augmentation to the surrogate\nmodels, aiming to boost overall generalization of ensemble models and reduce\nthe risk of adversarial overfitting. Meanwhile, observing that ensemble Vision\nTransformers (ViTs) gain less attention, we propose ViT-EnsembleAttack based on\nthe idea of model adversarial augmentation, the first ensemble-based attack\nmethod tailored for ViTs to the best of our knowledge. Our approach generates\naugmented models for each surrogate ViT using three strategies: Multi-head\ndropping, Attention score scaling, and MLP feature mixing, with the associated\nparameters optimized by Bayesian optimization. These adversarially augmented\nmodels are ensembled to generate adversarial examples. Furthermore, we\nintroduce Automatic Reweighting and Step Size Enlargement modules to boost\ntransferability. Extensive experiments demonstrate that ViT-EnsembleAttack\nsignificantly enhances the adversarial transferability of ensemble-based\nattacks on ViTs, outperforming existing methods by a substantial margin. Code\nis available at https://github.com/Trustworthy-AI-Group/TransferAttack.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u66ff\u4ee3ViT\u6a21\u578b\u8fdb\u884c\u5bf9\u6297\u589e\u5e3c\uff08\u591a\u5934\u6295\u5f03\u3001\u6ce8\u610f\u529b\u7ed9\u5206\u7f29\u653e\u3001MLP\u7279\u5f81\u6df7\u5408\uff09\uff0c\u7ec4\u5408\u81ea\u52a8\u91cd\u65b0\u52a0\u6743\u548c\u6b65\u957f\u6269\u5927\u6a21\u5757\uff0c\u63d0\u5347\u4e86ViT\u96c6\u6210\u653b\u51fb\u7684\u53ef\u8f6c\u79fb\u6027\u3002", "motivation": "\u73b0\u6709\u96c6\u6210\u653b\u51fb\u4e3b\u8981\u5173\u6ce8\u6743\u91cd\u7cbe\u70bc\u6216\u96c6\u6210\u8def\u5f84\u4f18\u5316\uff0c\u5ffd\u89c6\u4e86\u901a\u8fc7\u589e\u5f3a\u66ff\u4ee3\u6a21\u578b\u6765\u63d0\u5347\u653b\u51fb\u53ef\u8f6c\u79fb\u6027\u7684\u6f5c\u529b\u3002\u540c\u65f6ViT\u96c6\u6210\u653b\u51fb\u5f97\u5230\u7684\u5173\u6ce8\u8f83\u5c11\u3002", "method": "\u63d0\u51faViT-EnsembleAttack\uff1a1\uff09\u4f7f\u7528\u4e09\u79cd\u5bf9\u6297\u589e\u5e3c\u7b56\u7565\uff08\u591a\u5934\u6295\u5f03\u3001\u6ce8\u610f\u529b\u5206\u6570\u7f29\u653e\u3001MLP\u7279\u5f81\u6df7\u5408\uff09\u751f\u6210\u589e\u5f3a\u7684\u66ff\u4ee3\u6a21\u578b\uff1b2\uff09\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u8c03\u6574\u53c2\u6570\uff1b3\uff09\u96c6\u6210\u8fd9\u4e9b\u589e\u5f3a\u6a21\u578b\u751f\u6210\u5bf9\u6297\u6837\u672c\uff1b4\uff09\u6dfb\u52a0\u81ea\u52a8\u91cd\u65b0\u52a0\u6743\u548c\u6b65\u957f\u6269\u5927\u6a21\u5757\u6765\u63d0\u5347\u53ef\u8f6c\u79fb\u6027\u3002", "result": "\u5e9e\u5927\u5b9e\u9a8c\u8868\u660eViT-EnsembleAttack\u663e\u8457\u63d0\u5347\u4e86ViT\u96c6\u6210\u653b\u51fb\u7684\u5bf9\u6297\u53ef\u8f6c\u79fb\u6027\uff0c\u8fdc\u8d85\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u66ff\u4ee3ViT\u6a21\u578b\u8fdb\u884c\u5bf9\u6297\u589e\u5e3c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u96c6\u6210\u653b\u51fb\u7684\u53ef\u8f6c\u79fb\u6027\uff0c\u51cf\u5c11\u5bf9\u6297\u8fc7\u62df\u5408\u98ce\u9669\uff0c\u4e3aViT\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.12396", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12396", "abs": "https://arxiv.org/abs/2508.12396", "authors": ["Xiaochuan Lin", "Xiangyong Chen", "Xuan Li", "Yichen Su"], "title": "DeCoT: Decomposing Complex Instructions for Enhanced Text-to-Image Generation with Large Language Models", "comment": null, "summary": "Despite remarkable advancements, current Text-to-Image (T2I) models struggle\nwith complex, long-form textual instructions, frequently failing to accurately\nrender intricate details, spatial relationships, or specific constraints. This\nlimitation is highlighted by benchmarks such as LongBench-T2I, which reveal\ndeficiencies in handling composition, specific text, and fine textures. To\naddress this, we propose DeCoT (Decomposition-CoT), a novel framework that\nleverages Large Language Models (LLMs) to significantly enhance T2I models'\nunderstanding and execution of complex instructions. DeCoT operates in two core\nstages: first, Complex Instruction Decomposition and Semantic Enhancement,\nwhere an LLM breaks down raw instructions into structured, actionable semantic\nunits and clarifies ambiguities; second, Multi-Stage Prompt Integration and\nAdaptive Generation, which transforms these units into a hierarchical or\noptimized single prompt tailored for existing T2I models. Extensive experiments\non the LongBench-T2I dataset demonstrate that DeCoT consistently and\nsubstantially improves the performance of leading T2I models across all\nevaluated dimensions, particularly in challenging aspects like \"Text\" and\n\"Composition\". Quantitative results, validated by multiple MLLM evaluators\n(Gemini-2.0-Flash and InternVL3-78B), show that DeCoT, when integrated with\nInfinity-8B, achieves an average score of 3.52, outperforming the baseline\nInfinity-8B (3.44). Ablation studies confirm the critical contribution of each\nDeCoT component and the importance of sophisticated LLM prompting. Furthermore,\nhuman evaluations corroborate these findings, indicating superior perceptual\nquality and instruction fidelity. DeCoT effectively bridges the gap between\nhigh-level user intent and T2I model requirements, leading to more faithful and\naccurate image generation.", "AI": {"tldr": "DeCoT\u662f\u4e00\u4e2a\u901a\u8fc7LLM\u5206\u89e3\u590d\u6742\u6587\u672c\u6307\u4ee4\u4e3a\u7ed3\u6784\u5316\u8bed\u4e49\u5355\u5143\uff0c\u518d\u6574\u5408\u4e3a\u5206\u5c42\u63d0\u793a\u6765\u63d0\u5347T2I\u6a21\u578b\u6027\u80fd\u7684\u6846\u67b6\uff0c\u5728LongBench-T2I\u57fa\u51c6\u4e0a\u663e\u8457\u6539\u5584\u4e86\u6587\u672c\u548c\u6784\u56fe\u7b49\u6311\u6218\u6027\u4efb\u52a1\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524dT2I\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u957f\u6587\u672c\u6307\u4ee4\u65f6\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u7ecf\u5e38\u65e0\u6cd5\u51c6\u786e\u6e32\u67d3\u7ec6\u8282\u3001\u7a7a\u95f4\u5173\u7cfb\u548c\u7279\u5b9a\u7ea6\u675f\uff0cLongBench-T2I\u7b49\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u5728\u7ec4\u5408\u6027\u3001\u6587\u672c\u51c6\u786e\u6027\u548c\u7cbe\u7ec6\u7eb9\u7406\u65b9\u9762\u7684\u7f3a\u9677\u3002", "method": "DeCoT\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u9636\u6bb5\uff1a1\uff09\u590d\u6742\u6307\u4ee4\u5206\u89e3\u4e0e\u8bed\u4e49\u589e\u5f3a - \u4f7f\u7528LLM\u5c06\u539f\u59cb\u6307\u4ee4\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u64cd\u4f5c\u7684\u8bed\u4e49\u5355\u5143\u5e76\u6f84\u6e05\u6b67\u4e49\uff1b2\uff09\u591a\u9636\u6bb5\u63d0\u793a\u6574\u5408\u4e0e\u81ea\u9002\u5e94\u751f\u6210 - \u5c06\u8fd9\u4e9b\u5355\u5143\u8f6c\u6362\u4e3a\u5206\u5c42\u6216\u4f18\u5316\u7684\u5355\u4e00\u63d0\u793a\uff0c\u9002\u914d\u73b0\u6709T2I\u6a21\u578b\u3002", "result": "\u5728LongBench-T2I\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDeCoT\u663e\u8457\u63d0\u5347\u4e86\u9886\u5148T2I\u6a21\u578b\u5728\u6240\u6709\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\"\u6587\u672c\"\u548c\"\u6784\u56fe\"\u7b49\u6311\u6218\u6027\u65b9\u9762\u3002\u4e0eInfinity-8B\u96c6\u6210\u65f6\u5e73\u5747\u5f97\u5206\u4ece3.44\u63d0\u5347\u52303.52\uff0c\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86\u611f\u77e5\u8d28\u91cf\u548c\u6307\u4ee4\u4fdd\u771f\u5ea6\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "DeCoT\u6709\u6548\u5f25\u5408\u4e86\u9ad8\u7ea7\u7528\u6237\u610f\u56fe\u4e0eT2I\u6a21\u578b\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\u66f4\u5fe0\u5b9e\u548c\u51c6\u786e\u7684\u56fe\u50cf\u751f\u6210\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u5404\u7ec4\u4ef6\u7684\u5173\u952e\u8d21\u732e\u548c\u590d\u6742LLM\u63d0\u793a\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.12399", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12399", "abs": "https://arxiv.org/abs/2508.12399", "authors": ["Suraj Prasad", "Navyansh Mahla", "Sunny Gupta", "Amit Sethi"], "title": "Federated Cross-Modal Style-Aware Prompt Generation", "comment": null, "summary": "Prompt learning has propelled vision-language models like CLIP to excel in\ndiverse tasks, making them ideal for federated learning due to computational\nefficiency. However, conventional approaches that rely solely on final-layer\nfeatures miss out on rich multi-scale visual cues and domain-specific style\nvariations in decentralized client data. To bridge this gap, we introduce\nFedCSAP (Federated Cross-Modal Style-Aware Prompt Generation). Our framework\nharnesses low, mid, and high-level features from CLIP's vision encoder\nalongside client-specific style indicators derived from batch-level statistics.\nBy merging intricate visual details with textual context, FedCSAP produces\nrobust, context-aware prompt tokens that are both distinct and non-redundant,\nthereby boosting generalization across seen and unseen classes. Operating\nwithin a federated learning paradigm, our approach ensures data privacy through\nlocal training and global aggregation, adeptly handling non-IID class\ndistributions and diverse domain-specific styles. Comprehensive experiments on\nmultiple image classification datasets confirm that FedCSAP outperforms\nexisting federated prompt learning methods in both accuracy and overall\ngeneralization.", "AI": {"tldr": "FedCSAP\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528CLIP\u6a21\u578b\u7684\u591a\u5c3a\u5ea6\u89c6\u89c9\u7279\u5f81\u548c\u5ba2\u6237\u7aef\u7279\u5b9a\u98ce\u683c\u6307\u6807\u6765\u751f\u6210\u9c81\u68d2\u7684\u8de8\u6a21\u6001\u63d0\u793a\uff0c\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u4ec5\u4f7f\u7528\u6700\u7ec8\u5c42\u7279\u5f81\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u5c3a\u5ea6\u89c6\u89c9\u7ebf\u7d22\u548c\u5ba2\u6237\u7aef\u6570\u636e\u7684\u9886\u57df\u7279\u5b9a\u98ce\u683c\u53d8\u5316\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u975eIID\u6570\u636e\u5206\u5e03\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u4eceCLIP\u89c6\u89c9\u7f16\u7801\u5668\u63d0\u53d6\u4f4e\u3001\u4e2d\u3001\u9ad8\u4e09\u4e2a\u5c42\u6b21\u7684\u7279\u5f81\uff0c\u7ed3\u5408\u5ba2\u6237\u7aef\u6279\u5904\u7406\u7edf\u8ba1\u4fe1\u606f\u751f\u6210\u98ce\u683c\u6307\u6807\uff0c\u5c06\u7cbe\u7ec6\u89c6\u89c9\u7ec6\u8282\u4e0e\u6587\u672c\u4e0a\u4e0b\u6587\u878d\u5408\u6765\u751f\u6210\u72ec\u7279\u4e14\u975e\u5197\u4f59\u7684\u63d0\u793atoken\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedCSAP\u5728\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "FedCSAP\u901a\u8fc7\u6709\u6548\u6574\u5408\u591a\u5c3a\u5ea6\u89c6\u89c9\u7279\u5f81\u548c\u5ba2\u6237\u7aef\u98ce\u683c\u4fe1\u606f\uff0c\u5728\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e0b\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8de8\u57df\u6cdb\u5316\u6027\u80fd\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2508.12400", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12400", "abs": "https://arxiv.org/abs/2508.12400", "authors": ["Amirul Rahman", "Qiang Xu", "Xueying Huang"], "title": "MPCAR: Multi-Perspective Contextual Augmentation for Enhanced Visual Reasoning in Large Vision-Language Models", "comment": null, "summary": "Despite significant advancements, Large Vision-Language Models (LVLMs)\ncontinue to face challenges in complex visual reasoning tasks that demand deep\ncontextual understanding, multi-angle analysis, or meticulous detail\nrecognition. Existing approaches often rely on single-shot image encoding and\nprompts, limiting their ability to fully capture nuanced visual information.\nInspired by the notion that strategically generated \"additional\" information\ncan serve as beneficial contextual augmentation, we propose Multi-Perspective\nContextual Augmentation for Reasoning (MPCAR), a novel inference-time strategy\ndesigned to enhance LVLM performance. MPCAR operates in three stages: first, an\nLVLM generates N diverse and complementary descriptions or preliminary\nreasoning paths from various angles; second, these descriptions are\nintelligently integrated with the original question to construct a\ncomprehensive context-augmented prompt; and finally, this enriched prompt\nguides the ultimate LVLM for deep reasoning and final answer generation.\nCrucially, MPCAR achieves these enhancements without requiring any fine-tuning\nof the underlying LVLM's parameters. Extensive experiments on challenging\nVisual Question Answering (VQA) datasets, including GQA, VQA-CP v2, and\nScienceQA (Image-VQA), demonstrate that MPCAR consistently outperforms\nestablished baseline methods. Our quantitative results show significant\naccuracy gains, particularly on tasks requiring robust contextual\nunderstanding, while human evaluations confirm improved coherence and\ncompleteness of the generated answers. Ablation studies further highlight the\nimportance of diverse prompt templates and the number of generated\nperspectives. This work underscores the efficacy of leveraging LVLMs' inherent\ngenerative capabilities to enrich input contexts, thereby unlocking their\nlatent reasoning potential for complex multimodal tasks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u7684\u63a8\u7406\u65f6\u7b56\u7565MPCAR\uff0c\u901a\u8fc7\u591a\u89d2\u5ea6\u751f\u6210\u8865\u5145\u63cf\u8ff0\u6765\u589e\u5f3a\u5927\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u5355\u6b21\u56fe\u50cf\u7f16\u7801\u548c\u63d0\u793a\u65b9\u5f0f\u9650\u5236\u4e86\u5bf9\u7ec6\u817b\u89c6\u89c9\u4fe1\u606f\u7684\u5b8c\u6574\u6293\u53d6\u80fd\u529b\u3002", "method": "MPCAR\u7b97\u6cd5\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u4ece\u591a\u4e2a\u89d2\u5ea6\u751f\u6210N\u4e2a\u5f02\u8d28\u8865\u5145\u63cf\u8ff0\uff1b2\uff09\u5c06\u8fd9\u4e9b\u63cf\u8ff0\u4e0e\u539f\u95ee\u9898\u667a\u80fd\u6574\u5408\u6784\u5efa\u4e30\u5bcc\u63d0\u793a\uff1b3\uff09\u4f7f\u7528\u589e\u5f3a\u63d0\u793a\u8fdb\u884c\u6df1\u5ea6\u7406\u89e3\u548c\u7b54\u6848\u751f\u6210\u3002", "result": "\u5728GQA\u3001VQA-CP v2\u548cScienceQA\u7b49\u5177\u6709\u6311\u6218\u6027\u7684VQA\u6570\u636e\u96c6\u4e0a\uff0cMPCAR\u6301\u7eed\u8d85\u8fc7\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u9700\u8981\u6df1\u5ea6\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\uff0c\u4eba\u5de5\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86\u7b54\u6848\u7684\u8fde\u8d2f\u6027\u548c\u5b8c\u6574\u6027\u63d0\u5347\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\u4e86\u5229\u7528\u5927\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u6765\u4e30\u5bcc\u8f93\u5165\u4e0a\u4e0b\u6587\uff0c\u53ef\u4ee5\u91ca\u653e\u5176\u6f5c\u5728\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u591a\u6a21\u6001\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12404", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12404", "abs": "https://arxiv.org/abs/2508.12404", "authors": ["Nan Song", "Bozhou Zhang", "Xiatian Zhu", "Jiankang Deng", "Li Zhang"], "title": "LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving", "comment": "7 pages, 4 figures,", "summary": "Large vision-language models (VLMs) have shown promising capabilities in\nscene understanding, enhancing the explainability of driving behaviors and\ninteractivity with users. Existing methods primarily fine-tune VLMs on on-board\nmulti-view images and scene reasoning text, but this approach often lacks the\nholistic and nuanced scene recognition and powerful spatial awareness required\nfor autonomous driving, especially in complex situations. To address this gap,\nwe propose a novel vision-language framework tailored for autonomous driving,\ncalled LMAD. Our framework emulates modern end-to-end driving paradigms by\nincorporating comprehensive scene understanding and a task-specialized\nstructure with VLMs. In particular, we introduce preliminary scene interaction\nand specialized expert adapters within the same driving task structure, which\nbetter align VLMs with autonomous driving scenarios. Furthermore, our approach\nis designed to be fully compatible with existing VLMs while seamlessly\nintegrating with planning-oriented driving systems. Extensive experiments on\nthe DriveLM and nuScenes-QA datasets demonstrate that LMAD significantly boosts\nthe performance of existing VLMs on driving reasoning tasks,setting a new\nstandard in explainable autonomous driving.", "AI": {"tldr": "LMAD\u662f\u4e00\u4e2a\u4e13\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8bbe\u8ba1\u7684\u89c6\u89c9\u8bed\u8a00\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u521d\u6b65\u573a\u666f\u4ea4\u4e92\u548c\u4e13\u5bb6\u9002\u914d\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709VLM\u5728\u9a7e\u9a76\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5728\u591a\u89c6\u89d2\u56fe\u50cf\u548c\u573a\u666f\u63a8\u7406\u6587\u672c\u4e0a\u5fae\u8c03VLM\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u81ea\u52a8\u9a7e\u9a76\u6240\u9700\u7684\u6574\u4f53\u7ec6\u81f4\u573a\u666f\u8bc6\u522b\u548c\u5f3a\u5927\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u60c5\u51b5\u4e0b", "method": "\u63d0\u51faLMAD\u6846\u67b6\uff0c\u6a21\u62df\u73b0\u4ee3\u7aef\u5230\u7aef\u9a7e\u9a76\u8303\u5f0f\uff0c\u5305\u542b\u5168\u9762\u573a\u666f\u7406\u89e3\u548c\u4efb\u52a1\u4e13\u7528\u7ed3\u6784\uff0c\u5f15\u5165\u521d\u6b65\u573a\u666f\u4ea4\u4e92\u548c\u4e13\u5bb6\u9002\u914d\u5668\uff0c\u4e0e\u73b0\u6709VLM\u5b8c\u5168\u517c\u5bb9\u5e76\u53ef\u4e0e\u89c4\u5212\u5bfc\u5411\u9a7e\u9a76\u7cfb\u7edf\u65e0\u7f1d\u96c6\u6210", "result": "\u5728DriveLM\u548cnuScenes-QA\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLMAD\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709VLM\u5728\u9a7e\u9a76\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd", "conclusion": "LMAD\u4e3a\u53ef\u89e3\u91ca\u81ea\u52a8\u9a7e\u9a76\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u901a\u8fc7\u66f4\u597d\u7684VLM\u4e0e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2508.12409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12409", "abs": "https://arxiv.org/abs/2508.12409", "authors": ["Liang Lv", "Di Wang", "Jing Zhang", "Lefei Zhang"], "title": "S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing", "comment": null, "summary": "Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS)\nanalysis by leveraging unlabeled data through pseudo-labeling and consistency\nlearning. However, existing S4 studies often rely on small-scale datasets and\nmodels, limiting their practical applicability. To address this, we propose S5,\nthe first scalable framework for semi-supervised semantic segmentation in RS,\nwhich unlocks the potential of vast unlabeled Earth observation data typically\nunderutilized due to costly pixel-level annotations. Built upon existing\nlarge-scale RS datasets, S5 introduces a data selection strategy that\nintegrates entropy-based filtering and diversity expansion, resulting in the\nRS4P-1M dataset. Using this dataset, we systematically scales S4 methods by\npre-training RS foundation models (RSFMs) of varying sizes on this extensive\ncorpus, significantly boosting their performance on land cover segmentation and\nobject detection tasks. Furthermore, during fine-tuning, we incorporate a\nMixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which\nenables efficient adaptation to multiple RS benchmarks with fewer parameters.\nThis approach improves the generalization and versatility of RSFMs across\ndiverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance\nacross all benchmarks, underscoring the viability of scaling semi-supervised\nlearning for RS applications. All datasets, code, and models will be released\nat https://github.com/MiliLab/S5", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86S5\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u9501\u5927\u89c4\u6a21\u672a\u6807\u6ce8\u5730\u7403\u89c2\u6d4b\u6570\u636e\u7684\u6f5c\u529b\uff0c\u5b9e\u73b0\u4e86\u8fdc\u611f\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u53ef\u6269\u5c55\u6027\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u8fdc\u611f\u6d4b\u8bd5\u96c6\u4e0a\u521b\u9020\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u591a\u57fa\u4e8e\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u80fd\u529b\u3002\u9700\u8981\u89e3\u51b3\u5927\u89c4\u6a21\u672a\u6807\u6ce8\u5730\u7403\u89c2\u6d4b\u6570\u636e\u5229\u7528\u4e0d\u5145\u5206\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efaRS4P-1M\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7edf\u4e00\u9884\u8bad\u7ec3\u4e0d\u540c\u89c4\u6a21\u7684\u8fdc\u611f\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u5728\u5fae\u8c03\u9636\u6bb5\u91c7\u7528\u4e13\u5bb6\u6df7\u5408\u7b56\u7565\u8fdb\u884c\u591a\u6570\u636e\u96c6\u5fae\u8c03\u3002", "result": "\u5728\u571f\u5730\u8986\u76d6\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e0a\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5728\u6240\u6709\u8fdc\u611f\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u8bc1\u660e\u4e86\u901a\u8fc7\u6269\u5c55\u534a\u76d1\u7763\u5b66\u4e60\u6765\u5229\u7528\u5927\u89c4\u6a21\u672a\u6807\u6ce8\u8fdc\u611f\u6570\u636e\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u8fdc\u611f\u5206\u6790\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12410", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12410", "abs": "https://arxiv.org/abs/2508.12410", "authors": ["Jun Zeng", "Yannan Huang", "Elif Keles", "Halil Ertugrul Aktas", "Gorkem Durak", "Nikhil Kumar Tomar", "Quoc-Huy Trinh", "Deepak Ranjan Nayak", "Ulas Bagci", "Debesh Jha"], "title": "SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes", "comment": "9 pages, 4 figures", "summary": "Liver Cirrhosis plays a critical role in the prognosis of chronic liver\ndisease. Early detection and timely intervention are critical in significantly\nreducing mortality rates. However, the intricate anatomical architecture and\ndiverse pathological changes of liver tissue complicate the accurate detection\nand characterization of lesions in clinical settings. Existing methods\nunderutilize the spatial anatomical details in volumetric MRI data, thereby\nhindering their clinical effectiveness and explainability. To address this\nchallenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to\nmodel the spatial relationships within the complex anatomical structures of MRI\nvolumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba),\nSRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and\ncombines anatomical information from the sagittal, coronal, and axial planes to\nconstruct a global spatial context representation, enabling efficient\nvolumetric segmentation of pathological liver structures. Furthermore, we\nintroduce the Spatial Reverse Attention module (SRMA), designed to\nprogressively refine cirrhotic details in the segmentation map, utilizing both\nthe coarse segmentation map and hierarchical encoding features. Extensive\nexperiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods,\ndelivering exceptional performance in 3D pathological liver segmentation. Our\ncode is available for public:\n{\\color{blue}{https://github.com/JunZengz/SRMA-Mamba}}.", "AI": {"tldr": "\u63d0\u51faSRMA-Mamba\u7f51\u7edc\uff0c\u901a\u8fc7\u7a7a\u95f4\u89e3\u5256\u611f\u77e5\u7684Mamba\u6a21\u5757\u548c\u7a7a\u95f4\u53cd\u5411\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5b9e\u73b0\u809d\u810fMRI\u4f53\u79ef\u6570\u636e\u7684\u7cbe\u786e\u75c5\u7406\u5206\u5272\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u809d\u786c\u5316\u65e9\u671f\u68c0\u6d4b\u5bf9\u964d\u4f4e\u6b7b\u4ea1\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528MRI\u4f53\u79ef\u6570\u636e\u4e2d\u7684\u7a7a\u95f4\u89e3\u5256\u7ec6\u8282\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u6548\u679c\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u96c6\u6210SABMamba\u6a21\u5757\u5728\u809d\u786c\u5316\u7ec4\u7ec7\u5185\u8fdb\u884c\u9009\u62e9\u6027Mamba\u626b\u63cf\uff0c\u7ed3\u5408\u4e09\u5e73\u9762\u89e3\u5256\u4fe1\u606f\u6784\u5efa\u5168\u5c40\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff1b\u5f15\u5165SRMA\u6a21\u5757\u5229\u7528\u7c97\u5206\u5272\u56fe\u548c\u5206\u5c42\u7f16\u7801\u7279\u5f81\u9010\u6b65\u7ec6\u5316\u5206\u5272\u7ec6\u8282\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eSRMA-Mamba\u57283D\u75c5\u7406\u809d\u810f\u5206\u5272\u65b9\u9762\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "SRMA-Mamba\u901a\u8fc7\u6709\u6548\u5efa\u6a21MRI\u4f53\u79ef\u4e2d\u7684\u7a7a\u95f4\u89e3\u5256\u5173\u7cfb\uff0c\u4e3a\u809d\u786c\u5316\u75c5\u53d8\u7684\u7cbe\u786e\u68c0\u6d4b\u548c\u8868\u5f81\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12415", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12415", "abs": "https://arxiv.org/abs/2508.12415", "authors": ["Ke Xing", "Hanwen Liang", "Dejia Xu", "Yuyang Yin", "Konstantinos N. Plataniotis", "Yao Zhao", "Yunchao Wei"], "title": "TiP4GEN: Text to Immersive Panorama 4D Scene Generation", "comment": null, "summary": "With the rapid advancement and widespread adoption of VR/AR technologies,\nthere is a growing demand for the creation of high-quality, immersive dynamic\nscenes. However, existing generation works predominantly concentrate on the\ncreation of static scenes or narrow perspective-view dynamic scenes, falling\nshort of delivering a truly 360-degree immersive experience from any viewpoint.\nIn this paper, we introduce \\textbf{TiP4GEN}, an advanced text-to-dynamic\npanorama scene generation framework that enables fine-grained content control\nand synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN\nintegrates panorama video generation and dynamic scene reconstruction to create\n360-degree immersive virtual environments. For video generation, we introduce a\n\\textbf{Dual-branch Generation Model} consisting of a panorama branch and a\nperspective branch, responsible for global and local view generation,\nrespectively. A bidirectional cross-attention mechanism facilitates\ncomprehensive information exchange between the branches. For scene\nreconstruction, we propose a \\textbf{Geometry-aligned Reconstruction Model}\nbased on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using\nmetric depth maps and initializing scene cameras with estimated poses, our\nmethod ensures geometric consistency and temporal coherence for the\nreconstructed scenes. Extensive experiments demonstrate the effectiveness of\nour proposed designs and the superiority of TiP4GEN in generating visually\ncompelling and motion-coherent dynamic panoramic scenes. Our project page is at\nhttps://ke-xing.github.io/TiP4GEN/.", "AI": {"tldr": "TiP4GEN\u662f\u4e00\u4e2a\u6587\u672c\u5230\u52a8\u6001\u5168\u666f\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u751f\u6210\u6a21\u578b\u548c\u51e0\u4f55\u5bf9\u9f50\u91cd\u5efa\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86360\u5ea6\u6c89\u6d78\u5f0f\u52a8\u6001\u573a\u666f\u7684\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89d2\u5e7f\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740VR/AR\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5bf9\u9ad8\u8d28\u91cf\u6c89\u6d78\u5f0f\u52a8\u6001\u573a\u666f\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u573a\u666f\u6216\u7a84\u89c6\u89d2\u52a8\u6001\u573a\u666f\uff0c\u65e0\u6cd5\u63d0\u4f9b\u771f\u6b63\u7684360\u5ea6\u6c89\u6d78\u4f53\u9a8c\u3002", "method": "\u63d0\u51fa\u53cc\u5206\u652f\u751f\u6210\u6a21\u578b\uff08\u5168\u666f\u5206\u652f\u548c\u900f\u89c6\u5206\u652f\uff09\u8fdb\u884c\u89c6\u9891\u751f\u6210\uff0c\u901a\u8fc7\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u4fe1\u606f\u4ea4\u6362\uff1b\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u51e0\u4f55\u5bf9\u9f50\u91cd\u5efa\u6a21\u578b\uff0c\u5229\u7528\u5ea6\u91cf\u6df1\u5ea6\u56fe\u5bf9\u9f50\u65f6\u7a7a\u70b9\u4e91\uff0c\u786e\u4fdd\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u8bbe\u8ba1\u7684\u6709\u6548\u6027\uff0cTiP4GEN\u5728\u751f\u6210\u89c6\u89c9\u5438\u5f15\u4eba\u4e14\u8fd0\u52a8\u8fde\u8d2f\u7684\u52a8\u6001\u5168\u666f\u573a\u666f\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "TiP4GEN\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u6587\u672c\u751f\u6210\u9ad8\u8d28\u91cf360\u5ea6\u6c89\u6d78\u5f0f\u52a8\u6001\u573a\u666f\uff0c\u4e3aVR/AR\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12422", "abs": "https://arxiv.org/abs/2508.12422", "authors": ["Jianyi Yang", "Junyi Ye", "Ankan Dash", "Guiling Wang"], "title": "Illusions in Humans and AI: How Visual Perception Aligns and Diverges", "comment": null, "summary": "By comparing biological and artificial perception through the lens of\nillusions, we highlight critical differences in how each system constructs\nvisual reality. Understanding these divergences can inform the development of\nmore robust, interpretable, and human-aligned artificial intelligence (AI)\nvision systems. In particular, visual illusions expose how human perception is\nbased on contextual assumptions rather than raw sensory data. As artificial\nvision systems increasingly perform human-like tasks, it is important to ask:\ndoes AI experience illusions, too? Does it have unique illusions? This article\nexplores how AI responds to classic visual illusions that involve color, size,\nshape, and motion. We find that some illusion-like effects can emerge in these\nmodels, either through targeted training or as by-products of pattern\nrecognition. In contrast, we also identify illusions unique to AI, such as\npixel-level sensitivity and hallucinations, that lack human counterparts. By\nsystematically comparing human and AI responses to visual illusions, we uncover\nalignment gaps and AI-specific perceptual vulnerabilities invisible to human\nperception. These findings provide insights for future research on vision\nsystems that preserve human-beneficial perceptual biases while avoiding\ndistortions that undermine trust and safety.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u6bd4\u751f\u7269\u548c\u4eba\u5de5\u667a\u80fd\u89c6\u89c9\u7cfb\u7edf\u5728\u89c6\u89c9\u5e7b\u89c9\u4e0a\u7684\u5dee\u5f02\uff0c\u63ed\u793a\u4e86AI\u89c6\u89c9\u7684\u7279\u6709\u5f31\u70b9\u548c\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u5f00\u53d1\u66f4\u7a33\u5065\u53ef\u89e3\u91ca\u7684AI\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "motivation": "\u7406\u89e3\u4eba\u7c7b\u548cAI\u89c6\u89c9\u7cfb\u7edf\u5728\u6784\u5efa\u89c6\u89c9\u73b0\u5b9e\u65b9\u9762\u7684\u6839\u672c\u5dee\u5f02\uff0c\u4ee5\u53d1\u5c55\u66f4\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u6027\u66f4\u9ad8\u4e14\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u4eba\u5de5\u667a\u80fd\u89c6\u89c9\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5bf9\u6bd4\u4eba\u7c7b\u548cAI\u5bf9\u7ecf\u5178\u89c6\u89c9\u5e7b\u89c9\uff08\u989c\u8272\u3001\u5927\u5c0f\u3001\u5f62\u72b6\u3001\u8fd0\u52a8\uff09\u7684\u53cd\u5e94\uff0c\u5206\u6790AI\u6a21\u578b\u4e2d\u51fa\u73b0\u7684\u5e7b\u89c9\u6548\u5e94\u548c\u72ec\u7279\u5e7b\u89c9\u3002", "result": "\u53d1\u73b0AI\u4f1a\u51fa\u73b0\u67d0\u4e9b\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5e7b\u89c9\u6548\u5e94\uff0c\u540c\u65f6\u4e5f\u6709\u72ec\u7279\u7684AI\u5e7b\u89c9\uff08\u5982\u50cf\u7d20\u7ea7\u654f\u611f\u6027\u3001\u5e7b\u89c9\uff09\u3002\u63ed\u793a\u4e86AI\u7279\u6709\u7684\u611f\u77e5\u5f31\u70b9\u548c\u4e0e\u4eba\u7c7b\u7684\u5bf9\u9f50\u95f4\u9699\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5f00\u53d1\u4fdd\u6301\u4eba\u7c7b\u6709\u76ca\u611f\u77e5\u504f\u89c1\u3001\u907f\u514d\u7834\u574f\u4fe1\u4efb\u548c\u5b89\u5168\u7684\u6291\u523b\u5e7b\u89c9\u7684\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2508.12430", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12430", "abs": "https://arxiv.org/abs/2508.12430", "authors": ["Yahsin Yeh", "Yilun Wu", "Bokai Ruan", "Honghan Shuai"], "title": "Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations", "comment": null, "summary": "Natural language explanations in visual question answering (VQA-NLE) aim to\nmake black-box models more transparent by elucidating their decision-making\nprocesses. However, we find that existing VQA-NLE systems can produce\ninconsistent explanations and reach conclusions without genuinely understanding\nthe underlying context, exposing weaknesses in either their inference pipeline\nor explanation-generation mechanism. To highlight these vulnerabilities, we not\nonly leverage an existing adversarial strategy to perturb questions but also\npropose a novel strategy that minimally alters images to induce contradictory\nor spurious outputs. We further introduce a mitigation method that leverages\nexternal knowledge to alleviate these inconsistencies, thereby bolstering model\nrobustness. Extensive evaluations on two standard benchmarks and two widely\nused VQA-NLE models underscore the effectiveness of our attacks and the\npotential of knowledge-based defenses, ultimately revealing pressing security\nand reliability concerns in current VQA-NLE systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63ed\u793a\u4e86\u89c6\u89c9\u95ee\u7b54\u4e2d\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u7cfb\u7edf\u7684\u6f0f\u6d1e\uff0c\u901a\u8fc7\u653b\u51fb\u548c\u9632\u5fa1\u65b9\u6cd5\u63d0\u9ad8\u6a21\u578b\u7684\u7a33\u5065\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u7684VQA-NLE\u7cfb\u7edf\u5b58\u5728\u89e3\u91ca\u4e0d\u4e00\u81f4\u548c\u7406\u89e3\u4e0d\u6df1\u5165\u7684\u95ee\u9898\uff0c\u9700\u8981\u63ed\u793a\u8fd9\u4e9b\u6f0f\u6d1e\u5e76\u63d0\u51fa\u6539\u5584\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5bf9\u6297\u6027\u95ee\u9898\u6270\u52a8\u548c\u65b0\u7684\u56fe\u50cf\u6700\u5c0f\u4fee\u6539\u7b56\u7565\u6765\u5bfc\u81f4\u77db\u76fe\u8f93\u51fa\uff0c\u5e76\u91c7\u7528\u5916\u90e8\u77e5\u8bc6\u6765\u7f13\u89e3\u4e0d\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u6807\u51c6\u6d4b\u8bd5\u96c6\u548c\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684VQA-NLE\u6a21\u578b\u4e0a\uff0c\u653b\u51fb\u65b9\u6cd5\u663e\u793a\u51fa\u9ad8\u6548\u6027\uff0c\u77e5\u8bc6\u57fa\u7840\u9632\u5fa1\u65b9\u6848\u663e\u793a\u4e86\u63d0\u9ad8\u6a21\u578b\u7a33\u5065\u6027\u7684\u6f5c\u529b\u3002", "conclusion": "\u5f53\u524dVQA-NLE\u7cfb\u7edf\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u77e5\u8bc6\u57fa\u7840\u7684\u9632\u5fa1\u65b9\u6cd5\u4e3a\u63d0\u5347\u6a21\u578b\u7a33\u5065\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.12455", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12455", "abs": "https://arxiv.org/abs/2508.12455", "authors": ["Chee Ng", "Liliang Sun", "Shaoqing Tang"], "title": "X-Ray-CoT: Interpretable Chest X-ray Diagnosis with Vision-Language Models via Chain-of-Thought Reasoning", "comment": null, "summary": "Chest X-ray imaging is crucial for diagnosing pulmonary and cardiac diseases,\nyet its interpretation demands extensive clinical experience and suffers from\ninter-observer variability. While deep learning models offer high diagnostic\naccuracy, their black-box nature hinders clinical adoption in high-stakes\nmedical settings. To address this, we propose X-Ray-CoT (Chest X-Ray\nChain-of-Thought), a novel framework leveraging Vision-Language Large Models\n(LVLMs) for intelligent chest X-ray diagnosis and interpretable report\ngeneration. X-Ray-CoT simulates human radiologists' \"chain-of-thought\" by first\nextracting multi-modal features and visual concepts, then employing an\nLLM-based component with a structured Chain-of-Thought prompting strategy to\nreason and produce detailed natural language diagnostic reports. Evaluated on\nthe CORDA dataset, X-Ray-CoT achieves competitive quantitative performance,\nwith a Balanced Accuracy of 80.52% and F1 score of 78.65% for disease\ndiagnosis, slightly surpassing existing black-box models. Crucially, it\nuniquely generates high-quality, explainable reports, as validated by\npreliminary human evaluations. Our ablation studies confirm the integral role\nof each proposed component, highlighting the necessity of multi-modal fusion\nand CoT reasoning for robust and transparent medical AI. This work represents a\nsignificant step towards trustworthy and clinically actionable AI systems in\nmedical imaging.", "AI": {"tldr": "X-Ray-CoT\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u5927\u6a21\u578b\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u653e\u5c04\u79d1\u533b\u751f\u7684\u601d\u7ef4\u94fe\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u80f8\u90e8X\u5149\u7247\u7684\u667a\u80fd\u8bca\u65ad\u548c\u53ef\u89e3\u91ca\u62a5\u544a\u751f\u6210\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u8bca\u65ad\u51c6\u786e\u7387\u7684\u540c\u65f6\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u80f8\u90e8X\u5149\u5f71\u50cf\u8bca\u65ad\u9700\u8981\u4e30\u5bcc\u7684\u4e34\u5e8a\u7ecf\u9a8c\u4e14\u5b58\u5728\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\uff0c\u867d\u7136\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bca\u65ad\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u5176\u9ed1\u76d2\u7279\u6027\u963b\u788d\u4e86\u5728\u9ad8\u98ce\u9669\u533b\u7597\u73af\u5883\u4e2d\u7684\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u63d0\u51faX-Ray-CoT\u6846\u67b6\uff0c\u9996\u5148\u63d0\u53d6\u591a\u6a21\u6001\u7279\u5f81\u548c\u89c6\u89c9\u6982\u5ff5\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u7ec4\u4ef6\u914d\u5408\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63d0\u793a\u7b56\u7565\u8fdb\u884c\u63a8\u7406\uff0c\u751f\u6210\u8be6\u7ec6\u7684\u81ea\u7136\u8bed\u8a00\u8bca\u65ad\u62a5\u544a\u3002", "result": "\u5728CORDA\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u75be\u75c5\u8bca\u65ad\u7684\u5e73\u8861\u51c6\u786e\u7387\u8fbe\u523080.52%\uff0cF1\u5206\u6570\u4e3a78.65%\uff0c\u7565\u4f18\u4e8e\u73b0\u6709\u9ed1\u76d2\u6a21\u578b\uff0c\u5e76\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u53ef\u89e3\u91ca\u62a5\u544a\u3002", "conclusion": "\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u591a\u6a21\u6001\u878d\u5408\u548c\u601d\u7ef4\u94fe\u63a8\u7406\u5bf9\u4e8e\u6784\u5efa\u7a33\u5065\u900f\u660e\u533b\u7597AI\u7cfb\u7edf\u7684\u5fc5\u8981\u6027\uff0c\u8fd9\u9879\u5de5\u4f5c\u4ee3\u8868\u4e86\u533b\u5b66\u5f71\u50cf\u9886\u57df\u5411\u53ef\u4fe1\u8d56\u548c\u4e34\u5e8a\u53ef\u64cd\u4f5cAI\u7cfb\u7edf\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2508.12466", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12466", "abs": "https://arxiv.org/abs/2508.12466", "authors": ["Xuhui Zhan", "Tyler Derr"], "title": "Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping", "comment": "15pages, 3 figures", "summary": "Traditional multimodal learning approaches require expensive alignment\npre-training to bridge vision and language modalities, typically projecting\nvisual features into discrete text token spaces. We challenge both fundamental\nassumptions underlying this paradigm by proposing Inverse-LLaVA, a novel\napproach that eliminates alignment pre-training entirely while inverting the\nconventional mapping direction. Rather than projecting visual features to text\nspace, our method maps text embeddings into continuous visual representation\nspace and performs fusion within transformer intermediate layers. Through\nselective additive components in attention mechanisms, we enable dynamic\nintegration of visual and textual representations without requiring massive\nimage-text alignment datasets. Comprehensive experiments across nine multimodal\nbenchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves\nnotable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,\nVizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing\nexpected decreases in perception tasks requiring memorized visual-text\nassociations (celebrity recognition: -49.5%, OCR: -21.3%). These results\nprovide the first empirical evidence that alignment pre-training is not\nnecessary for effective multimodal learning, particularly for complex reasoning\ntasks. Our work establishes the feasibility of a new paradigm that reduces\ncomputational requirements by 45%, challenges conventional wisdom about\nmodality fusion, and opens new research directions for efficient multimodal\narchitectures that preserve modality-specific characteristics. Our project\nwebsite with code and additional resources is available at\nhttps://inverse-llava.github.io.", "AI": {"tldr": "Inverse-LLaVA\u662f\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u9700\u5bf9\u9f50\u9884\u8bad\u7ec3\uff0c\u5c06\u6587\u672c\u5d4c\u5165\u6620\u5c04\u5230\u89c6\u89c9\u8868\u793a\u7a7a\u95f4\u800c\u975e\u4f20\u7edf\u76f8\u53cd\u65b9\u5411\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u4f46\u611f\u77e5\u4efb\u52a1\u6709\u6240\u4e0b\u964d\u3002", "motivation": "\u6311\u6218\u4f20\u7edf\u591a\u6a21\u6001\u5b66\u4e60\u9700\u8981\u6602\u8d35\u5bf9\u9f50\u9884\u8bad\u7ec3\u7684\u5047\u8bbe\uff0c\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u5e76\u4fdd\u6301\u6a21\u6001\u7279\u6027\u3002", "method": "\u5c06\u6587\u672c\u5d4c\u5165\u6620\u5c04\u5230\u8fde\u7eed\u89c6\u89c9\u8868\u793a\u7a7a\u95f4\uff0c\u5728transformer\u4e2d\u95f4\u5c42\u8fdb\u884c\u878d\u5408\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u9009\u62e9\u6027\u52a0\u6cd5\u7ec4\u4ef6\u5b9e\u73b0\u52a8\u6001\u96c6\u6210\u3002", "result": "\u57289\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u6027\u80fd\u6743\u8861\uff1a\u63a8\u7406\u4efb\u52a1\u663e\u8457\u63d0\u5347\uff08MM-VET +0.2%, VizWiz +1.8%, ScienceQA +0.2%, \u8ba4\u77e5\u63a8\u7406 +27.2%\uff09\uff0c\u4f46\u611f\u77e5\u4efb\u52a1\u4e0b\u964d\uff08\u540d\u4eba\u8bc6\u522b -49.5%, OCR -21.3%\uff09\uff0c\u8ba1\u7b97\u9700\u6c42\u51cf\u5c1145%\u3002", "conclusion": "\u9996\u6b21\u8bc1\u660e\u5bf9\u9f50\u9884\u8bad\u7ec3\u5bf9\u6709\u6548\u591a\u6a21\u6001\u5b66\u4e60\u5e76\u975e\u5fc5\u8981\uff0c\u7279\u522b\u662f\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff1b\u5efa\u7acb\u65b0\u8303\u5f0f\u53ef\u884c\u6027\uff0c\u6311\u6218\u4f20\u7edf\u6a21\u6001\u878d\u5408\u89c2\u5ff5\uff0c\u4e3a\u9ad8\u6548\u591a\u6a21\u6001\u67b6\u6784\u5f00\u8f9f\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.12473", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12473", "abs": "https://arxiv.org/abs/2508.12473", "authors": ["Eranga Bandara", "Ross Gore", "Sachin Shetty", "Ravi Mukkamala", "Christopher Rhea", "Atmaram Yarlagadda", "Shaifali Kaushik", "L. H. M. P. De Silva", "Andriy Maznychenko", "Inna Sokolowska", "Amin Hass", "Kasun De Zoysa"], "title": "Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System", "comment": null, "summary": "Accurate assessment of neuromuscular reflexes, such as the H-reflex, plays a\ncritical role in sports science, rehabilitation, and clinical neurology.\nTraditional analysis of H-reflex EMG waveforms is subject to variability and\ninterpretation bias among clinicians and researchers, limiting reliability and\nstandardization. To address these challenges, we propose a Fine-Tuned\nVision-Language Model (VLM) Consortium and a reasoning Large-Language Model\n(LLM)-enabled Decision Support System for automated H-reflex waveform\ninterpretation and diagnosis. Our approach leverages multiple VLMs, each\nfine-tuned on curated datasets of H-reflex EMG waveform images annotated with\nclinical observations, recovery timelines, and athlete metadata. These models\nare capable of extracting key electrophysiological features and predicting\nneuromuscular states, including fatigue, injury, and recovery, directly from\nEMG images and contextual metadata. Diagnostic outputs from the VLM consortium\nare aggregated using a consensus-based method and refined by a specialized\nreasoning LLM, which ensures robust, transparent, and explainable decision\nsupport for clinicians and sports scientists. The end-to-end platform\norchestrates seamless communication between the VLM ensemble and the reasoning\nLLM, integrating prompt engineering strategies and automated reasoning\nworkflows using LLM Agents. Experimental results demonstrate that this hybrid\nsystem delivers highly accurate, consistent, and interpretable H-reflex\nassessments, significantly advancing the automation and standardization of\nneuromuscular diagnostics. To our knowledge, this work represents the first\nintegration of a fine-tuned VLM consortium with a reasoning LLM for image-based\nH-reflex analysis, laying the foundation for next-generation AI-assisted\nneuromuscular assessment and athlete monitoring platforms.", "AI": {"tldr": "\u4f7f\u7528\u7ec6\u8c03\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8054\u76df\u548c\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u81ea\u52a8\u5316H-\u53cd\u5c04\u6ce2\u5f62\u8bca\u65ad\u7cfb\u7edf\uff0c\u63d0\u9ad8\u795e\u7ecf\u808c\u8089\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u6807\u51c6\u5316", "motivation": "\u4f20\u7edfH-\u53cd\u5c04EMG\u6ce2\u5f62\u5206\u6790\u5b58\u5728\u4e3b\u89c2\u504f\u5dee\u548c\u53d8\u5f02\u6027\uff0c\u5f71\u54cd\u8bca\u65ad\u53ef\u9760\u6027\u548c\u6807\u51c6\u5316", "method": "\u591a\u4e2a\u7ec6\u8c03VLM\u6a21\u578b\u5206\u6790\u6ce2\u5f62\u56fe\u50cf\uff0c\u901a\u8fc7\u5171\u8bc6\u673a\u5236\u805a\u5408\u7ed3\u679c\uff0c\u518d\u7531\u4e13\u95e8\u63a8\u7406LLM\u7cbe\u70bc\u8bca\u65ad\u7ed3\u8bba\uff0c\u5f62\u6210\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u652f\u6301", "result": "\u6df7\u5408\u7cfb\u7edf\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u3001\u4e00\u81f4\u6027\u548c\u53ef\u89e3\u91ca\u7684H-\u53cd\u5c04\u8bc4\u4f30\uff0c\u5927\u5927\u63a8\u8fdb\u795e\u7ecf\u808c\u8089\u8bca\u65ad\u7684\u81ea\u52a8\u5316\u548c\u6807\u51c6\u5316", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5c06\u7ec6\u8c03VLM\u8054\u76df\u4e0e\u63a8\u7406LLM\u7ed3\u5408\u7528\u4e8e\u56fe\u50cf\u57faH-\u53cd\u5c04\u5206\u6790\uff0c\u4e3a\u4e0b\u4e00\u4ee3AI\u8f85\u52a9\u795e\u7ecf\u808c\u8089\u8bc4\u4f30\u5e73\u53f0\u5960\u5b9a\u57fa\u7840"}}
{"id": "2508.12484", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12484", "abs": "https://arxiv.org/abs/2508.12484", "authors": ["Shubhi Agarwal", "Amulya Kumar Mahto"], "title": "Skin Cancer Classification: Hybrid CNN-Transformer Models with KAN-Based Fusion", "comment": null, "summary": "Skin cancer classification is a crucial task in medical image analysis, where\nprecise differentiation between malignant and non-malignant lesions is\nessential for early diagnosis and treatment. In this study, we explore\nSequential and Parallel Hybrid CNN-Transformer models with Convolutional\nKolmogorov-Arnold Network (CKAN). Our approach integrates transfer learning and\nextensive data augmentation, where CNNs extract local spatial features,\nTransformers model global dependencies, and CKAN facilitates nonlinear feature\nfusion for improved representation learning. To assess generalization, we\nevaluate our models on multiple benchmark datasets (HAM10000,BCN20000 and\nPAD-UFES) under varying data distributions and class imbalances. Experimental\nresults demonstrate that hybrid CNN-Transformer architectures effectively\ncapture both spatial and contextual features, leading to improved\nclassification performance. Additionally, the integration of CKAN enhances\nfeature fusion through learnable activation functions, yielding more\ndiscriminative representations. Our proposed approach achieves competitive\nperformance in skin cancer classification, demonstrating 92.81% accuracy and\n92.47% F1-score on the HAM10000 dataset, 97.83% accuracy and 97.83% F1-score on\nthe PAD-UFES dataset, and 91.17% accuracy with 91.79% F1- score on the BCN20000\ndataset highlighting the effectiveness and generalizability of our model across\ndiverse datasets. This study highlights the significance of feature\nrepresentation and model design in advancing robust and accurate medical image\nclassification.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408CNN-Transformer\u6df7\u5408\u67b6\u6784\u548c\u5377\u79efKolmogorov-Arnold\u7f51\u7edc(CKAN)\u7684\u76ae\u80a4\u764c\u5206\u7c7b\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd", "motivation": "\u76ae\u80a4\u764c\u5206\u7c7b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7cbe\u786e\u533a\u5206\u6076\u6027\u548c\u975e\u6076\u6027\u75c5\u53d8\u4ee5\u5b9e\u73b0\u65e9\u671f\u8bca\u65ad\u548c\u6cbb\u7597\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u66f4\u597d\u5730\u6574\u5408\u5c40\u90e8\u7a7a\u95f4\u7279\u5f81\u548c\u5168\u5c40\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u987a\u5e8f\u548c\u5e76\u884c\u6df7\u5408CNN-Transformer\u6a21\u578b\uff0c\u7ed3\u5408\u5377\u79efKolmogorov-Arnold\u7f51\u7edc(CKAN)\u8fdb\u884c\u975e\u7ebf\u6027\u7279\u5f81\u878d\u5408\u3002\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u548c\u5e7f\u6cdb\u6570\u636e\u589e\u5f3a\uff0cCNN\u63d0\u53d6\u5c40\u90e8\u7a7a\u95f4\u7279\u5f81\uff0cTransformer\u5efa\u6a21\u5168\u5c40\u4f9d\u8d56\uff0cCKAN\u901a\u8fc7\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u589e\u5f3a\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728HAM10000\u6570\u636e\u96c6\u4e0a\u8fbe\u523092.81%\u51c6\u786e\u7387\u548c92.47% F1\u5206\u6570\uff0cPAD-UFES\u6570\u636e\u96c6\u4e0a97.83%\u51c6\u786e\u7387\u548cF1\u5206\u6570\uff0cBCN20000\u6570\u636e\u96c6\u4e0a91.17%\u51c6\u786e\u7387\u548c91.79% F1\u5206\u6570\uff0c\u5c55\u73b0\u4e86\u4f18\u5f02\u7684\u5206\u7c7b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6df7\u5408CNN-Transformer\u67b6\u6784\u80fd\u6709\u6548\u6355\u83b7\u7a7a\u95f4\u548c\u4e0a\u4e0b\u6587\u7279\u5f81\uff0cCKAN\u96c6\u6210\u901a\u8fc7\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u589e\u5f3a\u7279\u5f81\u878d\u5408\uff0c\u4ea7\u751f\u66f4\u5177\u5224\u522b\u6027\u7684\u8868\u793a\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u7279\u5f81\u8868\u793a\u548c\u6a21\u578b\u8bbe\u8ba1\u5728\u63a8\u8fdb\u7a33\u5065\u51c6\u786e\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.12506", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12506", "abs": "https://arxiv.org/abs/2508.12506", "authors": ["E. Ulises Moya-S\u00e1nchez", "Abraham S\u00e1nchez-Perez", "Ra\u00fal Nanclares Da Veiga", "Alejandro Zarate-Mac\u00edas", "Edgar Villareal", "Alejandro S\u00e1nchez-Montes", "Edtna Jauregui-Ulloa", "H\u00e9ctor Moreno", "Ulises Cort\u00e9s"], "title": "Design and Validation of a Responsible Artificial Intelligence-based System for the Referral of Diabetic Retinopathy Patients", "comment": "14 pages,3 figures, under review", "summary": "Diabetic Retinopathy (DR) is a leading cause of vision loss in working-age\nindividuals. Early detection of DR can reduce the risk of vision loss by up to\n95%, but a shortage of retinologists and challenges in timely examination\ncomplicate detection. Artificial Intelligence (AI) models using retinal fundus\nphotographs (RFPs) offer a promising solution. However, adoption in clinical\nsettings is hindered by low-quality data and biases that may lead AI systems to\nlearn unintended features. To address these challenges, we developed RAIS-DR, a\nResponsible AI System for DR screening that incorporates ethical principles\nacross the AI lifecycle. RAIS-DR integrates efficient convolutional models for\npreprocessing, quality assessment, and three specialized DR classification\nmodels. We evaluated RAIS-DR against the FDA-approved EyeArt system on a local\ndataset of 1,046 patients, unseen by both systems. RAIS-DR demonstrated\nsignificant improvements, with F1 scores increasing by 5-12%, accuracy by\n6-19%, and specificity by 10-20%. Additionally, fairness metrics such as\nDisparate Impact and Equal Opportunity Difference indicated equitable\nperformance across demographic subgroups, underscoring RAIS-DR's potential to\nreduce healthcare disparities. These results highlight RAIS-DR as a robust and\nethically aligned solution for DR screening in clinical settings. The code,\nweights of RAIS-DR are available at\nhttps://gitlab.com/inteligencia-gubernamental-jalisco/jalisco-retinopathy with\nRAIL.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86RAIS-DR\u7cfb\u7edf\uff0c\u4e00\u79cd\u8d1f\u8d23\u4efb\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\uff0c\u7528\u4e8e\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u7b5b\u67e5\uff0c\u5728\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u65b9\u9762\u90fd\u663e\u8457\u8d85\u8d8a\u4e86FDA\u6279\u51c6\u7684EyeArt\u7cfb\u7edf\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u662f\u5de5\u4f5c\u5e74\u9f84\u4eba\u53d8\u76ca\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u65e9\u671f\u53d1\u73b0\u53ef\u5927\u5e45\u964d\u4f4e\u5931\u660e\u98ce\u9669\u3002\u4f46\u773c\u79d1\u533b\u751f\u77ed\u7f3a\u548c\u8d28\u91cf\u504f\u5dee\u7684\u6570\u636e\u5f71\u54cd\u4e86AI\u7cfb\u7edf\u5728\u4e34\u5e8a\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86RAIS-DR\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u5728AI\u751f\u547d\u5468\u671f\u4e2d\u878d\u5165\u4f26\u7406\u539f\u5219\u7684\u8d1f\u8d23\u4efbAI\u7cfb\u7edf\u3002\u7cfb\u7edf\u96c6\u6210\u4e86\u9ad8\u6548\u5377\u79ef\u6a21\u578b\u7528\u4e8e\u9884\u5904\u7406\u3001\u8d28\u91cf\u8bc4\u4f30\u548c\u4e09\u4e2a\u4e13\u95e8\u7684DR\u5206\u7c7b\u6a21\u578b\u3002", "result": "\u57281,046\u540d\u60a3\u8005\u7684\u672c\u5730\u6570\u636e\u96c6\u4e0a\uff0cRAIS-DR\u4e0eFDA\u6279\u51c6\u7684EyeArt\u7cfb\u7edf\u76f8\u6bd4\uff0cF1\u5206\u6570\u63d0\u9ad85-12%\uff0c\u51c6\u786e\u6027\u63d0\u9ad86-19%\uff0c\u7279\u5f02\u6027\u63d0\u9ad810-20%\u3002\u540c\u65f6\u5728\u516c\u5e73\u6027\u6307\u6807\u4e0a\u4e5f\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "RAIS-DR\u4f5c\u4e3a\u4e00\u4e2a\u5065\u58ee\u4e14\u7b26\u5408\u4f26\u7406\u8981\u6c42\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u529b\u63a8\u52a8\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u7b5b\u67e5\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u4e14\u6240\u6709\u4ee3\u7801\u548c\u6a21\u578b\u90fd\u5f00\u6e90\u53ef\u7528\u3002"}}
{"id": "2508.12512", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12512", "abs": "https://arxiv.org/abs/2508.12512", "authors": ["Krishna Teja Chitty-Venkata", "Murali Emani", "Venkatram Vishwanath"], "title": "LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models", "comment": "Accepted by ICIP 2025 Conference", "summary": "Vision Language Models (VLMs) integrate visual and text modalities to enable\nmultimodal understanding and generation. These models typically combine a\nVision Transformer (ViT) as an image encoder and a Large Language Model (LLM)\nfor text generation. LoRA (Low-Rank Adaptation) is an efficient fine-tuning\nmethod to adapt pre-trained models to new tasks by introducing low-rank updates\nto their weights. While LoRA has emerged as a powerful technique for\nfine-tuning large models by introducing low-rank updates, current\nimplementations assume a fixed rank, potentially limiting flexibility and\nefficiency across diverse tasks. This paper introduces\n\\textit{LangVision-LoRA-NAS}, a novel framework that integrates Neural\nArchitecture Search (NAS) with LoRA to optimize VLMs for variable-rank\nadaptation. Our approach leverages NAS to dynamically search for the optimal\nLoRA rank configuration tailored to specific multimodal tasks, balancing\nperformance and computational efficiency. Through extensive experiments using\nthe LLaMA-3.2-11B model on several datasets, LangVision-LoRA-NAS demonstrates\nnotable improvement in model performance while reducing fine-tuning costs. Our\nBase and searched fine-tuned models on LLaMA-3.2-11B-Vision-Instruct can be\nfound\n\\href{https://huggingface.co/collections/krishnateja95/llama-32-11b-vision-instruct-langvision-lora-nas-6786cac480357a6a6fcc59ee}{\\textcolor{blue}{here}}\nand the code for LangVision-LoRA-NAS can be found\n\\href{https://github.com/krishnateja95/LangVision-NAS}{\\textcolor{blue}{here}}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LangVision-LoRA-NAS\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u52a8\u6001\u4f18\u5316LoRA\u79e9\u914d\u7f6e\uff0c\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u5fae\u8c03\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684LoRA\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u79e9\u8fdb\u884c\u5fae\u8c03\uff0c\u9650\u5236\u4e86\u5728\u4e0d\u540c\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6839\u636e\u5177\u4f53\u4efb\u52a1\u52a8\u6001\u8c03\u6574\u79e9\u914d\u7f6e\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u795e\u7ecf\u67b6\u6784\u641c\u7d22(NAS)\u4e0eLoRA\u76f8\u7ed3\u5408\uff0c\u52a8\u6001\u641c\u7d22\u6700\u4f18\u7684LoRA\u79e9\u914d\u7f6e\uff0c\u9488\u5bf9\u7279\u5b9a\u591a\u6a21\u6001\u4efb\u52a1\u5e73\u8861\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728LLaMA-3.2-11B\u6a21\u578b\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5fae\u8c03\u6210\u672c\u3002", "conclusion": "LangVision-LoRA-NAS\u6846\u67b6\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u79e9\u4f18\u5316\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2508.12520", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12520", "abs": "https://arxiv.org/abs/2508.12520", "authors": ["Felipe Carlos dos Santos", "Eric Aislan Antonelo", "Gustavo Claudio Karl Couto"], "title": "An Initial Study of Bird's-Eye View Generation for Autonomous Vehicles using Cross-View Transformers", "comment": "12 pages,submitted in ENIAC 2025", "summary": "Bird's-Eye View (BEV) maps provide a structured, top-down abstraction that is\ncrucial for autonomous-driving perception. In this work, we employ Cross-View\nTransformers (CVT) for learning to map camera images to three BEV's channels -\nroad, lane markings, and planned trajectory - using a realistic simulator for\nurban driving. Our study examines generalization to unseen towns, the effect of\ndifferent camera layouts, and two loss formulations (focal and L1). Using\ntraining data from only a town, a four-camera CVT trained with the L1 loss\ndelivers the most robust test performance, evaluated in a new town. Overall,\nour results underscore CVT's promise for mapping camera inputs to reasonably\naccurate BEV maps.", "AI": {"tldr": "\u4f7f\u7528\u4ea4\u53c9\u89c6\u56fe\u53d8\u6362\u5668(CVT)\u5c06\u76f8\u673a\u56fe\u50cf\u6620\u5c04\u5230\u9e1f\u77b0\u56fe(BEV)\u7684\u4e09\u4e2a\u901a\u9053\uff1a\u9053\u8def\u3001\u8f66\u9053\u6807\u8bb0\u548c\u89c4\u5212\u8f68\u8ff9\uff0c\u901a\u8fc7\u6a21\u62df\u5668\u9a8c\u8bc1\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u4e0d\u540c\u76f8\u673a\u914d\u7f6e\u7684\u6548\u679c\u3002", "motivation": "\u9e1f\u77b0\u56fe(BEV)\u4e3a\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u4fef\u89c6\u62bd\u8c61\u8868\u793a\uff0c\u4f46\u5982\u4f55\u4ece\u76f8\u673a\u56fe\u50cf\u51c6\u786e\u6620\u5c04\u5230BEV\u5730\u56fe\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002", "method": "\u91c7\u7528\u4ea4\u53c9\u89c6\u56fe\u53d8\u6362\u5668(CVT)\u67b6\u6784\uff0c\u4f7f\u7528\u57ce\u5e02\u9a7e\u9a76\u6a21\u62df\u5668\u751f\u6210\u6570\u636e\uff0c\u7814\u7a76\u4e0d\u540c\u76f8\u673a\u5e03\u5c40\u548c\u635f\u5931\u51fd\u6570\uff08\u7126\u70b9\u635f\u5931\u548cL1\u635f\u5931\uff09\u7684\u6548\u679c\u3002", "result": "\u5728\u4ec5\u4f7f\u7528\u4e00\u4e2a\u57ce\u9547\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u91c7\u7528\u56db\u76f8\u673a\u914d\u7f6e\u548cL1\u635f\u5931\u7684CVT\u6a21\u578b\u5728\u65b0\u57ce\u9547\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6700\u9c81\u68d2\u7684\u6027\u80fd\u3002", "conclusion": "\u4ea4\u53c9\u89c6\u56fe\u53d8\u6362\u5668\u5728\u5c06\u76f8\u673a\u8f93\u5165\u6620\u5c04\u5230\u76f8\u5bf9\u51c6\u786e\u7684BEV\u5730\u56fe\u65b9\u9762\u5c55\u73b0\u51fa\u826f\u597d\u6f5c\u529b\uff0cL1\u635f\u5931\u548c\u56db\u76f8\u673a\u914d\u7f6e\u662f\u6700\u4f73\u7ec4\u5408\u3002"}}
{"id": "2508.12522", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12522", "abs": "https://arxiv.org/abs/2508.12522", "authors": ["Muhammad Osama Zeeshan", "Natacha Gillet", "Alessandro Lameiras Koerich", "Marco Pedersoli", "Francois Bremond", "Eric Granger"], "title": "MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training", "comment": null, "summary": "Personalized expression recognition (ER) involves adapting a machine learning\nmodel to subject-specific data for improved recognition of expressions with\nconsiderable interpersonal variability. Subject-specific ER can benefit\nsignificantly from multi-source domain adaptation (MSDA) methods, where each\ndomain corresponds to a specific subject, to improve model accuracy and\nrobustness. Despite promising results, state-of-the-art MSDA approaches often\noverlook multimodal information or blend sources into a single domain, limiting\nsubject diversity and failing to explicitly capture unique subject-specific\ncharacteristics. To address these limitations, we introduce MuSACo, a\nmulti-modal subject-specific selection and adaptation method for ER based on\nco-training. It leverages complementary information across multiple modalities\nand multiple source domains for subject-specific adaptation. This makes MuSACo\nparticularly relevant for affective computing applications in digital health,\nsuch as patient-specific assessment for stress or pain, where subject-level\nnuances are crucial. MuSACo selects source subjects relevant to the target and\ngenerates pseudo-labels using the dominant modality for class-aware learning,\nin conjunction with a class-agnostic loss to learn from less confident target\nsamples. Finally, source features from each modality are aligned, while only\nconfident target features are combined. Our experimental results on challenging\nmultimodal ER datasets: BioVid and StressID, show that MuSACo can outperform\nUDA (blending) and state-of-the-art MSDA methods.", "AI": {"tldr": "MuSACo\u662f\u4e00\u4e2a\u57fa\u4e8e\u534f\u540c\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u4e2a\u6027\u5316\u8868\u60c5\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u76f8\u5173\u6e90\u4e3b\u4f53\u5e76\u5229\u7528\u591a\u6a21\u6001\u4e92\u8865\u4fe1\u606f\u8fdb\u884c\u4e3b\u4f53\u7279\u5f02\u6027\u9002\u5e94\uff0c\u5728BioVid\u548cStressID\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6e90\u57df\u9002\u5e94\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u591a\u6a21\u6001\u4fe1\u606f\u6216\u5c06\u591a\u4e2a\u6e90\u6df7\u5408\u4e3a\u5355\u4e00\u57df\uff0c\u9650\u5236\u4e86\u4e3b\u4f53\u591a\u6837\u6027\uff0c\u65e0\u6cd5\u660e\u786e\u6355\u6349\u4e3b\u4f53\u7279\u5f02\u6027\u7279\u5f81\u3002\u4e2a\u6027\u5316\u8868\u60c5\u8bc6\u522b\u9700\u8981\u9002\u5e94\u4e3b\u4f53\u95f4\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5728\u6570\u5b57\u5065\u5eb7\u5e94\u7528\u4e2d\u4e3b\u4f53\u7ea7\u522b\u7684\u7ec6\u5fae\u5dee\u522b\u81f3\u5173\u91cd\u8981\u3002", "method": "MuSACo\u57fa\u4e8e\u534f\u540c\u8bad\u7ec3\u6846\u67b6\uff0c\u9009\u62e9\u4e0e\u76ee\u6807\u76f8\u5173\u7684\u6e90\u4e3b\u4f53\uff0c\u4f7f\u7528\u4e3b\u5bfc\u6a21\u6001\u751f\u6210\u4f2a\u6807\u7b7e\u8fdb\u884c\u7c7b\u611f\u77e5\u5b66\u4e60\uff0c\u7ed3\u5408\u7c7b\u65e0\u5173\u635f\u5931\u4ece\u7f6e\u4fe1\u5ea6\u8f83\u4f4e\u7684\u76ee\u6807\u6837\u672c\u4e2d\u5b66\u4e60\u3002\u540c\u65f6\u5bf9\u9f50\u6bcf\u4e2a\u6a21\u6001\u7684\u6e90\u7279\u5f81\uff0c\u4ec5\u7ec4\u5408\u7f6e\u4fe1\u7684\u76ee\u6807\u7279\u5f81\u3002", "result": "\u5728BioVid\u548cStressID\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u6a21\u6001\u8868\u60c5\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\uff0cMuSACo\u8d85\u8d8a\u4e86\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08\u6df7\u5408\uff09\u548c\u6700\u5148\u8fdb\u7684\u591a\u6e90\u57df\u9002\u5e94\u65b9\u6cd5\u3002", "conclusion": "MuSACo\u901a\u8fc7\u6709\u6548\u5229\u7528\u591a\u6a21\u6001\u548c\u591a\u6e90\u57df\u4fe1\u606f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u4e3b\u4f53\u7279\u5f02\u6027\u7684\u8868\u60c5\u8bc6\u522b\u9002\u5e94\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6570\u5b57\u5065\u5eb7\u5e94\u7528\u4e2d\u9700\u8981\u6355\u6349\u4e3b\u4f53\u7ec6\u5fae\u5dee\u5f02\u7684\u573a\u666f\u3002"}}
{"id": "2508.12543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12543", "abs": "https://arxiv.org/abs/2508.12543", "authors": ["Ipsita Praharaj", "Yukta Butala", "Yash Butala"], "title": "REVEAL -- Reasoning and Evaluation of Visual Evidence through Aligned Language", "comment": "4 pages, 6 figures, International Conference on Computer Vision, ICCV\n  2025", "summary": "The rapid advancement of generative models has intensified the challenge of\ndetecting and interpreting visual forgeries, necessitating robust frameworks\nfor image forgery detection while providing reasoning as well as localization.\nWhile existing works approach this problem using supervised training for\nspecific manipulation or anomaly detection in the embedding space,\ngeneralization across domains remains a challenge. We frame this problem of\nforgery detection as a prompt-driven visual reasoning task, leveraging the\nsemantic alignment capabilities of large vision-language models. We propose a\nframework, `REVEAL` (Reasoning and Evaluation of Visual Evidence through\nAligned Language), that incorporates generalized guidelines. We propose two\ntangential approaches - (1) Holistic Scene-level Evaluation that relies on the\nphysics, semantics, perspective, and realism of the image as a whole and (2)\nRegion-wise anomaly detection that splits the image into multiple regions and\nanalyzes each of them. We conduct experiments over datasets from different\ndomains (Photoshop, DeepFake and AIGC editing). We compare the Vision Language\nModels against competitive baselines and analyze the reasoning provided by\nthem.", "AI": {"tldr": "REVEAL\u6846\u67b6\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\uff0c\u901a\u8fc7\u6574\u4f53\u573a\u666f\u8bc4\u4f30\u548c\u533a\u57df\u5f02\u5e38\u68c0\u6d4b\u4e24\u79cd\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97\u89c6\u89c9\u4f2a\u9020\u68c0\u6d4b\u548c\u89e3\u91ca\u53d8\u5f97\u66f4\u52a0\u56f0\u96be\uff0c\u9700\u8981\u65e2\u80fd\u68c0\u6d4b\u4f2a\u9020\u53c8\u80fd\u63d0\u4f9b\u63a8\u7406\u548c\u5b9a\u4f4d\u7684\u9c81\u68d2\u6846\u67b6\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u9886\u57df\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u5c06\u4f2a\u9020\u68c0\u6d4b\u6784\u5efa\u4e3a\u63d0\u793a\u9a71\u52a8\u7684\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\uff0c\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u5bf9\u9f50\u80fd\u529b\u3002\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a(1)\u6574\u4f53\u573a\u666f\u7ea7\u8bc4\u4f30\uff08\u57fa\u4e8e\u7269\u7406\u3001\u8bed\u4e49\u3001\u900f\u89c6\u548c\u771f\u5b9e\u6027\uff09(2)\u533a\u57df\u7ea7\u5f02\u5e38\u68c0\u6d4b\uff08\u5c06\u56fe\u50cf\u5206\u5272\u6210\u591a\u4e2a\u533a\u57df\u8fdb\u884c\u5206\u6790\uff09\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u6570\u636e\u96c6\uff08Photoshop\u3001DeepFake\u548cAIGC\u7f16\u8f91\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4e0e\u7ade\u4e89\u57fa\u7ebf\u6bd4\u8f83\u5e76\u5206\u6790\u6a21\u578b\u63d0\u4f9b\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "REVEAL\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u5bf9\u9f50\u80fd\u529b\uff0c\u5728\u8de8\u9886\u57df\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2508.12570", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12570", "abs": "https://arxiv.org/abs/2508.12570", "authors": ["Yingxue Pang", "Xin Jin", "Jun Fu", "Zhibo Chen"], "title": "Structure-preserving Feature Alignment for Old Photo Colorization", "comment": null, "summary": "Deep learning techniques have made significant advancements in\nreference-based colorization by training on large-scale datasets. However,\ndirectly applying these methods to the task of colorizing old photos is\nchallenging due to the lack of ground truth and the notorious domain gap\nbetween natural gray images and old photos. To address this issue, we propose a\nnovel CNN-based algorithm called SFAC, i.e., Structure-preserving Feature\nAlignment Colorizer. SFAC is trained on only two images for old photo\ncolorization, eliminating the reliance on big data and allowing direct\nprocessing of the old photo itself to overcome the domain gap problem. Our\nprimary objective is to establish semantic correspondence between the two\nimages, ensuring that semantically related objects have similar colors. We\nachieve this through a feature distribution alignment loss that remains robust\nto different metric choices. However, utilizing robust semantic correspondence\nto transfer color from the reference to the old photo can result in inevitable\nstructure distortions. To mitigate this, we introduce a structure-preserving\nmechanism that incorporates a perceptual constraint at the feature level and a\nfrozen-updated pyramid at the pixel level. Extensive experiments demonstrate\nthe effectiveness of our method for old photo colorization, as confirmed by\nqualitative and quantitative metrics.", "AI": {"tldr": "\u4e00\u79cd\u4ec5\u9700\u4e24\u5f20\u56fe\u7247\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5SFAC\uff0c\u7528\u4e8e\u8001\u7167\u7247\u7740\u8272\uff0c\u514d\u9664\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u57df\u95f4\u5dee\u95ee\u9898", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u8001\u7167\u7247\u7740\u8272\u4efb\u52a1\u4e2d\u9047\u5230\u56f0\u96be\uff0c\u5305\u62ec\u7f3a\u4e4f\u771f\u5b9e\u6807\u7b7e\u548c\u81ea\u7136\u7070\u5ea6\u56fe\u4e0e\u8001\u7167\u7247\u4e4b\u95f4\u7684\u57df\u95f4\u5dee", "method": "\u63d0\u51faSFAC\u7b97\u6cd5\uff0c\u901a\u8fc7\u7279\u5f81\u5206\u5e03\u5bf9\u9f50\u635f\u5931\u786e\u4fdd\u8bed\u4e49\u76f8\u5173\u5bf9\u8c61\u6709\u76f8\u4f3c\u989c\u8272\uff0c\u5e76\u4f7f\u7528\u7ed3\u6784\u4fdd\u6301\u673a\u5236\uff08\u7279\u5f81\u5c42\u611f\u77e5\u7ea6\u675f\u548c\u50cf\u7d20\u5c42\u51b2\u51bb-\u66f4\u65b0\u91d1\u5b57\u5854\uff09\u6765\u51cf\u5c11\u7ed3\u6784\u626d\u66f2", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u65b9\u6cd5\u5728\u8001\u7167\u7247\u7740\u8272\u4efb\u52a1\u4e0a\u6709\u6548\uff0c\u5b9a\u6027\u548c\u5b9a\u91cf\u6307\u6807\u90fd\u8bc1\u660e\u4e86\u5176\u4f18\u52bf", "conclusion": "SFAC\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8001\u7167\u7247\u7740\u8272\u7684\u6311\u6218\uff0c\u514d\u9664\u4e86\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u4f9d\u8d56\uff0c\u901a\u8fc7\u8bed\u4e49\u5bf9\u5e94\u548c\u7ed3\u6784\u4fdd\u6301\u673a\u5236\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u989c\u8272\u8f6c\u79fb"}}
{"id": "2508.12586", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12586", "abs": "https://arxiv.org/abs/2508.12586", "authors": ["Hongsong Wang", "Wanjiang Weng", "Junbo Wang", "Fang Zhao", "Guo-Sen Xie", "Xin Geng", "Liang Wang"], "title": "Foundation Model for Skeleton-Based Human Action Understanding", "comment": "Accepted by TPAMI, Code is available at:\n  https://github.com/wengwanjiang/FoundSkelModel", "summary": "Human action understanding serves as a foundational pillar in the field of\nintelligent motion perception. Skeletons serve as a modality- and\ndevice-agnostic representation for human modeling, and skeleton-based action\nunderstanding has potential applications in humanoid robot control and\ninteraction. \\RED{However, existing works often lack the scalability and\ngeneralization required to handle diverse action understanding tasks. There is\nno skeleton foundation model that can be adapted to a wide range of action\nunderstanding tasks}. This paper presents a Unified Skeleton-based Dense\nRepresentation Learning (USDRL) framework, which serves as a foundational model\nfor skeleton-based human action understanding. USDRL consists of a\nTransformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained Feature\nDecorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). The\nDSTE module adopts two parallel streams to learn temporal dynamic and spatial\nstructure features. The MG-FD module collaboratively performs feature\ndecorrelation across temporal, spatial, and instance domains to reduce\ndimensional redundancy and enhance information extraction. The MPCT module\nemploys both multi-view and multi-modal self-supervised consistency training.\nThe former enhances the learning of high-level semantics and mitigates the\nimpact of low-level discrepancies, while the latter effectively facilitates the\nlearning of informative multimodal features. We perform extensive experiments\non 25 benchmarks across across 9 skeleton-based action understanding tasks,\ncovering coarse prediction, dense prediction, and transferred prediction. Our\napproach significantly outperforms the current state-of-the-art methods. We\nhope that this work would broaden the scope of research in skeleton-based\naction understanding and encourage more attention to dense prediction tasks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u9aa8\u67b6\u57fa\u7840\u6a21\u578bUSDRL\uff0c\u901a\u8fc7Transformer\u7f16\u7801\u5668\u3001\u591a\u7c92\u5ea6\u7279\u5f81\u89e3\u76f8\u5173\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u8bad\u7ec3\uff0c\u572825\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5177\u6709\u826f\u597d\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\u7684\u9aa8\u67b6\u57fa\u7840\u6a21\u578b\uff0c\u65e0\u6cd5\u5904\u7406\u591a\u6837\u5316\u7684\u884c\u4e3a\u7406\u89e3\u4efb\u52a1\u3002\u9700\u8981\u4e00\u4e2a\u53ef\u9002\u914d\u5e7f\u6cdb\u884c\u4e3a\u7406\u89e3\u4efb\u52a1\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u8bbe\u8ba1\u4e86USDRL\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09Transformer\u57fa\u7840\u7684\u5bc6\u96c6\u65f6\u7a7a\u7f16\u7801\u5668\uff08DSTE\uff09\u7528\u4e8e\u5b66\u4e60\u65f6\u95f4\u52a8\u6001\u548c\u7a7a\u95f4\u7ed3\u6784\u7279\u5f81\uff1b2\uff09\u591a\u7c92\u5ea6\u7279\u5f81\u89e3\u76f8\u5173\uff08MG-FD\uff09\u6a21\u5757\u51cf\u5c11\u7ef4\u5ea6\u5197\u4f59\uff1b3\uff09\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u8bad\u7ec3\uff08MPCT\uff09\u6a21\u5757\u901a\u8fc7\u591a\u89c6\u89d2\u548c\u591a\u6a21\u6001\u81ea\u76d1\u7763\u63d0\u5347\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u57289\u79cd\u4e0d\u540c\u7684\u9aa8\u67b6\u57fa\u4e8e\u884c\u4e3a\u7406\u89e3\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u6db5\u76d6\u7c97\u7c92\u5ea6\u9884\u6d4b\u3001\u5bc6\u96c6\u9884\u6d4b\u548c\u8f6c\u79fb\u9884\u6d4b\u3002\u572825\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u9aa8\u67b6\u57fa\u7840\u884c\u4e3a\u7406\u89e3\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u7840\u6a21\u578b\uff0c\u62d3\u5bbd\u4e86\u7814\u7a76\u8303\u56f4\uff0c\u5e76\u9f13\u52b1\u66f4\u591a\u5173\u6ce8\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2508.12610", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12610", "abs": "https://arxiv.org/abs/2508.12610", "authors": ["Chen Qian", "Danyang Li", "Xinran Yu", "Zheng Yang", "Qiang Ma"], "title": "OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion", "comment": null, "summary": "Optical motion capture is a foundational technology driving advancements in\ncutting-edge fields such as virtual reality and film production. However,\nsystem performance suffers severely under large-scale marker occlusions common\nin real-world applications. An in-depth analysis identifies two primary\nlimitations of current models: (i) the lack of training datasets accurately\nreflecting realistic marker occlusion patterns, and (ii) the absence of\ntraining strategies designed to capture long-range dependencies among markers.\nTo tackle these challenges, we introduce the CMU-Occlu dataset, which\nincorporates ray tracing techniques to realistically simulate practical marker\nocclusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving\nmodel designed specifically for robust motion capture in environments with\nsignificant occlusions. Leveraging a marker-joint chain inference mechanism,\nOpenMoCap enables simultaneous optimization and construction of deep\nconstraints between markers and joints. Extensive comparative experiments\ndemonstrate that OpenMoCap consistently outperforms competing methods across\ndiverse scenarios, while the CMU-Occlu dataset opens the door for future\nstudies in robust motion solving. The proposed OpenMoCap is integrated into the\nMoSen MoCap system for practical deployment. The code is released at:\nhttps://github.com/qianchen214/OpenMoCap.", "AI": {"tldr": "\u901a\u8fc7\u63d0\u51faCMU-Occlu\u6570\u636e\u96c6\u548cOpenMoCap\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5149\u5b66\u52a8\u4f5c\u6293\u53d6\u4e2d\u5927\u89c4\u6a21\u6807\u8bb0\u70b9\u906e\u6321\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u52a8\u4f5c\u91cd\u5efa\u7cbe\u5ea6\u548c\u7a33\u5065\u6027\u3002", "motivation": "\u73b0\u6709\u5149\u5b66\u52a8\u4f5c\u6293\u53d6\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9047\u5230\u5927\u89c4\u6a21\u6807\u8bb0\u70b9\u906e\u6321\u65f6\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff0c\u4e3b\u8981\u56e0\u4e3a\u7f3a\u4e4f\u771f\u5b9e\u906e\u6321\u6a21\u5f0f\u7684\u8bad\u7ec3\u6570\u636e\u548c\u65e0\u6cd5\u6293\u53d6\u6807\u8bb0\u70b9\u95f4\u7684\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u5149\u7ebf\u8ffd\u8e2a\u6280\u672f\u6a21\u62df\u771f\u5b9e\u906e\u6321\u6a21\u5f0f\u6784\u5efaCMU-Occlu\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1OpenMoCap\u6a21\u578b\u901a\u8fc7\u6807\u8bb0\u70b9-\u5173\u8282\u94fe\u63a8\u7406\u673a\u5236\u5b9e\u73b0\u540c\u65f6\u4f18\u5316\u548c\u6784\u5efa\u6df1\u5ea6\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aOpenMoCap\u5728\u591a\u79cd\u573a\u666f\u4e0b\u90fd\u4e00\u81f4\u8d85\u8fc7\u4e86\u7ade\u4e89\u65b9\u6cd5\uff0cCMU-Occlu\u6570\u636e\u96c6\u4e3a\u7a33\u5065\u52a8\u4f5c\u89e3\u51b3\u7814\u7a76\u6253\u5f00\u4e86\u65b0\u5927\u95e8\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5149\u5b66\u52a8\u4f5c\u6293\u53d6\u4e2d\u906e\u6321\u95ee\u9898\u7684\u6838\u5fc3\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7a33\u5065\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.12587", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12587", "abs": "https://arxiv.org/abs/2508.12587", "authors": ["Tan-Hanh Pham", "Chris Ngo"], "title": "Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models", "comment": null, "summary": "Many reasoning techniques for large multimodal models adapt language model\napproaches, such as Chain-of-Thought (CoT) prompting, which express reasoning\nas word sequences. While effective for text, these methods are suboptimal for\nmultimodal contexts, struggling to align audio, visual, and textual information\ndynamically. To explore an alternative paradigm, we propose the Multimodal\nChain of Continuous Thought (MCOUT), which enables reasoning directly in a\njoint latent space rather than in natural language. In MCOUT, the reasoning\nstate is represented as a continuous hidden vector, iteratively refined and\naligned with visual and textual embeddings, inspired by human reflective\ncognition. We develop two variants: MCOUT-Base, which reuses the language\nmodel`s last hidden state as the continuous thought for iterative reasoning,\nand MCOUT-Multi, which integrates multimodal latent attention to strengthen\ncross-modal alignment between visual and textual features. Experiments on\nbenchmarks including MMMU, ScienceQA, and MMStar show that MCOUT consistently\nimproves multimodal reasoning, yielding up to 8.23% accuracy gains over strong\nbaselines and improving BLEU scores up to 8.27% across multiple-choice and\nopen-ended tasks. These findings highlight latent continuous reasoning as a\npromising direction for advancing LMMs beyond language-bound CoT, offering a\nscalable framework for human-like reflective multimodal inference. Code is\navailable at https://github.com/Hanhpt23/OmniMod.", "AI": {"tldr": "\u591a\u6a21\u6001\u8fde\u7eed\u601d\u7ef4\u94fe(MCOUT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8054\u5408\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8fde\u7eed\u5411\u91cf\u63a8\u7406\uff0c\u5145\u5206\u5229\u7528\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u7684\u5bf9\u9f50\uff0c\u5728\u591a\u6a21\u6001\u7406\u7531\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u7684\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u65b9\u6cd5(\u5982\u601d\u7ef4\u94fe\u63d0\u793a)\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u5b58\u5728\u9650\u5236\uff0c\u96be\u4ee5\u52a8\u6001\u5bf9\u9f50\u97f3\u9891\u3001\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u591a\u6a21\u6001\u63a8\u7406\u65b9\u6848\u3002", "method": "\u63d0\u51faMCOUT\u65b9\u6cd5\uff0c\u5c06\u63a8\u7406\u72b6\u6001\u8868\u793a\u4e3a\u8fde\u7eed\u9690\u85cf\u5411\u91cf\uff0c\u901a\u8fc7\u8fed\u4ee3\u7cbe\u70bc\u5e76\u4e0e\u89c6\u89c9\u3001\u6587\u672c\u5d4c\u5165\u5f62\u5f0f\u5bf9\u9f50\u3002\u5305\u62ec\u4e24\u4e2a\u53d8\u4f53\uff1aMCOUT-Base(\u91cd\u7528\u8bed\u8a00\u6a21\u578b\u9690\u85cf\u72b6\u6001)\u548cMCOUT-Multi(\u96c6\u6210\u591a\u6a21\u6001\u6f5c\u5728\u6ce8\u610f\u529b\u673a\u5236)\u3002", "result": "\u5728MMMU\u3001ScienceQA\u3001MMStar\u7b49\u6846\u67b6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cMCOUT\u5728\u591a\u6a21\u6001\u7406\u7531\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u4e00\u81f4\u6027\u63d0\u5347\uff0c\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u53478.23%\uff0cBLEU\u5206\u6570\u6700\u9ad8\u63d0\u53478.27%\u3002", "conclusion": "\u6f5c\u5728\u8fde\u7eed\u63a8\u7406\u662f\u63d0\u5347\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u6027\u80fd\u7684\u6709\u524d\u666f\u65b9\u5411\uff0cMCOUT\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u80fd\u591f\u8fdb\u884c\u7c7b\u4f3c\u4eba\u7c7b\u53cd\u601d\u7c7b\u578b\u7684\u591a\u6a21\u6001\u63a8\u7406\u3002"}}
{"id": "2508.12638", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12638", "abs": "https://arxiv.org/abs/2508.12638", "authors": ["Chen Qian", "Xinran Yu", "Zewen Huang", "Danyang Li", "Qiang Ma", "Fan Dang", "Xuan Ding", "Guangyong Shang", "Zheng Yang"], "title": "SpotVLM: Cloud-edge Collaborative Real-time VLM based on Context Transfer", "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly deployed in real-time\napplications such as autonomous driving and human-computer interaction, which\ndemand fast and reliable responses based on accurate perception. To meet these\nrequirements, existing systems commonly employ cloud-edge collaborative\narchitectures, such as partitioned Large Vision-Language Models (LVLMs) or task\noffloading strategies between Large and Small Vision-Language Models (SVLMs).\nHowever, these methods fail to accommodate cloud latency fluctuations and\noverlook the full potential of delayed but accurate LVLM responses. In this\nwork, we propose a novel cloud-edge collaborative paradigm for VLMs, termed\nContext Transfer, which treats the delayed outputs of LVLMs as historical\ncontext to provide real-time guidance for SVLMs inference. Based on this\nparadigm, we design SpotVLM, which incorporates both context replacement and\nvisual focus modules to refine historical textual input and enhance visual\ngrounding consistency. Extensive experiments on three real-time vision tasks\nacross four datasets demonstrate the effectiveness of the proposed framework.\nThe new paradigm lays the groundwork for more effective and latency-aware\ncollaboration strategies in future VLM systems.", "AI": {"tldr": "\u57fa\u4e8e\u4e91\u7aef\u534f\u540c\u7684Vision-Language\u6a21\u578b\u6846\u67b6SpotVLM\uff0c\u901a\u8fc7\u5c06\u5927\u578b\u6a21\u578b\u7684\u5ef6\u8fdf\u8f93\u51fa\u4f5c\u4e3a\u5386\u53f2\u4e0a\u4e0b\u6587\u6765\u6307\u5c0f\u578b\u6a21\u578b\u7684\u5b9e\u65f6\u63a8\u7406", "motivation": "\u73b0\u6709\u4e91\u7aef\u534f\u540c\u65b9\u6848\u65e0\u6cd5\u5904\u7406\u4e91\u7aef\u5ef6\u8fdf\u6ce2\u52a8\uff0c\u4e14\u6ca1\u6709\u5145\u5206\u5229\u7528\u5927\u578b\u6a21\u578b\u5ef6\u8fdf\u4f46\u51c6\u786e\u7684\u54cd\u5e94", "method": "\u63d0\u51faContext Transfer\u6846\u67b6\uff0c\u5c06\u5927\u578bLVLM\u7684\u5ef6\u8fdf\u8f93\u51fa\u4f5c\u4e3a\u5386\u53f2\u4e0a\u4e0b\u6587\u6307\u5c0f\u578bSVLM\u7684\u5b9e\u65f6\u63a8\u7406\uff0c\u8bbe\u8ba1\u4e86\u4e0a\u4e0b\u6587\u66ff\u6362\u548c\u89c6\u89c9\u805a\u7126\u6a21\u5757", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u7684\u4e09\u4e2a\u5b9e\u65f6\u89c6\u89c9\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u65b0\u6846\u67b6\u4e3a\u672a\u6765VLM\u7cfb\u7edf\u7684\u6548\u7387\u548c\u5ef6\u8fdf\u611f\u77e5\u534f\u540c\u7b56\u7565\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2508.12603", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12603", "abs": "https://arxiv.org/abs/2508.12603", "authors": ["Can Cui", "Yupeng Zhou", "Juntong Peng", "Sung-Yeon Park", "Zichong Yang", "Prashanth Sankaranarayanan", "Jiaru Zhang", "Ruqi Zhang", "Ziran Wang"], "title": "ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving systems built on Vision Language Models (VLMs)\nhave shown significant promise, yet their reliance on autoregressive\narchitectures introduces some limitations for real-world applications. The\nsequential, token-by-token generation process of these models results in high\ninference latency and cannot perform bidirectional reasoning, making them\nunsuitable for dynamic, safety-critical environments. To overcome these\nchallenges, we introduce ViLaD, a novel Large Vision Language Diffusion (LVLD)\nframework for end-to-end autonomous driving that represents a paradigm shift.\nViLaD leverages a masked diffusion model that enables parallel generation of\nentire driving decision sequences, significantly reducing computational\nlatency. Moreover, its architecture supports bidirectional reasoning, allowing\nthe model to consider both past and future simultaneously, and supports\nprogressive easy-first generation to iteratively improve decision quality. We\nconduct comprehensive experiments on the nuScenes dataset, where ViLaD\noutperforms state-of-the-art autoregressive VLM baselines in both planning\naccuracy and inference speed, while achieving a near-zero failure rate.\nFurthermore, we demonstrate the framework's practical viability through a\nreal-world deployment on an autonomous vehicle for an interactive parking task,\nconfirming its effectiveness and soundness for practical applications.", "AI": {"tldr": "ViLaD\u662f\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u6b21\u6563\u6a21\u578b\u7684\u65b0\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u751f\u6210\u9a7e\u9a76\u51b3\u7b56\u5e8f\u5217\uff0c\u89e3\u51b3\u4e86\u81ea\u56de\u5f52VLM\u6a21\u578b\u7684\u9ad8\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\uff0c\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u89c4\u5212\u51c6\u786e\u6027\u548c\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u81ea\u56de\u5f52\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u7684\u5b9e\u65f6\u6027\u95ee\u9898\uff0c\u5305\u62ec\u9ad8\u63a8\u7406\u5ef6\u8fdf\u3001\u7f3a\u4e4f\u53cc\u5411\u63a8\u7406\u80fd\u529b\uff0c\u4ee5\u53ca\u5728\u52a8\u6001\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faViLaD\u6846\u67b6\uff0c\u5229\u7528\u63a9\u7801\u6b21\u6563\u6a21\u578b\u5e76\u884c\u751f\u6210\u6574\u4e2a\u9a7e\u9a76\u51b3\u7b56\u5e8f\u5217\uff0c\u652f\u6301\u53cc\u5411\u63a8\u7406\u548c\u6e10\u8fdb\u5f0f\u751f\u6210\uff0c\u901a\u8fc7\u8003\u8651\u8fc7\u53bb\u548c\u672a\u6765\u6765\u8fed\u4ee3\u6539\u5584\u51b3\u7b56\u8d28\u91cf\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0cViLaD\u5728\u89c4\u5212\u51c6\u786e\u6027\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u81ea\u56de\u5f52VLM\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u4e86\u8fd1\u96f6\u5931\u8d25\u7387\u3002\u5b9e\u9645\u90e8\u7f72\u5728\u81ea\u4e3b\u9a7e\u9a76\u8f66\u8fdb\u884c\u4ea4\u4e92\u505c\u8f66\u4efb\u52a1\u65f6\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u6709\u6548\u6027\u3002", "conclusion": "ViLaD\u4ee3\u8868\u4e86\u81ea\u4e3b\u9a7e\u9a76\u7cfb\u7edf\u7684\u8303\u5f0f\u8f6c\u79fb\uff0c\u901a\u8fc7\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u63a8\u7406\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12690", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12690", "abs": "https://arxiv.org/abs/2508.12690", "authors": ["Dongjae Jeon", "Taeheon Kim", "Seongwon Cho", "Minhyuk Seo", "Jonghyun Choi"], "title": "TTA-DAME: Test-Time Adaptation with Domain Augmentation and Model Ensemble for Dynamic Driving Conditions", "comment": null, "summary": "Test-time Adaptation (TTA) poses a challenge, requiring models to dynamically\nadapt and perform optimally on shifting target domains. This task is\nparticularly emphasized in real-world driving scenes, where weather domain\nshifts occur frequently. To address such dynamic changes, our proposed method,\nTTA-DAME, leverages source domain data augmentation into target domains.\nAdditionally, we introduce a domain discriminator and a specialized domain\ndetector to mitigate drastic domain shifts, especially from daytime to\nnighttime conditions. To further improve adaptability, we train multiple\ndetectors and consolidate their predictions through Non-Maximum Suppression\n(NMS). Our empirical validation demonstrates the effectiveness of our method,\nshowing significant performance enhancements on the SHIFT Benchmark.", "AI": {"tldr": "TTA-DAME\u901a\u8fc7\u6e90\u57df\u6570\u636e\u589e\u5f3a\u3001\u57df\u5224\u522b\u5668\u548c\u4e13\u7528\u57df\u68c0\u6d4b\u5668\u5904\u7406\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5929\u6c14\u57df\u504f\u79fb\uff0c\u7279\u522b\u662f\u5728\u767d\u5929\u5230\u591c\u95f4\u7684\u5267\u70c8\u53d8\u5316\uff0c\u901a\u8fc7\u591a\u68c0\u6d4b\u5668\u96c6\u6210\u548cNMS\u63d0\u5347\u6027\u80fd\uff0c\u5728SHIFT\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u6d4b\u8bd5\u65f6\u9002\u5e94(TTA)\u5728\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u4e2d\u9891\u7e41\u51fa\u73b0\u7684\u5929\u6c14\u57df\u504f\u79fb\u95ee\u9898\uff0c\u7279\u522b\u662f\u5e94\u5bf9\u4ece\u767d\u5929\u5230\u591c\u95f4\u7b49\u5267\u70c8\u57df\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u6e90\u57df\u6570\u636e\u589e\u5f3a\u5230\u76ee\u6807\u57df\uff0c\u5f15\u5165\u57df\u5224\u522b\u5668\u548c\u4e13\u7528\u57df\u68c0\u6d4b\u5668\u6765\u7f13\u89e3\u5267\u70c8\u57df\u504f\u79fb\uff0c\u8bad\u7ec3\u591a\u4e2a\u68c0\u6d4b\u5668\u5e76\u901a\u8fc7\u975e\u6781\u5927\u503c\u6291\u5236(NMS)\u6574\u5408\u9884\u6d4b\u7ed3\u679c\u3002", "result": "\u5728SHIFT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "TTA-DAME\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u52a8\u6001\u57df\u53d8\u5316\uff0c\u7279\u522b\u662f\u5728\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u53d8\u5316\uff0c\u4e3a\u5b9e\u65f6\u81ea\u9002\u5e94\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12605", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12605", "abs": "https://arxiv.org/abs/2508.12605", "authors": ["Wenjie Liao", "Jieyu Yuan", "Yifang Xu", "Chunle Guo", "Zilong Zhang", "Jihong Li", "Jiachen Fu", "Haotian Fan", "Tao Li", "Junhui Cui", "Chongyi Li"], "title": "ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have introduced a\nparadigm shift for Image Quality Assessment (IQA) from unexplainable image\nquality scoring to explainable IQA, demonstrating practical applications like\nquality control and optimization guidance. However, current explainable IQA\nmethods not only inadequately use the same distortion criteria to evaluate both\nUser-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also\nlack detailed quality analysis for monitoring image quality and guiding image\nrestoration. In this study, we establish the first large-scale Visual\nDistortion Assessment Instruction Tuning Dataset for UGC images, termed\nViDA-UGC, which comprises 11K images with fine-grained quality grounding,\ndetailed quality perception, and reasoning quality description data. This\ndataset is constructed through a distortion-oriented pipeline, which involves\nhuman subject annotation and a Chain-of-Thought (CoT) assessment framework.\nThis framework guides GPT-4o to generate quality descriptions by identifying\nand analyzing UGC distortions, which helps capturing rich low-level visual\nfeatures that inherently correlate with distortion patterns. Moreover, we\ncarefully select 476 images with corresponding 6,149 question answer pairs from\nViDA-UGC and invite a professional team to ensure the accuracy and quality of\nGPT-generated information. The selected and revised data further contribute to\nthe first UGC distortion assessment benchmark, termed ViDA-UGC-Bench.\nExperimental results demonstrate the effectiveness of the ViDA-UGC and CoT\nframework for consistently enhancing various image quality analysis abilities\nacross multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing\nGPT-4o.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u7684\u7528\u6237\u751f\u6210\u5185\u5bb9\u56fe\u50cf\u89c6\u89c9\u5931\u771f\u8bc4\u4f30\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6ViDA-UGC\uff0c\u901a\u8fc7\u94fe\u5f0f\u601d\u7eea\u6846\u67b6\u6309\u53d8\u5f62\u6807\u51c6\u751f\u6210\u8be6\u7ec6\u7684\u8d28\u91cf\u63cf\u8ff0\uff0c\u5e76\u6784\u5efa\u4e86\u8bc4\u6d4b\u57fa\u51c6ViDA-UGC-Bench\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56fe\u50cf\u8d28\u91cf\u5206\u6790\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u53ef\u89e3\u91ca\u6027\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u4e00\u662f\u5c06\u7528\u6237\u751f\u6210\u5185\u5bb9(UGC)\u548cAI\u751f\u6210\u5185\u5bb9(AIGC)\u56fe\u50cf\u6df7\u4e3a\u8c08\uff0c\u4f7f\u7528\u76f8\u540c\u7684\u5931\u771f\u6807\u51c6\u8fdb\u884c\u8bc4\u4f30\uff1b\u4e8c\u662f\u7f3a\u4e4f\u8be6\u7ec6\u7684\u8d28\u91cf\u5206\u6790\u80fd\u529b\uff0c\u65e0\u6cd5\u6709\u6548\u76d1\u63a7\u56fe\u50cf\u8d28\u91cf\u548c\u6307\u5bfc\u56fe\u50cf\u6062\u590d\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u7684ViDA-UGC\u6570\u636e\u96c6\uff0c\u5305\u542b11K\u5f20\u56fe\u50cf\uff0c\u5177\u6709\u7ec6\u7c92\u5ea6\u8d28\u91cf\u57fa\u51c6\u3001\u8be6\u7ec6\u8d28\u91cf\u611f\u77e5\u548c\u63a8\u7406\u8d28\u91cf\u63cf\u8ff0\u3002\u901a\u8fc7\u5931\u771f\u5bfc\u5411\u7684\u6d41\u6c34\u7ebf\u6d41\u7a0b\uff0c\u7ed3\u5408\u4eba\u5de5\u6807\u6ce8\u548c\u94fe\u5f0f\u601d\u7eea(CoT)\u8bc4\u4f30\u6846\u67b6\uff0c\u6307\u5bfcGPT-4o\u8bc6\u522b\u548c\u5206\u6790UGC\u5931\u771f\u751f\u6210\u8d28\u91cf\u63cf\u8ff0\u3002\u4ece\u4e2d\u7cbe\u9009\u4e86476\u5f20\u56fe\u50cf\u548c6,149\u4e2a\u95ee\u7b54\u5bf9\uff0c\u6784\u5efa\u4e86ViDA-UGC-Bench\u8bc4\u6d4b\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cViDA-UGC\u6570\u636e\u96c6\u548cCoT\u6846\u67b6\u80fd\u591f\u4e00\u81f4\u5730\u63d0\u5347\u591a\u4e2a\u57fa\u7840MLLM\u6a21\u578b\u5728ViDA-UGC-Bench\u548cQ-Bench\u4e0a\u7684\u5404\u79cd\u56fe\u50cf\u8d28\u91cf\u5206\u6790\u80fd\u529b\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86GPT-4o\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aUGC\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u6570\u636e\u96c6\u548c\u8bc4\u6d4b\u57fa\u51c6\uff0c\u901a\u8fc7\u94fe\u5f0f\u601d\u7eea\u6846\u67b6\u751f\u6210\u7684\u8be6\u7ec6\u8d28\u91cf\u63cf\u8ff0\u80fd\u591f\u6293\u53d6\u4e0e\u5931\u771f\u6a21\u5f0f\u5185\u5728\u76f8\u5173\u7684\u4e30\u5bcc\u4f4e\u7ea7\u89c6\u89c9\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56fe\u50cf\u8d28\u91cf\u5206\u6790\u6027\u80fd\u3002"}}
{"id": "2508.12692", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12692", "abs": "https://arxiv.org/abs/2508.12692", "authors": ["Taeheon Kim", "San Kim", "Minhyuk Seo", "Dongjae Jeon", "Wonje Jeong", "Jonghyun Choi"], "title": "Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning", "comment": null, "summary": "Class-incremental with repetition (CIR), where previously trained classes\nrepeatedly introduced in future tasks, is a more realistic scenario than the\ntraditional class incremental setup, which assumes that each task contains\nunseen classes. CIR assumes that we can easily access abundant unlabeled data\nfrom external sources, such as the Internet. Therefore, we propose two\ncomponents that efficiently use the unlabeled data to ensure the high stability\nand the plasticity of models trained in CIR setup. First, we introduce\nmulti-level knowledge distillation (MLKD) that distills knowledge from multiple\nprevious models across multiple perspectives, including features and logits, so\nthe model can maintain much various previous knowledge. Moreover, we implement\ndynamic self-supervised loss (SSL) to utilize the unlabeled data that\naccelerates the learning of new classes, while dynamic weighting of SSL keeps\nthe focus of training to the primary task. Both of our proposed components\nsignificantly improve the performance in CIR setup, achieving 2nd place in the\nCVPR 5th CLVISION Challenge.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u7c7b\u522b\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u91cd\u590d\u7c7b\u95ee\u9898\uff1a\u591a\u7ea7\u77e5\u8bc6\u84c4\u7c89\u548c\u52a8\u6001\u81ea\u76d1\u7763\u635f\u5931\uff0c\u5229\u7528\u5916\u90e8\u672a\u6807\u6ce8\u6570\u636e\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7c7b\u522b\u589e\u91cf\u5b66\u4e60\u5047\u8bbe\u6bcf\u4e2a\u4efb\u52a1\u90fd\u5305\u542b\u65b0\u7c7b\u522b\uff0c\u800c\u5b9e\u9645\u60c5\u51b5\u4e0b\u65e7\u7c7b\u522b\u53ef\u80fd\u91cd\u590d\u51fa\u73b0\u3002\u8fd9\u79cd\u7c7b\u522b\u589e\u91cf\u91cd\u590d(CIR)\u573a\u666f\u66f4\u52a0\u73b0\u5b9e\uff0c\u4e14\u53ef\u4ee5\u5229\u7528\u7f51\u7edc\u7b49\u5916\u90e8\u6e90\u7684\u4e30\u5bcc\u672a\u6807\u6ce8\u6570\u636e\u3002", "method": "1. \u591a\u7ea7\u77e5\u8bc6\u84c4\u7c89(MLKD)\uff1a\u4ece\u591a\u4e2a\u5386\u53f2\u6a21\u578b\u4e2d\u84c4\u7c89\u77e5\u8bc6\uff0c\u5305\u62ec\u7279\u5f81\u548c\u903b\u8f91\u503c\u7b49\u591a\u4e2a\u89c6\u89d2\u3002\n2. \u52a8\u6001\u81ea\u76d1\u7763\u635f\u5931(SSL)\uff1a\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u52a0\u901f\u65b0\u7c7b\u522b\u5b66\u4e60\uff0c\u901a\u8fc7\u52a8\u6001\u6743\u91cd\u4fdd\u6301\u4e3b\u8981\u4efb\u52a1\u7684\u91cd\u70b9\u3002", "result": "\u65b9\u6cd5\u5728CIR\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u83b7\u5f97\u4e86CVPR\u7b2c5\u5c4aCLVISION\u6311\u6218\u8d5b\u7684\u7b2c2\u540d\u3002", "conclusion": "\u901a\u8fc7\u6709\u6548\u5229\u7528\u5916\u90e8\u672a\u6807\u6ce8\u6570\u636e\u548c\u591a\u6b21\u5e73\u6cbf\u7684\u77e5\u8bc6\u84c4\u7c89\u6280\u672f\uff0c\u53ef\u4ee5\u5728\u7c7b\u522b\u589e\u91cf\u91cd\u590d\u573a\u666f\u4e2d\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u3002"}}
{"id": "2508.12745", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12745", "abs": "https://arxiv.org/abs/2508.12745", "authors": ["Xizhan Gao", "Wei Hu"], "title": "DCSCR: A Class-Specific Collaborative Representation based Network for Image Set Classification", "comment": null, "summary": "Image set classification (ISC), which can be viewed as a task of comparing\nsimilarities between sets consisting of unordered heterogeneous images with\nvariable quantities and qualities, has attracted growing research attention in\nrecent years. How to learn effective feature representations and how to explore\nthe similarities between different image sets are two key yet challenging\nissues in this field. However, existing traditional ISC methods classify image\nsets based on raw pixel features, ignoring the importance of feature learning.\nExisting deep ISC methods can learn deep features, but they fail to adaptively\nadjust the features when measuring set distances, resulting in limited\nperformance in few-shot ISC. To address the above issues, this paper combines\ntraditional ISC methods with deep models and proposes a novel few-shot ISC\napproach called Deep Class-specific Collaborative Representation (DCSCR)\nnetwork to simultaneously learn the frame- and concept-level feature\nrepresentations of each image set and the distance similarities between\ndifferent sets. Specifically, DCSCR consists of a fully convolutional deep\nfeature extractor module, a global feature learning module, and a\nclass-specific collaborative representation-based metric learning module. The\ndeep feature extractor and global feature learning modules are used to learn\n(local and global) frame-level feature representations, while the\nclass-specific collaborative representation-based metric learning module is\nexploit to adaptively learn the concept-level feature representation of each\nimage set and thus obtain the distance similarities between different sets by\ndeveloping a new CSCR-based contrastive loss function. Extensive experiments on\nseveral well-known few-shot ISC datasets demonstrate the effectiveness of the\nproposed method compared with some state-of-the-art image set classification\nalgorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u7c7b\u522b\u7279\u5f02\u534f\u540c\u8868\u5f81\u7f51\u7edc(DCSCR)\uff0c\u7528\u4e8e\u89e3\u51b3\u5c11\u6837\u672c\u56fe\u50cf\u96c6\u5206\u7c7b\u95ee\u9898\uff0c\u540c\u65f6\u5b66\u4e60\u6846\u67b6\u7ea7\u548c\u6982\u5ff5\u7ea7\u7279\u5f81\u8868\u5f81\uff0c\u5e76\u901a\u8fc7\u7c7b\u522b\u7279\u5f02\u534f\u540c\u8868\u5f81\u5b9e\u73b0\u81ea\u9002\u5e94\u6027\u8ddd\u79bb\u6d4b\u91cf\u3002", "motivation": "\u56fe\u50cf\u96c6\u5206\u7c7b\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\u662f\u5982\u4f55\u5b66\u4e60\u6709\u6548\u7279\u5f81\u8868\u5f81\u548c\u63a2\u7d22\u4e0d\u540c\u56fe\u50cf\u96c6\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u5ffd\u89c6\u7279\u5f02\u5b66\u4e60\uff0c\u800c\u73b0\u6709\u6df1\u5ea6\u65b9\u6cd5\u5728\u6d4b\u91cf\u96c6\u8ddd\u79bb\u65f6\u65e0\u6cd5\u81ea\u9002\u5e94\u8c03\u6574\u7279\u5f02\uff0c\u5bfc\u81f4\u5c11\u6837\u672c\u6027\u80fd\u6709\u9650\u3002", "method": "\u7ed3\u5408\u4f20\u7edfISC\u65b9\u6cd5\u4e0e\u6df1\u5ea6\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e86DCSCR\u7f51\u7edc\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u5168\u5377\u79ef\u6df1\u5ea6\u7279\u5f02\u63d0\u53d6\u5668\u3001\u5168\u5c40\u7279\u5f02\u5b66\u4e60\u6a21\u5757\u548c\u57fa\u4e8e\u7c7b\u522b\u7279\u5f02\u534f\u540c\u8868\u5f81\u7684\u8ddd\u79bb\u5b66\u4e60\u6a21\u5757\u3002\u524d\u4e24\u4e2a\u6a21\u5757\u5b66\u4e60\u6846\u67b6\u7ea7\u7279\u5f02\uff0c\u540e\u8005\u901a\u8fc7\u65b0\u7684CSCR\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\u5b66\u4e60\u6982\u5ff5\u7ea7\u7279\u5f02\u8868\u5f81\u548c\u96c6\u8ddd\u79bb\u3002", "result": "\u5728\u591a\u4e2a\u77e5\u540d\u7684\u5c11\u6837\u672c\u56fe\u50cf\u96c6\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4e0e\u4e00\u4e9b\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u96c6\u5206\u7c7b\u7b97\u6cd5\u76f8\u6bd4\u5177\u6709\u663e\u8457\u7684\u6548\u679c\u6027\u3002", "conclusion": "DCSCR\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u5b66\u4e60\u6846\u67b6\u7ea7\u548c\u6982\u5ff5\u7ea7\u7279\u5f02\u8868\u5f81\uff0c\u5e76\u901a\u8fc7\u7c7b\u522b\u7279\u5f02\u534f\u540c\u8868\u5f81\u5b9e\u73b0\u81ea\u9002\u5e94\u6027\u7684\u96c6\u8ddd\u79bb\u6d4b\u91cf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c11\u6837\u672c\u56fe\u50cf\u96c6\u5206\u7c7b\u7684\u6311\u6218\u3002"}}
{"id": "2508.12615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12615", "abs": "https://arxiv.org/abs/2508.12615", "authors": ["Wenhao Zhang", "Hao Zhu", "Delong Wu", "Di Kang", "Linchao Bao", "Zhan Ma", "Xun Cao"], "title": "WIPES: Wavelet-based Visual Primitives", "comment": "IEEE/CVF International Conference on Computer Vision", "summary": "Pursuing a continuous visual representation that offers flexible frequency\nmodulation and fast rendering speed has recently garnered increasing attention\nin the fields of 3D vision and graphics. However, existing representations\noften rely on frequency guidance or complex neural network decoding, leading to\nspectrum loss or slow rendering. To address these limitations, we propose\nWIPES, a universal Wavelet-based vIsual PrimitivES for representing\nmulti-dimensional visual signals. Building on the spatial-frequency\nlocalization advantages of wavelets, WIPES effectively captures both the\nlow-frequency \"forest\" and the high-frequency \"trees.\" Additionally, we develop\na wavelet-based differentiable rasterizer to achieve fast visual rendering.\nExperimental results on various visual tasks, including 2D image\nrepresentation, 5D static and 6D dynamic novel view synthesis, demonstrate that\nWIPES, as a visual primitive, offers higher rendering quality and faster\ninference than INR-based methods, and outperforms Gaussian-based\nrepresentations in rendering quality.", "AI": {"tldr": "WIPES\u662f\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u7684\u901a\u7528\u89c6\u89c9\u57fa\u5143\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c0f\u6ce2\u7684\u7a7a\u95f4-\u9891\u7387\u5c40\u90e8\u5316\u4f18\u52bf\u6709\u6548\u6355\u6349\u9ad8\u4f4e\u9891\u4fe1\u606f\uff0c\u5e76\u5f00\u53d1\u4e86\u5c0f\u6ce2\u53ef\u5fae\u5206\u5149\u6805\u5316\u5668\u5b9e\u73b0\u5feb\u901f\u6e32\u67d3\uff0c\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6e32\u67d3\u8d28\u91cf\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8868\u793a\u65b9\u6cd5\u4f9d\u8d56\u9891\u7387\u5f15\u5bfc\u6216\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u89e3\u7801\uff0c\u5bfc\u81f4\u9891\u8c31\u635f\u5931\u6216\u6e32\u67d3\u901f\u5ea6\u6162\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u548c\u5feb\u901f\u63a8\u7406\u7684\u901a\u7528\u89c6\u89c9\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5c0f\u6ce2\u7684\u89c6\u89c9\u57fa\u5143\u8868\u793aWIPES\uff0c\u5229\u7528\u5c0f\u6ce2\u7684\u7a7a\u95f4-\u9891\u7387\u5c40\u90e8\u5316\u7279\u6027\u6355\u6349\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u5e76\u5f00\u53d1\u4e86\u5c0f\u6ce2\u53ef\u5fae\u5206\u5149\u6805\u5316\u5668\u6765\u5b9e\u73b0\u5feb\u901f\u89c6\u89c9\u6e32\u67d3\u3002", "result": "\u57282D\u56fe\u50cf\u8868\u793a\u30015D\u9759\u6001\u548c6D\u52a8\u6001\u65b0\u89c6\u89d2\u5408\u6210\u7b49\u89c6\u89c9\u4efb\u52a1\u4e2d\uff0cWIPES\u76f8\u6bd4\u57fa\u4e8eINR\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u6e32\u67d3\u8d28\u91cf\u548c\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u5728\u6e32\u67d3\u8d28\u91cf\u4e0a\u4e5f\u4f18\u4e8e\u57fa\u4e8e\u9ad8\u65af\u7684\u65b9\u6cd5\u3002", "conclusion": "WIPES\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u89c6\u89c9\u57fa\u5143\u8868\u793a\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9891\u8c31\u4fdd\u6301\u548c\u6e32\u67d3\u901f\u5ea6\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u591a\u7ef4\u5ea6\u89c6\u89c9\u4fe1\u53f7\u8868\u793a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12755", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12755", "abs": "https://arxiv.org/abs/2508.12755", "authors": ["Cristo J. van den Berg", "Frank G. te Nijenhuis", "Mirre J. Blaauboer", "Daan T. W. van Erp", "Carlijn M. Keppels", "Matthijs van der Sluijs", "Bob Roozenbeek", "Wim van Zwam", "Sandra Cornelissen", "Danny Ruijters", "Ruisheng Su", "Theo van Walsum"], "title": "CLAIRE-DSA: Fluoroscopic Image Classification for Quality Assurance of Computer Vision Pipelines in Acute Ischemic Stroke", "comment": "10 pages, 4 figures, workshop paper accepted at\n  https://switchmiccai.github.io/switch/", "summary": "Computer vision models can be used to assist during mechanical thrombectomy\n(MT) for acute ischemic stroke (AIS), but poor image quality often degrades\nperformance. This work presents CLAIRE-DSA, a deep learning--based framework\ndesigned to categorize key image properties in minimum intensity projections\n(MinIPs) acquired during MT for AIS, supporting downstream quality control and\nworkflow optimization. CLAIRE-DSA uses pre-trained ResNet backbone models,\nfine-tuned to predict nine image properties (e.g., presence of contrast,\nprojection angle, motion artefact severity). Separate classifiers were trained\non an annotated dataset containing $1,758$ fluoroscopic MinIPs. The model\nachieved excellent performance on all labels, with ROC-AUC ranging from $0.91$\nto $0.98$, and precision ranging from $0.70$ to $1.00$. The ability of\nCLAIRE-DSA to identify suitable images was evaluated on a segmentation task by\nfiltering poor quality images and comparing segmentation performance on\nfiltered and unfiltered datasets. Segmentation success rate increased from\n$42%$ to $69%$, $p < 0.001$. CLAIRE-DSA demonstrates strong potential as an\nautomated tool for accurately classifying image properties in DSA series of\nacute ischemic stroke patients, supporting image annotation and quality control\nin clinical and research applications. Source code is available at\nhttps://gitlab.com/icai-stroke-lab/wp3_neurointerventional_ai/claire-dsa.", "AI": {"tldr": "CLAIRE-DSA\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6025\u6027\u7f3a\u8840\u6027\u5352\u4e2d\u673a\u68b0\u53d6\u6813\u672f\u4e2d\u5206\u7c7b\u5173\u952e\u56fe\u50cf\u5c5e\u6027\uff0c\u63d0\u9ad8\u4e0b\u6e38\u8d28\u91cf\u63a7\u5236\u548c\u5de5\u4f5c\u6d41\u7a0b\u4f18\u5316\u3002", "motivation": "\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5728\u673a\u68b0\u53d6\u6813\u672f\u4e2d\u8f85\u52a9\u5e94\u7528\u65f6\uff0c\u56fe\u50cf\u8d28\u91cf\u5dee\u4f1a\u964d\u4f4e\u6027\u80fd\u8868\u73b0\uff0c\u9700\u8981\u81ea\u52a8\u5316\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684ResNet\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\uff0c\u9884\u6d4b\u4e5d\u4e2a\u56fe\u50cf\u5c5e\u6027\uff08\u5982\u5bf9\u6bd4\u5ea6\u5b58\u5728\u3001\u6295\u5f71\u89d2\u5ea6\u3001\u8fd0\u52a8\u4f2a\u5f71\u4e25\u91cd\u7a0b\u5ea6\u7b49\uff09\uff0c\u5728\u5305\u542b1,758\u5f20\u8367\u5149MinIPs\u7684\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5355\u72ec\u5206\u7c7b\u5668\u3002", "result": "\u6a21\u578b\u5728\u6240\u6709\u6807\u7b7e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cROC-AUC\u8303\u56f4\u4e3a0.91-0.98\uff0c\u7cbe\u786e\u5ea6\u8303\u56f4\u4e3a0.70-1.00\u3002\u5728\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u56fe\u50cf\u540e\u5206\u5272\u6210\u529f\u7387\u4ece42%\u63d0\u5347\u81f369%\uff08p<0.001\uff09\u3002", "conclusion": "CLAIRE-DSA\u4f5c\u4e3a\u81ea\u52a8\u5316\u5de5\u5177\u5728\u6025\u6027\u7f3a\u8840\u6027\u5352\u4e2d\u60a3\u8005DSA\u5e8f\u5217\u4e2d\u51c6\u786e\u5206\u7c7b\u56fe\u50cf\u5c5e\u6027\u65b9\u9762\u663e\u793a\u51fa\u5f3a\u5927\u6f5c\u529b\uff0c\u652f\u6301\u4e34\u5e8a\u548c\u7814\u7a76\u5e94\u7528\u4e2d\u7684\u56fe\u50cf\u6807\u6ce8\u548c\u8d28\u91cf\u63a7\u5236\u3002"}}
{"id": "2508.12628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12628", "abs": "https://arxiv.org/abs/2508.12628", "authors": ["Yukang Lin", "Xiang Zhang", "Shichang Jia", "Bowen Wan", "Chenghan Fu", "Xudong Ren", "Yueran Liu", "Wanxian Guan", "Pengji Wang", "Jian Xu", "Bo Zheng", "Baolin Liu"], "title": "Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning", "comment": null, "summary": "Creative image in advertising is the heart and soul of e-commerce platform.\nAn eye-catching creative image can enhance the shopping experience for users,\nboosting income for advertisers and advertising revenue for platforms. With the\nadvent of AIGC technology, advertisers can produce large quantities of creative\nimages at minimal cost. However, they struggle to assess the creative quality\nto select. Existing methods primarily focus on creative ranking, which fails to\naddress the need for explainable creative selection.\n  In this work, we propose the first paradigm for explainable creative\nassessment and selection. Powered by multimodal large language models (MLLMs),\nour approach integrates the assessment and selection of creative images into a\nnatural language generation task. To facilitate this research, we construct\nCreativePair, the first comparative reasoning-induced creative dataset\nfeaturing 8k annotated image pairs, with each sample including a label\nindicating which image is superior. Additionally, we introduce Creative4U\n(pronounced Creative for You), a MLLMs-based creative selector that takes into\naccount users' interests. Through Reason-to-Select RFT, which includes\nsupervised fine-tuning with Chain-of-Thought (CoT-SFT) and Group Relative\nPolicy Optimization (GRPO) based reinforcement learning, Creative4U is able to\nevaluate and select creative images accurately. Both offline and online\nexperiments demonstrate the effectiveness of our approach. Our code and dataset\nwill be made public to advance research and industrial applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u53ef\u89e3\u91ca\u6027\u521b\u610f\u56fe\u7247\u8bc4\u4f30\u4e0e\u9009\u62e9\u65b9\u6cd5Creative4U\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7Reason-to-Select RFT\u8bad\u7ec3\u65b9\u6cd5\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u521b\u610f\u56fe\u7247\u9009\u62e9\u3002", "motivation": "\u5f53\u524dAIGC\u6280\u672f\u80fd\u4ee5\u4f4e\u6210\u672c\u751f\u4ea7\u5927\u91cf\u521b\u610f\u56fe\u7247\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u9009\u62e9\u8d28\u91cf\u6700\u4f73\u7684\u521b\u610f\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6392\u540d\u800c\u65e0\u6cd5\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9009\u62e9\u3002", "method": "\u6784\u5efaCreativePair\u6570\u636e\u96c6\uff088k\u5e26\u6ce8\u91ca\u7684\u56fe\u7247\u5bf9\uff09\uff0c\u63d0\u51faCreative4U\u6a21\u578b\uff0c\u91c7\u7528Reason-to-Select RFT\u8bad\u7ec3\u65b9\u6cd5\uff08\u5305\u62ecCoT-SFT\u548cGRPO\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5c06\u521b\u610f\u8bc4\u4f30\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4efb\u52a1\u3002", "result": "\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u90fd\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u51c6\u786e\u8bc4\u4f30\u548c\u9009\u62e9\u521b\u610f\u56fe\u7247\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u521b\u610f\u56fe\u7247\u9009\u62e9\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7MLLMs\u6280\u672f\u5b9e\u73b0\u4e86\u8003\u8651\u7528\u6237\u5174\u8da3\u7684\u667a\u80fd\u9009\u62e9\uff0c\u5bf9\u4e92\u8054\u7f51\u5e7f\u544a\u884c\u4e1a\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.12766", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12766", "abs": "https://arxiv.org/abs/2508.12766", "authors": ["Peihao Li", "Yan Fang", "Man Liu", "Huihui Bai", "Anhong Wang", "Yunchao Wei", "Yao Zhao"], "title": "Harnessing Group-Oriented Consistency Constraints for Semi-Supervised Semantic Segmentation in CdZnTe Semiconductors", "comment": null, "summary": "Labeling Cadmium Zinc Telluride (CdZnTe) semiconductor images is challenging\ndue to the low-contrast defect boundaries, necessitating annotators to\ncross-reference multiple views. These views share a single ground truth (GT),\nforming a unique ``many-to-one'' relationship. This characteristic renders\nadvanced semi-supervised semantic segmentation (SSS) methods suboptimal, as\nthey are generally limited by a ``one-to-one'' relationship, where each image\nis independently associated with its GT. Such limitation may lead to error\naccumulation in low-contrast regions, further exacerbating confirmation bias.\nTo address this issue, we revisit the SSS pipeline from a group-oriented\nperspective and propose a human-inspired solution: the Intra-group Consistency\nAugmentation Framework (ICAF). First, we experimentally validate the inherent\nconsistency constraints within CdZnTe groups, establishing a group-oriented\nbaseline using the Intra-group View Sampling (IVS). Building on this insight,\nwe introduce the Pseudo-label Correction Network (PCN) to enhance consistency\nrepresentation, which consists of two key modules. The View Augmentation Module\n(VAM) improves boundary details by dynamically synthesizing a boundary-aware\nview through the aggregation of multiple views. In the View Correction Module\n(VCM), this synthesized view is paired with other views for information\ninteraction, effectively emphasizing salient regions while minimizing noise.\nExtensive experiments demonstrate the effectiveness of our solution for CdZnTe\nmaterials. Leveraging DeepLabV3+ with a ResNet-101 backbone as our segmentation\nmodel, we achieve a 70.6\\% mIoU on the CdZnTe dataset using only 2\ngroup-annotated data (5\\textperthousand). The code is available at\n\\href{https://github.com/pipixiapipi/ICAF}{https://github.com/pipixiapipi/ICAF}.", "AI": {"tldr": "\u63d0\u51fa\u4e86ICAF\u6846\u67b6\u89e3\u51b3CdZnTe\u534a\u5bfc\u4f53\u56fe\u50cf\u6807\u6ce8\u96be\u9898\uff0c\u901a\u8fc7\u7ec4\u5185\u4e00\u81f4\u6027\u589e\u5f3a\u548c\u591a\u89c6\u56fe\u4fe1\u606f\u4ea4\u4e92\uff0c\u5728\u4ec5\u4f7f\u75282%\u6807\u6ce8\u6570\u636e\u4e0b\u8fbe\u523070.6% mIoU", "motivation": "CdZnTe\u534a\u5bfc\u4f53\u56fe\u50cf\u5b58\u5728\u4f4e\u5bf9\u6bd4\u5ea6\u7f3a\u9677\u8fb9\u754c\uff0c\u9700\u8981\u591a\u89c6\u56fe\u4ea4\u53c9\u53c2\u8003\uff0c\u4f20\u7edf\u534a\u76d1\u7763\u5206\u5272\u65b9\u6cd5\u53d7\u9650\u4e8e\u4e00\u5bf9\u4e00\u5173\u7cfb\uff0c\u5728\u4f4e\u5bf9\u6bd4\u5ea6\u533a\u57df\u5bb9\u6613\u4ea7\u751f\u8bef\u5dee\u7d2f\u79ef", "method": "\u63d0\u51fa\u7ec4\u5185\u4e00\u81f4\u6027\u589e\u5f3a\u6846\u67b6ICAF\uff0c\u5305\u542b\u4f2a\u6807\u7b7e\u6821\u6b63\u7f51\u7edcPCN\uff0c\u901a\u8fc7\u89c6\u56fe\u589e\u5f3a\u6a21\u5757\u52a8\u6001\u5408\u6210\u8fb9\u754c\u611f\u77e5\u89c6\u56fe\uff0c\u89c6\u56fe\u6821\u6b63\u6a21\u5757\u8fdb\u884c\u4fe1\u606f\u4ea4\u4e92\u4ee5\u7a81\u51fa\u663e\u8457\u533a\u57df\u5e76\u51cf\u5c11\u566a\u58f0", "result": "\u4f7f\u7528DeepLabV3+\u548cResNet-101\u9aa8\u5e72\u7f51\u7edc\uff0c\u5728\u4ec52%\u7ec4\u6807\u6ce8\u6570\u636e\u4e0b\uff0c\u5728CdZnTe\u6570\u636e\u96c6\u4e0a\u8fbe\u523070.6% mIoU", "conclusion": "ICAF\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86CdZnTe\u6750\u6599\u7684\u591a\u89c6\u56fe\u6807\u6ce8\u6311\u6218\uff0c\u901a\u8fc7\u7ec4\u5185\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u89c6\u56fe\u5408\u6210\u6821\u6b63\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5bf9\u6bd4\u5ea6\u533a\u57df\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd"}}
{"id": "2508.12794", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12794", "abs": "https://arxiv.org/abs/2508.12794", "authors": ["Kyriaki", "Kokka", "Rahul Goel", "Ali Abbas", "Kerry A. Nice", "Luca Martial", "SM Labib", "Rihuan Ke", "Carola Bibiane Sch\u00f6nlieb", "James Woodcock"], "title": "Vehicle detection from GSV imagery: Predicting travel behaviour for cycling and motorcycling using Computer Vision", "comment": null, "summary": "Transportation influence health by shaping exposure to physical activity, air\npollution and injury risk.Comparative data on cycling and motorcycling\nbehaviours is scarce, particularly at a global scale.Street view imagery, such\nas Google Street View (GSV), combined with computer vision, is a valuable\nresource for efficiently capturing travel behaviour data.This study\ndemonstrates a novel approach using deep learning on street view images to\nestimate cycling and motorcycling levels across diverse cities worldwide.We\nutilized data from 185 global cities.The data on mode shares of cycling and\nmotorcycling estimated using travel surveys or censuses.We used GSV images to\ndetect cycles and motorcycles in sampled locations, using 8000 images per\ncity.The YOLOv4 model, fine-tuned using images from six cities, achieved a mean\naverage precision of 89% for detecting cycles and motorcycles in GSV images.A\nglobal prediction model was developed using beta regression with city-level\nmode shares as outcome, with log transformed explanatory variables of counts of\nGSV-detected images with cycles and motorcycles, while controlling for\npopulation density.We found strong correlations between GSV motorcycle counts\nand motorcycle mode share (0.78) and moderate correlations between GSV cycle\ncounts and cycling mode share (0.51).Beta regression models predicted mode\nshares with $R^2$ values of 0.614 for cycling and 0.612 for motorcycling,\nachieving median absolute errors (MDAE) of 1.3% and 1.4%,\nrespectively.Scatterplots demonstrated consistent prediction accuracy, though\ncities like Utrecht and Cali were outliers.The model was applied to 60 cities\nglobally for which we didn't have recent mode share data.We provided estimates\nfor some cities in the Middle East, Latin America and East Asia.With computer\nvision, GSV images capture travel modes and activity, providing insights\nalongside traditional data sources.", "AI": {"tldr": "\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5206\u6790Google\u8857\u666f\u56fe\u50cf\uff0c\u53d1\u5c55\u4e86\u4e00\u79cd\u5168\u7403\u89c4\u6a21\u4f30\u8ba1\u81ea\u884c\u8f66\u548c\u6469\u6258\u8f66\u4f7f\u7528\u6c34\u5e73\u7684\u65b0\u65b9\u6cd5\uff0c\u76f8\u5173\u6027\u9ad8\u4e14\u9884\u6d4b\u51c6\u786e\u5ea6\u826f\u597d", "motivation": "\u4ea4\u901a\u65b9\u5f0f\u5f71\u54cd\u5065\u5eb7\uff0c\u4f46\u5168\u7403\u8303\u56f4\u5185\u81ea\u884c\u8f66\u548c\u6469\u6258\u8f66\u884c\u4e3a\u6570\u636e\u7f3a\u4e4f\uff0c\u9700\u8981\u9ad8\u6548\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5", "method": "\u4f7f\u7528YOLOv4\u6a21\u578b\u5206\u6790185\u4e2a\u57ce\u5e02\u7684Google\u8857\u666f\u56fe\u50cf\uff0c\u6bcf\u4e2a\u57ce\u5e02\u91c7\u68378000\u5f20\u56fe\u7247\uff0c\u901a\u8fc7\u8c03\u6574\u6a21\u578b\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\uff0c\u4f7f\u7528beta\u56de\u5f52\u5efa\u7acb\u5168\u7403\u9884\u6d4b\u6a21\u578b", "result": "\u6469\u6258\u8f66\u68c0\u6d4b\u4e0e\u5b9e\u9645\u4f7f\u7528\u6c34\u5e73\u76f8\u5173\u7cfb\u6570\u9ad8\u8fbe0.78\uff0c\u81ea\u884c\u8f660.51\uff0c\u9884\u6d4b\u6a21\u578bR\u00b2\u503c\u5206\u522b\u4e3a0.614\u548c0.612\uff0c\u4e2d\u4f4d\u7edd\u5bf9\u8bef\u5dee\u4ec5\u4e3a1.3%\u548c1.4%", "conclusion": "\u8857\u666f\u56fe\u50cf\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u53ef\u4ee5\u9ad8\u6548\u6355\u83b7\u4ea4\u901a\u884c\u4e3a\u6570\u636e\uff0c\u4e3a\u4f20\u7edf\u6570\u636e\u6e90\u63d0\u4f9b\u8865\u5145\uff0c\u5c24\u5176\u5728\u6570\u636e\u7f3a\u4e4f\u5730\u533a\u5177\u6709\u91cd\u8981\u4ef7\u503c"}}
{"id": "2508.12640", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12640", "abs": "https://arxiv.org/abs/2508.12640", "authors": ["Bastian Brandst\u00f6tter", "Erich Kobler"], "title": "Synthesizing Accurate and Realistic T1-weighted Contrast-Enhanced MR Images using Posterior-Mean Rectified Flow", "comment": "12 pages, 3 figures, MICCAI workshops (SASHIMI) 2025", "summary": "Contrast-enhanced (CE) T1-weighted MRI is central to neuro-oncologic\ndiagnosis but requires gadolinium-based agents, which add cost and scan time,\nraise environmental concerns, and may pose risks to patients. In this work, we\npropose a two-stage Posterior-Mean Rectified Flow (PMRF) pipeline for\nsynthesizing volumetric CE brain MRI from non-contrast inputs. First, a\npatch-based 3D U-Net predicts the voxel-wise posterior mean (minimizing MSE).\nThen, this initial estimate is refined by a time-conditioned 3D rectified flow\nto incorporate realistic textures without compromising structural fidelity. We\ntrain this model on a multi-institutional collection of paired pre- and\npost-contrast T1w volumes (BraTS 2023-2025). On a held-out test set of 360\ndiverse volumes, our best refined outputs achieve an axial FID of $12.46$ and\nKID of $0.007$ ($\\sim 68.7\\%$ lower FID than the posterior mean) while\nmaintaining low volumetric MSE of $0.057$ ($\\sim 27\\%$ higher than the\nposterior mean). Qualitative comparisons confirm that our method restores\nlesion margins and vascular details realistically, effectively navigating the\nperception-distortion trade-off for clinical deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5PMRF\u7ba1\u9053\uff0c\u4ece\u975e\u5bf9\u6bd4MRI\u5408\u6210\u5bf9\u6bd4\u589e\u5f3a\u8111MRI\uff0c\u907f\u514d\u4f7f\u7528\u9486\u9020\u5f71\u5242", "motivation": "\u5bf9\u6bd4\u589e\u5f3aMRI\u9700\u8981\u9486\u9020\u5f71\u5242\uff0c\u589e\u52a0\u6210\u672c\u3001\u626b\u63cf\u65f6\u95f4\u3001\u73af\u5883\u95ee\u9898\uff0c\u5e76\u5bf9\u60a3\u8005\u6709\u6f5c\u5728\u98ce\u9669", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u5148\u75283D U-Net\u9884\u6d4b\u540e\u9a8c\u5747\u503c\uff0c\u518d\u7528\u65f6\u95f4\u6761\u4ef63D\u6574\u6d41\u6d41\u7ec6\u5316\u4ee5\u4fdd\u6301\u7ed3\u6784\u4fdd\u771f\u5ea6\u5e76\u589e\u52a0\u771f\u5b9e\u7eb9\u7406", "result": "\u5728360\u4e2a\u6d4b\u8bd5\u6837\u672c\u4e0a\uff0c\u6700\u4f73\u8f93\u51fa\u8fbe\u5230\u8f74\u5411FID 12.46\u548cKID 0.007\uff08\u6bd4\u540e\u9a8c\u5747\u503c\u4f4e68.7%\uff09\uff0c\u4f53\u79efMSE\u4e3a0.057\uff08\u6bd4\u540e\u9a8c\u5747\u503c\u9ad827%\uff09", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u771f\u5b9e\u6062\u590d\u75c5\u7076\u8fb9\u7f18\u548c\u8840\u7ba1\u7ec6\u8282\uff0c\u6709\u6548\u5e73\u8861\u611f\u77e5-\u5931\u771f\u6743\u8861\uff0c\u9002\u5408\u4e34\u5e8a\u90e8\u7f72"}}
{"id": "2508.12811", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12811", "abs": "https://arxiv.org/abs/2508.12811", "authors": ["Yikai Wang", "Zhouxia Wang", "Zhonghua Wu", "Qingyi Tao", "Kang Liao", "Chen Change Loy"], "title": "Next Visual Granularity Generation", "comment": null, "summary": "We propose a novel approach to image generation by decomposing an image into\na structured sequence, where each element in the sequence shares the same\nspatial resolution but differs in the number of unique tokens used, capturing\ndifferent level of visual granularity. Image generation is carried out through\nour newly introduced Next Visual Granularity (NVG) generation framework, which\ngenerates a visual granularity sequence beginning from an empty image and\nprogressively refines it, from global layout to fine details, in a structured\nmanner. This iterative process encodes a hierarchical, layered representation\nthat offers fine-grained control over the generation process across multiple\ngranularity levels. We train a series of NVG models for class-conditional image\ngeneration on the ImageNet dataset and observe clear scaling behavior. Compared\nto the VAR series, NVG consistently outperforms it in terms of FID scores (3.30\n-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to\nshowcase the capability and potential of the NVG framework. Our code and models\nwill be released.", "AI": {"tldr": "\u65b0\u7684NVG\u6846\u67b6\u901a\u8fc7\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u7684\u89c6\u89c9\u7c92\u5ea6\u5e8f\u5217\uff0c\u4ece\u5168\u5c40\u5e03\u5c40\u5230\u7ec6\u8282\u9010\u6b65\u7cbe\u7ec6\u5730\u751f\u6210\u56fe\u50cf\uff0c\u5728ImageNet\u4e0a\u8f83VAR\u7cfb\u5217\u83b7\u5f97\u66f4\u4f18\u7684FID\u6307\u6807\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u5c42\u6b21\u5316\u7684\u7c92\u5ea6\u63a7\u5236\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u4e0d\u540c\u7c92\u5ea6\u7ea7\u522b\u4e0a\u7cbe\u7ec6\u63a7\u5236\u751f\u6210\u8fc7\u7a0b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faNext Visual Granularity (NVG)\u751f\u6210\u6846\u67b6\uff0c\u5c06\u56fe\u50cf\u89e3\u6784\u4e3a\u7ed3\u6784\u5316\u5e8f\u5217\uff0c\u6bcf\u4e2a\u5143\u7d20\u5171\u4eab\u76f8\u540c\u7a7a\u95f4\u5206\u8fa8\u7387\u4f46\u4f7f\u7528\u4e0d\u540c\u6570\u91cf\u7684\u552f\u4e00token\u3002\u4ece\u7a7a\u767d\u56fe\u50cf\u5f00\u59cb\uff0c\u9010\u6b65\u7cbe\u7ec6\u5730\u751f\u6210\u4ece\u5168\u5c40\u5230\u7ec6\u8282\u7684\u89c6\u89c9\u7c92\u5ea6\u5e8f\u5217\u3002", "result": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684NVG\u6a21\u578b\u663e\u793a\u4e86\u6e05\u6670\u7684\u7f29\u653e\u884c\u4e3a\u3002\u4e0eVAR\u7cfb\u5217\u76f8\u6bd4\uff0cNVG\u5728FID\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u5bf9\u624b\uff083.30->3.03, 2.57->2.44, 2.09->2.06\uff09\u3002", "conclusion": "NVG\u6846\u67b6\u901a\u8fc7\u5c42\u6b21\u5316\u7684\u89c6\u89c9\u7c92\u5ea6\u5e8f\u5217\u751f\u6210\uff0c\u63d0\u4f9b\u4e86\u591a\u7c92\u5ea6\u7ea7\u522b\u7684\u7cbe\u7ec6\u63a7\u5236\u80fd\u529b\uff0c\u5728\u56fe\u50cf\u751f\u6210\u6027\u80fd\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.12643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12643", "abs": "https://arxiv.org/abs/2508.12643", "authors": ["Pinci Yang", "Peisong Wen", "Ke Ma", "Qianqian Xu"], "title": "Learn Faster and Remember More: Balancing Exploration and Exploitation for Continual Test-time Adaptation", "comment": null, "summary": "Continual Test-Time Adaptation (CTTA) aims to adapt a source pre-trained\nmodel to continually changing target domains during inference. As a fundamental\nprinciple, an ideal CTTA method should rapidly adapt to new domains\n(exploration) while retaining and exploiting knowledge from previously\nencountered domains to handle similar domains in the future. Despite\nsignificant advances, balancing exploration and exploitation in CTTA is still\nchallenging: 1) Existing methods focus on adjusting predictions based on\ndeep-layer outputs of neural networks. However, domain shifts typically affect\nshallow features, which are inefficient to be adjusted from deep predictions,\nleading to dilatory exploration; 2) A single model inevitably forgets knowledge\nof previous domains during the exploration, making it incapable of exploiting\nhistorical knowledge to handle similar future domains. To address these\nchallenges, this paper proposes a mean teacher framework that strikes an\nappropriate Balance between Exploration and Exploitation (BEE) during the CTTA\nprocess. For the former challenge, we introduce a Multi-level Consistency\nRegularization (MCR) loss that aligns the intermediate features of the student\nand teacher models, accelerating adaptation to the current domain. For the\nlatter challenge, we employ a Complementary Anchor Replay (CAR) mechanism to\nreuse historical checkpoints (anchors), recovering complementary knowledge for\ndiverse domains. Experiments show that our method significantly outperforms\nstate-of-the-art methods on several benchmarks, demonstrating its effectiveness\nfor CTTA tasks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u914d\u65b9\u6cd5BEE\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u4e00\u81f4\u6027\u6b63\u5219\u5316\u548c\u8865\u5145\u9510\u56de\u653e\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5728\u6301\u7eed\u9002\u914d\u4e2d\u5e73\u8861\u63a2\u7d22\u65b0\u57df\u548b\u5229\u7528\u5386\u53f2\u77e5\u8bc6\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u914d\u65b9\u6cd5\u5728\u5e73\u8861\u63a2\u7d22\u65b0\u57df\u548b\u5229\u7528\u5386\u53f2\u77e5\u8bc6\u65b9\u9762\u9047\u5230\u4e24\u5927\u6311\u6218\uff1a1\uff09\u57df\u504f\u79fb\u5f71\u54cd\u6d45\u5c42\u7279\u5f81\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6df1\u5c42\u8f93\u51fa\u8c03\u6574\uff0c\u5bfc\u81f4\u9002\u914d\u901f\u5ea6\u6162\uff1b2\uff09\u5355\u4e00\u6a21\u578b\u5728\u9002\u914d\u65b0\u57df\u65f6\u5bb9\u6613\u9057\u5fd8\u5386\u53f2\u77e5\u8bc6\u3002", "method": "\u63d0\u51faBEE\u6846\u67b6\uff0c\u91c7\u7528\u5747\u503c\u6559\u5e08\u6a21\u578b\u7ed3\u6784\u3002\u4f7f\u7528\u591a\u5c42\u6b21\u4e00\u81f4\u6027\u6b63\u5219\u5316(MCR)\u6355\u6349\u4e2d\u95f4\u7279\u5f81\u5e76\u52a0\u901f\u9002\u914d\u65b0\u57df\uff0c\u91c7\u7528\u8865\u5145\u9510\u56de\u653e(CAR)\u673a\u5236\u91cd\u7528\u5386\u53f2\u68c0\u67e5\u70b9\u6765\u6062\u590d\u591a\u6837\u5316\u7684\u57df\u77e5\u8bc6\u3002", "result": "\u5728\u591a\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\uff0cBEE\u65b9\u6cd5\u663e\u8457\u8d85\u8fc7\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u914d\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "BEE\u65b9\u6cd5\u901a\u8fc7\u5747\u503c\u6559\u5e08\u6846\u67b6\u3001\u591a\u5c42\u6b21\u7279\u5f81\u5bf9\u9f50\u548b\u8865\u5145\u5386\u53f2\u77e5\u8bc6\u91cd\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u9002\u914d\u4e2d\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3aCTTA\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12900", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12900", "abs": "https://arxiv.org/abs/2508.12900", "authors": ["Jiayi Wang", "Hadrien Reynaud", "Franciskus Xaverius Erick", "Bernhard Kainz"], "title": "CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis", "comment": null, "summary": "Generative modelling of entire CT volumes conditioned on clinical reports has\nthe potential to accelerate research through data augmentation,\nprivacy-preserving synthesis and reducing regulator-constraints on patient data\nwhile preserving diagnostic signals. With the recent release of CT-RATE, a\nlarge-scale collection of 3D CT volumes paired with their respective clinical\nreports, training large text-conditioned CT volume generation models has become\nachievable. In this work, we introduce CTFlow, a 0.5B latent flow matching\ntransformer model, conditioned on clinical reports. We leverage the A-VAE from\nFLUX to define our latent space, and rely on the CT-Clip text encoder to encode\nthe clinical reports. To generate consistent whole CT volumes while keeping the\nmemory constraints tractable, we rely on a custom autoregressive approach,\nwhere the model predicts the first sequence of slices of the volume from\ntext-only, and then relies on the previously generated sequence of slices and\nthe text, to predict the following sequence. We evaluate our results against\nstate-of-the-art generative CT model, and demonstrate the superiority of our\napproach in terms of temporal coherence, image diversity and text-image\nalignment, with FID, FVD, IS scores and CLIP score.", "AI": {"tldr": "CTFlow\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e34\u5e8a\u62a5\u544a\u751f\u62103D CT\u4f53\u79ef\u76840.5B\u6f5c\u5728\u6d41\u5339\u914d\u53d8\u6362\u5668\u6a21\u578b\uff0c\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u56fe\u50cf\u591a\u6837\u6027\u548c\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u901a\u8fc7\u57fa\u4e8e\u4e34\u5e8a\u62a5\u544a\u751f\u6210\u5b8c\u6574CT\u4f53\u79ef\uff0c\u53ef\u4ee5\u52a0\u901f\u533b\u5b66\u7814\u7a76\u3001\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u6570\u636e\u5408\u6210\uff0c\u5e76\u51cf\u5c11\u5bf9\u60a3\u8005\u6570\u636e\u7684\u76d1\u7ba1\u9650\u5236\uff0c\u540c\u65f6\u4fdd\u7559\u8bca\u65ad\u4fe1\u53f7", "method": "\u4f7f\u7528FLUX\u7684A-VAE\u5b9a\u4e49\u6f5c\u5728\u7a7a\u95f4\uff0cCT-Clip\u6587\u672c\u7f16\u7801\u5668\u7f16\u7801\u4e34\u5e8a\u62a5\u544a\u3002\u91c7\u7528\u81ea\u5b9a\u4e49\u81ea\u56de\u5f52\u65b9\u6cd5\u751f\u6210\u4e00\u81f4\u7684\u5168CT\u4f53\u79ef\uff0c\u5148\u57fa\u4e8e\u7eaf\u6587\u672c\u9884\u6d4b\u7b2c\u4e00\u4e2a\u5207\u7247\u5e8f\u5217\uff0c\u7136\u540e\u57fa\u4e8e\u5148\u524d\u751f\u6210\u7684\u5207\u7247\u5e8f\u5217\u548c\u6587\u672c\u6765\u9884\u6d4b\u540e\u7eed\u5e8f\u5217", "result": "\u5728FID\u3001FVD\u3001IS\u5206\u6570\u548cCLIP\u5206\u6570\u7b49\u6307\u6807\u4e0a\uff0cCTFlow\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u56fe\u50cf\u591a\u6837\u6027\u548c\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u65b9\u9762\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684CT\u751f\u6210\u6a21\u578b", "conclusion": "CTFlow\u6210\u529f\u5b9e\u73b0\u4e86\u57fa\u4e8e\u4e34\u5e8a\u62a5\u544a\u7684\u9ad8\u8d28\u91cf3D CT\u4f53\u79ef\u751f\u6210\uff0c\u4e3a\u533b\u5b66\u6570\u636e\u589e\u5f3a\u548c\u9690\u79c1\u4fdd\u62a4\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12644", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12644", "abs": "https://arxiv.org/abs/2508.12644", "authors": ["Hao Wen", "Hongbo Kang", "Jian Ma", "Jing Huang", "Yuanwang Yang", "Haozhe Lin", "Yu-Kun Lai", "Kun Li"], "title": "DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video", "comment": null, "summary": "3D reconstruction of dynamic crowds in large scenes has become increasingly\nimportant for applications such as city surveillance and crowd analysis.\nHowever, current works attempt to reconstruct 3D crowds from a static image,\ncausing a lack of temporal consistency and inability to alleviate the typical\nimpact caused by occlusions. In this paper, we propose DyCrowd, the first\nframework for spatio-temporally consistent 3D reconstruction of hundreds of\nindividuals' poses, positions and shapes from a large-scene video. We design a\ncoarse-to-fine group-guided motion optimization strategy for occlusion-robust\ncrowd reconstruction in large scenes. To address temporal instability and\nsevere occlusions, we further incorporate a VAE (Variational Autoencoder)-based\nhuman motion prior along with a segment-level group-guided optimization. The\ncore of our strategy leverages collective crowd behavior to address long-term\ndynamic occlusions. By jointly optimizing the motion sequences of individuals\nwith similar motion segments and combining this with the proposed Asynchronous\nMotion Consistency (AMC) loss, we enable high-quality unoccluded motion\nsegments to guide the motion recovery of occluded ones, ensuring robust and\nplausible motion recovery even in the presence of temporal desynchronization\nand rhythmic inconsistencies. Additionally, in order to fill the gap of no\nexisting well-annotated large-scene video dataset, we contribute a virtual\nbenchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction\nfrom large-scene videos. Experimental results demonstrate that the proposed\nmethod achieves state-of-the-art performance in the large-scene dynamic crowd\nreconstruction task. The code and dataset will be available for research\npurposes.", "AI": {"tldr": "DyCrowd\u662f\u9996\u4e2a\u4ece\u5927\u573a\u666f\u89c6\u9891\u4e2d\u91cd\u5efa\u6570\u767e\u4eba3D\u59ff\u6001\u3001\u4f4d\u7f6e\u548c\u5f62\u72b6\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u6846\u67b6\uff0c\u91c7\u7528\u7c97\u5230\u7ec6\u7684\u7fa4\u4f53\u5f15\u5bfc\u8fd0\u52a8\u4f18\u5316\u7b56\u7565\uff0c\u7ed3\u5408VAE\u8fd0\u52a8\u5148\u9a8c\u548c\u5f02\u6b65\u8fd0\u52a8\u4e00\u81f4\u6027\u635f\u5931\uff0c\u6709\u6548\u89e3\u51b3\u906e\u6321\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4ece\u9759\u6001\u56fe\u50cf\u91cd\u5efa3D\u4eba\u7fa4\u7f3a\u4e4f\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u906e\u6321\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4ece\u5927\u573a\u666f\u89c6\u9891\u4e2d\u91cd\u5efa\u52a8\u6001\u4eba\u7fa4\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7c97\u5230\u7ec6\u7684\u7fa4\u4f53\u5f15\u5bfc\u8fd0\u52a8\u4f18\u5316\u7b56\u7565\uff0c\u7ed3\u5408VAE\u8fd0\u52a8\u5148\u9a8c\u548c\u6bb5\u7ea7\u7fa4\u4f53\u5f15\u5bfc\u4f18\u5316\uff0c\u5229\u7528\u5f02\u6b65\u8fd0\u52a8\u4e00\u81f4\u6027\u635f\u5931(AMC)\u8ba9\u9ad8\u8d28\u91cf\u65e0\u906e\u6321\u8fd0\u52a8\u6bb5\u6307\u5bfc\u906e\u6321\u6bb5\u7684\u6062\u590d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5927\u573a\u666f\u52a8\u6001\u4eba\u7fa4\u91cd\u5efa\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u8d21\u732e\u4e86VirtualCrowd\u865a\u62df\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "DyCrowd\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u5927\u573a\u666f\u89c6\u9891\u4e2d\u7684\u52a8\u6001\u4eba\u7fa4\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u65f6\u95f4\u4e0d\u7a33\u5b9a\u6027\u548c\u4e25\u91cd\u906e\u6321\u95ee\u9898\uff0c\u4e3a\u57ce\u5e02\u76d1\u63a7\u548c\u4eba\u7fa4\u5206\u6790\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2508.12932", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12932", "abs": "https://arxiv.org/abs/2508.12932", "authors": ["Hongyang Chen", "Shaoling Pu", "Lingyu Zheng", "Zhongwu Sun"], "title": "SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for Class Incremental Learning with Small Memory", "comment": "Accepted by ICONIP2025", "summary": "In incremental learning, enhancing the generality of knowledge is crucial for\nadapting to dynamic data inputs. It can develop generalized representations or\nmore balanced decision boundaries, preventing the degradation of long-term\nknowledge over time and thus mitigating catastrophic forgetting. Some emerging\nincremental learning methods adopt an encoder-decoder architecture and have\nachieved promising results. In the encoder-decoder achitecture, improving the\ngeneralization capabilities of both the encoder and decoder is critical, as it\nhelps preserve previously learned knowledge while ensuring adaptability and\nrobustness to new, diverse data inputs. However, many existing continual\nmethods focus solely on enhancing one of the two components, which limits their\neffectiveness in mitigating catastrophic forgetting. And these methods perform\neven worse in small-memory scenarios, where only a limited number of historical\nsamples can be stored. To mitigate this limitation, we introduces SEDEG, a\ntwo-stage training framework for vision transformers (ViT), focusing on\nsequentially improving the generality of both Decoder and Encoder. Initially,\nSEDEG trains an ensembled encoder through feature boosting to learn generalized\nrepresentations, which subsequently enhance the decoder's generality and\nbalance the classifier. The next stage involves using knowledge distillation\n(KD) strategies to compress the ensembled encoder and develop a new, more\ngeneralized encoder. This involves using a balanced KD approach and feature KD\nfor effective knowledge transfer. Extensive experiments on three benchmark\ndatasets show SEDEG's superior performance, and ablation studies confirm the\nefficacy of its components. The code is available at\nhttps://github.com/ShaolingPu/CIL.", "AI": {"tldr": "SEDEG\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u5347\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u6cdb\u5316\u80fd\u529b\u6765\u7f13\u89e3\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u7279\u522b\u5728\u5c0f\u5185\u5b58\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709\u7684\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u7f16\u7801\u5668\u6216\u89e3\u7801\u5668\u4e2d\u7684\u4e00\u4e2a\u7ec4\u4ef6\uff0c\u9650\u5236\u4e86\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u7684\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u5c0f\u5185\u5b58\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u5dee", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u7279\u5f81\u589e\u5f3a\u8bad\u7ec3\u96c6\u6210\u7f16\u7801\u5668\u5b66\u4e60\u6cdb\u5316\u8868\u793a\uff0c\u63d0\u5347\u89e3\u7801\u5668\u6cdb\u5316\u80fd\u529b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u538b\u7f29\u96c6\u6210\u7f16\u7801\u5668\uff0c\u5f00\u53d1\u65b0\u7684\u6cdb\u5316\u7f16\u7801\u5668", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793aSEDEG\u5177\u6709\u4f18\u8d8a\u6027\u80fd\uff0c\u6d88\u878d\u7814\u7a76\u786e\u8ba4\u4e86\u5404\u7ec4\u4ef6\u6709\u6548\u6027", "conclusion": "SEDEG\u901a\u8fc7\u987a\u5e8f\u63d0\u5347\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u7279\u522b\u5728\u5c0f\u5185\u5b58\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa"}}
{"id": "2508.12663", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12663", "abs": "https://arxiv.org/abs/2508.12663", "authors": ["Seung Young Noh", "Ju Yong Chang"], "title": "Stable Diffusion-Based Approach for Human De-Occlusion", "comment": "MM 2025", "summary": "Humans can infer the missing parts of an occluded object by leveraging prior\nknowledge and visible cues. However, enabling deep learning models to\naccurately predict such occluded regions remains a challenging task.\nDe-occlusion addresses this problem by reconstructing both the mask and RGB\nappearance. In this work, we focus on human de-occlusion, specifically\ntargeting the recovery of occluded body structures and appearances. Our\napproach decomposes the task into two stages: mask completion and RGB\ncompletion. The first stage leverages a diffusion-based human body prior to\nprovide a comprehensive representation of body structure, combined with\noccluded joint heatmaps that offer explicit spatial cues about missing regions.\nThe reconstructed amodal mask then serves as a conditioning input for the\nsecond stage, guiding the model on which areas require RGB reconstruction. To\nfurther enhance RGB generation, we incorporate human-specific textual features\nderived using a visual question answering (VQA) model and encoded via a CLIP\nencoder. RGB completion is performed using Stable Diffusion, with decoder\nfine-tuning applied to mitigate pixel-level degradation in visible regions -- a\nknown limitation of prior diffusion-based de-occlusion methods caused by latent\nspace transformations. Our method effectively reconstructs human appearances\neven under severe occlusions and consistently outperforms existing methods in\nboth mask and RGB completion. Moreover, the de-occluded images generated by our\napproach can improve the performance of downstream human-centric tasks, such as\n2D pose estimation and 3D human reconstruction. The code will be made publicly\navailable.", "AI": {"tldr": "\u4e00\u79cd\u4e24\u9636\u6bb5\u6f14\u8fdb\u5f0f\u4eba\u4f53\u53bb\u906e\u853d\u65b9\u6cd5\uff0c\u5148\u901a\u8fc7\u6b21\u6563\u6a21\u578b\u5b8c\u6210\u906e\u853d\u533a\u57df\u7684\u63a9\u7801\u91cd\u5efa\uff0c\u518d\u4f7f\u7528\u7a33\u5b9a\u6b21\u6563\u6a21\u578b\u8fdb\u884cRGB\u5916\u89c2\u751f\u6210\uff0c\u5e76\u901a\u8fc7VQA\u6a21\u578b\u63d0\u53d6\u4eba\u4f53\u7279\u5f81\u63d0\u5347\u6548\u679c", "motivation": "\u4eba\u7c7b\u80fd\u591f\u901a\u8fc7\u5148\u9a8c\u77e5\u8bc6\u63a8\u65ad\u906e\u853d\u7269\u4f53\u7684\u7f3a\u5931\u90e8\u5206\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u51c6\u786e\u9884\u6d4b\u906e\u853d\u533a\u57df\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4eba\u4f53\u53bb\u906e\u853d\u4efb\u52a1\u4e2d", "method": "1. \u63a8\u7406\u9636\u6bb5\u5206\u89e3\uff1a\u5148\u63a9\u7801\u5b8c\u6210\u9636\u6bb5\uff0c\u4f7f\u7528\u6b21\u6563\u6a21\u578b\u7ed9\u51fa\u4eba\u4f53\u7ed3\u6784\u7684\u5b8c\u6574\u8868\u793a\uff0c\u7ed3\u5408\u906e\u853d\u5173\u8282\u70ed\u529b\u56fe\u63d0\u4f9b\u7a7a\u95f4\u7ebf\u7d22\n2. RGB\u5b8c\u6210\u9636\u6bb5\uff1a\u4f7f\u7528\u7a33\u5b9a\u6b21\u6563\u6a21\u578b\uff0c\u4ee5\u91cd\u5efa\u7684\u65e0\u6a21\u63a9\u7801\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\n3. \u63d0\u53d6\u4eba\u4f53\u7279\u5f81\uff1a\u901a\u8fc7VQA\u6a21\u578b\u63d0\u53d6\u4eba\u4f53\u7279\u5f81\u5e76\u7528CLIP\u7f16\u7801\u5668\u7f16\u7801\n4. \u7ec6\u8c03\u89e3\u7801\u5668\uff1a\u51cf\u5c11\u53ef\u89c1\u533a\u57df\u7684\u50cf\u7d20\u7ea7\u9000\u5316", "result": "\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u91cd\u5efa\u4e25\u91cd\u906e\u853d\u4e0b\u7684\u4eba\u4f53\u5916\u89c2\uff0c\u5728\u63a9\u7801\u548cRGB\u5b8c\u6210\u4efb\u52a1\u4e2d\u90fd\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u751a\u81f3\u80fd\u591f\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u59822D\u59ff\u52bf\u4f30\u8ba1\u548c3D\u4eba\u4f53\u91cd\u5efa\u7684\u6027\u80fd", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4eba\u4f53\u53bb\u906e\u853d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6f14\u8fdb\u5f0f\u63a5\u8fd1\u548c\u4eba\u4f53\u7279\u5f81\u63d0\u53d6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u906e\u853d\u533a\u57df\u9884\u6d4b\u7684\u6311\u6218\uff0c\u4e3a\u4eba\u5458\u4e2d\u5fc3\u8ba1\u7b97\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u5de5\u5177"}}
{"id": "2508.12962", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12962", "abs": "https://arxiv.org/abs/2508.12962", "authors": ["Dominic LaBella", "Keshav Jha", "Jared Robbins", "Esther Yu"], "title": "Multi-Phase Automated Segmentation of Dental Structures in CBCT Using a Lightweight Auto3DSeg and SegResNet Implementation", "comment": "MICCAI. ToothFairy3, 16 pages, 5 figures, 1 table", "summary": "Cone-beam computed tomography (CBCT) has become an invaluable imaging\nmodality in dentistry, enabling 3D visualization of teeth and surrounding\nstructures for diagnosis and treatment planning. Automated segmentation of\ndental structures in CBCT can efficiently assist in identifying pathology\n(e.g., pulpal or periapical lesions) and facilitate radiation therapy planning\nin head and neck cancer patients. We describe the DLaBella29 team's approach\nfor the MICCAI 2025 ToothFairy3 Challenge, which involves a deep learning\npipeline for multi-class tooth segmentation. We utilized the MONAI Auto3DSeg\nframework with a 3D SegResNet architecture, trained on a subset of the\nToothFairy3 dataset (63 CBCT scans) with 5-fold cross-validation. Key\npreprocessing steps included image resampling to 0.6 mm isotropic resolution\nand intensity clipping. We applied an ensemble fusion using Multi-Label STAPLE\non the 5-fold predictions to infer a Phase 1 segmentation and then conducted\ntight cropping around the easily segmented Phase 1 mandible to perform Phase 2\nsegmentation on the smaller nerve structures. Our method achieved an average\nDice of 0.87 on the ToothFairy3 challenge out-of-sample validation set. This\npaper details the clinical context, data preparation, model development,\nresults of our approach, and discusses the relevance of automated dental\nsegmentation for improving patient care in radiation oncology.", "AI": {"tldr": "DLaBella29\u56e2\u961f\u5728MICCAI 2025 ToothFairy3\u6311\u6218\u8d5b\u4e2d\u63d0\u51fa\u57fa\u4e8e3D SegResNet\u7684\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\uff0c\u7528\u4e8eCBCT\u7259\u9f7f\u591a\u7c7b\u522b\u5206\u5272\uff0c\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u5230\u5e73\u5747Dice\u7cfb\u65700.87", "motivation": "CBCT\u5728\u7259\u79d1\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u81ea\u52a8\u5316\u7259\u9f7f\u7ed3\u6784\u5206\u5272\u53ef\u5e2e\u52a9\u8bc6\u522b\u75c5\u7406\uff08\u5982\u7259\u9ad3\u6216\u6839\u5c16\u5468\u75c5\u53d8\uff09\u5e76\u4fc3\u8fdb\u5934\u9888\u764c\u60a3\u8005\u7684\u653e\u5c04\u6cbb\u7597\u89c4\u5212", "method": "\u4f7f\u7528MONAI Auto3DSeg\u6846\u67b6\u548c3D SegResNet\u67b6\u6784\uff0c\u91c7\u75285\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8bad\u7ec3\uff0c\u5173\u952e\u9884\u5904\u7406\u5305\u62ec\u56fe\u50cf\u91cd\u91c7\u6837\u548c\u5f3a\u5ea6\u88c1\u526a\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5206\u5272\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528Multi-Label STAPLE\u96c6\u6210\u878d\u5408\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5bf9\u4e0b\u988c\u9aa8\u8fdb\u884c\u7d27\u5bc6\u88c1\u526a\u4ee5\u5206\u5272\u795e\u7ecf\u7ed3\u6784", "result": "\u5728ToothFairy3\u6311\u6218\u8d5b\u7684\u6837\u672c\u5916\u9a8c\u8bc1\u96c6\u4e0a\u83b7\u5f97\u4e860.87\u7684\u5e73\u5747Dice\u7cfb\u6570", "conclusion": "\u81ea\u52a8\u5316\u7259\u9f7f\u5206\u5272\u65b9\u6cd5\u5728CBCT\u56fe\u50cf\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5bf9\u6539\u5584\u653e\u5c04\u80bf\u7624\u5b66\u60a3\u8005\u62a4\u7406\u5177\u6709\u91cd\u8981\u4e34\u5e8a\u610f\u4e49"}}
{"id": "2508.12668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12668", "abs": "https://arxiv.org/abs/2508.12668", "authors": ["Abhijay Ghildyal", "Li-Yun Wang", "Feng Liu"], "title": "WP-CLIP: Leveraging CLIP to Predict W\u00f6lfflin's Principles in Visual Art", "comment": "ICCV 2025 AI4VA workshop (oral), Code:\n  https://github.com/abhijay9/wpclip", "summary": "W\\\"olfflin's five principles offer a structured approach to analyzing\nstylistic variations for formal analysis. However, no existing metric\neffectively predicts all five principles in visual art. Computationally\nevaluating the visual aspects of a painting requires a metric that can\ninterpret key elements such as color, composition, and thematic choices. Recent\nadvancements in vision-language models (VLMs) have demonstrated their ability\nto evaluate abstract image attributes, making them promising candidates for\nthis task. In this work, we investigate whether CLIP, pre-trained on\nlarge-scale data, can understand and predict W\\\"olfflin's principles. Our\nfindings indicate that it does not inherently capture such nuanced stylistic\nelements. To address this, we fine-tune CLIP on annotated datasets of real art\nimages to predict a score for each principle. We evaluate our model, WP-CLIP,\non GAN-generated paintings and the Pandora-18K art dataset, demonstrating its\nability to generalize across diverse artistic styles. Our results highlight the\npotential of VLMs for automated art analysis.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76CLIP\u6a21\u578b\u5728\u827a\u672f\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u5fae\u8c03CLIP\u6765\u9884\u6d4bW\u00f6lfflin\u4e94\u5927\u98ce\u683c\u539f\u5219\uff0c\u5e76\u5728GAN\u751f\u6210\u6d77\u62a5\u548cPandora-18K\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u867d\u7136W\u00f6lfflin\u4e94\u5927\u539f\u5219\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u98ce\u683c\u5206\u6790\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u8ba1\u91cf\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u9884\u6d4b\u6240\u6709\u539f\u5219\u3002\u9700\u8981\u80fd\u591f\u89e3\u91ca\u989c\u8272\u3001\u6784\u56fe\u7b49\u5173\u952e\u5143\u7d20\u7684\u8ba1\u91cf\u65b9\u6cd5\u6765\u8ba1\u7b97\u5206\u6790\u7ed8\u753b\u7684\u89c6\u89c9\u65b9\u9762\u3002", "method": "\u4f7f\u7528CLIP\u6a21\u578b\uff0c\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u540e\uff0c\u901a\u8fc7\u5728\u771f\u5b9e\u827a\u672f\u56fe\u50cf\u6ce8\u91ca\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u6765\u9884\u6d4b\u6bcf\u4e2aW\u00f6lfflin\u539f\u5219\u7684\u5206\u6570\u3002\u5f00\u53d1\u4e86WP-CLIP\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0CLIP\u6a21\u578b\u672c\u8eab\u65e0\u6cd5\u6293\u53d6W\u00f6lfflin\u539f\u5219\u7684\u7ec6\u81f4\u98ce\u683c\u5143\u7d20\u3002\u4f46\u901a\u8fc7\u5fae\u8c03\u540e\u7684WP-CLIP\u6a21\u578b\u5728GAN\u751f\u6210\u6d77\u62a5\u548cPandora-18K\u827a\u672f\u6570\u636e\u96c6\u4e0a\u90fd\u8868\u73b0\u51fa\u826f\u597d\u7684\u6a21\u6027\u9002\u5e94\u80fd\u529b\uff0c\u80fd\u591f\u6a21\u7cca\u9006\u5408\u591a\u6837\u7684\u827a\u672f\u98ce\u683c\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5448\u73b0\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u827a\u672f\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u6709\u76ee\u6807\u7684\u5fae\u8c03\u53ef\u4ee5\u8ba9\u5927\u578b\u6a21\u578b\u7406\u89e3\u548c\u9884\u6d4b\u7ec6\u81f4\u7684\u827a\u672f\u98ce\u683c\u539f\u5219\u3002"}}
{"id": "2508.12680", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12680", "abs": "https://arxiv.org/abs/2508.12680", "authors": ["Yuheng Zha", "Kun Zhou", "Yujia Wu", "Yushu Wang", "Jie Feng", "Zhi Xu", "Shibo Hao", "Zhengzhong Liu", "Eric P. Xing", "Zhiting Hu"], "title": "Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation", "comment": null, "summary": "Despite their success, current training pipelines for reasoning VLMs focus on\na limited range of tasks, such as mathematical and logical reasoning. As a\nresult, these models face difficulties in generalizing their reasoning\ncapabilities to a wide range of domains, primarily due to the scarcity of\nreadily available and verifiable reward data beyond these narrowly defined\nareas. Moreover, integrating data from multiple domains is challenging, as the\ncompatibility between domain-specific datasets remains uncertain. To address\nthese limitations, we build a comprehensive RL-ready visual reasoning dataset\nfrom 46 data sources across 8 dimensions, covering a wide range of tasks such\nas infographic, mathematical, spatial, cross-image, graphic user interface,\nmedical, common sense and general science. We propose an influence function\nbased data selection and difficulty based filtering strategy to identify\nhigh-quality training samples from this dataset. Subsequently, we train the\nVLM, referred to as Vision-G1, using multi-round RL with a data curriculum to\niteratively improve its visual reasoning capabilities. Our model achieves\nstate-of-the-art performance across various visual reasoning benchmarks,\noutperforming similar-sized VLMs and even proprietary models like GPT-4o and\nGemini-1.5 Flash. The model, code and dataset are publicly available at\nhttps://github.com/yuh-zha/Vision-G1.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86Vision-G1\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u57dfRL\u8bad\u7ec3\u6765\u63d0\u5347\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u89c6\u89c9\u7406\u89e3\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u6700\u4f73\u6027\u80fd", "motivation": "\u89e3\u51b3\u73b0\u6709\u7406\u89e3VLM\u6a21\u578b\u5728\u8303\u56f4\u9650\u5236\u4e0b\u8bad\u7ec3\u3001\u6570\u636e\u8d44\u6e90\u7f3a\u4e4f\u4ee5\u53ca\u591a\u57df\u6570\u636e\u6574\u5408\u56f0\u96be\u7684\u95ee\u9898", "method": "\u6784\u5efa\u5305\u542b46\u4e2a\u6570\u636e\u6e90\u30018\u4e2a\u7ef4\u5ea6\u7684\u7406\u89e3\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u5f71\u54cd\u51fd\u6570\u6570\u636e\u9009\u62e9\u548c\u96be\u5ea6\u7b5b\u9009\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u8f6eRL\u6570\u636e\u8bfe\u7a0b\u8fdb\u884c\u8fed\u4ee3\u8bad\u7ec3", "result": "Vision-G1\u6a21\u578b\u5728\u5404\u79cd\u89c6\u89c9\u7406\u89e3\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff0c\u8d85\u8fc7\u540c\u89c4\u6a21\u6a21\u578b\u751a\u81f3GPT-4o\u548cGemini-1.5 Flash", "conclusion": "\u901a\u8fc7\u591a\u57dfRL\u8bad\u7ec3\u548c\u6570\u636e\u8bfe\u7a0b\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347VLM\u7684\u89c6\u89c9\u7406\u89e3\u901a\u7528\u80fd\u529b"}}
{"id": "2508.12684", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12684", "abs": "https://arxiv.org/abs/2508.12684", "authors": ["Zhongyao Li", "Peirui Cheng", "Liangjin Zhao", "Chen Chen", "Yundu Li", "Zhechao Wang", "Xue Yang", "Xian Sun", "Zhirui Wang"], "title": "Refine-and-Contrast: Adaptive Instance-Aware BEV Representations for Multi-UAV Collaborative Object Detection", "comment": "9 pages", "summary": "Multi-UAV collaborative 3D detection enables accurate and robust perception\nby fusing multi-view observations from aerial platforms, offering significant\nadvantages in coverage and occlusion handling, while posing new challenges for\ncomputation on resource-constrained UAV platforms. In this paper, we present\nAdaBEV, a novel framework that learns adaptive instance-aware BEV\nrepresentations through a refine-and-contrast paradigm. Unlike existing methods\nthat treat all BEV grids equally, AdaBEV introduces a Box-Guided Refinement\nModule (BG-RM) and an Instance-Background Contrastive Learning (IBCL) to\nenhance semantic awareness and feature discriminability. BG-RM refines only BEV\ngrids associated with foreground instances using 2D supervision and spatial\nsubdivision, while IBCL promotes stronger separation between foreground and\nbackground features via contrastive learning in BEV space. Extensive\nexperiments on the Air-Co-Pred dataset demonstrate that AdaBEV achieves\nsuperior accuracy-computation trade-offs across model scales, outperforming\nother state-of-the-art methods at low resolutions and approaching upper bound\nperformance while maintaining low-resolution BEV inputs and negligible\noverhead.", "AI": {"tldr": "AdaBEV\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u591a\u65e0\u4eba\u673a3D\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5b9e\u4f8b\u611f\u77e5\u7684BEV\u8868\u793a\u548c\u7cbe\u5316\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0f\uff0c\u5728\u4fdd\u6301\u4f4e\u5206\u8fa8\u7387BEV\u8f93\u5165\u7684\u540c\u65f6\u5b9e\u73b0\u4f18\u8d8a\u7684\u7cbe\u5ea6-\u8ba1\u7b97\u6743\u8861\u3002", "motivation": "\u591a\u65e0\u4eba\u673a\u534f\u540c3D\u68c0\u6d4b\u867d\u7136\u80fd\u901a\u8fc7\u878d\u5408\u591a\u89c6\u89d2\u89c2\u6d4b\u63d0\u4f9b\u51c6\u786e\u9c81\u68d2\u7684\u611f\u77e5\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u4eba\u673a\u5e73\u53f0\u4e0a\u8ba1\u7b97\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5e73\u7b49\u5bf9\u5f85\u6240\u6709BEV\u7f51\u683c\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faBox-Guided Refinement Module (BG-RM) \u4ec5\u7cbe\u5316\u4e0e\u524d\u666f\u5b9e\u4f8b\u76f8\u5173\u7684BEV\u7f51\u683c\uff0c\u4f7f\u75282D\u76d1\u7763\u548c\u7a7a\u95f4\u7ec6\u5206\uff1bInstance-Background Contrastive Learning (IBCL) \u901a\u8fc7BEV\u7a7a\u95f4\u4e2d\u7684\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u524d\u666f\u548c\u80cc\u666f\u7279\u5f81\u7684\u53ef\u533a\u5206\u6027\u3002", "result": "\u5728Air-Co-Pred\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAdaBEV\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\u90fd\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u7cbe\u5ea6-\u8ba1\u7b97\u6743\u8861\uff0c\u5728\u4f4e\u5206\u8fa8\u7387\u4e0b\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u63a5\u8fd1\u4e0a\u9650\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u5206\u8fa8\u7387BEV\u8f93\u5165\u548c\u53ef\u5ffd\u7565\u7684\u5f00\u9500\u3002", "conclusion": "AdaBEV\u901a\u8fc7\u81ea\u9002\u5e94\u5b9e\u4f8b\u611f\u77e5\u7684BEV\u8868\u793a\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u65e0\u4eba\u673a3D\u68c0\u6d4b\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u9ad8\u6027\u80fd\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12695", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12695", "abs": "https://arxiv.org/abs/2508.12695", "authors": ["Felix Embacher", "David Holtz", "Jonas Uhrig", "Marius Cordts", "Markus Enzweiler"], "title": "Neural Rendering for Sensor Adaptation in 3D Object Detection", "comment": "Accepted at IEEE Intelligent Vehicles Symposium (IV) 2025", "summary": "Autonomous vehicles often have varying camera sensor setups, which is\ninevitable due to restricted placement options for different vehicle types.\nTraining a perception model on one particular setup and evaluating it on a new,\ndifferent sensor setup reveals the so-called cross-sensor domain gap, typically\nleading to a degradation in accuracy. In this paper, we investigate the impact\nof the cross-sensor domain gap on state-of-the-art 3D object detectors. To this\nend, we introduce CamShift, a dataset inspired by nuScenes and created in CARLA\nto specifically simulate the domain gap between subcompact vehicles and sport\nutility vehicles (SUVs). Using CamShift, we demonstrate significant\ncross-sensor performance degradation, identify robustness dependencies on model\narchitecture, and propose a data-driven solution to mitigate the effect. On the\none hand, we show that model architectures based on a dense Bird's Eye View\n(BEV) representation with backward projection, such as BEVFormer, are the most\nrobust against varying sensor configurations. On the other hand, we propose a\nnovel data-driven sensor adaptation pipeline based on neural rendering, which\ncan transform entire datasets to match different camera sensor setups. Applying\nthis approach improves performance across all investigated 3D object detectors,\nmitigating the cross-sensor domain gap by a large margin and reducing the need\nfor new data collection by enabling efficient data reusability across vehicles\nwith different sensor setups. The CamShift dataset and the sensor adaptation\nbenchmark are available at https://dmholtz.github.io/camshift/.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u81ea\u4e3b\u9a7e\u9a76\u8f66\u4e0d\u540c\u611f\u77e5\u5668\u914d\u7f6e\u5bfc\u81f4\u7684\u8de8\u611f\u77e5\u5668\u57df\u95f4\u95f4\u9694\u95ee\u9898\uff0c\u63d0\u51fa\u4e86CamShift\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u795e\u7ecf\u6e32\u67d3\u7684\u6570\u636e\u9a71\u52a8\u611f\u77e5\u5668\u9002\u914d\u65b9\u6848\uff0c\u6709\u6548\u51cf\u5c11\u8de8\u611f\u77e5\u5668\u6027\u80fd\u6cc4\u6f0f\u3002", "motivation": "\u81ea\u4e3b\u9a7e\u9a76\u8f66\u7684\u611f\u77e5\u5668\u914d\u7f6e\u56e0\u8f66\u8f86\u7c7b\u578b\u4e0d\u540c\u800c\u5f02\uff0c\u5bfc\u81f4\u5728\u4e00\u79cd\u611f\u77e5\u5668\u914d\u7f6e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5176\u4ed6\u914d\u7f6e\u4e0a\u6027\u80fd\u6cc4\u6f0f\uff0c\u9700\u8981\u89e3\u51b3\u8de8\u611f\u77e5\u5668\u57df\u95f4\u95f4\u9694\u95ee\u9898\u3002", "method": "\u521b\u5efaCamShift\u6570\u636e\u96c6\u6a21\u62df\u4e0d\u540c\u8f66\u8f86\u7c7b\u578b\u7684\u611f\u77e5\u5668\u914d\u7f6e\u5dee\u5f02\uff0c\u5206\u6790\u5404\u79cd3D\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u8de8\u611f\u77e5\u5668\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u6e32\u67d3\u7684\u6570\u636e\u9a71\u52a8\u611f\u77e5\u5668\u9002\u914d\u6d41\u6c34\u7ebf\u3002", "result": "\u53d1\u73b0\u57fa\u4e8e\u5bc6\u96c6BEV\u8868\u793a\u7684BEVFormer\u6a21\u578b\u6700\u7a33\u5065\uff0c\u800c\u63d0\u51fa\u7684\u611f\u77e5\u5668\u9002\u914d\u65b9\u6848\u80fd\u5927\u5e45\u63d0\u5347\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u7684\u8de8\u611f\u77e5\u5668\u6027\u80fd\uff0c\u51cf\u5c11\u6570\u636e\u91cd\u65b0\u6536\u96c6\u9700\u6c42\u3002", "conclusion": "\u901a\u8fc7\u795e\u7ecf\u6e32\u67d3\u6280\u672f\u5b9e\u73b0\u6570\u636e\u96c6\u8de8\u611f\u77e5\u5668\u9002\u914d\uff0c\u53ef\u6709\u6548\u51cf\u5c11\u81ea\u4e3b\u9a7e\u9a76\u8f66\u4e0d\u540c\u611f\u77e5\u5668\u914d\u7f6e\u5bfc\u81f4\u7684\u6027\u80fd\u6cc4\u6f0f\uff0c\u63d0\u9ad8\u6570\u636e\u91cd\u7528\u6027\u3002"}}
{"id": "2508.12711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12711", "abs": "https://arxiv.org/abs/2508.12711", "authors": ["Fanxiao Li", "Jiaying Wu", "Tingchao Fu", "Yunyun Dong", "Bingbing Song", "Wei Zhou"], "title": "Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection", "comment": null, "summary": "The proliferation of multimodal misinformation poses growing threats to\npublic discourse and societal trust. While Large Vision-Language Models (LVLMs)\nhave enabled recent progress in multimodal misinformation detection (MMD), the\nrise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven\nnews diversity, characterized by highly varied and complex content. We show\nthat this diversity induces multi-level drift, comprising (1) model-level\nmisperception drift, where stylistic variations disrupt a model's internal\nreasoning, and (2) evidence-level drift, where expression diversity degrades\nthe quality or relevance of retrieved external evidence. These drifts\nsignificantly degrade the robustness of current LVLM-based MMD systems. To\nsystematically study this problem, we introduce DriftBench, a large-scale\nbenchmark comprising 16,000 news instances across six categories of\ndiversification. We design three evaluation tasks: (1) robustness of truth\nverification under multi-level drift; (2) susceptibility to adversarial\nevidence contamination generated by GenAI; and (3) analysis of reasoning\nconsistency across diverse inputs. Experiments with six state-of-the-art\nLVLM-based detectors show substantial performance drops (average F1 -14.8%) and\nincreasingly unstable reasoning traces, with even more severe failures under\nadversarial evidence injection. Our findings uncover fundamental\nvulnerabilities in existing MMD systems and suggest an urgent need for more\nresilient approaches in the GenAI era.", "AI": {"tldr": "GenAI\u9a71\u52a8\u7684\u65b0\u95fb\u591a\u6837\u6027\u5bfc\u81f4\u591a\u7ea7\u6f02\u79fb\uff0c\u663e\u8457\u964d\u4f4e\u73b0\u6709LVLM\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u6027\u80fd\u5e73\u5747\u4e0b\u964d14.8%", "motivation": "\u751f\u6210\u5f0fAI\u5de5\u5177\u5e26\u6765\u7684\u65b0\u95fb\u5185\u5bb9\u591a\u6837\u6027\u5bf9\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u6784\u6210\u4e86\u65b0\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u5176\u5bf9\u68c0\u6d4b\u7cfb\u7edf\u7684\u5f71\u54cd", "method": "\u6784\u5efaDriftBench\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0816,000\u4e2a\u65b0\u95fb\u5b9e\u4f8b\uff0c6\u4e2a\u591a\u6837\u5316\u7c7b\u522b\uff09\uff0c\u8bbe\u8ba1\u4e09\u4e2a\u8bc4\u4f30\u4efb\u52a1\uff1a\u771f\u5b9e\u6027\u9a8c\u8bc1\u9c81\u68d2\u6027\u3001\u5bf9\u6297\u6027\u8bc1\u636e\u6c61\u67d3\u654f\u611f\u6027\u3001\u63a8\u7406\u4e00\u81f4\u6027\u5206\u6790", "result": "\u5b9e\u9a8c\u663e\u793a6\u4e2a\u6700\u5148\u8fdb\u7684LVLM\u68c0\u6d4b\u5668\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08\u5e73\u5747F1\u4e0b\u964d14.8%\uff09\uff0c\u63a8\u7406\u8f68\u8ff9\u4e0d\u7a33\u5b9a\uff0c\u5728\u5bf9\u6297\u6027\u8bc1\u636e\u6ce8\u5165\u4e0b\u8868\u73b0\u66f4\u5dee", "conclusion": "\u73b0\u6709MMD\u7cfb\u7edf\u5b58\u5728\u6839\u672c\u6027\u6f0f\u6d1e\uff0c\u5728GenAI\u65f6\u4ee3\u8feb\u5207\u9700\u8981\u66f4\u5177\u5f39\u6027\u7684\u65b9\u6cd5"}}
{"id": "2508.12713", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12713", "abs": "https://arxiv.org/abs/2508.12713", "authors": ["Brandone Fonya"], "title": "Real-Time Sign Language Gestures to Speech Transcription using Deep Learning", "comment": "Course related research project", "summary": "Communication barriers pose significant challenges for individuals with\nhearing and speech impairments, often limiting their ability to effectively\ninteract in everyday environments. This project introduces a real-time\nassistive technology solution that leverages advanced deep learning techniques\nto translate sign language gestures into textual and audible speech. By\nemploying convolution neural networks (CNN) trained on the Sign Language MNIST\ndataset, the system accurately classifies hand gestures captured live via\nwebcam. Detected gestures are instantaneously translated into their\ncorresponding meanings and transcribed into spoken language using\ntext-to-speech synthesis, thus facilitating seamless communication.\nComprehensive experiments demonstrate high model accuracy and robust real-time\nperformance with some latency, highlighting the system's practical\napplicability as an accessible, reliable, and user-friendly tool for enhancing\nthe autonomy and integration of sign language users in diverse social settings.", "AI": {"tldr": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5b9e\u65f6\u624b\u8bed\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7CNN\u7b97\u6cd5\u5c06\u624b\u52bf\u8f6c\u6362\u4e3a\u6587\u672c\u548c\u8bed\u97f3\uff0c\u4fc3\u8fdb\u542c\u529b\u8bed\u8a00\u969c\u788d\u8005\u7684\u6c9f\u901a\u878d\u5165", "motivation": "\u89e3\u51b3\u542c\u529b\u548c\u8bed\u8a00\u969c\u788d\u8005\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u6c9f\u901a\u56f0\u96be\uff0c\u63d0\u5347\u4ed6\u4eec\u7684\u81ea\u4e3b\u6027\u548c\u793e\u4ea4\u878d\u5165\u80fd\u529b", "method": "\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\u8bad\u7ec3\u5728Sign Language MNIST\u6570\u636e\u96c6\u4e0a\uff0c\u901a\u8fc7\u6444\u50cf\u5934\u5b9e\u65f6\u6355\u83b7\u624b\u52bf\u5e76\u8fdb\u884c\u5206\u7c7b\uff0c\u7136\u540e\u4f7f\u7528\u6587\u672c\u8f6c\u8bed\u97f3\u6280\u672f\u751f\u6210\u8bed\u97f3\u8f93\u51fa", "result": "\u7cfb\u7edf\u5c55\u73b0\u51fa\u9ad8\u51c6\u786e\u7387\u548c\u7a33\u5b9a\u7684\u5b9e\u65f6\u6027\u80fd\uff0c\u867d\u7136\u5b58\u5728\u4e00\u5b9a\u5ef6\u8fdf\uff0c\u4f46\u5177\u6709\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027", "conclusion": "\u8be5\u7cfb\u7edf\u4f5c\u4e3a\u4e00\u79cd\u53ef\u8bbf\u3001\u53ef\u9760\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u8f85\u52a9\u6280\u672f\u5de5\u5177\uff0c\u6709\u6548\u63d0\u5347\u4e86\u624b\u8bed\u4f7f\u7528\u8005\u5728\u591a\u6837\u5316\u793e\u4ea4\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u6027\u548c\u878d\u5165\u6027"}}
{"id": "2508.12718", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12718", "abs": "https://arxiv.org/abs/2508.12718", "authors": ["Syed Muhmmad Israr", "Feng Zhao"], "title": "Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score", "comment": null, "summary": "Large-scale text-to-image generative models have shown remarkable ability to\nsynthesize diverse and high-quality images. However, it is still challenging to\ndirectly apply these models for editing real images for two reasons. First, it\nis difficult for users to come up with a perfect text prompt that accurately\ndescribes every visual detail in the input image. Second, while existing models\ncan introduce desirable changes in certain regions, they often dramatically\nalter the input content and introduce unexpected changes in unwanted regions.\nTo address these challenges, we present Dual Contrastive Denoising Score, a\nsimple yet powerful framework that leverages the rich generative prior of\ntext-to-image diffusion models. Inspired by contrastive learning approaches for\nunpaired image-to-image translation, we introduce a straightforward dual\ncontrastive loss within the proposed framework. Our approach utilizes the\nextensive spatial information from the intermediate representations of the\nself-attention layers in latent diffusion models without depending on auxiliary\nnetworks. Our method achieves both flexible content modification and structure\npreservation between input and output images, as well as zero-shot\nimage-to-image translation. Through extensive experiments, we show that our\napproach outperforms existing methods in real image editing while maintaining\nthe capability to directly utilize pretrained text-to-image diffusion models\nwithout further training.", "AI": {"tldr": "\u63d0\u51faDual Contrastive Denoising Score\u6846\u67b6\uff0c\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u5148\u9a8c\uff0c\u901a\u8fc7\u53cc\u91cd\u5bf9\u6bd4\u635f\u5931\u5b9e\u73b0\u771f\u5b9e\u56fe\u50cf\u7f16\u8f91\uff0c\u65e2\u80fd\u7075\u6d3b\u4fee\u6539\u5185\u5bb9\u53c8\u80fd\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u7f16\u8f91\u771f\u5b9e\u56fe\u50cf\u65f6\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u7528\u6237\u96be\u4ee5\u51c6\u786e\u63cf\u8ff0\u56fe\u50cf\u7ec6\u8282\u7684\u6587\u672c\u63d0\u793a\uff0c\u4ee5\u53ca\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7ecf\u5e38\u610f\u5916\u6539\u53d8\u4e0d\u9700\u8981\u4fee\u6539\u7684\u533a\u57df\u3002", "method": "\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5f15\u5165\u53cc\u91cd\u5bf9\u6bd4\u635f\u5931\uff0c\u5229\u7528\u81ea\u6ce8\u610f\u529b\u5c42\u7684\u7a7a\u95f4\u4fe1\u606f\uff0c\u65e0\u9700\u4f9d\u8d56\u8f85\u52a9\u7f51\u7edc\u3002", "result": "\u65b9\u6cd5\u5728\u771f\u5b9e\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u96f6\u6837\u672c\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\uff0c\u540c\u65f6\u4fdd\u6301\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u800c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u7b80\u5355\u800c\u5f3a\u5927\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u771f\u5b9e\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8f93\u5165\u8f93\u51fa\u56fe\u50cf\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u5185\u5bb9\u4fee\u6539\u3002"}}
{"id": "2508.12720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12720", "abs": "https://arxiv.org/abs/2508.12720", "authors": ["Kangjie Chen", "Yingji Zhong", "Zhihao Li", "Jiaqi Lin", "Youyu Chen", "Minghan Qin", "Haoqian Wang"], "title": "Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting", "comment": "Under review. Project page:\n  https://chenkangjie1123.github.io/Co-Adaptation-3DGS/", "summary": "3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel\nview synthesis under dense-view settings. However, in sparse-view scenarios,\ndespite the realistic renderings in training views, 3DGS occasionally manifests\nappearance artifacts in novel views. This paper investigates the appearance\nartifacts in sparse-view 3DGS and uncovers a core limitation of current\napproaches: the optimized Gaussians are overly-entangled with one another to\naggressively fit the training views, which leads to a neglect of the real\nappearance distribution of the underlying scene and results in appearance\nartifacts in novel views. The analysis is based on a proposed metric, termed\nCo-Adaptation Score (CA), which quantifies the entanglement among Gaussians,\ni.e., co-adaptation, by computing the pixel-wise variance across multiple\nrenderings of the same viewpoint, with different random subsets of Gaussians.\nThe analysis reveals that the degree of co-adaptation is naturally alleviated\nas the number of training views increases. Based on the analysis, we propose\ntwo lightweight strategies to explicitly mitigate the co-adaptation in\nsparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise\ninjection to the opacity. Both strategies are designed to be plug-and-play, and\ntheir effectiveness is validated across various methods and benchmarks. We hope\nthat our insights into the co-adaptation effect will inspire the community to\nachieve a more comprehensive understanding of sparse-view 3DGS.", "AI": {"tldr": "\u4e09\u7ef4\u9ad8\u65af\u62d3\u6251\u5728\u7a00\u758f\u89c6\u89d2\u573a\u666f\u4e2d\u5b58\u5728\u5916\u89c2\u4f2a\u50cf\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u9ad8\u65af\u5143\u7d20\u8fc7\u5ea6\u7c98\u8054\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848", "motivation": "\u89e3\u51b33DGS\u5728\u7a00\u758f\u89c6\u89d2\u4e0b\u7684\u5916\u89c2\u4f2a\u50cf\u95ee\u9898\uff0c\u63a2\u7d22\u9ad8\u65af\u5143\u7d20\u8fc7\u5ea6\u7c98\u8054\u5bfc\u81f4\u7684\u6027\u80fd\u964d\u7ea7", "method": "\u63d0\u51fa\u534f\u540c\u9002\u5e94\u5206\u6570(CA)\u6307\u6807\u6765\u91cf\u5316\u9ad8\u65af\u5143\u7d20\u7c98\u8054\u7a0b\u5ea6\uff0c\u5e76\u63d0\u51fa\u968f\u673a\u9ad8\u65af\u6295\u5f03\u548c\u4e0d\u900f\u660e\u5ea6\u4e58\u6027\u566a\u58f0\u6ce8\u5165\u4e24\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5", "result": "\u5206\u6790\u663e\u793a\u8bad\u7ec3\u89c6\u89d2\u6570\u91cf\u589e\u52a0\u80fd\u81ea\u7136\u7f13\u89e3\u534f\u540c\u9002\u5e94\uff0c\u63d0\u51fa\u7684\u4e24\u79cd\u7b56\u7565\u5728\u591a\u4e2a\u65b9\u6cd5\u548c\u6d4b\u8bd5\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548", "conclusion": "\u672c\u6587\u63ed\u793a\u4e863DGS\u5728\u7a00\u758f\u89c6\u89d2\u4e0b\u7684\u6838\u5fc3\u9650\u5236\uff0c\u63d0\u51fa\u7684\u534f\u540c\u9002\u5e94\u5206\u6790\u6846\u67b6\u548c\u7b80\u5355\u6709\u6548\u7684\u7b56\u7565\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3"}}
{"id": "2508.12736", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12736", "abs": "https://arxiv.org/abs/2508.12736", "authors": ["Ying Zhang", "Xiongxin Tang", "Chongyi Li", "Qiao Chen", "Yuquan Wu"], "title": "Frequency-Driven Inverse Kernel Prediction for Single Image Defocus Deblurring", "comment": null, "summary": "Single image defocus deblurring aims to recover an all-in-focus image from a\ndefocus counterpart, where accurately modeling spatially varying blur kernels\nremains a key challenge. Most existing methods rely on spatial features for\nkernel estimation, but their performance degrades in severely blurry regions\nwhere local high-frequency details are missing. To address this, we propose a\nFrequency-Driven Inverse Kernel Prediction network (FDIKP) that incorporates\nfrequency-domain representations to enhance structural identifiability in\nkernel modeling. Given the superior discriminative capability of the frequency\ndomain for blur modeling, we design a Dual-Branch Inverse Kernel Prediction\n(DIKP) strategy that improves the accuracy of kernel estimation while\nmaintaining stability. Moreover, considering the limited number of predicted\ninverse kernels, we introduce a Position Adaptive Convolution (PAC) to enhance\nthe adaptability of the deconvolution process. Finally, we propose a\nDual-Domain Scale Recurrent Module (DSRM) to fuse deconvolution results and\nprogressively improve deblurring quality from coarse to fine. Extensive\nexperiments demonstrate that our method outperforms existing approaches. Code\nwill be made publicly available.", "AI": {"tldr": "\u63d0\u51faFDIKP\u7f51\u7edc\uff0c\u901a\u8fc7\u9891\u7387\u57df\u8868\u793a\u589e\u5f3a\u6838\u4f30\u8ba1\u7684\u7ed3\u6784\u53ef\u8bc6\u522b\u6027\uff0c\u91c7\u7528\u53cc\u5206\u652f\u9006\u6838\u9884\u6d4b\u7b56\u7565\u548c\u4f4d\u7f6e\u81ea\u9002\u5e94\u5377\u79ef\uff0c\u5728\u5355\u56fe\u50cf\u6563\u7126\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7a7a\u95f4\u7279\u5f81\u8fdb\u884c\u6838\u4f30\u8ba1\uff0c\u4f46\u5728\u4e25\u91cd\u6a21\u7cca\u533a\u57df\u7531\u4e8e\u5c40\u90e8\u9ad8\u9891\u7ec6\u8282\u7f3a\u5931\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u5229\u7528\u9891\u7387\u57df\u7684\u5224\u522b\u80fd\u529b\u6765\u589e\u5f3a\u6838\u5efa\u6a21\u3002", "method": "\u63d0\u51fa\u9891\u7387\u9a71\u52a8\u7684\u9006\u6838\u9884\u6d4b\u7f51\u7edc(FDIKP)\uff0c\u5305\u542b\u53cc\u5206\u652f\u9006\u6838\u9884\u6d4b\u7b56\u7565(DIKP)\u3001\u4f4d\u7f6e\u81ea\u9002\u5e94\u5377\u79ef(PAC)\u548c\u53cc\u57df\u5c3a\u5ea6\u5faa\u73af\u6a21\u5757(DSRM)\uff0c\u878d\u5408\u9891\u7387\u57df\u4fe1\u606f\u5e76\u9010\u6b65\u6539\u8fdb\u53bb\u6a21\u7cca\u8d28\u91cf\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u6563\u7126\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u9891\u7387\u57df\u8868\u793a\u548c\u521b\u65b0\u7684\u7f51\u7edc\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e25\u91cd\u6a21\u7cca\u533a\u57df\u7684\u6838\u4f30\u8ba1\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5355\u56fe\u50cf\u6563\u7126\u53bb\u6a21\u7cca\u7684\u6027\u80fd\u3002"}}
{"id": "2508.12750", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12750", "abs": "https://arxiv.org/abs/2508.12750", "authors": ["Linhao Li", "Boya Jin", "Zizhe Li", "Lanqing Guo", "Hao Cheng", "Bo Li", "Yongfeng Dong"], "title": "D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal", "comment": "Paper Under Review", "summary": "Shadow removal aims to restore images that are partially degraded by shadows,\nwhere the degradation is spatially localized and non-uniform. Unlike general\nrestoration tasks that assume global degradation, shadow removal can leverage\nabundant information from non-shadow regions for guidance. However, the\ntransformation required to correct shadowed areas often differs significantly\nfrom that of well-lit regions, making it challenging to apply uniform\ncorrection strategies. This necessitates the effective integration of non-local\ncontextual cues and adaptive modeling of region-specific transformations. To\nthis end, we propose a novel Mamba-based network featuring dual-scale fusion\nand dual-path scanning to selectively propagate contextual information based on\ntransformation similarity across regions. Specifically, the proposed Dual-Scale\nFusion Mamba Block (DFMB) enhances multi-scale feature representation by fusing\noriginal features with low-resolution features, effectively reducing boundary\nartifacts. The Dual-Path Mamba Group (DPMG) captures global features via\nhorizontal scanning and incorporates a mask-aware adaptive scanning strategy,\nwhich improves structural continuity and fine-grained region modeling.\nExperimental results demonstrate that our method significantly outperforms\nexisting state-of-the-art approaches on shadow removal benchmarks.", "AI": {"tldr": "\u57fa\u4e8eMamba\u7684\u53cc\u8def\u5f84\u626b\u63cf\u7f51\u7edc\uff0c\u901a\u8fc7\u53cc\u89d2\u5ea6\u878d\u5408\u548c\u9002\u914d\u626b\u63cf\u7b56\u7565\uff0c\u9ad8\u6548\u5b8c\u6210\u9634\u5f71\u79fb\u9664\u4efb\u52a1", "motivation": "\u9634\u5f71\u79fb\u9664\u4efb\u52a1\u4e2d\u975e\u9634\u5f71\u533a\u57df\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4f46\u4e0d\u540c\u533a\u57df\u9700\u8981\u4e0d\u540c\u7684\u53d8\u6362\u7b56\u7565\uff0c\u5fc5\u987b\u6709\u6548\u6574\u5408\u8fd9\u4e9b\u4fe1\u606f\u5e76\u9002\u914d\u5730\u5efa\u6a21\u533a\u57df\u7279\u5f02\u53d8\u6362", "method": "\u63d0\u51fa\u53cc\u8def\u5f84Mamba\u7ec4(DPMG)\u901a\u8fc7\u6c34\u5e73\u626b\u63cf\u6355\u83b7\u5168\u5c40\u7279\u5f81\uff0c\u91c7\u7528\u9762\u5177\u611f\u77e5\u9002\u914d\u626b\u63cf\u7b56\u7565\uff1b\u53cc\u5c3a\u5ea6\u878d\u5408Mamba\u5757(DFMB)\u901a\u8fc7\u878d\u5408\u539f\u59cb\u7279\u5f81\u4e0e\u4f4e\u5206\u8fa8\u7387\u7279\u5f81\u63d0\u5347\u591a\u5c3a\u5ea6\u8868\u5f81\u80fd\u529b", "result": "\u5728\u9634\u5f71\u79fb\u9664\u6807\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u663e\u8457\u8d85\u8fc7\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u901a\u8fc7\u53cc\u8def\u5f84\u626b\u63cf\u548c\u53cc\u5c3a\u5ea6\u878d\u5408\u673a\u5236\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u9634\u5f71\u79fb\u9664\u4e2d\u7684\u8fb9\u754c\u504f\u5dee\u548c\u7ed3\u6784\u4e0d\u8fde\u7eed\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u79c0\u7684\u6062\u590d\u6548\u679c"}}
{"id": "2508.12777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12777", "abs": "https://arxiv.org/abs/2508.12777", "authors": ["Wenguang Tao", "Xiaotian Wang", "Tian Yan", "Jie Yan", "Guodong Li", "Kun Bai"], "title": "SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior", "comment": null, "summary": "As a key research direction in the field of multi-object tracking (MOT),\nUAV-based multi-object tracking has significant application value in the\nanalysis and understanding of urban intelligent transportation systems.\nHowever, in complex UAV perspectives, challenges such as small target scale\nvariations, occlusions, nonlinear crossing motions, and motion blur severely\nhinder the stability of multi-object tracking. To address these challenges,\nthis paper proposes a novel multi-object tracking framework, SocialTrack, aimed\nat enhancing the tracking accuracy and robustness of small targets in complex\nurban traffic environments. The specialized small-target detector enhances the\ndetection performance by employing a multi-scale feature enhancement mechanism.\nThe Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of\ntrajectory prediction by incorporating a velocity dynamic modeling mechanism.\nThe Group Motion Compensation Strategy (GMCS) models social group motion priors\nto provide stable state update references for low-quality tracks, significantly\nimproving the target association accuracy in complex dynamic environments.\nFurthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical\ntrajectory information to predict the future state of low-quality tracks,\neffectively mitigating identity switching issues. Extensive experiments on the\nUAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing\nstate-of-the-art (SOTA) methods across several key metrics. Significant\nimprovements in MOTA and IDF1, among other core performance indicators,\nhighlight its superior robustness and adaptability. Additionally, SocialTrack\nis highly modular and compatible, allowing for seamless integration with\nexisting trackers to further enhance performance.", "AI": {"tldr": "SocialTrack\u662f\u4e00\u4e2a\u9488\u5bf9\u65e0\u4eba\u673a\u89c6\u89d2\u4e0b\u590d\u6742\u57ce\u5e02\u4ea4\u901a\u73af\u5883\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u589e\u5f3a\u3001\u901f\u5ea6\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u3001\u7fa4\u4f53\u8fd0\u52a8\u8865\u507f\u548c\u65f6\u7a7a\u8bb0\u5fc6\u9884\u6d4b\u7b49\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u76ee\u6807\u8ddf\u8e2a\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u65e0\u4eba\u673a\u591a\u76ee\u6807\u8ddf\u8e2a\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u590d\u6742\u89c6\u89d2\u4e0b\u7684\u5c0f\u76ee\u6807\u5c3a\u5ea6\u53d8\u5316\u3001\u906e\u6321\u3001\u975e\u7ebf\u6027\u4ea4\u53c9\u8fd0\u52a8\u548c\u8fd0\u52a8\u6a21\u7cca\u7b49\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u4e86\u8ddf\u8e2a\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faSocialTrack\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u591a\u5c3a\u5ea6\u7279\u5f81\u589e\u5f3a\u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\u5668\uff1b2\uff09\u901f\u5ea6\u81ea\u9002\u5e94\u7acb\u65b9\u5361\u5c14\u66fc\u6ee4\u6ce2(VACKF)\u7528\u4e8e\u8f68\u8ff9\u9884\u6d4b\uff1b3\uff09\u7fa4\u4f53\u8fd0\u52a8\u8865\u507f\u7b56\u7565(GMCS)\u5efa\u6a21\u793e\u4f1a\u7fa4\u4f53\u8fd0\u52a8\u5148\u9a8c\uff1b4\uff09\u65f6\u7a7a\u8bb0\u5fc6\u9884\u6d4b(STMP)\u5229\u7528\u5386\u53f2\u8f68\u8ff9\u4fe1\u606f\u9884\u6d4b\u672a\u6765\u72b6\u6001\u3002", "result": "\u5728UAVDT\u548cMOT17\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSocialTrack\u5728\u591a\u4e2a\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728MOTA\u548cIDF1\u7b49\u6838\u5fc3\u6027\u80fd\u6307\u6807\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "SocialTrack\u6846\u67b6\u5177\u6709\u4f18\u5f02\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u540c\u65f6\u5177\u6709\u9ad8\u5ea6\u6a21\u5757\u5316\u548c\u517c\u5bb9\u6027\uff0c\u53ef\u4e0e\u73b0\u6709\u8ddf\u8e2a\u5668\u65e0\u7f1d\u96c6\u6210\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2508.12784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12784", "abs": "https://arxiv.org/abs/2508.12784", "authors": ["Dan Ruta", "Abdelaziz Djelouah", "Raphael Ortiz", "Christopher Schroers"], "title": "Leveraging Diffusion Models for Stylization using Multiple Style Images", "comment": null, "summary": "Recent advances in latent diffusion models have enabled exciting progress in\nimage style transfer. However, several key issues remain. For example, existing\nmethods still struggle to accurately match styles. They are often limited in\nthe number of style images that can be used. Furthermore, they tend to entangle\ncontent and style in undesired ways. To address this, we propose leveraging\nmultiple style images which helps better represent style features and prevent\ncontent leaking from the style images. We design a method that leverages both\nimage prompt adapters and statistical alignment of the features during the\ndenoising process. With this, our approach is designed such that it can\nintervene both at the cross-attention and the self-attention layers of the\ndenoising UNet. For the statistical alignment, we employ clustering to distill\na small representative set of attention features from the large number of\nattention values extracted from the style samples. As demonstrated in our\nexperimental section, the resulting method achieves state-of-the-art results\nfor stylization.", "AI": {"tldr": "\u901a\u8fc7\u591a\u6837\u5f0f\u56fe\u50cf\u7ed3\u5408\u56fe\u50cf\u63d0\u793a\u9002\u914d\u5668\u548c\u7edf\u8ba1\u5bf9\u9f50\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u66f4\u51c6\u786e\u5339\u914d\u98ce\u683c\u3001\u9632\u6b62\u5185\u5bb9\u6f0f\u6cc4\u7684\u56fe\u50cf\u98ce\u683c\u8f6c\u6362\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u73b0\u6709\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u98ce\u683c\u8f6c\u6362\u4e2d\u5b58\u5728\u7684\u95ee\u9898\uff1a\u98ce\u683c\u5339\u914d\u4e0d\u51c6\u786e\u3001\u53ef\u4f7f\u7528\u98ce\u683c\u56fe\u50cf\u6570\u91cf\u6709\u9650\u3001\u5185\u5bb9\u4e0e\u98ce\u683c\u6742\u4e71\u7b49", "method": "\u5229\u7528\u591a\u4e2a\u98ce\u683c\u56fe\u50cf\u6765\u66f4\u597d\u8868\u5f81\u98ce\u683c\u7279\u5f81\uff0c\u8bbe\u8ba1\u4e86\u7ed3\u5408\u56fe\u50cf\u63d0\u793a\u9002\u914d\u5668\u548c\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7279\u5f81\u7edf\u8ba1\u5bf9\u9f50\u7684\u65b9\u6cd5\uff0c\u5728\u53bb\u566aUNet\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u81ea\u6ce8\u610f\u529b\u5c42\u8fdb\u884c\u5e72\u9884\uff0c\u91c7\u7528\u805a\u7c7b\u6280\u672f\u4ece\u5927\u91cf\u6ce8\u610f\u529b\u503c\u4e2d\u7cbe\u70bc\u5c0f\u96c6\u4ee3\u8868\u6027\u7279\u5f81", "result": "\u8be5\u65b9\u6cd5\u5728\u98ce\u683c\u5316\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u76ee\u524d\u6700\u5148\u8fdb\u7684\u7ed3\u679c", "conclusion": "\u901a\u8fc7\u591a\u98ce\u683c\u56fe\u50cf\u7ed3\u5408\u7edf\u8ba1\u5bf9\u9f50\u6280\u672f\uff0c\u6210\u529f\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u66f4\u51c6\u786e\u8fdb\u884c\u56fe\u50cf\u98ce\u683c\u8f6c\u6362\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u98ce\u683c\u5339\u914d\u4e0d\u51c6\u548c\u5185\u5bb9\u6f0f\u6cc4\u7b49\u95ee\u9898"}}
{"id": "2508.12802", "categories": ["cs.CV", "astro-ph.IM", "astro-ph.SR", "I.5.1; J.2"], "pdf": "https://arxiv.org/pdf/2508.12802", "abs": "https://arxiv.org/abs/2508.12802", "authors": ["\u0160tefan Parimucha", "Maksim Gabdeev", "Yanna Markus", "Martin Va\u0148ko", "Pavol Gajdo\u0161"], "title": "Morphological classification of eclipsing binary stars using computer vision methods", "comment": "19 pages, 4 figures, 4 tables", "summary": "We present an application of computer vision methods to classify the light\ncurves of eclipsing binaries (EB). We have used pre-trained models based on\nconvolutional neural networks ($\\textit{ResNet50}$) and vision transformers\n($\\textit{vit\\_base\\_patch16\\_224}$), which were fine-tuned on images created\nfrom synthetic datasets. To improve model generalisation and reduce\noverfitting, we developed a novel image representation by transforming\nphase-folded light curves into polar coordinates combined with hexbin\nvisualisation. Our hierarchical approach in the first stage classifies systems\ninto detached and overcontact types, and in the second stage identifies the\npresence or absence of spots. The binary classification models achieved high\naccuracy ($>96\\%$) on validation data across multiple passbands (Gaia~$G$, $I$,\nand $TESS$) and demonstrated strong performance ($>94\\%$, up to $100\\%$ for\n$TESS$) when tested on extensive observational data from the OGLE, DEBCat, and\nWUMaCat catalogues. While the primary binary classification was highly\nsuccessful, the secondary task of automated spot detection performed poorly,\nrevealing a significant limitation of our models for identifying subtle\nphotometric features. This study highlights the potential of computer vision\nfor EB morphological classification in large-scale surveys, but underscores the\nneed for further research into robust, automated spot detection.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.12813", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12813", "abs": "https://arxiv.org/abs/2508.12813", "authors": ["Friedhelm Hamann", "Emil Mededovic", "Fabian G\u00fclhan", "Yuli Wu", "Johannes Stegmaier", "Jing He", "Yiqing Wang", "Kexin Zhang", "Lingling Li", "Licheng Jiao", "Mengru Ma", "Hongxiang Huang", "Yuhao Yan", "Hongwei Ren", "Xiaopeng Lin", "Yulong Huang", "Bojun Cheng", "Se Hyun Lee", "Gyu Sung Ham", "Kanghan Oh", "Gi Hyun Lim", "Boxuan Yang", "Bowen Du", "Guillermo Gallego"], "title": "SIS-Challenge: Event-based Spatio-temporal Instance Segmentation Challenge at the CVPR 2025 Event-based Vision Workshop", "comment": "13 pages, 7 figures, 7 tables", "summary": "We present an overview of the Spatio-temporal Instance Segmentation (SIS)\nchallenge held in conjunction with the CVPR 2025 Event-based Vision Workshop.\nThe task is to predict accurate pixel-level segmentation masks of defined\nobject classes from spatio-temporally aligned event camera and grayscale camera\ndata. We provide an overview of the task, dataset, challenge details and\nresults. Furthermore, we describe the methods used by the top-5 ranking teams\nin the challenge. More resources and code of the participants' methods are\navailable here:\nhttps://github.com/tub-rip/MouseSIS/blob/main/docs/challenge_results.md", "AI": {"tldr": "CVPR 2025\u4e8b\u4ef6\u89c6\u89c9\u7814\u8ba8\u4f1a\u4e3e\u529e\u7684\u65f6\u7a7a\u5b9e\u4f8b\u5206\u5272\u6311\u6218\u8d5b\u6982\u8ff0\uff0c\u5305\u542b\u4efb\u52a1\u5b9a\u4e49\u3001\u6570\u636e\u96c6\u3001\u6311\u6218\u7ec6\u8282\u548c\u7ed3\u679c\u5206\u6790\uff0c\u4ee5\u53ca\u524d5\u540d\u56e2\u961f\u7684\u65b9\u6cd5\u4ecb\u7ecd", "motivation": "\u63a8\u52a8\u4e8b\u4ef6\u76f8\u673a\u548c\u7070\u5ea6\u76f8\u673a\u6570\u636e\u878d\u5408\u7684\u65f6\u7a7a\u5b9e\u4f8b\u5206\u5272\u6280\u672f\u53d1\u5c55\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u793e\u533a\u63d0\u4f9b\u6807\u51c6\u57fa\u51c6\u548c\u8bc4\u4f30\u5e73\u53f0", "method": "\u7ec4\u7ec7\u6311\u6218\u8d5b\uff0c\u63d0\u4f9b\u65f6\u7a7a\u5bf9\u9f50\u7684\u4e8b\u4ef6\u76f8\u673a\u548c\u7070\u5ea6\u76f8\u673a\u6570\u636e\u96c6\uff0c\u5b9a\u4e49\u50cf\u7d20\u7ea7\u5206\u5272\u4efb\u52a1\uff0c\u6536\u96c6\u5e76\u8bc4\u4f30\u53c2\u8d5b\u56e2\u961f\u7684\u7b97\u6cd5\u65b9\u6848", "result": "\u6210\u529f\u4e3e\u529e\u4e86SIS\u6311\u6218\u8d5b\uff0c\u83b7\u5f97\u4e86\u591a\u4e2a\u56e2\u961f\u53c2\u4e0e\uff0c\u63d0\u4f9b\u4e86\u524d5\u540d\u56e2\u961f\u7684\u65b9\u6cd5\u7ec6\u8282\u548c\u6027\u80fd\u7ed3\u679c\uff0c\u5efa\u7acb\u4e86\u516c\u5f00\u7684\u8d44\u6e90\u5e93", "conclusion": "\u8be5\u6311\u6218\u8d5b\u6709\u6548\u4fc3\u8fdb\u4e86\u4e8b\u4ef6\u89c6\u89c9\u9886\u57df\u7684\u53d1\u5c55\uff0c\u4e3a\u65f6\u7a7a\u5b9e\u4f8b\u5206\u5272\u6280\u672f\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u7b97\u6cd5\u6bd4\u8f83\u5e73\u53f0"}}
{"id": "2508.12824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12824", "abs": "https://arxiv.org/abs/2508.12824", "authors": ["Shuang Chen", "Ronald Thenius", "Farshad Arvin", "Amir Atapour-Abarghouei"], "title": "DEEP-SEA: Deep-Learning Enhancement for Environmental Perception in Submerged Aquatics", "comment": null, "summary": "Continuous and reliable underwater monitoring is essential for assessing\nmarine biodiversity, detecting ecological changes and supporting autonomous\nexploration in aquatic environments. Underwater monitoring platforms rely on\nmainly visual data for marine biodiversity analysis, ecological assessment and\nautonomous exploration. However, underwater environments present significant\nchallenges due to light scattering, absorption and turbidity, which degrade\nimage clarity and distort colour information, which makes accurate observation\ndifficult. To address these challenges, we propose DEEP-SEA, a novel deep\nlearning-based underwater image restoration model to enhance both low- and\nhigh-frequency information while preserving spatial structures. The proposed\nDual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator aims to\nadaptively refine feature representations in frequency domains and\nsimultaneously spatial information for better structural preservation. Our\ncomprehensive experiments on EUVP and LSUI datasets demonstrate the superiority\nover the state of the art in restoring fine-grained image detail and structural\nconsistency. By effectively mitigating underwater visual degradation, DEEP-SEA\nhas the potential to improve the reliability of underwater monitoring platforms\nfor more accurate ecological observation, species identification and autonomous\nnavigation.", "AI": {"tldr": "DEEP-SEA\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6d77\u5e95\u56fe\u50cf\u6062\u590d\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u9891\u589e\u5f3a\u81ea\u6ce8\u610f\u529b\u7a7a\u95f4\u548c\u9891\u7387\u8c03\u5236\u5668\u6765\u540c\u65f6\u589e\u5f3a\u4f4e\u9891\u548c\u9ad8\u9891\u4fe1\u606f\uff0c\u4fdd\u6301\u7a7a\u95f4\u7ed3\u6784\uff0c\u6709\u6548\u89e3\u51b3\u6c34\u4e0b\u89c6\u89c9\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u5b58\u5728\u5149\u6563\u5c04\u3001\u5438\u6536\u548c\u6d51\u6d4a\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u56fe\u50cf\u6e05\u6670\u5ea6\u4e0b\u964d\u548c\u989c\u8272\u5931\u771f\uff0c\u5f71\u54cd\u6d77\u6d0b\u751f\u7269\u591a\u6837\u6027\u5206\u6790\u3001\u751f\u6001\u8bc4\u4f30\u548c\u81ea\u4e3b\u63a2\u7d22\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faDEEP-SEA\u6a21\u578b\uff0c\u91c7\u7528\u53cc\u9891\u589e\u5f3a\u81ea\u6ce8\u610f\u529b\u7a7a\u95f4\u548c\u9891\u7387\u8c03\u5236\u5668\uff0c\u81ea\u9002\u5e94\u5730\u5728\u9891\u57df\u7ec6\u5316\u7279\u5f81\u8868\u793a\uff0c\u540c\u65f6\u5904\u7406\u7a7a\u95f4\u4fe1\u606f\u4ee5\u66f4\u597d\u5730\u4fdd\u6301\u7ed3\u6784\u3002", "result": "\u5728EUVP\u548cLSUI\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u6062\u590d\u7cbe\u7ec6\u56fe\u50cf\u7ec6\u8282\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "DEEP-SEA\u901a\u8fc7\u6709\u6548\u7f13\u89e3\u6c34\u4e0b\u89c6\u89c9\u9000\u5316\uff0c\u6709\u6f5c\u529b\u63d0\u9ad8\u6c34\u4e0b\u76d1\u6d4b\u5e73\u53f0\u7684\u53ef\u9760\u6027\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u751f\u6001\u89c2\u6d4b\u3001\u7269\u79cd\u8bc6\u522b\u548c\u81ea\u4e3b\u5bfc\u822a\u3002"}}
{"id": "2508.12842", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.12842", "abs": "https://arxiv.org/abs/2508.12842", "authors": ["Ronghao Lin", "Sijie Mai", "Ying Zeng", "Qiaolin He", "Aolin Xiong", "Haifeng Hu"], "title": "Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection", "comment": "Accepted at ACM MM 2025 SVC Workshop", "summary": "This paper presents the winning approach for the 1st MultiModal Deception\nDetection (MMDD) Challenge at the 1st Workshop on Subtle Visual Computing\n(SVC). Aiming at the domain shift issue across source and target domains, we\npropose a Multi-source Multimodal Progressive Domain Adaptation (MMPDA)\nframework that transfers the audio-visual knowledge from diverse source domains\nto the target domain. By gradually aligning source and the target domain at\nboth feature and decision levels, our method bridges domain shifts across\ndiverse multimodal datasets. Extensive experiments demonstrate the\neffectiveness of our approach securing Top-2 place. Our approach reaches 60.43%\non accuracy and 56.99\\% on F1-score on competition stage 2, surpassing the 1st\nplace team by 5.59% on F1-score and the 3rd place teams by 6.75% on accuracy.\nOur code is available at https://github.com/RH-Lin/MMPDA.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6e90\u591a\u6a21\u6001\u6e10\u8fdb\u57df\u9002\u5e94\u6846\u67b6MMPDA\uff0c\u5728\u591a\u6a21\u6001\u6b3a\u9a97\u68c0\u6d4b\u6311\u6218\u4e2d\u83b7\u5f97\u7b2c\u4e8c\u540d\uff0c\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u8d85\u8d8a\u7b2c\u4e00\u540d\u56e2\u961f", "motivation": "\u89e3\u51b3\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5c06\u97f3\u9891-\u89c6\u89c9\u77e5\u8bc6\u4ece\u591a\u6837\u5316\u7684\u6e90\u57df\u8fc1\u79fb\u5230\u76ee\u6807\u57df", "method": "\u591a\u6e90\u591a\u6a21\u6001\u6e10\u8fdb\u57df\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7279\u5f81\u548c\u51b3\u7b56\u5c42\u9762\u9010\u6b65\u5bf9\u9f50\u6e90\u57df\u548c\u76ee\u6807\u57df\u6765\u5f25\u5408\u57df\u504f\u79fb", "result": "\u5728\u7ade\u8d5b\u7b2c\u4e8c\u9636\u6bb5\u8fbe\u523060.43%\u7684\u51c6\u786e\u7387\u548c56.99%\u7684F1\u5206\u6570\uff0cF1\u5206\u6570\u6bd4\u7b2c\u4e00\u540d\u56e2\u961f\u9ad85.59%\uff0c\u51c6\u786e\u7387\u6bd4\u7b2c\u4e09\u540d\u56e2\u961f\u9ad86.75%", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u6b3a\u9a97\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u57df\u8fc1\u79fb\u95ee\u9898\uff0c\u83b7\u5f97\u4e86Top-2\u7684\u6210\u7ee9"}}
{"id": "2508.12861", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12861", "abs": "https://arxiv.org/abs/2508.12861", "authors": ["Dexia Chen", "Wentao Zhang", "Qianjie Zhu", "Ping Hu", "Weibing Li", "Tong Zhang", "Ruixuan Wang"], "title": "Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) pre-trained on natural image and language data,\nsuch as CLIP, have exhibited significant potential in few-shot image\nrecognition tasks, leading to development of various efficient transfer\nlearning methods. These methods exploit inherent pre-learned knowledge in VLMs\nand have achieved strong performance on standard image datasets. However, their\neffectiveness is often limited when confronted with cross-domain tasks where\nimaging domains differ from natural images. To address this limitation, we\npropose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), a\nnovel fine-tuning strategy for VLMs. This strategy employs two functionally\ncomplementary expert modules to extract multi-view features, while\nincorporating prior knowledge-based consistency constraints and information\ngeometry-based consensus mechanisms to enhance the robustness of feature\nlearning. Additionally, a new cross-domain few-shot benchmark is established to\nhelp comprehensively evaluate methods on imaging domains distinct from natural\nimages. Extensive empirical evaluations on both existing and newly proposed\nbenchmarks suggest CoMuCo consistently outperforms current methods in few-shot\ntasks. The code and benchmark will be released.", "AI": {"tldr": "\u63d0\u51faCoMuCo\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u534f\u4f5c\u4f18\u5316\u548c\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u57df\u5c11\u6837\u672c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8de8\u57df\u4efb\u52a1\uff08\u6210\u50cf\u57df\u4e0d\u540c\u4e8e\u81ea\u7136\u56fe\u50cf\uff09\u4e2d\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u65b0\u7684\u5fae\u8c03\u7b56\u7565\u6765\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027", "method": "CoMuCo\u65b9\u6cd5\u4f7f\u7528\u4e24\u4e2a\u529f\u80fd\u4e92\u8865\u7684\u4e13\u5bb6\u6a21\u5757\u63d0\u53d6\u591a\u89c6\u56fe\u7279\u5f81\uff0c\u7ed3\u5408\u57fa\u4e8e\u5148\u9a8c\u77e5\u8bc6\u7684\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u4fe1\u606f\u51e0\u4f55\u7684\u5171\u8bc6\u673a\u5236\u6765\u589e\u5f3a\u7279\u5f81\u5b66\u4e60\u7684\u9c81\u68d2\u6027", "result": "\u5728\u73b0\u6709\u548c\u65b0\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u8868\u660eCoMuCo\u5728\u5c11\u6837\u672c\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5f53\u524d\u65b9\u6cd5", "conclusion": "CoMuCo\u662f\u4e00\u79cd\u6709\u6548\u7684\u8de8\u57df\u5c11\u6837\u672c\u5b66\u4e60\u5fae\u8c03\u7b56\u7565\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u8de8\u57df\u57fa\u51c6\u6765\u5168\u9762\u8bc4\u4f30\u65b9\u6cd5\u6027\u80fd"}}
{"id": "2508.12877", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12877", "abs": "https://arxiv.org/abs/2508.12877", "authors": ["Dexia Chen", "Qianjie Zhu", "Weibing Li", "Yue Yu", "Tong Zhang", "Ruixuan Wang"], "title": "Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning", "comment": null, "summary": "Pretrained vision-language models (VLMs), such as CLIP, have shown remarkable\npotential in few-shot image classification and led to numerous effective\ntransfer learning strategies. These methods leverage the pretrained knowledge\nof VLMs to enable effective domain adaptation while mitigating overfitting\nthrough parameter-efficient tuning or instance-based consistency constraints.\nHowever, such regularizations often neglect the geometric structure of data\ndistribution, which may lead to distortion of the overall semantic\nrepresentation. To overcome this limitation, we propose a novel fine-tuning\nmethod, Manifold-Preserving and Sculpting Tuning (MPS-Tuning). Regarding the\ndata distribution in feature space as a semantic manifold, MPS-Tuning\nexplicitly constrains the intrinsic geometry of this manifold while further\nsculpting it to enhance class separability. Specifically, MPS-Tuning preserves\nboth macroscopic and microscopic topological structures of the original\nmanifold by aligning Gram matrices of features before and after fine-tuning.\nTheoretically, this constraint is shown to approximate an upper bound of the\nGromov-Wasserstein distance. Furthermore, features from the image and text\nmodalities are paired, and pairwise similarities are optimized to enhance the\nmanifold's class discriminability. Extensive experiments demonstrate that\nMPS-Tuning significantly improves model performance while effectively\npreserving the structure of the semantic manifold. The code will be released.", "AI": {"tldr": "\u57fa\u4e8eCLIP\u7b49\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u7cbe\u7ec6\u8c03\u6574\u65b9\u6cd5MPS-Tuning\uff0c\u901a\u8fc7\u4fdd\u6301\u8bed\u4e49\u6d41\u5f62\u7684\u51e0\u4f55\u7ed3\u6784\u548c\u589e\u5f3a\u7c7b\u522b\u53ef\u5206\u8fa8\u6027\u6765\u63d0\u5347\u57df\u9002\u914d\u6027\u80fd", "motivation": "\u73b0\u6709\u7684\u53c2\u6570\u6548\u7387\u8c03\u6574\u6216\u4e00\u81f4\u6027\u7ea6\u675f\u65b9\u6cd5\u5ffd\u89c6\u4e86\u6570\u636e\u5206\u5e03\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5bfc\u81f4\u6574\u4f53\u8bed\u4e49\u8868\u5f81\u7684\u626d\u66f2", "method": "\u5c06\u7279\u5f81\u7a7a\u95f4\u7684\u6570\u636e\u5206\u5e03\u89c6\u4e3a\u8bed\u4e49\u6d41\u5f62\uff0c\u901a\u8fc7\u5bf9\u9f50\u7cbe\u7ec6\u8c03\u6574\u524d\u540e\u7279\u5f81\u7684Gram\u77e9\u9635\u6765\u4fdd\u6301\u539f\u59cb\u6d41\u5f62\u7684\u5b8f\u89c2\u548c\u5fae\u89c2\u62d3\u6251\u7ed3\u6784\uff0c\u540c\u65f6\u4f18\u5316\u56fe\u50cf\u548c\u6587\u672c\u7279\u5f81\u7684\u6210\u5bf9\u76f8\u4f3c\u6027\u6765\u589e\u5f3a\u7c7b\u522b\u53ef\u5206\u8fa8\u6027", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eMPS-Tuning\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u6709\u6548\u4fdd\u6301\u4e86\u8bed\u4e49\u6d41\u5f62\u7684\u7ed3\u6784", "conclusion": "MPS-Tuning\u901a\u8fc7\u660e\u786e\u7ea6\u675f\u8bed\u4e49\u6d41\u5f62\u7684\u672c\u8d28\u51e0\u4f55\u7ed3\u6784\u548c\u589e\u5f3a\u7c7b\u522b\u53ef\u5206\u8fa8\u6027\uff0c\u4e3a\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u7cbe\u7ec6\u8c03\u6574\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5"}}
{"id": "2508.12880", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12880", "abs": "https://arxiv.org/abs/2508.12880", "authors": ["Chubin Chen", "Jiashu Zhu", "Xiaokun Feng", "Nisha Huang", "Meiqi Wu", "Fangyuan Mao", "Jiahong Wu", "Xiangxiang Chu", "Xiu Li"], "title": "S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models", "comment": null, "summary": "Classifier-free Guidance (CFG) is a widely used technique in modern diffusion\nmodels for enhancing sample quality and prompt adherence. However, through an\nempirical analysis on Gaussian mixture modeling with a closed-form solution, we\nobserve a discrepancy between the suboptimal results produced by CFG and the\nground truth. The model's excessive reliance on these suboptimal predictions\noften leads to semantic incoherence and low-quality outputs. To address this\nissue, we first empirically demonstrate that the model's suboptimal predictions\ncan be effectively refined using sub-networks of the model itself. Building on\nthis insight, we propose S^2-Guidance, a novel method that leverages stochastic\nblock-dropping during the forward process to construct stochastic sub-networks,\neffectively guiding the model away from potential low-quality predictions and\ntoward high-quality outputs. Extensive qualitative and quantitative experiments\non text-to-image and text-to-video generation tasks demonstrate that\nS^2-Guidance delivers superior performance, consistently surpassing CFG and\nother advanced guidance strategies. Our code will be released.", "AI": {"tldr": "S^2-Guidance\u662f\u4e00\u79cd\u65b0\u7684\u6269\u6563\u6a21\u578b\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u5757\u4e22\u5f03\u6784\u5efa\u5b50\u7f51\u7edc\u6765\u4f18\u5316\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86CFG\u65b9\u6cd5\u4e2d\u7684\u6b21\u4f18\u9884\u6d4b\u95ee\u9898\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf\u548c\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0Classifier-free Guidance (CFG)\u65b9\u6cd5\u5728\u6269\u6563\u6a21\u578b\u4e2d\u4f1a\u4ea7\u751f\u6b21\u4f18\u9884\u6d4b\uff0c\u5bfc\u81f4\u8bed\u4e49\u4e0d\u8fde\u8d2f\u548c\u4f4e\u8d28\u91cf\u8f93\u51fa\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u5f15\u5bfc\u7b56\u7565\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51faS^2-Guidance\u65b9\u6cd5\uff0c\u5229\u7528\u524d\u5411\u8fc7\u7a0b\u4e2d\u7684\u968f\u673a\u5757\u4e22\u5f03\u6765\u6784\u5efa\u968f\u673a\u5b50\u7f51\u7edc\uff0c\u6709\u6548\u5f15\u5bfc\u6a21\u578b\u8fdc\u79bb\u4f4e\u8d28\u91cf\u9884\u6d4b\uff0c\u671d\u5411\u9ad8\u8d28\u91cf\u8f93\u51fa\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u548c\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cS^2-Guidance\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u90fd\u4f18\u4e8eCFG\u548c\u5176\u4ed6\u5148\u8fdb\u5f15\u5bfc\u7b56\u7565\u3002", "conclusion": "S^2-Guidance\u901a\u8fc7\u5229\u7528\u6a21\u578b\u81ea\u8eab\u7684\u5b50\u7f51\u7edc\u6765\u4f18\u5316\u9884\u6d4b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6269\u6563\u6a21\u578b\u5f15\u5bfc\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2508.12891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12891", "abs": "https://arxiv.org/abs/2508.12891", "authors": ["Sankar Behera", "Yamuna Prasad"], "title": "ONG: One-Shot NMF-based Gradient Masking for Efficient Model Sparsification", "comment": "7 pages", "summary": "Deep Neural Networks (DNNs) have achieved remarkable success but their large\nsize poses deployment challenges. While various pruning techniques exist, many\ninvolve complex iterative processes, specialized criteria, or struggle to\nmaintain sparsity effectively during training. We introduce ONG (One-shot\nNMF-based Gradient Masking), a novel sparsification strategy that identifies\nsalient weight structures using Non-negative Matrix Factorization (NMF) for\none-shot pruning at the outset of training. Subsequently, ONG employs a precise\ngradient masking mechanism to ensure that only unpruned weights are updated,\nstrictly preserving the target sparsity throughout the training phase. We\nintegrate ONG into the BIMP comparative framework and evaluate it on CIFAR-10\nand CIFAR-100 with ResNet56, ResNet34, and ResNet18 against established stable\nsparsification methods. Our experiments demonstrate ONG's ability to achieve\ncomparable or superior performance at various sparsity levels while maintaining\nstructural integrity post-pruning and offering a clear mechanism for targeting\ndesired sparsities.", "AI": {"tldr": "ONG\u662f\u4e00\u79cd\u57fa\u4e8e\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u7684\u4e00\u6b21\u6027\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u63a9\u7801\u673a\u5236\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e25\u683c\u4fdd\u6301\u76ee\u6807\u7a00\u758f\u5ea6\uff0c\u5728CIFAR\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5c3a\u5bf8\u5e9e\u5927\u5bfc\u81f4\u90e8\u7f72\u56f0\u96be\uff0c\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u5b58\u5728\u8fed\u4ee3\u8fc7\u7a0b\u590d\u6742\u3001\u6807\u51c6\u4e13\u4e1a\u6216\u96be\u4ee5\u5728\u8bad\u7ec3\u4e2d\u6709\u6548\u4fdd\u6301\u7a00\u758f\u5ea6\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u7a00\u758f\u5316\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u975e\u8d1f\u77e9\u9635\u5206\u89e3(NMF)\u8bc6\u522b\u91cd\u8981\u6743\u91cd\u7ed3\u6784\u8fdb\u884c\u4e00\u6b21\u6027\u521d\u59cb\u526a\u679d\uff0c\u7136\u540e\u91c7\u7528\u7cbe\u786e\u7684\u68af\u5ea6\u63a9\u7801\u673a\u5236\u786e\u4fdd\u53ea\u66f4\u65b0\u672a\u526a\u679d\u6743\u91cd\uff0c\u4e25\u683c\u4fdd\u6301\u76ee\u6807\u7a00\u758f\u5ea6\u3002", "result": "\u5728CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\u4f7f\u7528ResNet56\u3001ResNet34\u548cResNet18\u8fdb\u884c\u6d4b\u8bd5\uff0cONG\u5728\u4e0d\u540c\u7a00\u758f\u5ea6\u6c34\u5e73\u4e0b\u90fd\u80fd\u8fbe\u5230\u4e0e\u73b0\u6709\u7a33\u5b9a\u7a00\u758f\u5316\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "ONG\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4e00\u6b21\u6027\u526a\u679d\u65b9\u6cd5\uff0c\u80fd\u591f\u4fdd\u6301\u526a\u679d\u540e\u7684\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u673a\u5236\u6765\u8fbe\u5230\u76ee\u6807\u7a00\u758f\u5ea6\uff0c\u89e3\u51b3\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7a00\u758f\u5ea6\u4fdd\u6301\u7684\u96be\u9898\u3002"}}
{"id": "2508.12917", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12917", "abs": "https://arxiv.org/abs/2508.12917", "authors": ["Zhiwei Ning", "Zhaojiang Liu", "Xuanang Gao", "Yifan Zuo", "Jie Yang", "Yuming Fang", "Wei Liu"], "title": "CMF-IoU: Multi-Stage Cross-Modal Fusion 3D Object Detection with IoU Joint Prediction", "comment": "The Paper is Accepted by TCSVT", "summary": "Multi-modal methods based on camera and LiDAR sensors have garnered\nsignificant attention in the field of 3D detection. However, many prevalent\nworks focus on single or partial stage fusion, leading to insufficient feature\nextraction and suboptimal performance. In this paper, we introduce a\nmulti-stage cross-modal fusion 3D detection framework, termed CMF-IOU, to\neffectively address the challenge of aligning 3D spatial and 2D semantic\ninformation. Specifically, we first project the pixel information into 3D space\nvia a depth completion network to get the pseudo points, which unifies the\nrepresentation of the LiDAR and camera information. Then, a bilateral\ncross-view enhancement 3D backbone is designed to encode LiDAR points and\npseudo points. The first sparse-to-distant (S2D) branch utilizes an\nencoder-decoder structure to reinforce the representation of sparse LiDAR\npoints. The second residual view consistency (ResVC) branch is proposed to\nmitigate the influence of inaccurate pseudo points via both the 3D and 2D\nconvolution processes. Subsequently, we introduce an iterative voxel-point\naware fine grained pooling module, which captures the spatial information from\nLiDAR points and textural information from pseudo points in the proposal\nrefinement stage. To achieve more precise refinement during iteration, an\nintersection over union (IoU) joint prediction branch integrated with a novel\nproposals generation technique is designed to preserve the bounding boxes with\nboth high IoU and classification scores. Extensive experiments show the\nsuperior performance of our method on the KITTI, nuScenes and Waymo datasets.", "AI": {"tldr": "CMF-IOU\u662f\u4e00\u4e2a\u591a\u9636\u6bb5\u8de8\u6a21\u6001\u878d\u5408\u76843D\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u8865\u5168\u7f51\u7edc\u5c06\u50cf\u7d20\u4fe1\u606f\u6295\u5f71\u52303D\u7a7a\u95f4\u83b7\u5f97\u4f2a\u70b9\u4e91\uff0c\u8bbe\u8ba1\u53cc\u8fb9\u4ea4\u53c9\u89c6\u56fe\u589e\u5f3a3D\u9aa8\u5e72\u7f51\u7edc\uff0c\u5e76\u5f15\u5165\u8fed\u4ee3\u4f53\u7d20\u70b9\u611f\u77e5\u7ec6\u7c92\u5ea6\u6c60\u5316\u6a21\u5757\uff0c\u5728KITTI\u3001nuScenes\u548cWaymo\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u9636\u6bb5\u6216\u90e8\u5206\u9636\u6bb5\u878d\u5408\uff0c\u5bfc\u81f4\u7279\u5f81\u63d0\u53d6\u4e0d\u8db3\u548c\u6027\u80fd\u6b20\u4f73\uff0c\u9700\u8981\u89e3\u51b33D\u7a7a\u95f4\u4fe1\u606f\u4e0e2D\u8bed\u4e49\u4fe1\u606f\u5bf9\u9f50\u7684\u6311\u6218\u3002", "method": "1) \u901a\u8fc7\u6df1\u5ea6\u8865\u5168\u7f51\u7edc\u5c06\u50cf\u7d20\u4fe1\u606f\u6295\u5f71\u52303D\u7a7a\u95f4\u83b7\u5f97\u4f2a\u70b9\u4e91\uff1b2) \u8bbe\u8ba1\u53cc\u8fb9\u4ea4\u53c9\u89c6\u56fe\u589e\u5f3a3D\u9aa8\u5e72\u7f51\u7edc\uff08S2D\u5206\u652f\u548cResVC\u5206\u652f\uff09\uff1b3) \u5f15\u5165\u8fed\u4ee3\u4f53\u7d20\u70b9\u611f\u77e5\u7ec6\u7c92\u5ea6\u6c60\u5316\u6a21\u5757\uff1b4) \u8bbe\u8ba1IoU\u8054\u5408\u9884\u6d4b\u5206\u652f\u548c\u65b0\u7684\u5019\u9009\u6846\u751f\u6210\u6280\u672f\u3002", "result": "\u5728KITTI\u3001nuScenes\u548cWaymo\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "CMF-IOU\u6846\u67b6\u901a\u8fc7\u591a\u9636\u6bb5\u8de8\u6a21\u6001\u878d\u5408\u6709\u6548\u89e3\u51b3\u4e863D\u7a7a\u95f4\u548c2D\u8bed\u4e49\u4fe1\u606f\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2508.12919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12919", "abs": "https://arxiv.org/abs/2508.12919", "authors": ["Elena Izzo", "Luca Parolari", "Davide Vezzaro", "Lamberto Ballan"], "title": "7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models", "comment": "Accepted to ICIAP 2025", "summary": "Layout-guided text-to-image models offer greater control over the generation\nprocess by explicitly conditioning image synthesis on the spatial arrangement\nof elements. As a result, their adoption has increased in many computer vision\napplications, ranging from content creation to synthetic data generation. A\ncritical challenge is achieving precise alignment between the image, textual\nprompt, and layout, ensuring semantic fidelity and spatial accuracy. Although\nrecent benchmarks assess text alignment, layout alignment remains overlooked,\nand no existing benchmark jointly evaluates both. This gap limits the ability\nto evaluate a model's spatial fidelity, which is crucial when using\nlayout-guided generation for synthetic data, as errors can introduce noise and\ndegrade data quality. In this work, we introduce 7Bench, the first benchmark to\nassess both semantic and spatial alignment in layout-guided text-to-image\ngeneration. It features text-and-layout pairs spanning seven challenging\nscenarios, investigating object generation, color fidelity, attribute\nrecognition, inter-object relationships, and spatial control. We propose an\nevaluation protocol that builds on existing frameworks by incorporating the\nlayout alignment score to assess spatial accuracy. Using 7Bench, we evaluate\nseveral state-of-the-art diffusion models, uncovering their respective\nstrengths and limitations across diverse alignment tasks. The benchmark is\navailable at https://github.com/Elizzo/7Bench.", "AI": {"tldr": "7Bench\u662f\u9996\u4e2a\u8bc4\u4f30\u5e03\u5c40\u5f15\u5bfc\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8bed\u4e49\u548c\u7a7a\u95f4\u5bf9\u9f50\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b7\u4e2a\u6311\u6218\u6027\u573a\u666f\uff0c\u8bc4\u4f30\u5bf9\u8c61\u751f\u6210\u3001\u989c\u8272\u4fdd\u771f\u5ea6\u3001\u5c5e\u6027\u8bc6\u522b\u3001\u5bf9\u8c61\u95f4\u5173\u7cfb\u548c\u7a7a\u95f4\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u53ea\u8bc4\u4f30\u6587\u672c\u5bf9\u9f50\uff0c\u800c\u5ffd\u7565\u4e86\u5e03\u5c40\u5bf9\u9f50\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u7684\u7a7a\u95f4\u4fdd\u771f\u5ea6\uff0c\u8fd9\u5728\u5408\u6210\u6570\u636e\u751f\u6210\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e867Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u6587\u672c-\u5e03\u5c40\u5bf9\uff0c\u6db5\u76d67\u4e2a\u6311\u6218\u6027\u573a\u666f\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u5408\u5e03\u5c40\u5bf9\u9f50\u5206\u6570\u7684\u8bc4\u4f30\u534f\u8bae\u6765\u8bc4\u4f30\u7a7a\u95f4\u51c6\u786e\u6027\u3002", "result": "\u4f7f\u75287Bench\u8bc4\u4f30\u4e86\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u5bf9\u9f50\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "7Bench\u586b\u8865\u4e86\u5e03\u5c40\u5f15\u5bfc\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u6a21\u578b\u7684\u7a7a\u95f4\u4fdd\u771f\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5408\u6210\u6570\u636e\u8d28\u91cf\u3002"}}
{"id": "2508.12931", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12931", "abs": "https://arxiv.org/abs/2508.12931", "authors": ["Ximiao Zhang", "Min Xu", "Xiuzhuang Zhou"], "title": "Towards High-Resolution Industrial Image Anomaly Detection", "comment": null, "summary": "Current anomaly detection methods primarily focus on low-resolution\nscenarios. For high-resolution images, conventional downsampling often results\nin missed detections of subtle anomalous regions due to the loss of\nfine-grained discriminative information. Despite some progress, recent studies\nhave attempted to improve detection resolution by employing lightweight\nnetworks or using simple image tiling and ensemble methods. However, these\napproaches still struggle to meet the practical demands of industrial scenarios\nin terms of detection accuracy and efficiency. To address the above issues, we\npropose HiAD, a general framework for high-resolution anomaly detection. HiAD\nis capable of detecting anomalous regions of varying sizes in high-resolution\nimages under limited computational resources. Specifically, HiAD employs a\ndual-branch architecture that integrates anomaly cues across different scales\nto comprehensively capture both subtle and large-scale anomalies. Furthermore,\nit incorporates a multi-resolution feature fusion strategy to tackle the\nchallenges posed by fine-grained texture variations in high-resolution images.\nTo enhance both adaptability and efficiency, HiAD utilizes a detector pool in\nconjunction with various detector assignment strategies, enabling detectors to\nbe adaptively assigned based on patch features, ensuring detection performance\nwhile effectively controlling computational costs. We conduct extensive\nexperiments on our specifically constructed high-resolution anomaly detection\nbenchmarks, including MVTec-HD, VisA-HD, and the real-world benchmark\nRealIAD-HD, demonstrating the superior performance of HiAD. The code is\navailable at https://github.com/cnulab/HiAD.", "AI": {"tldr": "HiAD\u662f\u4e00\u4e2a\u9488\u5bf9\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u7684\u53cc\u5206\u652f\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u548c\u81ea\u9002\u5e94\u68c0\u6d4b\u5668\u5206\u914d\u7b56\u7565\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u6709\u6548\u68c0\u6d4b\u4e0d\u540c\u5927\u5c0f\u7684\u5f02\u5e38\u533a\u57df\u3002", "motivation": "\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u4f4e\u5206\u8fa8\u7387\u573a\u666f\uff0c\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0b\u4f20\u7edf\u4e0b\u91c7\u6837\u4f1a\u5bfc\u81f4\u7ec6\u7c92\u5ea6\u5f02\u5e38\u4fe1\u606f\u4e22\u5931\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u65e0\u6cd5\u6ee1\u8db3\u5de5\u4e1a\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\u6574\u5408\u591a\u5c3a\u5ea6\u5f02\u5e38\u7ebf\u7d22\uff0c\u7ed3\u5408\u591a\u5206\u8fa8\u7387\u7279\u5f81\u878d\u5408\u7b56\u7565\u5904\u7406\u7ec6\u7c92\u5ea6\u7eb9\u7406\u53d8\u5316\uff0c\u4f7f\u7528\u68c0\u6d4b\u5668\u6c60\u548c\u81ea\u9002\u5e94\u5206\u914d\u7b56\u7565\u6839\u636e\u56fe\u50cf\u5757\u7279\u5f81\u52a8\u6001\u5206\u914d\u68c0\u6d4b\u5668\u3002", "result": "\u5728\u4e13\u95e8\u6784\u5efa\u7684\u9ad8\u5206\u8fa8\u7387\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\uff08MVTec-HD\u3001VisA-HD\u548cRealIAD-HD\uff09\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86HiAD\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "HiAD\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.12942", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12942", "abs": "https://arxiv.org/abs/2508.12942", "authors": ["Kyriaki-Margarita Bintsi", "Ya\u00ebl Balbastre", "Jingjing Wu", "Julia F. Lehman", "Suzanne N. Haber", "Anastasia Yendiki"], "title": "Fully Automated Segmentation of Fiber Bundles in Anatomic Tracing Data", "comment": "Accepted at CDMRI, MICCAI 2025", "summary": "Anatomic tracer studies are critical for validating and improving diffusion\nMRI (dMRI) tractography. However, large-scale analysis of data from such\nstudies is hampered by the labor-intensive process of annotating fiber bundles\nmanually on histological slides. Existing automated methods often miss sparse\nbundles or require complex post-processing across consecutive sections,\nlimiting their flexibility and generalizability. We present a streamlined,\nfully automated framework for fiber bundle segmentation in macaque tracer data,\nbased on a U-Net architecture with large patch sizes, foreground aware\nsampling, and semisupervised pre-training. Our approach eliminates common\nerrors such as mislabeling terminals as bundles, improves detection of sparse\nbundles by over 20% and reduces the False Discovery Rate (FDR) by 40% compared\nto the state-of-the-art, all while enabling analysis of standalone slices. This\nnew framework will facilitate the automated analysis of anatomic tracing data\nat a large scale, generating more ground-truth data that can be used to\nvalidate and optimize dMRI tractography methods.", "AI": {"tldr": "\u57fa\u4e8eU-Net\u7f51\u7edc\u7684\u5168\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u8865\u4e01\u5c3a\u5bf8\u3001\u524d\u666f\u611f\u77e5\u91c7\u6837\u548c\u534a\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u9ad8\u9e45\u5c0f\u7334\u795e\u7ecf\u8e2a\u8ff9\u6570\u636e\u4e2d\u7ea4\u675f\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6548\u7387", "motivation": "\u89e3\u51b3\u5386\u53f2\u5b66\u5206\u6790\u4e2d\u624b\u52a8\u6ce8\u91ca\u7ea4\u675f\u7684\u52b3\u52a8\u5bc6\u96c6\u95ee\u9898\uff0c\u5145\u5206\u5229\u7528\u795e\u7ecf\u8e2a\u8ff9\u6570\u636e\u6765\u9a8c\u8bc1\u548c\u6539\u8fdb\u6eff\u88c1MRI\u8d68\u6563\u8ff7\u8def\u6280\u672f", "method": "\u91c7\u7528U-Net\u7f51\u7edc\u6784\u9020\uff0c\u7ed3\u5408\u5927\u8865\u4e01\u5c3a\u5bf8\u3001\u524d\u666f\u611f\u77e5\u91c7\u6837\u7b56\u7565\u548c\u534a\u76d1\u7763\u9884\u8bad\u7ec3\u6280\u672f", "result": "\u7a00\u758f\u7ea4\u675f\u68c0\u6d4b\u63d0\u9ad820%\u4ee5\u4e0a\uff0c\u5047\u53d1\u73b0\u7387\u964d\u4f4e40%\uff0c\u907f\u514d\u4e86\u7ec8\u7aef\u88ab\u8bef\u6807\u4e3a\u7ea4\u675f\u7684\u5e38\u89c1\u9519\u8bef\uff0c\u652f\u6301\u5355\u72ec\u5207\u7247\u5206\u6790", "conclusion": "\u8be5\u6846\u67b6\u80fd\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u5904\u7406\u89e3\u5256\u5b66\u8e2a\u8ff9\u6570\u636e\uff0c\u4e3a\u6eff\u88c1MRI\u8d68\u6563\u8ff7\u8def\u6280\u672f\u63d0\u4f9b\u66f4\u591a\u7684\u771f\u5b9e\u5730\u9762\u771f\u5b9e\u6570\u636e\uff0c\u6709\u52a9\u4e8e\u8be5\u6280\u672f\u7684\u9a8c\u8bc1\u548c\u4f18\u5316"}}
{"id": "2508.12945", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12945", "abs": "https://arxiv.org/abs/2508.12945", "authors": ["Jianshu Zeng", "Yuxuan Liu", "Yutong Feng", "Chenxuan Miao", "Zixiang Gao", "Jiwang Qu", "Jianzhang Zhang", "Bin Wang", "Kun Yuan"], "title": "Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models", "comment": "15 pages, 7 figures", "summary": "Video relighting is a challenging yet valuable task, aiming to replace the\nbackground in videos while correspondingly adjusting the lighting in the\nforeground with harmonious blending. During translation, it is essential to\npreserve the original properties of the foreground, e.g., albedo, and propagate\nconsistent relighting among temporal frames. In this paper, we propose Lumen,\nan end-to-end video relighting framework developed on large-scale video\ngenerative models, receiving flexible textual description for instructing the\ncontrol of lighting and background. Considering the scarcity of high-qualified\npaired videos with the same foreground in various lighting conditions, we\nconstruct a large-scale dataset with a mixture of realistic and synthetic\nvideos. For the synthetic domain, benefiting from the abundant 3D assets in the\ncommunity, we leverage advanced 3D rendering engine to curate video pairs in\ndiverse environments. For the realistic domain, we adapt a HDR-based lighting\nsimulation to complement the lack of paired in-the-wild videos. Powered by the\naforementioned dataset, we design a joint training curriculum to effectively\nunleash the strengths of each domain, i.e., the physical consistency in\nsynthetic videos, and the generalized domain distribution in realistic videos.\nTo implement this, we inject a domain-aware adapter into the model to decouple\nthe learning of relighting and domain appearance distribution. We construct a\ncomprehensive benchmark to evaluate Lumen together with existing methods, from\nthe perspectives of foreground preservation and video consistency assessment.\nExperimental results demonstrate that Lumen effectively edit the input into\ncinematic relighted videos with consistent lighting and strict foreground\npreservation. Our project page: https://lumen-relight.github.io/", "AI": {"tldr": "Lumen\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u89c6\u9891\u91cd\u5149\u7167\u6846\u67b6\uff0c\u57fa\u4e8e\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\u63a7\u5236\u5149\u7167\u548c\u80cc\u666f\uff0c\u5728\u4fdd\u6301\u524d\u666f\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u548c\u8c10\u7684\u89c6\u9891\u91cd\u5149\u7167\u6548\u679c\u3002", "motivation": "\u89c6\u9891\u91cd\u5149\u7167\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u4f46\u6709\u4ef7\u503c\u7684\u4efb\u52a1\uff0c\u9700\u8981\u5728\u66ff\u6362\u89c6\u9891\u80cc\u666f\u7684\u540c\u65f6\u76f8\u5e94\u5730\u8c03\u6574\u524d\u666f\u5149\u7167\u5e76\u5b9e\u73b0\u548c\u8c10\u878d\u5408\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u914d\u5bf9\u89c6\u9891\u6570\u636e\uff0c\u4e14\u96be\u4ee5\u4fdd\u6301\u65f6\u95f4\u5e27\u95f4\u7684\u4e00\u81f4\u6027\u548c\u524d\u666f\u5c5e\u6027\uff08\u5982\u53cd\u7167\u7387\uff09\u7684\u4fdd\u7559\u3002", "method": "\u6784\u5efa\u6df7\u5408\u771f\u5b9e\u548c\u5408\u6210\u89c6\u9891\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff1b\u5229\u7528\u5148\u8fdb3D\u6e32\u67d3\u5f15\u64ce\u521b\u5efa\u591a\u6837\u5316\u73af\u5883\u7684\u5408\u6210\u89c6\u9891\u5bf9\uff1b\u91c7\u7528HDR\u5149\u7167\u6a21\u62df\u8865\u5145\u771f\u5b9e\u89c6\u9891\u6570\u636e\uff1b\u8bbe\u8ba1\u8054\u5408\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u6ce8\u5165\u9886\u57df\u611f\u77e5\u9002\u914d\u5668\u6765\u89e3\u8026\u91cd\u5149\u7167\u548c\u9886\u57df\u5916\u89c2\u5206\u5e03\u7684\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLumen\u80fd\u591f\u6709\u6548\u5730\u5c06\u8f93\u5165\u89c6\u9891\u7f16\u8f91\u4e3a\u5177\u6709\u4e00\u81f4\u5149\u7167\u548c\u4e25\u683c\u524d\u666f\u4fdd\u7559\u7684\u7535\u5f71\u7ea7\u91cd\u5149\u7167\u89c6\u9891\uff0c\u5728\u524d\u666f\u4fdd\u6301\u548c\u89c6\u9891\u4e00\u81f4\u6027\u8bc4\u4f30\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Lumen\u6846\u67b6\u901a\u8fc7\u5927\u89c4\u6a21\u6df7\u5408\u6570\u636e\u96c6\u548c\u9886\u57df\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u91cd\u5149\u7167\u4e2d\u7684\u4e00\u81f4\u6027\u548c\u524d\u666f\u4fdd\u7559\u95ee\u9898\uff0c\u4e3a\u89c6\u9891\u7f16\u8f91\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12948", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12948", "abs": "https://arxiv.org/abs/2508.12948", "authors": ["Wei Wei", "Shaojie Zhang", "Yonghao Dang", "Jianqin Yin"], "title": "MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation", "comment": "Accepted to IROS 2025", "summary": "Human action recognition is a crucial task for intelligent robotics,\nparticularly within the context of human-robot collaboration research. In\nself-supervised skeleton-based action recognition, the mask-based\nreconstruction paradigm learns the spatial structure and motion patterns of the\nskeleton by masking joints and reconstructing the target from unlabeled data.\nHowever, existing methods focus on a limited set of joints and low-order motion\npatterns, limiting the model's ability to understand complex motion patterns.\nTo address this issue, we introduce MaskSem, a novel semantic-guided masking\nmethod for learning 3D hybrid high-order motion representations. This novel\nframework leverages Grad-CAM based on relative motion to guide the masking of\njoints, which can be represented as the most semantically rich temporal\norgions. The semantic-guided masking process can encourage the model to explore\nmore discriminative features. Furthermore, we propose using hybrid high-order\nmotion as the reconstruction target, enabling the model to learn multi-order\nmotion patterns. Specifically, low-order motion velocity and high-order motion\nacceleration are used together as the reconstruction target. This approach\noffers a more comprehensive description of the dynamic motion process,\nenhancing the model's understanding of motion patterns. Experiments on the\nNTU60, NTU120, and PKU-MMD datasets show that MaskSem, combined with a vanilla\ntransformer, improves skeleton-based action recognition, making it more\nsuitable for applications in human-robot interaction.", "AI": {"tldr": "\u63d0\u51faMaskSem\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u5bfc\u5411\u7684\u62e6\u622a\u548c\u6df7\u5408\u9ad8\u9636\u8fd0\u52a8\u91cd\u6784\u5b66\u4e60\uff0c\u6539\u5584\u4e86\u81ea\u76d1\u7763\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u7684\u81ea\u76d1\u7763\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6709\u9650\u5173\u8282\u548c\u4f4e\u9636\u8fd0\u52a8\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u590d\u6742\u8fd0\u52a8\u6a21\u5f0f\u7684\u7406\u89e3\u80fd\u529b", "method": "\u4f7f\u7528Grad-CAM\u57fa\u4e8e\u76f8\u5bf9\u8fd0\u52a8\u6765\u5bfc\u5411\u62e6\u622a\u6700\u5177\u8bed\u4e49\u4e30\u5bcc\u7684\u65f6\u95f4\u533a\u57df\uff0c\u5e76\u4ee5\u4f4e\u9636\u901f\u5ea6\u548c\u9ad8\u9636\u52a0\u901f\u5ea6\u7684\u6df7\u5408\u9ad8\u9636\u8fd0\u52a8\u4f5c\u4e3a\u91cd\u6784\u76ee\u6807", "result": "\u5728NTU60\u3001NTU120\u548cPKU-MMD\u6570\u636e\u96c6\u4e0a\u8bc6\u522b\u6027\u80fd\u63d0\u5347\uff0c\u66f4\u9002\u5408\u4eba\u673a\u4ea4\u4e92\u5e94\u7528", "conclusion": "MaskSem\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u5bfc\u5411\u62e6\u622a\u548c\u591a\u9636\u8fd0\u52a8\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u76d1\u7763\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u7684\u6027\u80fd"}}
{"id": "2508.12957", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12957", "abs": "https://arxiv.org/abs/2508.12957", "authors": ["Yizhou Liu", "Jingwei Wei", "Zizhi Chen", "Minghao Han", "Xukun Zhang", "Keliang Liu", "Lihua Zhang"], "title": "Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination", "comment": null, "summary": "Reinforcement learning (RL) with rule-based rewards has demonstrated strong\npotential in enhancing the reasoning and generalization capabilities of\nvision-language models (VLMs) and large language models (LLMs), while reducing\ncomputational overhead. However, its application in medical imaging remains\nunderexplored. Existing reinforcement fine-tuning (RFT) approaches in this\ndomain primarily target closed-ended visual question answering (VQA), limiting\ntheir applicability to real-world clinical reasoning. In contrast, open-ended\nmedical VQA better reflects clinical practice but has received limited\nattention. While some efforts have sought to unify both formats via\nsemantically guided RL, we observe that model-based semantic rewards often\nsuffer from reward collapse, where responses with significant semantic\ndifferences receive similar scores. To address this, we propose ARMed (Adaptive\nReinforcement for Medical Reasoning), a novel RL framework for open-ended\nmedical VQA. ARMed first incorporates domain knowledge through supervised\nfine-tuning (SFT) on chain-of-thought data, then applies reinforcement learning\nwith textual correctness and adaptive semantic rewards to enhance reasoning\nquality. We evaluate ARMed on six challenging medical VQA benchmarks. Results\nshow that ARMed consistently boosts both accuracy and generalization, achieving\na 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain\nbenchmarks. These results highlight the critical role of reward\ndiscriminability in medical RL and the promise of semantically guided rewards\nfor enabling robust and clinically meaningful multimodal reasoning.", "AI": {"tldr": "ARMed\u662f\u4e00\u4e2a\u7528\u4e8e\u5f00\u653e\u5f0f\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u6b63\u786e\u6027\u548c\u81ea\u9002\u5e94\u8bed\u4e49\u5956\u52b1\u6765\u63d0\u5347\u533b\u5b66\u63a8\u7406\u8d28\u91cf\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5c01\u95ed\u5f0f\u89c6\u89c9\u95ee\u7b54\uff0c\u800c\u5f00\u653e\u5f0f\u533b\u5b66VQA\u66f4\u80fd\u53cd\u6620\u4e34\u5e8a\u5b9e\u8df5\u4f46\u5173\u6ce8\u6709\u9650\u3002\u57fa\u4e8e\u6a21\u578b\u7684\u8bed\u4e49\u5956\u52b1\u5b58\u5728\u5956\u52b1\u5d29\u6e83\u95ee\u9898\uff0c\u5373\u8bed\u4e49\u5dee\u5f02\u663e\u8457\u7684\u54cd\u5e94\u83b7\u5f97\u76f8\u4f3c\u5206\u6570\u3002", "method": "ARMed\u9996\u5148\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5728\u601d\u7ef4\u94fe\u6570\u636e\u4e2d\u878d\u5165\u9886\u57df\u77e5\u8bc6\uff0c\u7136\u540e\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u6587\u672c\u6b63\u786e\u6027\u548c\u81ea\u9002\u5e94\u8bed\u4e49\u5956\u52b1\u6765\u589e\u5f3a\u63a8\u7406\u8d28\u91cf\u3002", "result": "\u5728\u516d\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u533b\u5b66VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cARMed\u6301\u7eed\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u57df\u5185\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e8632.64%\u7684\u6539\u8fdb\uff0c\u5728\u57df\u5916\u57fa\u51c6\u4e0a\u83b7\u5f97\u4e8611.65%\u7684\u63d0\u5347\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86\u5956\u52b1\u53ef\u533a\u5206\u6027\u5728\u533b\u5b66\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4ee5\u53ca\u8bed\u4e49\u5f15\u5bfc\u5956\u52b1\u5728\u5b9e\u73b0\u7a33\u5065\u4e14\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.12966", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12966", "abs": "https://arxiv.org/abs/2508.12966", "authors": ["Ryan Anthony Jalova de Belen", "Gelareh Mohammadi", "Arcot Sowmya"], "title": "GazeDETR: Gaze Detection using Disentangled Head and Gaze Representations", "comment": null, "summary": "Gaze communication plays a crucial role in daily social interactions.\nQuantifying this behavior can help in human-computer interaction and digital\nphenotyping. While end-to-end models exist for gaze target detection, they only\nutilize a single decoder to simultaneously localize human heads and predict\ntheir corresponding gaze (e.g., 2D points or heatmap) in a scene. This\nmultitask learning approach generates a unified and entangled representation\nfor human head localization and gaze location prediction. Herein, we propose\nGazeDETR, a novel end-to-end architecture with two disentangled decoders that\nindividually learn unique representations and effectively utilize coherent\nattentive fields for each subtask. More specifically, we demonstrate that its\nhuman head predictor utilizes local information, while its gaze decoder\nincorporates both local and global information. Our proposed architecture\nachieves state-of-the-art results on the GazeFollow, VideoAttentionTarget and\nChildPlay datasets. It outperforms existing end-to-end models with a notable\nmargin.", "AI": {"tldr": "GazeDETR\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u4f7f\u7528\u4e24\u4e2a\u89e3\u8026\u7684\u89e3\u7801\u5668\u5206\u522b\u5904\u7406\u5934\u90e8\u5b9a\u4f4d\u548c\u89c6\u7ebf\u9884\u6d4b\u4efb\u52a1\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u89c6\u7ebf\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u4f7f\u7528\u5355\u4e00\u89e3\u7801\u5668\u540c\u65f6\u5b9a\u4f4d\u5934\u90e8\u548c\u9884\u6d4b\u89c6\u7ebf\uff0c\u5bfc\u81f4\u8868\u793a\u7ea0\u7f20\u3002\u9700\u8981\u89e3\u8026\u8fd9\u4e24\u4e2a\u4efb\u52a1\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faGazeDETR\u67b6\u6784\uff0c\u5305\u542b\u4e24\u4e2a\u72ec\u7acb\u7684\u89e3\u7801\u5668\uff1a\u4e00\u4e2a\u7528\u4e8e\u5934\u90e8\u5b9a\u4f4d\uff08\u5229\u7528\u5c40\u90e8\u4fe1\u606f\uff09\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u89c6\u7ebf\u9884\u6d4b\uff08\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\uff09\uff0c\u5e76\u6709\u6548\u5229\u7528\u8fde\u8d2f\u6ce8\u610f\u529b\u573a\u3002", "result": "\u5728GazeFollow\u3001VideoAttentionTarget\u548cChildPlay\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u7aef\u5230\u7aef\u6a21\u578b\u3002", "conclusion": "\u89e3\u8026\u7684\u67b6\u6784\u8bbe\u8ba1\u80fd\u591f\u4e3a\u6bcf\u4e2a\u5b50\u4efb\u52a1\u5b66\u4e60\u72ec\u7279\u7684\u8868\u793a\uff0c\u5934\u90e8\u9884\u6d4b\u5668\u4f7f\u7528\u5c40\u90e8\u4fe1\u606f\uff0c\u89c6\u7ebf\u89e3\u7801\u5668\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u89c6\u7ebf\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.12969", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12969", "abs": "https://arxiv.org/abs/2508.12969", "authors": ["Qirui Li", "Guangcong Zheng", "Qi Zhao", "Jie Li", "Bin Dong", "Yiwu Yao", "Xi Li"], "title": "Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation", "comment": null, "summary": "The computational demands of self-attention mechanisms pose a critical\nchallenge for transformer-based video generation, particularly in synthesizing\nultra-long sequences. Current approaches, such as factorized attention and\nfixed sparse patterns, fail to fully exploit the inherent spatio-temporal\nredundancies in video data. Through systematic analysis of video diffusion\ntransformers (DiT), we uncover a key insight: Attention matrices exhibit\nstructured, yet heterogeneous sparsity patterns, where specialized heads\ndynamically attend to distinct spatiotemporal regions (e.g., local pattern,\ncross-shaped pattern, or global pattern). Existing sparse attention methods\neither impose rigid constraints or introduce significant overhead, limiting\ntheir effectiveness. To address this, we propose Compact Attention, a\nhardware-aware acceleration framework featuring three innovations: 1) Adaptive\ntiling strategies that approximate diverse spatial interaction patterns via\ndynamic tile grouping, 2) Temporally varying windows that adjust sparsity\nlevels based on frame proximity, and 3) An automated configuration search\nalgorithm that optimizes sparse patterns while preserving critical attention\npathways. Our method achieves 1.6~2.5x acceleration in attention computation on\nsingle-GPU setups while maintaining comparable visual quality with\nfull-attention baselines. This work provides a principled approach to unlocking\nefficient long-form video generation through structured sparsity exploitation.\nProject Page: https://yo-ava.github.io/Compact-Attention.github.io/", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86Compact Attention\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6c34\u5e73\u7ec4\u5408\u3001\u65f6\u95f4\u53d8\u5316\u7a97\u53e3\u548c\u81ea\u52a8\u914d\u7f6e\u641c\u7d22\uff0c\u5b9e\u73b0\u4e86\u6ce8\u610f\u529b\u673a\u5236\u7684\u9ad8\u6548\u52a0\u901f\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u83b7\u5f971.6~2.5\u500d\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\u8981\u6c42\u5bf9\u4e8e\u53d8\u6362\u5668\u57fa\u4e8e\u89c6\u9891\u751f\u6210\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u751f\u6210\u8d85\u957f\u5e8f\u5217\u65f6\u3002\u73b0\u6709\u7684\u56e0\u5b50\u5316\u6ce8\u610f\u529b\u548c\u56fa\u5b9a\u7a00\u758f\u6a21\u5f0f\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5229\u7528\u89c6\u9891\u6570\u636e\u4e2d\u5185\u5728\u7684\u7a7a\u95f4-\u65f6\u95f4\u5197\u4f59\u6027\u3002", "method": "\u63d0\u51faCompact Attention\u786c\u4ef6\u611f\u77e5\u52a0\u901f\u6846\u67b6\uff1a1)\u9002\u5e94\u6027\u5206\u5757\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u5757\u7ec4\u5408\u8fd1\u4f3c\u591a\u6837\u5316\u7a7a\u95f4\u4ea4\u4e92\u6a21\u5f0f\uff1b2)\u65f6\u95f4\u53d8\u5316\u7a97\u53e3\uff0c\u6839\u636e\u5e27\u8ddd\u79bb\u8c03\u6574\u7a00\u758f\u7a0b\u5ea6\uff1b3)\u81ea\u52a8\u5316\u914d\u7f6e\u641c\u7d22\u7b97\u6cd5\uff0c\u5728\u4fdd\u7559\u5173\u952e\u6ce8\u610f\u529b\u9014\u5f84\u7684\u540c\u65f6\u4f18\u5316\u7a00\u758f\u6a21\u5f0f\u3002", "result": "\u5728\u5355GPU\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u6ce8\u610f\u529b\u8ba1\u7b971.6~2.5\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u5168\u6ce8\u610f\u529b\u57fa\u7ebf\u76f8\u5f53\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5316\u7a00\u758f\u5229\u7528\u7684\u539f\u7406\u6027\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f00\u542f\u9ad8\u6548\u7684\u957f\u5f62\u5f0f\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2508.12977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12977", "abs": "https://arxiv.org/abs/2508.12977", "authors": ["Rohan Asthana", "Joschua Conrad", "Maurits Ortmanns", "Vasileios Belagiannis"], "title": "Dextr: Zero-Shot Neural Architecture Search with Singular Value Decomposition and Extrinsic Curvature", "comment": "Accepted at Transactions on Machine Learning Research (TMLR)", "summary": "Zero-shot Neural Architecture Search (NAS) typically optimises the\narchitecture search process by exploiting the network or gradient properties at\ninitialisation through zero-cost proxies. The existing proxies often rely on\nlabelled data, which is usually unavailable in real-world settings.\nFurthermore, the majority of the current methods focus either on optimising the\nconvergence and generalisation attributes or solely on the expressivity of the\nnetwork architectures. To address both limitations, we first demonstrate how\nchannel collinearity affects the convergence and generalisation properties of a\nneural network. Then, by incorporating the convergence, generalisation and\nexpressivity in one approach, we propose a zero-cost proxy that omits the\nrequirement of labelled data for its computation. In particular, we leverage\nthe Singular Value Decomposition (SVD) of the neural network layer features and\nthe extrinsic curvature of the network output to design our proxy. %As a\nresult, the proposed proxy is formulated as the simplified harmonic mean of the\nlogarithms of two key components: the sum of the inverse of the feature\ncondition number and the extrinsic curvature of the network output. Our\napproach enables accurate prediction of network performance on test data using\nonly a single label-free data sample. Our extensive evaluation includes a total\nof six experiments, including the Convolutional Neural Network (CNN) search\nspace, i.e. DARTS and the Transformer search space, i.e. AutoFormer. The\nproposed proxy demonstrates a superior performance on multiple correlation\nbenchmarks, including NAS-Bench-101, NAS-Bench-201, and\nTransNAS-Bench-101-micro; as well as on the NAS task within the DARTS and the\nAutoFormer search space, all while being notably efficient. The code is\navailable at https://github.com/rohanasthana/Dextr.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6807\u7b7e\u6570\u636e\u7684\u96f6\u6837\u672c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u641c\u7d22\u4ee3\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6c47\u805a\u6027\u3001\u6cdb\u5316\u6027\u548c\u8868\u8fbe\u80fd\u529b\u6765\u9884\u6d4b\u7f51\u7edc\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u4e14\u6548\u7387\u9ad8\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u96f6\u6837\u672cNAS\u4ee3\u7406\u65b9\u6cd5\u4f9d\u8d56\u6807\u7b7e\u6570\u636e\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u4ec5\u5173\u6ce8\u6c47\u805a\u6027/\u6cdb\u5316\u6027\u6216\u5355\u7eaf\u8868\u8fbe\u80fd\u529b\u7684\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u7efc\u5408\u8003\u8651\u5404\u65b9\u9762\u6027\u80fd\u7684\u65e0\u6807\u7b7e\u4ee3\u7406\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u9891\u9053\u5171\u7ebf\u6027\u5bf9\u7f51\u7edc\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u7ed3\u5408\u5c42\u7279\u5f81\u7684\u5947\u5f02\u503c\u5206\u89e3(SVD)\u548c\u7f51\u7edc\u8f93\u51fa\u7684\u5916\u5728\u66f2\u7387\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b80\u5316\u8c03\u548c\u5e73\u5747\u5f62\u5f0f\u7684\u4ee3\u7406\u6307\u6807\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u5355\u4e2a\u65e0\u6807\u7b7e\u6570\u636e\u6837\u672c\u5373\u53ef\u9884\u6d4b\u7f51\u7edc\u6027\u80fd\u3002", "result": "\u5728NAS-Bench-101\u3001NAS-Bench-201\u3001TransNAS-Bench-101-micro\u7b49\u591a\u4e2a\u76f8\u5173\u6027\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5728DARTS\u5377\u79ef\u7f51\u7edc\u548cAutoFormer\u53d8\u6362\u5668\u641c\u7d22\u7a7a\u95f4\u7684NAS\u4efb\u52a1\u4e2d\u4e5f\u663e\u793a\u51fa\u826f\u597d\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65e0\u6807\u7b7e\u6570\u636e\u96f6\u6837\u672cNAS\u4ee3\u7406\u65b9\u6cd5\u80fd\u591f\u7efc\u5408\u8003\u8651\u7f51\u7edc\u7684\u6c47\u805a\u6027\u3001\u6cdb\u5316\u6027\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u9884\u6d4b\u6548\u679c\uff0c\u4e3a\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u641c\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13000", "abs": "https://arxiv.org/abs/2508.13000", "authors": ["Zhangyong Tang", "Tianyang Xu", "Xuefeng Zhu", "Hui Li", "Shaochuan Zhao", "Tao Zhou", "Chunyang Cheng", "Xiaojun Wu", "Josef Kittler"], "title": "Omni Survey for Multimodality Analysis in Visual Object Tracking", "comment": "The first comprehensive survey for multi-modal visual object\n  tracking; 6 multi-modal tasks; 338 references", "summary": "The development of smart cities has led to the generation of massive amounts\nof multi-modal data in the context of a range of tasks that enable a\ncomprehensive monitoring of the smart city infrastructure and services. This\npaper surveys one of the most critical tasks, multi-modal visual object\ntracking (MMVOT), from the perspective of multimodality analysis. Generally,\nMMVOT differs from single-modal tracking in four key aspects, data collection,\nmodality alignment and annotation, model designing, and evaluation.\nAccordingly, we begin with an introduction to the relevant data modalities,\nlaying the groundwork for their integration. This naturally leads to a\ndiscussion of challenges of multi-modal data collection, alignment, and\nannotation. Subsequently, existing MMVOT methods are categorised, based on\ndifferent ways to deal with visible (RGB) and X modalities: programming the\nauxiliary X branch with replicated or non-replicated experimental\nconfigurations from the RGB branch. Here X can be thermal infrared (T), depth\n(D), event (E), near infrared (NIR), language (L), or sonar (S). The final part\nof the paper addresses evaluation and benchmarking. In summary, we undertake an\nomni survey of all aspects of multi-modal visual object tracking (VOT),\ncovering six MMVOT tasks and featuring 338 references in total. In addition, we\ndiscuss the fundamental rhetorical question: Is multi-modal tracking always\nguaranteed to provide a superior solution to unimodal tracking with the help of\ninformation fusion, and if not, in what circumstances its application is\nbeneficial. Furthermore, for the first time in this field, we analyse the\ndistributions of the object categories in the existing MMVOT datasets,\nrevealing their pronounced long-tail nature and a noticeable lack of animal\ncategories when compared with RGB datasets.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4efd\u5173\u4e8e\u591a\u6a21\u6001\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a(MMVOT)\u7684\u7efc\u8ff0\u6027\u8bba\u6587\uff0c\u4ece\u6570\u636e\u6536\u96c6\u3001\u6a21\u6001\u5bf9\u9f50\u3001\u6a21\u578b\u8bbe\u8ba1\u548c\u8bc4\u4f30\u56db\u4e2a\u5173\u952e\u65b9\u9762\u5168\u9762\u5206\u6790\u4e86\u8be5\u9886\u57df\uff0c\u5305\u542b6\u4e2aMMVOT\u4efb\u52a1\u548c338\u4e2a\u53c2\u8003\u6587\u732e\u3002", "motivation": "\u667a\u6167\u57ce\u5e02\u53d1\u5c55\u4ea7\u751f\u4e86\u5927\u91cf\u591a\u6a21\u6001\u6570\u636e\uff0c\u9700\u8981\u7efc\u5408\u76d1\u63a7\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u548c\u670d\u52a1\u3002\u591a\u6a21\u6001\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u4f5c\u4e3a\u5173\u952e\u4efb\u52a1\uff0c\u9700\u8981\u4ece\u591a\u6a21\u6001\u89d2\u5ea6\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u6790\u548c\u8bc4\u4f30\u3002", "method": "\u8bba\u6587\u91c7\u7528\u7efc\u8ff0\u6027\u7814\u7a76\u65b9\u6cd5\uff0c\u9996\u5148\u4ecb\u7ecd\u76f8\u5173\u6570\u636e\u6a21\u6001\uff0c\u5206\u6790\u591a\u6a21\u6001\u6570\u636e\u6536\u96c6\u3001\u5bf9\u9f50\u548c\u6807\u6ce8\u7684\u6311\u6218\uff0c\u7136\u540e\u57fa\u4e8e\u5904\u7406\u53ef\u89c1\u5149(RGB)\u548c\u5176\u4ed6X\u6a21\u6001\u7684\u4e0d\u540c\u65b9\u5f0f\u5bf9\u73b0\u6709MMVOT\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u8bba\u6587\u5b8c\u6574\u8c03\u67e5\u4e86\u591a\u6a21\u6001\u89c6\u89c9\u8ddf\u8e2a\u7684\u6240\u6709\u65b9\u9762\uff0c\u63ed\u793a\u4e86\u73b0\u6709MMVOT\u6570\u636e\u96c6\u4e2d\u5bf9\u8c61\u7c7b\u522b\u7684\u660e\u663e\u957f\u5c3e\u5206\u5e03\u7279\u5f81\uff0c\u4ee5\u53ca\u4e0eRGB\u6570\u636e\u96c6\u76f8\u6bd4\u52a8\u7269\u7c7b\u522b\u663e\u8457\u7f3a\u4e4f\u7684\u95ee\u9898\u3002", "conclusion": "\u591a\u6a21\u6001\u8ddf\u8e2a\u5e76\u975e\u603b\u662f\u6bd4\u5355\u6a21\u6001\u8ddf\u8e2a\u66f4\u4f18\u79f0\uff0c\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b\u591a\u6a21\u6001\u8ddf\u8e2a\u80fd\u591f\u901a\u8fc7\u4fe1\u606f\u878d\u5408\u63d0\u4f9b\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13005", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13005", "abs": "https://arxiv.org/abs/2508.13005", "authors": ["Jiawen Xu", "Odej Kao"], "title": "Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition and Continual Learning", "comment": null, "summary": "Open set recognition (OSR) and continual learning are two critical challenges\nin machine learning, focusing respectively on detecting novel classes at\ninference time and updating models to incorporate the new classes. While many\nrecent approaches have addressed these problems, particularly OSR, by\nheuristically promoting feature diversity, few studies have directly examined\nthe role that feature diversity plays in tackling them. In this work, we\nprovide empirical evidence that enhancing feature diversity improves the\nrecognition of open set samples. Moreover, increased feature diversity also\nfacilitates both the retention of previously learned data and the integration\nof new data in continual learning. We hope our findings can inspire further\nresearch into both practical methods and theoretical understanding in these\ndomains.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u589e\u5f3a\u7279\u5f81\u591a\u6837\u6027\u53ef\u4ee5\u663e\u8457\u6539\u5584\u5f00\u653e\u96c6\u8bc6\u522b\u548c\u6301\u7eed\u5b66\u4e60\u6027\u80fd\uff0c\u7279\u5f81\u591a\u6837\u6027\u6709\u52a9\u4e8e\u68c0\u6d4b\u65b0\u7c7b\u522b\u548c\u6574\u5408\u65b0\u6570\u636e\u3002", "motivation": "\u5f00\u653e\u96c6\u8bc6\u522b(OSR)\u548c\u6301\u7eed\u5b66\u4e60\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff0c\u867d\u7136\u5df2\u6709\u8bb8\u591a\u65b9\u6cd5\u901a\u8fc7\u542f\u53d1\u5f0f\u65b9\u5f0f\u4fc3\u8fdb\u7279\u5f81\u591a\u6837\u6027\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u76f4\u63a5\u63a2\u8ba8\u7279\u5f81\u591a\u6837\u6027\u5728\u5176\u4e2d\u626e\u6f14\u7684\u89d2\u8272\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u7279\u5f81\u591a\u6837\u6027\u5bf9\u5f00\u653e\u96c6\u8bc6\u522b\u548c\u6301\u7eed\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u63d0\u4f9b\u5b9e\u9a8c\u8bc1\u636e\u6765\u9a8c\u8bc1\u7279\u5f81\u591a\u6837\u6027\u7684\u4f5c\u7528\u3002", "result": "\u589e\u5f3a\u7279\u5f81\u591a\u6837\u6027\u53ef\u4ee5\u6539\u5584\u5f00\u653e\u96c6\u6837\u672c\u7684\u8bc6\u522b\uff0c\u540c\u65f6\u4e5f\u6709\u5229\u4e8e\u6301\u7eed\u5b66\u4e60\u4e2d\u65e7\u77e5\u8bc6\u7684\u4fdd\u6301\u548c\u65b0\u6570\u636e\u7684\u6574\u5408\u3002", "conclusion": "\u7279\u5f81\u591a\u6837\u6027\u5728\u5f00\u653e\u96c6\u8bc6\u522b\u548c\u6301\u7eed\u5b66\u4e60\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u8fd9\u4e00\u53d1\u73b0\u53ef\u4e3a\u8fd9\u4e24\u4e2a\u9886\u57df\u7684\u5b9e\u8df5\u65b9\u6cd5\u548c\u7406\u8bba\u7406\u89e3\u63d0\u4f9b\u65b0\u7684\u7814\u7a76\u7075\u611f\u3002"}}
{"id": "2508.13007", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13007", "abs": "https://arxiv.org/abs/2508.13007", "authors": ["Melih Yazgan", "Qiyuan Wu", "Iramm Hamdard", "Shiqi Li", "J. Marius Zoellner"], "title": "SlimComm: Doppler-Guided Sparse Queries for Bandwidth-Efficient Cooperative 3-D Perception", "comment": "Accepted by ICCV - Drive2X Workshop", "summary": "Collaborative perception allows connected autonomous vehicles (CAVs) to\novercome occlusion and limited sensor range by sharing intermediate features.\nYet transmitting dense Bird's-Eye-View (BEV) feature maps can overwhelm the\nbandwidth available for inter-vehicle communication. We present SlimComm, a\ncommunication-efficient framework that integrates 4D radar Doppler with a\nquery-driven sparse scheme. SlimComm builds a motion-centric dynamic map to\ndistinguish moving from static objects and generates two query types: (i)\nreference queries on dynamic and high-confidence regions, and (ii) exploratory\nqueries probing occluded areas via a two-stage offset. Only query-specific BEV\nfeatures are exchanged and fused through multi-scale gated deformable\nattention, reducing payload while preserving accuracy. For evaluation, we\nrelease OPV2V-R and Adver-City-R, CARLA-based datasets with per-point Doppler\nradar. SlimComm achieves up to 90% lower bandwidth than full-map sharing while\nmatching or surpassing prior baselines across varied traffic densities and\nocclusions. Dataset and code will be available at: https://url.fzi.de/SlimComm.", "AI": {"tldr": "SlimComm\u662f\u4e00\u4e2a\u901a\u4fe1\u9ad8\u6548\u7684\u534f\u4f5c\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc74D\u96f7\u8fbe\u591a\u666e\u52d2\u548c\u67e5\u8be2\u9a71\u52a8\u7a00\u758f\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u51cf\u5c1190%\u5e26\u5bbd\u4f7f\u7528", "motivation": "\u89e3\u51b3\u534f\u4f5c\u611f\u77e5\u4e2d\u5bc6\u96c6BEV\u7279\u5f81\u56fe\u4f20\u8f93\u5bf9\u8f66\u8054\u7f51\u5e26\u5bbd\u7684\u6311\u6218\uff0c\u514b\u670d\u906e\u6321\u548c\u4f20\u611f\u5668\u8303\u56f4\u9650\u5236", "method": "\u6784\u5efa\u8fd0\u52a8\u4e2d\u5fc3\u52a8\u6001\u5730\u56fe\u533a\u5206\u52a8\u9759\u7269\u4f53\uff0c\u751f\u6210\u53c2\u8003\u67e5\u8be2\u548c\u63a2\u7d22\u67e5\u8be2\uff0c\u4ec5\u4ea4\u6362\u67e5\u8be2\u7279\u5b9aBEV\u7279\u5f81\u5e76\u901a\u8fc7\u591a\u5c3a\u5ea6\u95e8\u63a7\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u878d\u5408", "result": "\u5728OPV2V-R\u548cAdver-City-R\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5e26\u5bbd\u964d\u4f4e90%\uff0c\u5728\u4e0d\u540c\u4ea4\u901a\u5bc6\u5ea6\u548c\u906e\u6321\u60c5\u51b5\u4e0b\u6027\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u57fa\u7ebf", "conclusion": "SlimComm\u6210\u529f\u5b9e\u73b0\u4e86\u901a\u4fe1\u6548\u7387\u4e0e\u611f\u77e5\u7cbe\u5ea6\u7684\u5e73\u8861\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2508.13009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13009", "abs": "https://arxiv.org/abs/2508.13009", "authors": ["Xianglong He", "Chunli Peng", "Zexiang Liu", "Boyang Wang", "Yifan Zhang", "Qi Cui", "Fei Kang", "Biao Jiang", "Mengyin An", "Yangyang Ren", "Baixin Xu", "Hao-Xiang Guo", "Kaixiong Gong", "Cyrus Wu", "Wei Li", "Xuchen Song", "Yang Liu", "Eric Li", "Yahui Zhou"], "title": "Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model", "comment": "Project Page: https://matrix-game-v2.github.io", "summary": "Recent advances in interactive video generations have demonstrated diffusion\nmodel's potential as world models by capturing complex physical dynamics and\ninteractive behaviors. However, existing interactive world models depend on\nbidirectional attention and lengthy inference steps, severely limiting\nreal-time performance. Consequently, they are hard to simulate real-world\ndynamics, where outcomes must update instantaneously based on historical\ncontext and current actions. To address this, we present Matrix-Game 2.0, an\ninteractive world model generates long videos on-the-fly via few-step\nauto-regressive diffusion. Our framework consists of three key components: (1)\nA scalable data production pipeline for Unreal Engine and GTA5 environments to\neffectively produce massive amounts (about 1200 hours) of video data with\ndiverse interaction annotations; (2) An action injection module that enables\nframe-level mouse and keyboard inputs as interactive conditions; (3) A few-step\ndistillation based on the casual architecture for real-time and streaming video\ngeneration. Matrix Game 2.0 can generate high-quality minute-level videos\nacross diverse scenes at an ultra-fast speed of 25 FPS. We open-source our\nmodel weights and codebase to advance research in interactive world modeling.", "AI": {"tldr": "Matrix-Game 2.0\u662f\u4e00\u4e2a\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u5c11\u6b65\u81ea\u56de\u5f52\u6269\u6563\u751f\u6210\u9ad8\u8d28\u91cf\u957f\u89c6\u9891\uff0c\u901f\u5ea6\u8fbe\u523025FPS", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\u4f9d\u8d56\u53cc\u5411\u6ce8\u610f\u529b\u548c\u5197\u957f\u63a8\u7406\u6b65\u9aa4\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u6027\u80fd\uff0c\u96be\u4ee5\u6a21\u62df\u9700\u8981\u5373\u65f6\u66f4\u65b0\u7684\u771f\u5b9e\u4e16\u754c\u52a8\u6001", "method": "\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1)\u53ef\u6269\u5c55\u7684Unreal Engine\u548cGTA5\u6570\u636e\u751f\u4ea7\u6d41\u6c34\u7ebf\uff1b2)\u5e27\u7ea7\u9f20\u6807\u952e\u76d8\u8f93\u5165\u7684\u52a8\u4f5c\u6ce8\u5165\u6a21\u5757\uff1b3)\u57fa\u4e8e\u56e0\u679c\u67b6\u6784\u7684\u5c11\u6b65\u84b8\u998f\u5b9e\u65f6\u6d41\u5f0f\u89c6\u9891\u751f\u6210", "result": "\u80fd\u591f\u751f\u6210\u5206\u949f\u7ea7\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u4ee525FPS\u7684\u8d85\u5feb\u901f\u5ea6\u8fd0\u884c", "conclusion": "\u8be5\u6846\u67b6\u63a8\u8fdb\u4e86\u4ea4\u4e92\u5f0f\u4e16\u754c\u5efa\u6a21\u7814\u7a76\uff0c\u5f00\u6e90\u4e86\u6a21\u578b\u6743\u91cd\u548c\u4ee3\u7801\u5e93"}}
{"id": "2508.13013", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13013", "abs": "https://arxiv.org/abs/2508.13013", "authors": ["Jingqiao Xiu", "Fangzhou Hong", "Yicong Li", "Mengze Li", "Wentao Wang", "Sirui Han", "Liang Pan", "Ziwei Liu"], "title": "EgoTwin: Dreaming Body and View in First Person", "comment": null, "summary": "While exocentric video synthesis has achieved great progress, egocentric\nvideo generation remains largely underexplored, which requires modeling\nfirst-person view content along with camera motion patterns induced by the\nwearer's body movements. To bridge this gap, we introduce a novel task of joint\negocentric video and human motion generation, characterized by two key\nchallenges: 1) Viewpoint Alignment: the camera trajectory in the generated\nvideo must accurately align with the head trajectory derived from human motion;\n2) Causal Interplay: the synthesized human motion must causally align with the\nobserved visual dynamics across adjacent video frames. To address these\nchallenges, we propose EgoTwin, a joint video-motion generation framework built\non the diffusion transformer architecture. Specifically, EgoTwin introduces a\nhead-centric motion representation that anchors the human motion to the head\njoint and incorporates a cybernetics-inspired interaction mechanism that\nexplicitly captures the causal interplay between video and motion within\nattention operations. For comprehensive evaluation, we curate a large-scale\nreal-world dataset of synchronized text-video-motion triplets and design novel\nmetrics to assess video-motion consistency. Extensive experiments demonstrate\nthe effectiveness of the EgoTwin framework.", "AI": {"tldr": "\u63d0\u51faEgoTwin\u6846\u67b6\u89e3\u51b3\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u4e0e\u4eba\u4f53\u8fd0\u52a8\u8054\u5408\u751f\u6210\u4efb\u52a1\uff0c\u901a\u8fc7\u5934\u4e2d\u5fc3\u8fd0\u52a8\u8868\u793a\u548c\u7f51\u7edc\u4ea4\u4e92\u673a\u5236\u5b9e\u73b0\u89c6\u89d2\u5bf9\u9f50\u548c\u56e0\u679c\u4ea4\u4e92", "motivation": "\u73b0\u6709\u5916\u4e2d\u5fc3\u89c6\u9891\u5408\u6210\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u751f\u6210\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u540c\u65f6\u5efa\u6a21\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u5185\u5bb9\u548c\u7a7f\u6234\u8005\u8eab\u4f53\u8fd0\u52a8\u5f15\u8d77\u7684\u76f8\u673a\u8fd0\u52a8\u6a21\u5f0f", "method": "\u57fa\u4e8e\u6269\u6563transformer\u67b6\u6784\u6784\u5efaEgoTwin\u6846\u67b6\uff0c\u5f15\u5165\u5934\u4e2d\u5fc3\u8fd0\u52a8\u8868\u793a\u5c06\u4eba\u4f53\u8fd0\u52a8\u951a\u5b9a\u5230\u5934\u90e8\u5173\u8282\uff0c\u5e76\u91c7\u7528\u7f51\u7edc\u4ea4\u4e92\u673a\u5236\u5728\u6ce8\u610f\u529b\u64cd\u4f5c\u4e2d\u663e\u5f0f\u6355\u83b7\u89c6\u9891\u4e0e\u8fd0\u52a8\u95f4\u7684\u56e0\u679c\u4ea4\u4e92", "result": "\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u540c\u6b65\u6587\u672c-\u89c6\u9891-\u8fd0\u52a8\u4e09\u5143\u7ec4\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e86\u65b0\u9896\u6307\u6807\u8bc4\u4f30\u89c6\u9891-\u8fd0\u52a8\u4e00\u81f4\u6027\uff0c\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86EgoTwin\u6846\u67b6\u7684\u6709\u6548\u6027", "conclusion": "EgoTwin\u6210\u529f\u89e3\u51b3\u4e86\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u4e0e\u4eba\u4f53\u8fd0\u52a8\u8054\u5408\u751f\u6210\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u89c6\u89d2\u5bf9\u9f50\u548c\u56e0\u679c\u4ea4\u4e92\uff0c\u4e3a\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.13026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13026", "abs": "https://arxiv.org/abs/2508.13026", "authors": ["Ruru Xu", "Ilkay Oksuz"], "title": "HierAdaptMR: Cross-Center Cardiac MRI Reconstruction with Hierarchical Feature Adapters", "comment": "MICCAI 2025, CMRxRecon2025 Challenge paper", "summary": "Deep learning-based cardiac MRI reconstruction faces significant domain shift\nchallenges when deployed across multiple clinical centers with heterogeneous\nscanner configurations and imaging protocols. We propose HierAdaptMR, a\nhierarchical feature adaptation framework that addresses multi-level domain\nvariations through parameter-efficient adapters. Our method employs\nProtocol-Level Adapters for sequence-specific characteristics and Center-Level\nAdapters for scanner-dependent variations, built upon a variational unrolling\nbackbone. A Universal Adapter enables generalization to entirely unseen centers\nthrough stochastic training that learns center-invariant adaptations. The\nframework utilizes multi-scale SSIM loss with frequency domain enhancement and\ncontrast-adaptive weighting for robust optimization. Comprehensive evaluation\non the CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9\nmodalities demonstrates superior cross-center generalization while maintaining\nreconstruction quality. code: https://github.com/Ruru-Xu/HierAdaptMR", "AI": {"tldr": "HierAdaptMR\u662f\u4e00\u4e2a\u5206\u5c42\u7279\u5f81\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u9002\u914d\u5668\u89e3\u51b3\u591a\u4e2d\u5fc3\u5fc3\u810fMRI\u91cd\u5efa\u4e2d\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5728CMRxRecon2025\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u8de8\u4e2d\u5fc3\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5fc3\u810fMRI\u91cd\u5efa\u5728\u591a\u4e34\u5e8a\u4e2d\u5fc3\u90e8\u7f72\u65f6\u9762\u4e34\u663e\u8457\u7684\u57df\u504f\u79fb\u6311\u6218\uff0c\u4e0d\u540c\u626b\u63cf\u4eea\u914d\u7f6e\u548c\u6210\u50cf\u534f\u8bae\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u9002\u914d\u5668\u6846\u67b6\uff1a\u534f\u8bae\u7ea7\u9002\u914d\u5668\u5904\u7406\u5e8f\u5217\u7279\u5b9a\u7279\u5f81\uff0c\u4e2d\u5fc3\u7ea7\u9002\u914d\u5668\u5904\u7406\u626b\u63cf\u4eea\u76f8\u5173\u53d8\u5316\uff0c\u57fa\u4e8e\u53d8\u5206\u5c55\u5f00\u9aa8\u5e72\u7f51\u7edc\u3002\u901a\u7528\u9002\u914d\u5668\u901a\u8fc7\u968f\u673a\u8bad\u7ec3\u5b9e\u73b0\u672a\u89c1\u4e2d\u5fc3\u7684\u6cdb\u5316\uff0c\u91c7\u7528\u591a\u5c3a\u5ea6SSIM\u635f\u5931\u548c\u9891\u57df\u589e\u5f3a\u4f18\u5316\u3002", "result": "\u5728CMRxRecon2025\u6570\u636e\u96c6\uff085+\u4e2d\u5fc3\u300110+\u626b\u63cf\u4eea\u30019\u79cd\u6a21\u6001\uff09\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u663e\u793a\u51fa\u5353\u8d8a\u7684\u8de8\u4e2d\u5fc3\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "HierAdaptMR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4e2d\u5fc3\u5fc3\u810fMRI\u91cd\u5efa\u7684\u57df\u9002\u5e94\u95ee\u9898\uff0c\u4e3a\u4e34\u5e8a\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13043", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13043", "abs": "https://arxiv.org/abs/2508.13043", "authors": ["Ayaka Yasunaga", "Hideo Saito", "Dieter Schmalstieg", "Shohei Mori"], "title": "IntelliCap: Intelligent Guidance for Consistent View Sampling", "comment": "This work is a pre-print version of a paper that has been accepted to\n  the IEEE International Symposium on Mixed and Augmented Reality for future\n  publication. Project Page:\n  https://mediated-reality.github.io/projects/yasunaga_ismar25/", "summary": "Novel view synthesis from images, for example, with 3D Gaussian splatting,\nhas made great progress. Rendering fidelity and speed are now ready even for\ndemanding virtual reality applications. However, the problem of assisting\nhumans in collecting the input images for these rendering algorithms has\nreceived much less attention. High-quality view synthesis requires uniform and\ndense view sampling. Unfortunately, these requirements are not easily addressed\nby human camera operators, who are in a hurry, impatient, or lack understanding\nof the scene structure and the photographic process. Existing approaches to\nguide humans during image acquisition concentrate on single objects or neglect\nview-dependent material characteristics. We propose a novel situated\nvisualization technique for scanning at multiple scales. During the scanning of\na scene, our method identifies important objects that need extended image\ncoverage to properly represent view-dependent appearance. To this end, we\nleverage semantic segmentation and category identification, ranked by a\nvision-language model. Spherical proxies are generated around highly ranked\nobjects to guide the user during scanning. Our results show superior\nperformance in real scenes compared to conventional view sampling strategies.", "AI": {"tldr": "\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u591a\u5c3a\u5ea6\u626b\u63cf\u6307\u5bfc\u6280\u672f\uff0c\u901a\u8fc7\u8bc6\u522b\u91cd\u8981\u7269\u4f53\u5e76\u751f\u6210\u7403\u5f62\u4ee3\u7406\u6765\u6307\u5bfc\u7528\u6237\u83b7\u53d6\u66f4\u5305\u5bb9\u89c6\u89d2\u53d8\u5316\u7279\u6027\u7684\u56fe\u50cf\u96c6\u3002", "motivation": "\u9ad8\u8d28\u91cf\u65b0\u89c6\u89d2\u5408\u6210\u9700\u8981\u5747\u5300\u5bc6\u96c6\u7684\u89c6\u89d2\u91c7\u6837\uff0c\u4f46\u4eba\u7c7b\u62cd\u6444\u8005\u5e38\u56e0\u7d27\u5feb\u3001\u8010\u5fc3\u4e0d\u8db3\u6216\u4e0d\u7406\u89e3\u573a\u666f\u7ed3\u6784\u800c\u65e0\u6cd5\u6ee1\u8db3\u8981\u6c42\u3002\u73b0\u6709\u65b9\u6cd5\u5bf9\u591a\u7269\u4f53\u573a\u666f\u548c\u89c6\u89d2\u4f9d\u8d56\u6750\u8d28\u7279\u6027\u7684\u652f\u6301\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u8bed\u4e49\u5206\u5272\u548c\u7c7b\u522b\u8bc6\u522b\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8bc4\u5206\u91cd\u8981\u7269\u4f53\u3002\u4e3a\u9ad8\u5206\u7269\u4f53\u751f\u6210\u7403\u5f62\u4ee3\u7406\uff0c\u5728\u626b\u63cf\u8fc7\u7a0b\u4e2d\u6307\u5bfc\u7528\u6237\u83b7\u53d6\u66f4\u5b8c\u6574\u7684\u56fe\u50cf\u8986\u76d6\u3002", "result": "\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u89c6\u89d2\u91c7\u6837\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u7269\u4f53\u573a\u666f\u626b\u63cf\u4e2d\u7684\u89c6\u89d2\u91c7\u6837\u95ee\u9898\uff0c\u63d0\u9ad8\u65b0\u89c6\u89d2\u5408\u6210\u7684\u8d28\u91cf\u3002"}}
{"id": "2508.13065", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13065", "abs": "https://arxiv.org/abs/2508.13065", "authors": ["Siddharth Khandelwal", "Sridhar Kamath", "Arjun Jain"], "title": "Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping", "comment": null, "summary": "Human shape editing enables controllable transformation of a person's body\nshape, such as thin, muscular, or overweight, while preserving pose, identity,\nclothing, and background. Unlike human pose editing, which has advanced\nrapidly, shape editing remains relatively underexplored. Current approaches\ntypically rely on 3D morphable models or image warping, often introducing\nunrealistic body proportions, texture distortions, and background\ninconsistencies due to alignment errors and deformations. A key limitation is\nthe lack of large-scale, publicly available datasets for training and\nevaluating body shape manipulation methods. In this work, we introduce the\nfirst large-scale dataset of 18,573 images across 1523 subjects, specifically\ndesigned for controlled human shape editing. It features diverse variations in\nbody shape, including fat, muscular and thin, captured under consistent\nidentity, clothing, and background conditions. Using this dataset, we propose\nOdo, an end-to-end diffusion-based method that enables realistic and intuitive\nbody reshaping guided by simple semantic attributes. Our approach combines a\nfrozen UNet that preserves fine-grained appearance and background details from\nthe input image with a ControlNet that guides shape transformation using target\nSMPL depth maps. Extensive experiments demonstrate that our method outperforms\nprior approaches, achieving per-vertex reconstruction errors as low as 7.5mm,\nsignificantly lower than the 13.6mm observed in baseline methods, while\nproducing realistic results that accurately match the desired target shapes.", "AI": {"tldr": "\u57fa\u4e8e\u6e29\u7a33UNet\u548cControlNet\u7684\u7aef\u5230\u7aef\u6c1b\u6563\u65b9\u6cd5Odo\uff0c\u901a\u8fc7\u76ee\u6807SMPL\u6df1\u5ea6\u5730\u56fe\u6307\u5bfc\u4eba\u4f53\u5f62\u72b6\u7f16\u8f91\uff0c\u5728\u4fdd\u6301\u5916\u89c2\u548c\u80cc\u666f\u7ec6\u8282\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u8f83\u4f4e\u7684\u91cd\u5efa\u8bef\u5dee\uff087.5mm\uff09\u3002", "motivation": "\u4eba\u4f53\u5f62\u72b6\u7f16\u8f91\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u516c\u5f00\u6570\u636e\u96c6\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8eab\u4f53\u6bd4\u4f8b\u4e0d\u771f\u5b9e\u3001\u7eb9\u7406\u626d\u66f2\u548c\u80cc\u666f\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\uff0c\u9700\u8981\u53d1\u5c55\u66f4\u73b0\u5b9e\u7684\u5f62\u72b6\u7f16\u8f91\u6280\u672f\u3002", "method": "\u6784\u5efa\u5305\u542b18,573\u5f20\u56fe\u7247\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u63d0\u51faOdo\u6a21\u578b\uff1a\u4f7f\u7528\u51bb\u7ed3\u7684UNet\u4fdd\u7559\u5916\u89c2\u548c\u80cc\u666f\u7ec6\u8282\uff0c\u901a\u8fc7ControlNet\u4f7f\u7528\u76ee\u6807SMPL\u6df1\u5ea6\u5730\u56fe\u6307\u5bfc\u5f62\u72b6\u53d8\u6362\uff0c\u652f\u6301\u8bed\u4e49\u5c5e\u6027\u6307\u5bfc\u7684\u76f4\u89c2\u8eab\u4f53\u91cd\u5851\u3002", "result": "\u65b9\u6cd5\u5728\u91cd\u5efa\u8bef\u5dee\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u6bcf\u9876\u70b9\u8bef\u5dee7.5mm vs 13.6mm\uff09\uff0c\u80fd\u591f\u751f\u6210\u73b0\u5b9e\u7684\u7ed3\u679c\u5e76\u51c6\u786e\u5339\u914d\u76ee\u6807\u5f62\u72b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u4eba\u4f53\u5f62\u72b6\u7f16\u8f91\u6570\u636e\u96c6\u548c\u9ad8\u6548\u7684\u6c1b\u6563\u57fa\u65b9\u6cd5Odo\uff0c\u4e3a\u4eba\u4f53\u5f62\u72b6\u64cd\u7eb5\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6491\u3002"}}
{"id": "2508.13068", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13068", "abs": "https://arxiv.org/abs/2508.13068", "authors": ["Tanjim Islam Riju", "Shuchismita Anwar", "Saman Sarker Joy", "Farig Sadeque", "Swakkhar Shatabda"], "title": "Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation", "comment": null, "summary": "We propose a two-stage multimodal framework that enhances disease\nclassification and region-aware radiology report generation from chest X-rays,\nleveraging the MIMIC-Eye dataset. In the first stage, we introduce a\ngaze-guided contrastive learning architecture for disease classification. It\nintegrates visual features, clinical labels, bounding boxes, and radiologist\neye-tracking signals and is equipped with a novel multi-term gaze-attention\nloss combining MSE, KL divergence, correlation, and center-of-mass alignment.\nIncorporating fixations improves F1 score from 0.597 to 0.631 (+5.70%) and AUC\nfrom 0.821 to 0.849 (+3.41%), while also improving precision and recall,\nhighlighting the effectiveness of gaze-informed attention supervision. In the\nsecond stage, we present a modular report generation pipeline that extracts\nconfidence-weighted diagnostic keywords, maps them to anatomical regions using\na curated dictionary constructed from domain-specific priors, and generates\nregion-aligned sentences via structured prompts. This pipeline improves report\nquality as measured by clinical keyword recall and ROUGE overlap. Our results\ndemonstrate that integrating gaze data improves both classification performance\nand the interpretability of generated medical reports.", "AI": {"tldr": "\u4e00\u4e2a\u4e24\u9636\u6bb5\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u7ebf\u5bfc\u5411\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u80ba\u90e8X\u5149\u75be\u75c5\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6a21\u5757\u5316\u62a5\u544a\u751f\u6210\u7ba1\u9053\u63d0\u9ad8\u533b\u5b66\u62a5\u544a\u8d28\u91cf\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5229\u7528\u653b\u4e0b\u5e08\u773c\u52a8\u4fe1\u53f7\u6765\u63d0\u5347\u533b\u5b66\u5f71\u50cf\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u75be\u75c5\u5206\u7c7b\u548c\u62a5\u544a\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\uff1a\u89c6\u7ebf\u5bfc\u5411\u5bf9\u6bd4\u5b66\u4e60\u67b6\u6784\uff0c\u6574\u5408\u89c6\u89c9\u7279\u5f81\u3001\u4e34\u5e8a\u6807\u7b7e\u3001\u76f2\u6846\u548c\u773c\u52a8\u4fe1\u53f7\uff0c\u91c7\u7528\u591a\u9879\u89c6\u7ebf\u6ce8\u610f\u529b\u635f\u5931\u51fd\u6570\u3002\u7b2c\u4e8c\u9636\u6bb5\uff1a\u6a21\u5757\u5316\u62a5\u544a\u751f\u6210\u7ba1\u9053\uff0c\u63d0\u53d6\u4fe1\u5fc3\u5ea6\u52a0\u6743\u8bca\u65ad\u5173\u952e\u8bcd\uff0c\u6620\u5c04\u5230\u89e3\u5256\u533a\u57df\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u751f\u6210\u533a\u57df\u5bf9\u9f50\u53e5\u5b50\u3002", "result": "\u7edf\u8ba1\u5206\u6790\u663e\u793a\uff1a\u7ed3\u5408\u773c\u52a8\u6570\u636e\u540e\uff0cF1\u5206\u6570\u4ece0.597\u63d0\u5347\u52300.631\uff08+5.70%\uff09\uff0cAUC\u4ece0.821\u63d0\u5347\u52300.849\uff08+3.41%\uff09\uff0c\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u90fd\u6709\u663e\u8457\u6539\u5584\u3002\u62a5\u544a\u751f\u6210\u8d28\u91cf\u5728\u4e34\u5e8a\u5173\u952e\u8bcd\u53ec\u56de\u7387\u548cROUGE\u91cd\u5408\u6307\u6807\u4e0a\u90fd\u6709\u63d0\u5347\u3002", "conclusion": "\u96c6\u6210\u773c\u52a8\u6570\u636e\u80fd\u591f\u540c\u65f6\u63d0\u5347\u75be\u75c5\u5206\u7c7b\u6027\u80fd\u548c\u751f\u6210\u533b\u5b66\u62a5\u544a\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8bc1\u660e\u4e86\u89c6\u7ebf\u5bfc\u5411\u6ce8\u610f\u529b\u76d1\u7763\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.13078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13078", "abs": "https://arxiv.org/abs/2508.13078", "authors": ["Qingwen Zeng", "Juan E. Tapia", "Izan Garcia", "Juan M. Espin", "Christoph Busch"], "title": "ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset", "comment": null, "summary": "Nowadays, the development of a Presentation Attack Detection (PAD) system for\nID cards presents a challenge due to the lack of images available to train a\nrobust PAD system and the increase in diversity of possible attack instrument\nspecies. Today, most algorithms focus on generating attack samples and do not\ntake into account the limited number of bona fide images. This work is one of\nthe first to propose a method for mimicking bona fide images by generating\nsynthetic versions of them using Stable Diffusion, which may help improve the\ngeneralisation capabilities of the detector. Furthermore, the new images\ngenerated are evaluated in a system trained from scratch and in a commercial\nsolution. The PAD system yields an interesting result, as it identifies our\nimages as bona fide, which has a positive impact on detection performance and\ndata restrictions.", "AI": {"tldr": "\u4f7f\u7528Stable Diffusion\u751f\u6210\u5408\u6210\u771f\u5b9eID\u5361\u56fe\u50cf\u6765\u89e3\u51b3\u5b9e\u9645\u771f\u5b9e\u6837\u672c\u6570\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u9a8c\u8bc1\u653b\u51fb\u68c0\u6d4b\u7cfb\u7edf\u7684\u6027\u80fd", "motivation": "ID\u5361\u9a8c\u8bc1\u653b\u51fb\u68c0\u6d4b(PAD)\u7cfb\u7edf\u9762\u4e34\u771f\u5b9e\u6837\u672c\u6570\u91cf\u4e0d\u8db3\u548c\u653b\u51fb\u624b\u6bb5\u591a\u6837\u5316\u7684\u6311\u6218\uff0c\u73b0\u6709\u7b97\u6cd5\u591a\u5173\u6ce8\u653b\u51fb\u6837\u672c\u751f\u6210\u800c\u5ffd\u89c6\u4e86\u771f\u5b9e\u6837\u672c\u7684\u9650\u5236", "method": "\u91c7\u7528Stable Diffusion\u751f\u6210\u5668\u751f\u6210\u5408\u6210\u771f\u5b9eID\u5361\u56fe\u50cf\uff0c\u5e76\u5728\u4ece\u5934\u8bad\u7ec3\u7684\u7cfb\u7edf\u548c\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\u4e2d\u8bc4\u4f30\u8fd9\u4e9b\u65b0\u56fe\u50cf", "result": "\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u88abPAD\u7cfb\u7edf\u8bc6\u522b\u4e3a\u771f\u5b9e\u6837\u672c\uff0c\u5bf9\u68c0\u6d4b\u6027\u80fd\u4ea7\u751f\u79ef\u6781\u5f71\u54cd\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6570\u636e\u9650\u5236\u95ee\u9898", "conclusion": "\u901a\u8fc7\u751f\u6210\u5f0fAI\u6280\u672f\u751f\u6210\u5408\u6210\u771f\u5b9e\u6837\u672c\u662f\u63d0\u5347ID\u5361PAD\u7cfb\u7edf\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u89e3\u51b3\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2508.13086", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13086", "abs": "https://arxiv.org/abs/2508.13086", "authors": ["Lucrezia Tosato", "Christel Tartini Chappuis", "Syrielle Montariol", "Flora Weissgerber", "Sylvain Lobry", "Devis Tuia"], "title": "Checkmate: interpretable and explainable RSVQA is the endgame", "comment": null, "summary": "Remote Sensing Visual Question Answering (RSVQA) presents unique challenges\nin ensuring that model decisions are both understandable and grounded in visual\ncontent. Current models often suffer from a lack of interpretability and\nexplainability, as well as from biases in dataset distributions that lead to\nshortcut learning. In this work, we tackle these issues by introducing a novel\nRSVQA dataset, Chessboard, designed to minimize biases through 3'123'253\nquestions and a balanced answer distribution. Each answer is linked to one or\nmore cells within the image, enabling fine-grained visual reasoning.\n  Building on this dataset, we develop an explainable and interpretable model\ncalled Checkmate that identifies the image cells most relevant to its\ndecisions. Through extensive experiments across multiple model architectures,\nwe show that our approach improves transparency and supports more trustworthy\ndecision-making in RSVQA systems.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u7684Chessboard\u6570\u636e\u96c6\u548cCheckmate\u6a21\u578b\uff0c\u89e3\u51b3\u8fdc\u611f\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u504f\u5dee\u95ee\u9898", "motivation": "\u8fdc\u611f\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u3001\u6a21\u578b\u51b3\u7b56\u4e0d\u53ef\u4fe1\u8d56\u4ee5\u53ca\u6570\u636e\u96c6\u504f\u5dee\u5bfc\u81f4\u7684\u77ed\u8def\u5b66\u4e60\u95ee\u9898", "method": "\u521b\u5efa\u5305\u542b312\u4e07\u4e2a\u95ee\u9898\u7684Chessboard\u6570\u636e\u96c6\uff0c\u7b54\u6848\u5206\u5e03\u5747\u8861\u4e14\u4e0e\u56fe\u50cf\u50cf\u7d20\u76f8\u5173\u8054\uff1b\u5f00\u53d1Checkmate\u6a21\u578b\uff0c\u80fd\u591f\u8bc6\u522b\u51b3\u7b56\u7684\u5173\u952e\u56fe\u50cf\u533a\u57df", "result": "\u901a\u8fc7\u591a\u79cd\u6a21\u578b\u67b6\u6784\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7cfb\u7edf\u900f\u660e\u5ea6\u548c\u51b3\u7b56\u53ef\u4fe1\u8d56\u6027", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8fdc\u611f\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u89e3\u91ca\u3001\u66f4\u53ef\u4fe1\u8d56\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u89c6\u89c9\u63a8\u7406\u6539\u5584\u4e86\u6a21\u578b\u7684\u53ef\u7406\u89e3\u6027"}}
{"id": "2508.13091", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13091", "abs": "https://arxiv.org/abs/2508.13091", "authors": ["Zihua Liu", "Yizhou Li", "Songyan Zhang", "Masatoshi Okutomi"], "title": "DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation", "comment": null, "summary": "While supervised stereo matching and monocular depth estimation have advanced\nsignificantly with learning-based algorithms, self-supervised methods using\nstereo images as supervision signals have received relatively less focus and\nrequire further investigation. A primary challenge arises from ambiguity\nintroduced during photometric reconstruction, particularly due to missing\ncorresponding pixels in ill-posed regions of the target view, such as\nocclusions and out-of-frame areas. To address this and establish explicit\nphotometric correspondences, we propose DMS, a model-agnostic approach that\nutilizes geometric priors from diffusion models to synthesize novel views along\nthe epipolar direction, guided by directional prompts. Specifically, we\nfinetune a Stable Diffusion model to simulate perspectives at key positions:\nleft-left view shifted from the left camera, right-right view shifted from the\nright camera, along with an additional novel view between the left and right\ncameras. These synthesized views supplement occluded pixels, enabling explicit\nphotometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play''\nmethod that seamlessly enhances self-supervised stereo matching and monocular\ndepth estimation, and relies solely on unlabeled stereo image pairs for both\ntraining and synthesizing. Extensive experiments demonstrate the effectiveness\nof our approach, with up to 35% outlier reduction and state-of-the-art\nperformance across multiple benchmark datasets.", "AI": {"tldr": "\u63d0\u51faDMS\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u65b0\u89c6\u89d2\u56fe\u50cf\u6765\u89e3\u51b3\u81ea\u76d1\u7763\u7acb\u4f53\u5339\u914d\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u7684\u906e\u6321\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u6570\u636e\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u81ea\u76d1\u7763\u7acb\u4f53\u5339\u914d\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5728\u906e\u6321\u533a\u57df\u5b58\u5728\u5bf9\u5e94\u50cf\u7d20\u7f3a\u5931\u95ee\u9898\uff0c\u5bfc\u81f4\u5149\u5ea6\u91cd\u5efa\u6a21\u7cca\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u6765\u5efa\u7acb\u660e\u786e\u7684\u5149\u5ea6\u5bf9\u5e94\u5173\u7cfb\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u751f\u6210\u6cbf\u6781\u7ebf\u65b9\u5411\u7684\u65b0\u89c6\u89d2\u56fe\u50cf\uff08\u5de6-\u5de6\u89c6\u56fe\u3001\u53f3-\u53f3\u89c6\u56fe\u548c\u4e2d\u95f4\u89c6\u56fe\uff09\uff0c\u8865\u5145\u906e\u6321\u50cf\u7d20\uff0c\u5b9e\u73b0\u660e\u786e\u7684\u5149\u5ea6\u91cd\u5efa\u3002\u8be5\u65b9\u6cd5\u4e0e\u6a21\u578b\u65e0\u5173\uff0c\u4ec5\u9700\u672a\u6807\u6ce8\u7684\u7acb\u4f53\u56fe\u50cf\u5bf9\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5f02\u5e38\u503c\u51cf\u5c11\u9ad8\u8fbe35%\uff0c\u8bc1\u660e\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "DMS\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u514d\u8d39\u65b9\u6cd5\uff0c\u80fd\u65e0\u7f1d\u589e\u5f3a\u81ea\u76d1\u7763\u7acb\u4f53\u5339\u914d\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd\uff0c\u4ec5\u4f9d\u8d56\u672a\u6807\u6ce8\u7acb\u4f53\u56fe\u50cf\u5bf9\u5373\u53ef\u5b9e\u73b0\u8bad\u7ec3\u548c\u5408\u6210\u3002"}}
{"id": "2508.13101", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13101", "abs": "https://arxiv.org/abs/2508.13101", "authors": ["Miftahul Huda", "Arsyiah Azahra", "Putri Maulida Chairani", "Dimas Rizky Ramadhani", "Nabila Azhari", "Ade Lailani"], "title": "Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants", "comment": null, "summary": "Coastal pollution is a pressing global environmental issue, necessitating\nscalable and automated solutions for monitoring and management. This study\ninvestigates the efficacy of the Real-Time Detection Transformer (RT-DETR), a\nstate-of-the-art, end-to-end object detection model, for the automated\ndetection and counting of beach litter. A rigorous comparative analysis is\nconducted between two model variants, RT-DETR-Large (RT-DETR-L) and\nRT-DETR-Extra-Large (RT-DETR-X), trained on a publicly available dataset of\ncoastal debris. The evaluation reveals that the RT-DETR-X model achieves\nmarginally superior accuracy, with a mean Average Precision at 50\\% IoU\n(mAP@50) of 0.816 and a mAP@50-95 of 0.612, compared to the RT-DETR-L model's\n0.810 and 0.606, respectively. However, this minor performance gain is realized\nat a significant computational cost; the RT-DETR-L model demonstrates a\nsubstantially faster inference time of 20.1 ms versus 34.5 ms for the\nRT-DETR-X. The findings suggest that the RT-DETR-L model offers a more\npractical and efficient solution for real-time, in-field deployment due to its\nsuperior balance of processing speed and detection accuracy. This research\nprovides valuable insights into the application of advanced Transformer-based\ndetectors for environmental conservation, highlighting the critical trade-offs\nbetween model complexity and operational viability.", "AI": {"tldr": "RT-DETR-L\u6a21\u578b\u5728\u68c0\u6d4b\u7cbe\u5ea6\u7565\u4f4e\u4e8eRT-DETR-X\u7684\u60c5\u51b5\u4e0b\uff0c\u5177\u6709\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u66f4\u9002\u5408\u5b9e\u65f6\u6d77\u6ee9\u5783\u573e\u68c0\u6d4b\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "motivation": "\u6d77\u5cb8\u6c61\u67d3\u662f\u5168\u7403\u7d27\u8feb\u7684\u73af\u5883\u95ee\u9898\uff0c\u9700\u8981\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u6700\u5148\u8fdb\u7684RT-DETR\u6a21\u578b\u5728\u81ea\u52a8\u5316\u6d77\u6ee9\u5783\u573e\u68c0\u6d4b\u548c\u8ba1\u6570\u4e2d\u7684\u6548\u679c\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u7684\u6d77\u5cb8\u5783\u573e\u6570\u636e\u96c6\uff0c\u5bf9RT-DETR-Large\u548cRT-DETR-Extra-Large\u4e24\u4e2a\u53d8\u4f53\u8fdb\u884c\u4e25\u683c\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u8bc4\u4f30\u5176\u68c0\u6d4b\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u3002", "result": "RT-DETR-X\u6a21\u578b\u83b7\u5f97\u7565\u9ad8\u7684\u7cbe\u5ea6\uff08mAP@50: 0.816, mAP@50-95: 0.612\uff09\uff0c\u4f46RT-DETR-L\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u66f4\u5feb\uff0820.1ms vs 34.5ms\uff09\u3002", "conclusion": "RT-DETR-L\u6a21\u578b\u5728\u5904\u7406\u901f\u5ea6\u548c\u68c0\u6d4b\u7cbe\u5ea6\u4e4b\u95f4\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u66f4\u9002\u5408\u5b9e\u65f6\u73b0\u573a\u90e8\u7f72\uff0c\u4e3a\u57fa\u4e8eTransformer\u7684\u5148\u8fdb\u68c0\u6d4b\u5668\u5728\u73af\u5883\u4fdd\u62a4\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2508.13104", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13104", "abs": "https://arxiv.org/abs/2508.13104", "authors": ["Yuang Wang", "Chao Wen", "Haoyu Guo", "Sida Peng", "Minghan Qin", "Hujun Bao", "Xiaowei Zhou", "Ruizhen Hu"], "title": "Precise Action-to-Video Generation Through Visual Action Prompts", "comment": "Accepted to ICCV 2025. Project page: https://zju3dv.github.io/VAP/", "summary": "We present visual action prompts, a unified action representation for\naction-to-video generation of complex high-DoF interactions while maintaining\ntransferable visual dynamics across domains. Action-driven video generation\nfaces a precision-generality trade-off: existing methods using text, primitive\nactions, or coarse masks offer generality but lack precision, while\nagent-centric action signals provide precision at the cost of cross-domain\ntransferability. To balance action precision and dynamic transferability, we\npropose to \"render\" actions into precise visual prompts as domain-agnostic\nrepresentations that preserve both geometric precision and cross-domain\nadaptability for complex actions; specifically, we choose visual skeletons for\ntheir generality and accessibility. We propose robust pipelines to construct\nskeletons from two interaction-rich data sources - human-object interactions\n(HOI) and dexterous robotic manipulation - enabling cross-domain training of\naction-driven generative models. By integrating visual skeletons into\npretrained video generation models via lightweight fine-tuning, we enable\nprecise action control of complex interaction while preserving the learning of\ncross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the\neffectiveness of our proposed approach. Project page:\nhttps://zju3dv.github.io/VAP/.", "AI": {"tldr": "\u89c6\u89c9\u52a8\u4f5c\u63d0\u793a\uff08VAP\uff09\u901a\u8fc7\u5c06\u52a8\u4f5c\u6e32\u67d3\u4e3a\u7cbe\u786e\u7684\u89c6\u89c9\u9aa8\u67b6\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u52a8\u4f5c\u5230\u89c6\u9891\u751f\u6210\u4e2d\u7684\u7cbe\u5ea6\u4e0e\u901a\u7528\u6027\u4e4b\u95f4\u7684\u4ea4\u6362\u95ee\u9898\uff0c\u652f\u6301\u590d\u6742\u9ad8\u81ea\u7531\u5ea6\u4ea4\u4e92\u52a8\u4f5c\u7684\u7cbe\u786e\u63a7\u5236\u548c\u8de8\u57df\u52a8\u6001\u8f6c\u79fb\u3002", "motivation": "\u73b0\u6709\u7684\u52a8\u4f5c\u9a71\u52a8\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u9047\u5230\u4e86\u7cbe\u5ea6\u4e0e\u901a\u7528\u6027\u7684\u4e24\u96be\u95ee\u9898\uff1a\u6587\u672c\u3001\u539f\u59cb\u52a8\u4f5c\u6216\u7c97\u7cd5\u63a9\u7801\u65b9\u6cd5\u901a\u7528\u6027\u597d\u4f46\u7f3a\u4e4f\u7cbe\u5ea6\uff0c\u800c\u4ee3\u7406\u4e2d\u5fc3\u7684\u52a8\u4f5c\u4fe1\u53f7\u867d\u6709\u7cbe\u5ea6\u4f46\u8de8\u57df\u8f6c\u79fb\u6027\u5dee\u3002\u9700\u8981\u627e\u5230\u4e00\u79cd\u65b9\u6cd5\u6765\u5e73\u8861\u52a8\u4f5c\u7cbe\u5ea6\u548c\u52a8\u6001\u8f6c\u79fb\u6027\u3002", "method": "\u63d0\u51fa\u5c06\u52a8\u4f5c\"\u6e32\u67d3\"\u4e3a\u7cbe\u786e\u7684\u89c6\u89c9\u63d0\u793a\uff08\u89c6\u89c9\u9aa8\u67b6\uff09\u4f5c\u4e3a\u57df\u65e0\u5173\u8868\u793a\uff0c\u4fdd\u6301\u51e0\u4f55\u7cbe\u5ea6\u548c\u8de8\u57df\u9002\u914d\u6027\u3002\u6784\u5efa\u4e86\u4ece\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u548c\u7075\u5de7\u673a\u5668\u4eba\u64cd\u63a7\u6570\u636e\u6e90\u751f\u6210\u9aa8\u67b6\u7684\u7a33\u5065\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5fae\u8c03\u5c06\u89c6\u89c9\u9aa8\u67b6\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u3002", "result": "\u5728EgoVid\u3001RT-1\u548cDROID\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u590d\u6742\u4ea4\u4e92\u52a8\u4f5c\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u8de8\u57df\u52a8\u6001\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "\u89c6\u89c9\u52a8\u4f5c\u63d0\u793a\u4f5c\u4e3a\u7edf\u4e00\u7684\u52a8\u4f5c\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u4f5c\u9a71\u52a8\u89c6\u9891\u751f\u6210\u4e2d\u7684\u7cbe\u5ea6-\u901a\u7528\u6027\u4ea4\u6362\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u9ad8\u81ea\u7531\u5ea6\u4ea4\u4e92\u52a8\u4f5c\u7684\u7cbe\u786e\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13139", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13139", "abs": "https://arxiv.org/abs/2508.13139", "authors": ["Ling-Hao Chen", "Yuhong Zhang", "Zixin Yin", "Zhiyang Dou", "Xin Chen", "Jingbo Wang", "Taku Komura", "Lei Zhang"], "title": "Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence", "comment": "SIGGRAPH Asia 2025", "summary": "This work studies the challenge of transfer animations between characters\nwhose skeletal topologies differ substantially. While many techniques have\nadvanced retargeting techniques in decades, transfer motions across diverse\ntopologies remains less-explored. The primary obstacle lies in the inherent\ntopological inconsistency between source and target skeletons, which restricts\nthe establishment of straightforward one-to-one bone correspondences. Besides,\nthe current lack of large-scale paired motion datasets spanning different\ntopological structures severely constrains the development of data-driven\napproaches. To address these limitations, we introduce Motion2Motion, a novel,\ntraining-free framework. Simply yet effectively, Motion2Motion works with only\none or a few example motions on the target skeleton, by accessing a sparse set\nof bone correspondences between the source and target skeletons. Through\ncomprehensive qualitative and quantitative evaluations, we demonstrate that\nMotion2Motion achieves efficient and reliable performance in both\nsimilar-skeleton and cross-species skeleton transfer scenarios. The practical\nutility of our approach is further evidenced by its successful integration in\ndownstream applications and user interfaces, highlighting its potential for\nindustrial applications. Code and data are available at\nhttps://lhchen.top/Motion2Motion.", "AI": {"tldr": "Motion2Motion\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6e90\u9aa8\u9abc\u548c\u76ee\u6807\u9aa8\u9abc\u4e4b\u95f4\u5efa\u7acb\u7a00\u758f\u9aa8\u9abc\u5bf9\u5e94\u5173\u7cfb\uff0c\u5b9e\u73b0\u4e0d\u540c\u62d3\u6251\u7ed3\u6784\u89d2\u8272\u4e4b\u95f4\u7684\u52a8\u753b\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540c\u9aa8\u9abc\u62d3\u6251\u7ed3\u6784\u89d2\u8272\u95f4\u52a8\u753b\u8fc1\u79fb\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u6e90\u9aa8\u9abc\u548c\u76ee\u6807\u9aa8\u9abc\u62d3\u6251\u4e0d\u4e00\u81f4\u65f6\uff0c\u96be\u4ee5\u5efa\u7acb\u76f4\u63a5\u7684\u4e00\u5bf9\u4e00\u9aa8\u9abc\u5bf9\u5e94\u5173\u7cfb\uff0c\u4e14\u7f3a\u4e4f\u5927\u89c4\u6a21\u914d\u5bf9\u8fd0\u52a8\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u514d\u8d39\u7684Motion2Motion\u6846\u67b6\uff0c\u4ec5\u9700\u76ee\u6807\u9aa8\u9abc\u4e0a\u7684\u4e00\u4e2a\u6216\u51e0\u4e2a\u793a\u4f8b\u8fd0\u52a8\uff0c\u901a\u8fc7\u5efa\u7acb\u6e90\u9aa8\u9abc\u548c\u76ee\u6807\u9aa8\u9abc\u4e4b\u95f4\u7684\u7a00\u758f\u9aa8\u9abc\u5bf9\u5e94\u5173\u7cfb\u6765\u5b9e\u73b0\u52a8\u753b\u8fc1\u79fb\u3002", "result": "\u901a\u8fc7\u5168\u9762\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\uff0c\u8bc1\u660eMotion2Motion\u5728\u76f8\u4f3c\u9aa8\u9abc\u548c\u8de8\u7269\u79cd\u9aa8\u9abc\u8fc1\u79fb\u573a\u666f\u4e2d\u90fd\u80fd\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0b\u6e38\u5e94\u7528\u548c\u7528\u6237\u754c\u9762\u4e2d\u6210\u529f\u96c6\u6210\uff0c\u5c55\u73b0\u4e86\u5de5\u4e1a\u5e94\u7528\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.13142", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13142", "abs": "https://arxiv.org/abs/2508.13142", "authors": ["Zhongang Cai", "Yubo Wang", "Qingping Sun", "Ruisi Wang", "Chenyang Gu", "Wanqi Yin", "Zhiqian Lin", "Zhitao Yang", "Chen Wei", "Xuanke Shi", "Kewang Deng", "Xiaoyang Han", "Zukai Chen", "Jiaqi Li", "Xiangyu Fan", "Hanming Deng", "Lewei Lu", "Bo Li", "Ziwei Liu", "Quan Wang", "Dahua Lin", "Lei Yang"], "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study", "comment": null, "summary": "Multi-modal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, which are fundamental capabilities to achieving\nartificial general intelligence. With the recent release of GPT-5, allegedly\nthe most powerful AI model to date, it is timely to examine where the leading\nmodels stand on the path toward spatial intelligence. First, we propose a\ncomprehensive taxonomy of spatial tasks that unifies existing benchmarks and\ndiscuss the challenges in ensuring fair evaluation. We then evaluate\nstate-of-the-art proprietary and open-source models on eight key benchmarks, at\na cost exceeding one billion total tokens. Our empirical study reveals that (1)\nGPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)\nstill falls short of human performance across a broad spectrum of tasks.\nMoreover, we (3) identify the more challenging spatial intelligence problems\nfor multi-modal models, and (4) proprietary models do not exhibit a decisive\nadvantage when facing the most difficult problems. In addition, we conduct a\nqualitative evaluation across a diverse set of scenarios that are intuitive for\nhumans yet fail even the most advanced multi-modal models.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30GPT-5\u5728\u7a7a\u95f4\u667a\u80fd\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u867d\u7136\u5f3a\u5927\u4f46\u4ecd\u843d\u540e\u4e8e\u4eba\u7c7b\u6c34\u5e73\uff0c\u5e76\u6307\u51fa\u4e13\u6709\u6a21\u578b\u5728\u6781\u96be\u95ee\u9898\u4e0a\u6ca1\u6709\u660e\u663e\u4f18\u52bf", "motivation": "\u591a\u6a21\u6001\u6a21\u578b\u5728\u7a7a\u95f4\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u663e\u8457\u5c40\u9650\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30GPT-5\u7b49\u9886\u5148\u6a21\u578b\u7684\u7a7a\u95f4\u667a\u80fd\u6c34\u5e73", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u7a7a\u95f4\u4efb\u52a1\u5206\u7c7b\u6cd5\uff0c\u57288\u4e2a\u5173\u952e\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u4e13\u6709\u548c\u5f00\u6e90\u6a21\u578b\uff0c\u6d88\u8017\u8d85\u8fc710\u4ebftoken\uff0c\u5e76\u8fdb\u884c\u5b9a\u6027\u5206\u6790", "result": "GPT-5\u5c55\u73b0\u524d\u6240\u672a\u6709\u7684\u7a7a\u95f4\u667a\u80fd\u5b9e\u529b\uff0c\u4f46\u5728\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u4ecd\u843d\u540e\u4e8e\u4eba\u7c7b\uff1b\u4e13\u6709\u6a21\u578b\u5728\u6781\u96be\u95ee\u9898\u4e0a\u6ca1\u6709\u51b3\u5b9a\u6027\u4f18\u52bf\uff1b\u8bc6\u522b\u51fa\u591a\u6a21\u6001\u6a21\u578b\u7684\u5177\u4f53\u6311\u6218\u95ee\u9898", "conclusion": "\u591a\u6a21\u6001\u6a21\u578b\u5728\u7a7a\u95f4\u667a\u80fd\u65b9\u9762\u4ecd\u6709\u663e\u8457\u7f3a\u53e3\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u7f29\u5c0f\u4e0e\u4eba\u7c7b\u80fd\u529b\u7684\u5dee\u8ddd"}}
{"id": "2508.13153", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13153", "abs": "https://arxiv.org/abs/2508.13153", "authors": ["Wenhao Hu", "Zesheng Li", "Haonan Zhou", "Liu Liu", "Xuexiang Wen", "Zhizhong Su", "Xi Li", "Gaoang Wang"], "title": "IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion", "comment": "Project page: https://whhu7.github.io/IGFuse", "summary": "Reconstructing complete and interactive 3D scenes remains a fundamental\nchallenge in computer vision and robotics, particularly due to persistent\nobject occlusions and limited sensor coverage. Multiview observations from a\nsingle scene scan often fail to capture the full structural details. Existing\napproaches typically rely on multi stage pipelines, such as segmentation,\nbackground completion, and inpainting or require per-object dense scanning,\nboth of which are error-prone, and not easily scalable. We propose IGFuse, a\nnovel framework that reconstructs interactive Gaussian scene by fusing\nobservations from multiple scans, where natural object rearrangement between\ncaptures reveal previously occluded regions. Our method constructs segmentation\naware Gaussian fields and enforces bi-directional photometric and semantic\nconsistency across scans. To handle spatial misalignments, we introduce a\npseudo-intermediate scene state for unified alignment, alongside collaborative\nco-pruning strategies to refine geometry. IGFuse enables high fidelity\nrendering and object level scene manipulation without dense observations or\ncomplex pipelines. Extensive experiments validate the framework's strong\ngeneralization to novel scene configurations, demonstrating its effectiveness\nfor real world 3D reconstruction and real-to-simulation transfer. Our project\npage is available online.", "AI": {"tldr": "IGFuse\u662f\u4e00\u4e2a\u65b0\u9896\u76843D\u573a\u666f\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u591a\u89c6\u89d2\u626b\u63cf\u6570\u636e\u6765\u89e3\u51b3\u7269\u4f53\u906e\u6321\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6e32\u67d3\u548c\u7269\u4f53\u7ea7\u573a\u666f\u64cd\u4f5c", "motivation": "\u73b0\u67093D\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u5b58\u5728\u7269\u4f53\u906e\u6321\u3001\u4f20\u611f\u5668\u8986\u76d6\u6709\u9650\u3001\u591a\u9636\u6bb5\u6d41\u7a0b\u590d\u6742\u4e14\u5bb9\u6613\u51fa\u9519\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u91cd\u5efa\u5b8c\u6574\u4ea4\u4e92\u5f0f3D\u573a\u666f", "method": "\u6784\u5efa\u5206\u5272\u611f\u77e5\u7684\u9ad8\u65af\u573a\uff0c\u901a\u8fc7\u53cc\u5411\u5149\u5ea6\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u5f15\u5165\u4f2a\u4e2d\u95f4\u573a\u666f\u72b6\u6001\u8fdb\u884c\u7edf\u4e00\u5bf9\u9f50\uff0c\u91c7\u7528\u534f\u4f5c\u5171\u526a\u679d\u7b56\u7565\u4f18\u5316\u51e0\u4f55\u7ed3\u6784", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5bf9\u65b0\u573a\u666f\u914d\u7f6e\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u771f\u5b9e\u4e16\u754c3D\u91cd\u5efa\u548c\u771f\u5b9e\u5230\u4eff\u771f\u7684\u8f6c\u6362\u4e2d\u8868\u73b0\u51fa\u8272", "conclusion": "IGFuse\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u5bc6\u96c6\u89c2\u6d4b\u6216\u590d\u6742\u6d41\u7a0b\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u573a\u666f\u91cd\u5efa\u548c\u4ea4\u4e92\u64cd\u4f5c\uff0c\u4e3a3D\u89c6\u89c9\u548c\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.13154", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13154", "abs": "https://arxiv.org/abs/2508.13154", "authors": ["Zhaoxi Chen", "Tianqi Liu", "Long Zhuo", "Jiawei Ren", "Zeng Tao", "He Zhu", "Fangzhou Hong", "Liang Pan", "Ziwei Liu"], "title": "4DNeX: Feed-Forward 4D Generative Modeling Made Easy", "comment": "Project Page: https://4dnex.github.io/", "summary": "We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,\ndynamic 3D) scene representations from a single image. In contrast to existing\nmethods that rely on computationally intensive optimization or require\nmulti-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D\ngeneration by fine-tuning a pretrained video diffusion model. Specifically, 1)\nto alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale\ndataset with high-quality 4D annotations generated using advanced\nreconstruction approaches. 2) we introduce a unified 6D video representation\nthat jointly models RGB and XYZ sequences, facilitating structured learning of\nboth appearance and geometry. 3) we propose a set of simple yet effective\nadaptation strategies to repurpose pretrained video diffusion models for 4D\nmodeling. 4DNeX produces high-quality dynamic point clouds that enable\nnovel-view video synthesis. Extensive experiments demonstrate that 4DNeX\noutperforms existing 4D generation methods in efficiency and generalizability,\noffering a scalable solution for image-to-4D modeling and laying the foundation\nfor generative 4D world models that simulate dynamic scene evolution.", "AI": {"tldr": "4DNeX\u662f\u9996\u4e2a\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u62104D\uff08\u52a8\u60013D\uff09\u573a\u666f\u7684\u524d\u9988\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u56fe\u50cf\u52304D\u751f\u6210", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8ba1\u7b97\u5bc6\u96c6\u578b\u4f18\u5316\u6216\u9700\u8981\u591a\u5e27\u89c6\u9891\u8f93\u5165\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u9ad8\u6548\u7684\u56fe\u50cf\u52304D\u751f\u6210\u65b9\u6848", "method": "1)\u6784\u5efa\u5927\u89c4\u6a214D\u6570\u636e\u96c64DNeX-10M\uff1b2)\u5f15\u5165\u7edf\u4e006D\u89c6\u9891\u8868\u793a\u8054\u5408\u5efa\u6a21RGB\u548cXYZ\u5e8f\u5217\uff1b3)\u63d0\u51fa\u9002\u914d\u7b56\u7565\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u91cd\u7528\u4e8e4D\u5efa\u6a21", "result": "\u751f\u6210\u9ad8\u8d28\u91cf\u52a8\u6001\u70b9\u4e91\uff0c\u652f\u6301\u65b0\u89c6\u89d2\u89c6\u9891\u5408\u6210\uff0c\u5728\u6548\u7387\u548c\u6cdb\u5316\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u67094D\u751f\u6210\u65b9\u6cd5", "conclusion": "\u4e3a\u56fe\u50cf\u52304D\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u751f\u6210\u5f0f4D\u4e16\u754c\u6a21\u578b\u6a21\u62df\u52a8\u6001\u573a\u666f\u6f14\u5316\u5960\u5b9a\u4e86\u57fa\u7840"}}
