<div id=toc></div>

# Table of Contents

- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [cs.NE](#cs.NE) [Total: 5]
- [cs.CV](#cs.CV) [Total: 204]
- [cs.AI](#cs.AI) [Total: 55]


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [1] [Numerical Assessment of Advective and Diffusive Dynamics of Interacting and Isolated Prototypical Convectively Initiated Circulations](https://arxiv.org/abs/2512.11828)
*Matthew R. Igel,Joseph A. Biello,Adele L. Igel*

Main category: physics.ao-ph

TL;DR: KRoNUT模型能有效表示对流云的整体环流，包括上升、下沉和水平运动，相比传统羽流模型更准确，能揭示无浮力条件下环流演化的关键机制。


<details>
  <summary>Details</summary>
Motivation: 传统对流模型主要关注上升气流和云内区域，忽略了补偿下沉、云外空气和水平运动等完整环流结构，需要更全面的表示方法来理解对流系统的整体动力学。

Method: 开发KRoNUT（非旋转上升气流环面运动学表示）模型，与高分辨率全物理海洋热带对流模拟结果对比，评估其相对于各种羽流模型的性能，并用于分析孤立和相互作用对流环流的平流扩散动力学。

Result: KRoNUT能准确表示对流环流，垂直平流在云内最重要，但水平环流分量在无浮力条件下对环流演化起关键作用。地表和上升气流核心附近的强曲率导致尺度依赖的强扩散趋势。相互作用环流表现出丰富动力学，包括几何稳定性保持和环流中心聚类。

Conclusion: KRoNUT模型为理解对流系统的完整环流提供了有效工具，揭示了水平环流分量在无浮力条件下的重要性，以及相互作用环流的复杂动力学行为，为改进对流参数化提供了新视角。

Abstract: The bulk circulation associated with convective clouds includes not only a region of updraft and cloudy air but also a region of compensating descent and cloud-free air and horizontal motions coupling these regions. The Kinematic Representation of Non-rotating Updraft Tori (KRoNUT) model is a simple representation of this entire flow. First, the skill of the KRoNUT in representing flows from a high resolution full-physics simulation of marine tropical convection is compared to various plume representations of convection. Then the KRoNUT is used to construct bulk descriptions of the dry dynamics of isolated and interacting convective circulations under the influence of advection and diffusion (only). Cross sections of advective and diffusive tendencies show that while vertical advection of the vertical wind is the most important advective tendency in clouds, the horizontal component of the convective circulation and advection thereof plays a crucial role in the evolution of circulations in the absence of buoyancy. Strong curvature of the flow near the surface and near the updraft core results in locally strong diffusive tendencies that depend on scale. Cross sections of tendencies from the KRoNUT compare favourably to results from the simulation. Interacting circulations are shown to exhibit a wide range of dynamics with some cases of interactions leading to unique stability of geometric properties of otherwise evolving flows and some leading to geometric clustering of circulation centers.

</details>


### [2] [Considerations of Earth climate sensitivity based on peculiarities of planetary heat capacity using system identification method: Runaway greenhouse effect scenario is still possible](https://arxiv.org/abs/2512.12438)
*Alexei V Karnaukhov,Sergei F Lyuksyutov,Artem V Aliakin,Mikhail E Prokhorov,Sergei I Blinnikov*

Main category: physics.ao-ph

TL;DR: 使用系统辨识方法评估地球平衡气候敏感性，发现其范围为2-7°C。分析显示水汽、甲烷和地球反照率的无量纲反馈系数总和可能小于1，但二氧化碳的正反馈效应增加了失控温室效应的概率。


<details>
  <summary>Details</summary>
Motivation: 评估地球平衡气候敏感性（ECS）并分析各种反馈机制对全球变暖的影响，特别是二氧化碳、水汽和甲烷的正反馈效应，以了解失控温室效应的可能性。

Method: 采用系统辨识方法（SIM）评估平衡气候敏感性，基于IPCC6实验数据分析海洋、大气、陆地和冰冻圈的热量库存变化，推导全球表面温度方程并分析各种反馈系数。

Result: 平衡气候敏感性估计在2-7°C之间；水汽、甲烷和地球反照率的反馈系数总和可能小于1；但二氧化碳的正反馈效应显著增加了失控温室效应的概率，尽管仍低于不考虑二氧化碳、水汽和甲烷累积反馈时的临界值。

Conclusion: 虽然水汽、甲烷和反照率的反馈总和可能有限，但二氧化碳的正反馈效应使得失控温室效应的风险显著增加，需要特别关注二氧化碳排放对气候系统稳定性的影响。

Abstract: System identification method (SIM) was used to evaluate the Earth equilibrium climate sensitivity. According to our simulations, the equilibrium climate sensitivity was found to be between 2 deg C and 7 deg C. Analysis of the changes in heat inventory of oceans, atmosphere, land, and cryosphere was based on the experimental data of IPCC6.
  The equation derived for Earth's global surface temperature (GST) shows that the sum of the dimensionless feedback coefficients from water vapor, methane, and Earth albedo could be less than 1. However, due to the positive feedback from carbon dioxide (the combined greenhouse catastrophe) and the revised equilibrium climate sensitivity estimate based on an increase in equilibrium climate sensitivity, the probability of the runaway greenhouse effect increases significantly. It is still less than the critical number when not considering the feedback associated with carbon dioxide, water vapor, and methane buildup in Earth's atmosphere.
  The analysis considers the thermal dynamics of the oceans and other factors, including the exponential growth of the Earth's global temperature based on IPCC6 data.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [3] [Spiking Manifesto](https://arxiv.org/abs/2512.11843)
*Eugene Izhikevich*

Main category: cs.NE

TL;DR: 该论文提出将传统AI模型转换为脉冲神经网络架构，以实现千倍能效提升


<details>
  <summary>Details</summary>
Motivation: 当前AI模型（人工神经网络）的能效比大脑低1000倍，而大脑使用脉冲神经网络且无需矩阵乘法，具有显著能效优势

Method: 提出一个框架，将流行AI模型用脉冲网络和多时间同步化概念重新解释，将脉冲活动视为自然实现的查找表

Result: 提供了一种将AI模型转换为新型架构的方法，承诺实现千倍能效改进，并在GitHub上提供代码

Conclusion: 通过将传统AI模型转换为脉冲神经网络架构，可以大幅提升计算能效，接近大脑的效率水平

Abstract: Practically everything computers do is better, faster, and more power-efficient than the brain. For example, a calculator crunches numbers more energy-efficiently than any human. Yet AI models are a thousand times less efficient than the brain. These models use artificial neural networks (ANNs) and require GPUs for the multiplication of huge matrices. In contrast, spiking neural networks (SNNs) of the brain have no matrix multiplication and much smaller energy requirements. This manifesto proposes a framework for thinking about popular AI models in terms of spiking networks and polychronization, and for interpreting spiking activity as nature's way of implementing look-up tables. This offers a way to convert AI models into a novel type of architecture with the promise of a thousandfold improvement in efficiency. Code is available at https://github.com/izhikevich/SNN

</details>


### [4] [Evolving Deep Learning Optimizers](https://arxiv.org/abs/2512.11853)
*Mitchell Marfinetz*

Main category: cs.NE

TL;DR: 使用遗传算法自动发现深度学习优化器，通过进化搜索发现的新优化器在多个视觉任务上优于Adam，性能提升2.6%


<details>
  <summary>Details</summary>
Motivation: 传统深度学习优化器（如Adam）是人工设计的，可能存在局限性。希望通过进化算法自动发现更优的优化算法，并揭示不同于人工设计的新设计原则。

Method: 将优化器编码为基因组，包含原始更新项（梯度、动量、RMS归一化、Adam风格自适应项、符号更新）的组合、超参数和调度选项。通过50代进化搜索，每代50个个体，在多个视觉任务上进行评估。

Result: 发现的新优化器在综合适应度上比Adam提升2.6%，在CIFAR-10上相对提升7.7%。该优化器结合符号梯度项和自适应矩估计，使用比Adam更低的动量系数（β₁=0.86，β₂=0.94），禁用偏差校正，启用学习率预热和余弦衰减。

Conclusion: 进化搜索能够发现具有竞争力的优化算法，揭示出与人工设计优化器不同的设计原则，为深度学习优化器的自动设计提供了新途径。

Abstract: We present a genetic algorithm framework for automatically discovering deep learning optimization algorithms. Our approach encodes optimizers as genomes that specify combinations of primitive update terms (gradient, momentum, RMS normalization, Adam-style adaptive terms, and sign-based updates) along with hyperparameters and scheduling options. Through evolutionary search over 50 generations with a population of 50 individuals, evaluated across multiple vision tasks, we discover an evolved optimizer that outperforms Adam by 2.6% in aggregate fitness and achieves a 7.7% relative improvement on CIFAR-10. The evolved optimizer combines sign-based gradient terms with adaptive moment estimation, uses lower momentum coefficients than Adam ($β_1$=0.86, $β_2$=0.94), and notably disables bias correction while enabling learning rate warmup and cosine decay. Our results demonstrate that evolutionary search can discover competitive optimization algorithms and reveal design principles that differ from hand-crafted optimizers. Code is available at https://github.com/mmarfinetz/evo-optimizer.

</details>


### [5] [Self-Motivated Growing Neural Network for Adaptive Architecture via Local Structural Plasticity](https://arxiv.org/abs/2512.12713)
*Yiyang Jia,Chengxu Zhou*

Main category: cs.NE

TL;DR: SMGrNN是一种自激励生长神经网络控制器，通过局部结构可塑性模块在线演化网络拓扑，无需手动调整架构即可调节网络容量，在控制任务中表现优于传统多层感知机。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习中的控制策略通常使用固定容量的多层感知机，通过反向传播训练，缺乏结构可塑性且依赖全局误差信号。这限制了网络的自适应能力和学习效率。

Method: 提出自激励生长神经网络(SMGrNN)，包含结构可塑性模块(SPM)。SPM监控神经元激活和边权重更新统计信息，基于短期时间窗口触发神经元插入和剪枝操作，同时使用标准梯度优化器更新突触权重。

Result: 在控制基准测试中，SMGrNN通过策略蒸馏实现与多层感知机基线相似或更高的回报、更低的方差，并自动学习到适合任务的网络规模。消融研究表明自适应拓扑能提高奖励稳定性。

Conclusion: SMGrNN的局部模块化设计支持未来集成Hebbian可塑性和脉冲时序依赖可塑性模块，使其能够支持基于局部规则的人工和脉冲神经网络实现，为自适应控制策略提供新途径。

Abstract: Control policies in deep reinforcement learning are often implemented with fixed-capacity multilayer perceptrons trained by backpropagation, which lack structural plasticity and depend on global error signals. This paper introduces the Self-Motivated Growing Neural Network (SMGrNN), a controller whose topology evolves online through a local Structural Plasticity Module (SPM). The SPM monitors neuron activations and edge-wise weight update statistics over short temporal windows and uses these signals to trigger neuron insertion and pruning, while synaptic weights are updated by a standard gradient-based optimizer. This allows network capacity to be regulated during learning without manual architectural tuning.
  SMGrNN is evaluated on control benchmarks via policy distillation. Compared with multilayer perceptron baselines, it achieves similar or higher returns, lower variance, and task-appropriate network sizes. Ablation studies with growth disabled and growth-only variants isolate the role of structural plasticity, showing that adaptive topology improves reward stability. The local and modular design of SPM enables future integration of a Hebbian plasticity module and spike-timing-dependent plasticity, so that SMGrNN can support both artificial and spiking neural implementations driven by local rules.

</details>


### [6] [OPAL: Operator-Programmed Algorithms for Landscape-Aware Black-Box Optimization](https://arxiv.org/abs/2512.12809)
*Junbo Jacob Lian,Mingyang Yu,Kaichen Ouyang,Shengwei Fu,Rui Zhong,Yujun Zhang,Jun Zhang,Huiling Chen*

Main category: cs.NE

TL;DR: OPAL是一个基于算子编程的黑盒优化框架，通过少量设计预算探测问题景观，使用图神经网络编码轨迹，元学习器生成算子调度策略，在CEC 2017测试集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统进化算法和群智能算法的性能高度依赖于具体问题，作者希望开发一种能够针对每个问题实例自适应学习优化策略的框架，超越基于隐喻的启发式算法。

Method: OPAL框架：1) 使用少量设计预算和差分进化基线探测问题景观；2) 构建采样点的k近邻图；3) 用图神经网络编码轨迹；4) 元学习器将表示映射为分阶段的探索、重启和局部搜索算子调度策略。

Result: 在CEC 2017测试套件上，单个元训练的OPAL策略与最先进的自适应差分进化变体统计相当，在非参数测试中显著优于简单基线。消融研究验证了设计阶段、轨迹图和算子编程表示的选择。

Conclusion: 算子编程、景观感知的按实例设计是超越临时性隐喻算法的一种实用方法，元组件仅增加适度的计算开销，为黑盒优化提供了新方向。

Abstract: Black-box optimization often relies on evolutionary and swarm algorithms whose performance is highly problem dependent. We view an optimizer as a short program over a small vocabulary of search operators and learn this operator program separately for each problem instance. We instantiate this idea in Operator-Programmed Algorithms (OPAL), a landscape-aware framework for continuous black-box optimization that uses a small design budget with a standard differential evolution baseline to probe the landscape, builds a $k$-nearest neighbor graph over sampled points, and encodes this trajectory with a graph neural network. A meta-learner then maps the resulting representation to a phase-wise schedule of exploration, restart, and local search operators. On the CEC~2017 test suite, a single meta-trained OPAL policy is statistically competitive with state-of-the-art adaptive differential evolution variants and achieves significant improvements over simpler baselines under nonparametric tests. Ablation studies on CEC~2017 justify the choices for the design phase, the trajectory graph, and the operator-program representation, while the meta-components add only modest wall-clock overhead. Overall, the results indicate that operator-programmed, landscape-aware per-instance design is a practical way forward beyond ad hoc metaphor-based algorithms in black-box optimization.

</details>


### [7] [Reproducing and Dissecting Denoising Language Models for Speech Recognition](https://arxiv.org/abs/2512.13576)
*Dorian Koch,Albert Zeyer,Nick Rossenbach,Ralf Schlüter,Hermann Ney*

Main category: cs.NE

TL;DR: DLMs在ASR中优于传统语言模型，但需要达到计算临界点；提出DLM-sum解码方法，性能优于DSR解码


<details>
  <summary>Details</summary>
Motivation: DLMs在ASR中具有利用双向上下文和适应ASR错误模式的能力，但其复杂训练流程阻碍了广泛研究，需要进行系统实证研究

Method: 构建可复现的完整流程，系统研究关键设计选择的影响；评估数十种配置，包括数据增强技术、TTS系统和解码策略；提出DLM-sum方法，从多个ASR假设进行解码

Result: DLMs在达到计算临界点后优于传统LMs；传统LMs在较低预算下更高效，但DLMs在长时间训练下扩展性更好；DLM-sum方法一致优于先前提出的DSR解码方法

Conclusion: DLMs性能提升的关键是条件化于ASR假设空间的更丰富信息，而非单一最佳猜测；研究为社区理解、改进和构建这类模型提供了重要基础

Abstract: Denoising language models (DLMs) have been proposed as a powerful alternative to traditional language models (LMs) for automatic speech recognition (ASR), motivated by their ability to use bidirectional context and adapt to a specific ASR model's error patterns. However, the complexity of the DLM training pipeline has hindered wider investigation. This paper presents the first independent, large-scale empirical study of DLMs. We build and release a complete, reproducible pipeline to systematically investigate the impact of key design choices. We evaluate dozens of configurations across multiple axes, including various data augmentation techniques (e.g., SpecAugment, dropout, mixup), different text-to-speech systems, and multiple decoding strategies. Our comparative analysis in a common subword vocabulary setting demonstrates that DLMs outperform traditional LMs, but only after a distinct compute tipping point. While LMs are more efficient at lower budgets, DLMs scale better with longer training, mirroring behaviors observed in diffusion language models. However, we observe smaller improvements than those reported in prior character-based work, which indicates that the DLM's performance is conditional on factors such as the vocabulary. Our analysis reveals that a key factor for improving performance is to condition the DLM on richer information from the ASR's hypothesis space, rather than just a single best guess. To this end, we introduce DLM-sum, a novel method for decoding from multiple ASR hypotheses, which consistently outperforms the previously proposed DSR decoding method. We believe our findings and public pipeline provide a crucial foundation for the community to better understand, improve, and build upon this promising class of models. The code is publicly available at https://github.com/rwth-i6/2025-denoising-lm/.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation](https://arxiv.org/abs/2512.11865)
*Ju-Young Kim,Ji-Hong Park,Myeongjun Kim,Gun-Woo Kim*

Main category: cs.CV

TL;DR: 提出基于OpenVLA-OFT框架的可解释对抗鲁棒视觉-语言-动作模型，通过Evidence-3模块检测光度扰动并生成自然语言解释，提升智能农业系统在对抗条件下的动作预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 智能农业中依赖RGB摄像头和机器人操纵器的系统容易受到色调、光照、噪声等光度扰动的对抗攻击，导致系统故障。需要增强系统在对抗条件下的鲁棒性和可解释性。

Method: 基于OpenVLA-OFT框架构建可解释对抗鲁棒视觉-语言-动作模型，集成Evidence-3模块检测光度扰动，并生成自然语言解释说明扰动的原因和影响。

Result: 相比基线模型，当前动作L1损失降低21.7%，下一动作L1损失降低18.4%，表明在对抗条件下动作预测准确性和可解释性得到显著提升。

Conclusion: 提出的可解释对抗鲁棒视觉-语言-动作模型能有效应对智能农业系统中的光度扰动对抗攻击，提高系统鲁棒性和可解释性，为智能农业的可靠部署提供支持。

Abstract: Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.

</details>


### [9] [MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater](https://arxiv.org/abs/2512.12142)
*Björn Lütjens,Patrick Alexander,Raf Antwerpen,Til Widmann,Guido Cervone,Marco Tedesco*

Main category: cs.CV

TL;DR: 开发深度学习模型融合多源遥感数据，生成格陵兰冰盖每日100米分辨率融水分布图，相比传统方法精度显著提升


<details>
  <summary>Details</summary>
Motivation: 格陵兰冰盖加速融化过程尚未完全理解且难以测量，现有融水分布图无法同时具备高时空分辨率，需要开发新方法解决这一局限性

Method: 使用深度学习模型融合区域气候模型(RCM)、合成孔径雷达(SAR)、被动微波(PMW)和数字高程模型(DEM)数据，对Helheim冰川2017-2023年数据进行时空降尺度处理，以SAR数据为"地面真值"

Result: 深度学习融合方法在测试区域达到95%准确率，显著优于仅使用区域气候模型(83%)或被动微波观测(72%)的传统方法；同时发布了MeltwaterBench基准数据集

Conclusion: 深度学习融合多源数据的方法能够生成高时空分辨率的融水分布图，为理解冰盖融化过程提供重要工具，并建立了公开基准促进后续研究

Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.

</details>


### [10] [Temporal-Anchor3DLane: Enhanced 3D Lane Detection with Multi-Task Losses and LSTM Fusion](https://arxiv.org/abs/2512.11869)
*D. Shainu Suhas,G. Rahul,K. Muni*

Main category: cs.CV

TL;DR: Temporal-Anchor3DLane通过改进损失函数和加入轻量级时序融合模块，显著提升了单目3D车道线检测的精度和时序稳定性


<details>
  <summary>Details</summary>
Motivation: 单目3D车道线检测面临深度模糊、遮挡和时序不稳定等挑战。现有的Anchor3DLane方法虽然表现不错，但仍存在对回归异常值敏感、全局曲线几何监督弱、多损失项平衡困难以及时序连续性利用有限等问题

Method: 提出Temporal-Anchor3DLane框架，包含三个关键改进：(1) 多任务损失改进：平衡L1回归、Chamfer点集距离、基于不确定性的损失加权，以及分类和可见性的focal和Dice组件；(2) 轻量级时序LSTM融合模块：跨帧聚合每个锚点的特征，替代较重的Transformer风格时序融合；(3) ESCOP风格训练优化：将曲线级监督与时序一致性相结合

Result: 在OpenLane数据集上，Temporal-Anchor3DLane将F1分数提升了+6.2，并产生了更平滑的时序轨迹。这表明小的架构和损失改进能显著增强3D车道线鲁棒性，无需额外传感器或扩展规模

Conclusion: 通过精心设计的损失函数改进和轻量级时序融合，Temporal-Anchor3DLane显著提升了单目3D车道线检测的性能和时序稳定性，证明了在现有框架上进行小规模优化的有效性

Abstract: Monocular 3D lane detection remains challenging due to depth ambiguity, occlusion, and temporal instability across frames. Anchor-based approaches such as Anchor3DLane have demonstrated strong performance by regressing continuous 3D lane curves from multi-camera surround views. However, the baseline model still exhibits (i) sensitivity to regression outliers, (ii) weak supervision of global curve geometry, (iii) difficulty in balancing multiple loss terms, and (iv) limited exploitation of temporal continuity. We propose Temporal-Anchor3DLane, an enhanced 3D lane detection framework that extends Anchor3DLane with three key contributions: (1) a set of multi-task loss improvements, including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, together with focal and Dice components for classification and visibility; (2) a lightweight Temporal LSTM Fusion module that aggregates per-anchor features across frames, replacing a heavier Transformer-style temporal fusion; and (3) ESCOP-style training refinements that couple curve-level supervision with temporal consistency. On OpenLane, Temporal-Anchor3DLane improves F1 by +6.2 and yields smoother temporal trajectories, showing that small architectural and loss refinements significantly enhance 3D lane robustness without extra sensors or scaling.

</details>


### [11] [Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops](https://arxiv.org/abs/2512.11871)
*Tekleab G. Gebremedhin,Hailom S. Asegede,Bruh W. Tesheme,Tadesse B. Gebremichael,Kalayu G. Redae*

Main category: cs.CV

TL;DR: 研究人员为埃塞俄比亚提格雷地区开发了一个离线作物病害检测系统，专注于本地仙人掌无花果，使用三种移动高效架构进行基准测试，并在本地化Flutter应用中部署。


<details>
  <summary>Details</summary>
Motivation: 提格雷地区80%以上人口依赖农业，但基础设施中断限制了专家作物病害诊断的获取。需要为冲突后边缘环境开发离线检测系统。

Method: 创建了包含3,587张田间图像的本地仙人掌无花果数据集。在部署约束下，基准测试了三种移动高效架构：自定义轻量CNN、EfficientNet-Lite1和CNN-Transformer混合模型MobileViT-XS。

Result: EfficientNet-Lite1达到90.7%测试准确率，轻量CNN达到89.5%且部署最优（42ms推理延迟，4.8MB模型大小），MobileViT-XS达到97.3%平均交叉验证准确率，显示MHSA全局推理优于局部纹理CNN。

Conclusion: ARM兼容模型在本地化Flutter应用中部署，支持Cortex-A53设备完全离线推理，增强了粮食安全关键诊断的包容性。MobileViT-XS的全局注意力机制能更好地区分害虫集群和真菌病变。

Abstract: Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.

</details>


### [12] [Pseudo-Label Refinement for Robust Wheat Head Segmentation via Two-Stage Hybrid Training](https://arxiv.org/abs/2512.11874)
*Jiahao Jiang,Zhangrui Yang,Xuanhan Wang,Jingkuan Song*

Main category: cs.CV

TL;DR: 提出一个用于小麦语义分割竞赛的自训练框架，结合两阶段混合训练策略与数据增强，使用SegFormer模型，通过师生迭代循环提升精度


<details>
  <summary>Details</summary>
Motivation: 解决Global Wheat Full Semantic Segmentation Competition中的小麦语义分割问题，需要高效利用有限数据并提升模型性能

Method: 采用系统化的自训练框架，结合两阶段混合训练策略和广泛的数据增强，核心模型为SegFormer（MiT-B4骨干网络），通过迭代的师生循环逐步优化模型精度

Result: 在开发阶段和测试阶段数据集上都取得了有竞争力的性能表现

Conclusion: 提出的自训练框架结合混合训练策略和数据增强，能够有效提升小麦语义分割任务的性能，在竞赛中表现优异

Abstract: This extended abstract details our solution for the Global Wheat Full Semantic Segmentation Competition. We developed a systematic self-training framework. This framework combines a two-stage hybrid training strategy with extensive data augmentation. Our core model is SegFormer with a Mix Transformer (MiT-B4) backbone. We employ an iterative teacher-student loop. This loop progressively refines model accuracy. It also maximizes data utilization. Our method achieved competitive performance. This was evident on both the Development and Testing Phase datasets.

</details>


### [13] [Generalization vs. Specialization: Evaluating Segment Anything Model (SAM3) Zero-Shot Segmentation Against Fine-Tuned YOLO Detectors](https://arxiv.org/abs/2512.11884)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.CV

TL;DR: SAM3零样本分割与YOLO11微调模型在密集苹果实例分割任务上的对比研究，发现YOLO在检测完整性上更优，而SAM3在边界稳定性上表现更佳


<details>
  <summary>Details</summary>
Motivation: 比较专门化微调模型（YOLO）与通用基础模型（SAM3）在密集实例分割任务上的性能差异，为实际应用提供选择指导

Method: 在MinneApple数据集（670张果园图像，28,179个苹果实例）上评估SAM3零样本模式与YOLO11三个变体（nano、medium、large）的微调模型，分析不同IoU阈值下的性能表现

Result: 在IoU=0.15时，YOLO模型F1分数为68.9%-72.2%，SAM3为59.8%；但YOLO在IoU变化时性能下降48-50点，而SAM3仅下降4点，显示SAM3边界稳定性是YOLO的12倍

Conclusion: SAM3在掩码精度和边界稳定性方面表现优异，而YOLO在检测完整性方面更专业；研究提供了何时选择专门化模型或通用模型的指导，并开源了代码和评估流程

Abstract: Deep learning has advanced two fundamentally different paradigms for instance segmentation: specialized models optimized through task-specific fine-tuning and generalist foundation models capable of zero-shot segmentation. This work presents a comprehensive comparison between SAM3 (Segment Anything Model, also called SAMv3) operating in zero-shot mode and three variants of Ultralytics YOLO11 (nano, medium, and large) fine-tuned for instance segmentation. The evaluation is conducted on the MinneApple dataset, a dense benchmark comprising 670 orchard images with 28,179 annotated apple instances, enabling rigorous validation of model behavior under high object density and occlusion. Our analysis shows IoU choices can inflate performance gaps by up to 30%. At the appropriate IoU = 0.15 threshold, YOLO models achieve 68.9%, 72.2%, and 71.9% F1, while SAM3 reaches 59.8% in pure zero-shot mode. However, YOLO exhibits steep degradation 48-50 points across IoU ranges whereas SAM3 drops only 4 points, revealing 12 times superior boundary stability of SAM3. This highlights the strength of SAMv3 in mask precision versus specialization in detection completeness of YOLO11. We provide open-source code, evaluation pipelines, and methodological recommendations, contributing to a deeper understanding of when specialized fine-tuned models or generalist foundation models are preferable for dense instance segmentation tasks. This project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors

</details>


### [14] [mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description](https://arxiv.org/abs/2512.11894)
*Mahathir Monjur,Shahriar Nirjon*

Main category: cs.CV

TL;DR: mmWeaver：基于隐式神经表示和超网络的毫米波雷达信号生成框架，通过环境上下文和人体运动特征合成真实信号，实现高效数据增强


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达信号在活动识别和姿态估计等应用中需要多样化的环境特定数据集，但物理模拟计算成本高，信号复杂稀疏且高维，需要高效的数据增强方法

Method: 使用隐式神经表示将毫米波信号建模为连续函数，通过超网络根据RGB-D图像提取的环境上下文和MotionGPT文本到姿态生成的运动特征动态生成INR参数，实现多分辨率I/Q信号合成

Result: 达到49倍压缩比，复杂SSIM 0.88，PSNR 35 dB，活动识别准确率提升7%，人体姿态估计误差降低15%，比模拟方法快6-35倍

Conclusion: mmWeaver通过结合语义和几何先验，实现了高效、真实的毫米波信号合成，为雷达应用提供了有效的数据增强解决方案

Abstract: Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.

</details>


### [15] [Hot Hém: Sài Gòn Giũa Cái Nóng Hông Còng Bàng -- Saigon in Unequal Heat](https://arxiv.org/abs/2512.11896)
*Tessa Vu*

Main category: cs.CV

TL;DR: Hot Hém是一个GeoAI工作流，结合Google街景、语义分割和遥感数据，为胡志明市行人网络预测地表温度并实现热感知路径规划


<details>
  <summary>Details</summary>
Motivation: 热带高密度城市中行人热暴露是重要的健康风险，但标准路径规划算法往往忽略微观尺度的热变化

Method: 结合Google街景图像、语义图像分割和遥感技术，训练两个XGBoost模型预测地表温度，并在OSMnx生成的行人网络节点上部署，实现热感知路径规划

Result: 开发了一个空间数据科学流程，能够识别城市走廊中温度异常高的区域，为热感知路径规划提供基础

Conclusion: Hot Hém工作流为理解城市基础设施尺度上热暴露的空间分布提供了基础，有助于识别温度异常高的城市走廊并探究其原因

Abstract: Pedestrian heat exposure is a critical health risk in dense tropical cities, yet standard routing algorithms often ignore micro-scale thermal variation. Hot Hém is a GeoAI workflow that estimates and operationalizes pedestrian heat exposure in Hô Chí Minh City (HCMC), Vi\d{e}t Nam, colloquially known as Sài Gòn. This spatial data science pipeline combines Google Street View (GSV) imagery, semantic image segmentation, and remote sensing. Two XGBoost models are trained to predict land surface temperature (LST) using a GSV training dataset in selected administrative wards, known as phŏng, and are deployed in a patchwork manner across all OSMnx-derived pedestrian network nodes to enable heat-aware routing. This is a model that, when deployed, can provide a foundation for pinpointing where and further understanding why certain city corridors may experience disproportionately higher temperatures at an infrastructural scale.

</details>


### [16] [Microscopic Vehicle Trajectory Datasets from UAV-collected Video for Heterogeneous, Area-Based Urban Traffic](https://arxiv.org/abs/2512.11898)
*Yawar Ali,K. Ramachandra Rao,Ashish Bhaskar,Niladri Chatterjee*

Main category: cs.CV

TL;DR: 该论文提供了使用无人机在异构、区域型城市交通条件下收集的开放微观车辆轨迹数据集，包含时间戳位置、速度、加速度和车辆分类信息。


<details>
  <summary>Details</summary>
Motivation: 传统路边视频采集在密集混合交通中常因遮挡、视角有限和车辆不规则运动而失败，需要无人机俯拍视角来减少这些问题并捕捉丰富的时空动态。

Method: 使用无人机和Data from Sky平台在印度国家首都区的六个路段位置收集数据，以30帧/秒分辨率提取车辆位置、速度、加速度和分类信息，并通过人工计数、空间平均速度和探测轨迹进行验证。

Result: 数据集包含时间戳车辆位置、速度、纵向和横向加速度及车辆分类，覆盖不同交通组成和密度水平，分析揭示了车道保持偏好、速度分布和横向操纵等行为模式。

Conclusion: 这些开放数据集为全球研究社区提供了资源，支持区域型交通条件下的仿真建模、安全评估和行为研究，为准确表示复杂城市交通环境提供了独特机会。

Abstract: This paper offers openly available microscopic vehicle trajectory (MVT) datasets collected using unmanned aerial vehicles (UAVs) in heterogeneous, area-based urban traffic conditions. Traditional roadside video collection often fails in dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. UAV-based recording provides a top-down perspective that reduces these issues and captures rich spatial and temporal dynamics. The datasets described here were extracted using the Data from Sky (DFS) platform and validated against manual counts, space mean speeds, and probe trajectories in earlier work. Each dataset contains time-stamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a resolution of 30 frames per second. Data were collected at six mid-block locations in the national capital region of India, covering diverse traffic compositions and density levels. Exploratory analyses highlight key behavioural patterns, including lane-keeping preferences, speed distributions, and lateral manoeuvres typical of heterogeneous and area-based traffic settings. These datasets are intended as a resource for the global research community to support simulation modelling, safety assessment, and behavioural studies under area-based traffic conditions. By making these empirical datasets openly available, this work offers researchers a unique opportunity to develop, test, and validate models that more accurately represent complex urban traffic environments.

</details>


### [17] [Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models](https://arxiv.org/abs/2512.11899)
*Futa Waseda,Shojiro Yamabe,Daiki Shiono,Kento Sasaki,Tsubasa Takahashi*

Main category: cs.CV

TL;DR: 论文提出RIO-VQA任务和RIO-Bench基准，解决大视觉语言模型中文本阅读与抗攻击的平衡问题，要求模型根据上下文选择性读取或忽略图像中的文本。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型易受排版攻击，现有评估和防御方法主要关注物体识别，鼓励忽略文本以获得鲁棒性，但现实场景需要同时处理物体和文本（如识别行人同时读取交通标志）。

Method: 提出RIO-VQA任务，形式化视觉问答中的选择性文本使用；创建RIO-Bench基准数据集，为每个真实图像提供相同场景的反事实（读取/忽略），仅改变文本内容和问题类型；基于此基准开发数据驱动的自适应选择性文本使用防御方法。

Result: 实验显示现有强大LVLMs和防御方法无法平衡排版攻击鲁棒性和文本阅读能力；RIO-Bench支持的新型数据驱动防御方法能实现自适应选择性文本使用，超越先前非自适应、忽略文本的防御方法。

Conclusion: 这项工作揭示了现有评估范围与现实需求之间的根本性错位，为构建可靠的大视觉语言模型提供了原则性路径，强调需要平衡文本阅读能力和抗攻击鲁棒性。

Abstract: Large vision-language models (LVLMs) are vulnerable to typographic attacks, where misleading text within an image overrides visual understanding. Existing evaluation protocols and defenses, largely focused on object recognition, implicitly encourage ignoring text to achieve robustness; however, real-world scenarios often require joint reasoning over both objects and text (e.g., recognizing pedestrians while reading traffic signs). To address this, we introduce a novel task, Read-or-Ignore VQA (RIO-VQA), which formalizes selective text use in visual question answering (VQA): models must decide, from context, when to read text and when to ignore it. For evaluation, we present the Read-or-Ignore Benchmark (RIO-Bench), a standardized dataset and protocol that, for each real image, provides same-scene counterfactuals (read / ignore) by varying only the textual content and question type. Using RIO-Bench, we show that strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability, highlighting the need for improved approaches. Finally, RIO-Bench enables a novel data-driven defense that learns adaptive selective text use, moving beyond prior non-adaptive, text-ignoring defenses. Overall, this work reveals a fundamental misalignment between the existing evaluation scope and real-world requirements, providing a principled path toward reliable LVLMs. Our Project Page is at https://turingmotors.github.io/rio-vqa/.

</details>


### [18] [CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities](https://arxiv.org/abs/2512.11901)
*Santosh Patapati*

Main category: cs.CV

TL;DR: CLARGA是一个通用的多模态融合架构，能够处理任意数量和类型的模态，通过图注意力网络构建模态间的注意力加权图进行信息传递，具有亚二次复杂度，支持缺失模态，并在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法通常针对特定模态组合设计，缺乏通用性。需要一种能够处理任意数量、类型模态的通用框架，同时支持缺失模态输入，并在多种任务中保持高效和鲁棒。

Method: CLARGA为每个样本构建模态特征的注意力加权图，使用多头图注意力网络在图上传递信息。通过可学习掩码适应缺失模态输入，采用监督任务损失和对比InfoNCE损失的混合目标进行训练。

Result: 在金融、人机交互、多媒体分类、情感计算等7个数据集上，CLARGA持续优于基线方法、SOTA模型和消融实验。同时展示了其对缺失输入的鲁棒性和在特定任务上的优异表现。

Conclusion: CLARGA是一个通用、高效、鲁棒的多模态融合架构，可轻松集成到各种机器学习模型中，在广泛任务中实现有效的跨模态表示学习。

Abstract: We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.

</details>


### [19] [Smartphone monitoring of smiling as a behavioral proxy of well-being in everyday life](https://arxiv.org/abs/2512.11905)
*Ming-Zher Poh,Shun Liao,Marco Andreetto,Daniel McDuff,Jonathan Wang,Paolo Di Achille,Jiang Wu,Yun Liu,Lawrence Cai,Eric Teasley,Mark Malhotra,Anupam Pathak,Shwetak Patel*

Main category: cs.CV

TL;DR: 通过智能手机被动捕捉的自然微笑可作为客观、可扩展的主观幸福感行为指标，与全国调查数据高度相关


<details>
  <summary>Details</summary>
Motivation: 传统主观幸福感测量依赖自我报告方法，存在回忆偏差和参与者负担重的问题，缺乏对日常生活的真实理解

Method: 分析233名参与者一周内被动记录的405,448个视频片段，使用深度学习模型量化微笑强度，研究昼夜和日常模式

Result: 微笑强度的日常模式与全国幸福感调查数据高度相关(r=0.92)，昼夜节律与日重建方法结果高度一致(r=0.80)。微笑强度与体力活动和光照正相关，与智能手机使用无关

Conclusion: 被动智能手机传感可作为研究情感行为动态的强大生态学有效方法，为在人口规模上理解这种行为打开了大门

Abstract: Subjective well-being is a cornerstone of individual and societal health, yet its scientific measurement has traditionally relied on self-report methods prone to recall bias and high participant burden. This has left a gap in our understanding of well-being as it is expressed in everyday life. We hypothesized that candid smiles captured during natural smartphone interactions could serve as a scalable, objective behavioral correlate of positive affect. To test this, we analyzed 405,448 video clips passively recorded from 233 consented participants over one week. Using a deep learning model to quantify smile intensity, we identified distinct diurnal and daily patterns. Daily patterns of smile intensity across the week showed strong correlation with national survey data on happiness (r=0.92), and diurnal rhythms documented close correspondence with established results from the day reconstruction method (r=0.80). Higher daily mean smile intensity was significantly associated with more physical activity (Beta coefficient = 0.043, 95% CI [0.001, 0.085]) and greater light exposure (Beta coefficient = 0.038, [0.013, 0.063]), whereas no significant effects were found for smartphone use. These findings suggest that passive smartphone sensing could serve as a powerful, ecologically valid methodology for studying the dynamics of affective behavior and open the door to understanding this behavior at a population scale.

</details>


### [20] [MPath: Multimodal Pathology Report Generation from Whole Slide Images](https://arxiv.org/abs/2512.11906)
*Noorul Wahab,Nasir Rajpoot*

Main category: cs.CV

TL;DR: MPath是一个轻量级多模态框架，通过视觉前缀提示机制将WSI特征注入预训练的生物医学语言模型，用于从全切片图像自动生成病理诊断报告。


<details>
  <summary>Details</summary>
Motivation: 从全切片图像自动生成病理诊断报告是计算病理学的新方向，但由于组织形态变异大和病理叙述结构复杂，将高分辨率组织模式转化为临床连贯文本仍然困难。

Method: MPath采用轻量级多模态框架，通过学习的视觉前缀提示机制，将WSI特征（CONCH + Titan）注入冻结的BioBART语言模型，使用紧凑投影模块而非端到端视觉语言预训练。

Result: 在RED 2025 Grand Challenge数据集上开发评估，在Test Phase 2中排名第4，尽管提交机会有限。结果显示了基于提示的多模态条件化作为可扩展且可解释的病理报告生成策略的潜力。

Conclusion: 基于提示的多模态条件化是病理报告生成的可扩展且可解释策略，MPath框架展示了将WSI特征有效注入预训练语言模型的能力。

Abstract: Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.

</details>


### [21] [FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications](https://arxiv.org/abs/2512.11925)
*Mozhgan Hadadi,Talukder Z. Jubery,Patrick S. Schnable,Arti Singh,Bedrich Benes,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: FloraForge是一个LLM辅助框架，让领域专家通过自然语言交互生成参数化3D植物模型，无需编程专业知识，结合了学习方法和程序化建模的优点。


<details>
  <summary>Details</summary>
Motivation: 当前3D植物建模方法存在局限：基于学习的方法需要大量物种特定数据且不可编辑；程序化建模需要几何建模专业知识和复杂规则理解，对领域科学家不友好。需要一种能让植物科学家轻松创建精确参数化模型的方法。

Method: 利用LLM辅助协同设计，通过迭代自然语言植物精炼(PR)生成Python脚本，创建分层B样条曲面表示的参数化植物几何体，具有植物学约束、显式控制点和参数变形函数。使用植物描述符(PD)文件进行手动精炼，拟合到点云数据。

Result: 框架在玉米、大豆和绿豆上成功演示，生成双重输出：用于可视化的三角网格和用于定量分析的带参数元数据的三角网格。实现了数学连续表示，支持表型分析和渲染。

Conclusion: FloraForge结合了LLM辅助模板创建、数学连续表示和直接参数控制，使植物科学领域的复杂几何建模民主化，同时保持数学严谨性，兼容功能结构植物分析工作流程。

Abstract: Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.

</details>


### [22] [TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder](https://arxiv.org/abs/2512.11926)
*Qinghao Meng,Chenming Wu,Liangjun Zhang,Jianbing Shen*

Main category: cs.CV

TL;DR: TransBridge：一种联合完成与检测的3D物体检测框架，通过transformer上采样块融合检测和补全网络特征，提升稀疏区域检测性能


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的3D物体检测在远距离稀疏LiDAR点云区域面临挑战，现有方法通过点云稠密化解决稀疏性问题，但需要在不增加成本的情况下提升稀疏区域检测特征

Method: 提出TransBridge框架：1）TransBridge transformer上采样块融合检测和补全网络特征；2）动态-静态重建模块生成稠密LiDAR数据；3）利用transformer机制建立通道和空间关系，生成高分辨率特征图用于补全

Result: 在nuScenes和Waymo数据集上验证，框架能持续提升端到端3D物体检测性能，mAP提升0.7-1.5点，对于两阶段检测框架mAP提升最高达5.78点

Conclusion: TransBridge框架通过联合完成与检测，有效提升稀疏点云区域的3D物体检测性能，具有良好泛化能力，为自动驾驶中的远距离物体检测提供了有效解决方案

Abstract: 3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.

</details>


### [23] [MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion](https://arxiv.org/abs/2512.11928)
*Alexander Peysakhovich,William Berman,Joseph Rufo,Felix Wong,Maxwell Z. Wilson*

Main category: cs.CV

TL;DR: 开发MONET扩散模型，从明场图像预测细胞染色图像，解决传统细胞染色技术耗时且无法研究细胞动态的问题


<details>
  <summary>Details</summary>
Motivation: 传统细胞染色技术存在两大问题：1) 劳动密集型，耗时耗力；2) 需要化学固定，无法研究细胞动态变化。需要一种能够从常规明场图像生成高质量细胞染色图像的方法

Method: 使用大规模数据集训练扩散模型MONET，采用一致性架构从明场图像预测细胞染色通道，该架构还能生成时间序列视频并支持上下文学习

Result: 模型质量随规模扩大而提升；一致性架构能够生成时间序列视频（尽管缺乏视频训练数据）；支持上下文学习，使模型能够部分适应分布外的细胞系和成像协议

Conclusion: 虚拟细胞染色不是要完全取代物理细胞染色，而是作为补充工具，为生物学研究提供新的工作流程，特别是在研究细胞动态变化方面

Abstract: Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.

</details>


### [24] [Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains](https://arxiv.org/abs/2512.11939)
*Clément Fernandes,Wojciech Pieczynski*

Main category: cs.CV

TL;DR: 本文提出了一种新的HEMC-CPS模型，结合了上下文Peano扫描和证据隐马尔可夫链，用于无监督图像分割，相比传统方法效果更好且计算更快。


<details>
  <summary>Details</summary>
Motivation: 传统隐马尔可夫场（HMFs）图像分割方法计算复杂，而基于Peano扫描的隐马尔可夫链（HMCs）方法虽然更快，但仍有改进空间。最近发展的上下文Peano扫描（CPS）和证据隐马尔可夫链（HEMCs）分别显示出改进潜力，因此研究将两者结合的新模型。

Method: 提出HEMC-CPS模型，同时考虑上下文Peano扫描和证据隐马尔可夫链。采用贝叶斯最大后验概率（MPM）分割，使用随机期望最大化（SEM）方法进行无监督参数估计。

Result: 在合成图像和真实图像上验证了HEMC-CPS模型的有效性，显示出比传统方法更好的分割性能。新模型对复杂图像（如三维或多传感器多分辨率图像）具有建模潜力。

Conclusion: HEMC-CPS模型结合了上下文Peano扫描和证据隐马尔可夫链的优势，为图像分割提供了更有效的无监督方法，且不仅限于图像分割，可应用于任何空间相关数据。

Abstract: Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.

</details>


### [25] [DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition](https://arxiv.org/abs/2512.11941)
*Jingmin Zhu,Anqi Zhu,James Bailey,Jun Liu,Hossein Rahmani,Mohammed Bennamoun,Farid Boussaid,Qiuhong Ke*

Main category: cs.CV

TL;DR: DynaPURLS是一个用于零样本骨架动作识别的新框架，通过多尺度视觉-语义对齐和动态细化来提升未见类别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有零样本骨架动作识别方法依赖于骨架特征与静态类别级语义的对齐，这种粗粒度对齐无法弥合可见和未见类别之间的领域偏移，阻碍了细粒度视觉知识的有效迁移。

Method: 1) 使用大语言模型生成包含全局运动和局部身体部位动态的层次化文本描述；2) 自适应分区模块通过语义分组骨架关节产生细粒度视觉表示；3) 动态细化模块在推理时通过轻量级可学习投影将文本特征适配到视觉流；4) 置信感知的类别平衡记忆库稳定细化过程，减轻噪声伪标签的错误传播。

Result: 在NTU RGB+D 60/120和PKU-MMD三个大规模基准数据集上的广泛实验表明，DynaPURLS显著优于现有方法，创造了新的最先进记录。

Conclusion: DynaPURLS通过建立鲁棒的多尺度视觉-语义对应关系并在推理时动态细化，有效解决了零样本骨架动作识别中的领域偏移问题，实现了更好的泛化性能。

Abstract: Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS

</details>


### [26] [A Comparative Analysis of Semiconductor Wafer Map Defect Detection with Image Transformer](https://arxiv.org/abs/2512.11977)
*Sushmita Nath*

Main category: cs.CV

TL;DR: 本研究探讨了在数据受限条件下使用DeiT（数据高效图像变换器）进行晶圆图缺陷分类，相比传统CNN模型（VGG-19、SqueezeNet、Xception等）在准确率、F1分数和训练收敛速度方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 半导体晶圆缺陷检测是预测性维护的重要领域，但传统CNN模型在数据有限且不平衡的情况下性能下降。需要探索在数据约束条件下更有效的分类方法。

Method: 使用Data-Efficient Image Transformer (DeiT)模型对半导体晶圆图缺陷进行分类，并与多种CNN模型（VGG-19、Xception、SqueezeNet和混合模型）进行对比实验。

Result: DeiT模型取得了90.83%的最高分类准确率，显著优于CNN模型（VGG-19:65%、SqueezeNet:82%、Xception:66%、混合模型:67%）。DeiT还表现出90.78%的F1分数、更快的训练收敛速度，以及在检测少数缺陷类别方面更强的鲁棒性。

Conclusion: 基于变换器的DeiT模型在半导体晶圆缺陷检测中展现出优越性能，特别是在数据受限条件下，为半导体制造过程中的预测性维护策略提供了有力支持。

Abstract: Predictive maintenance is an important sector in modern industries which improves fault detection and cost reduction processes. By using machine learning algorithms in the whole process, the defects detection process can be implemented smoothly. Semiconductor is a sensitive maintenance field that requires predictability in work. While convolutional neural networks (CNNs) such as VGG-19, Xception and Squeeze-Net have demonstrated solid performance in image classification for semiconductor wafer industry, their effectiveness often declines in scenarios with limited and imbalanced data. This study investigates the use of the Data-Efficient Image Transformer (DeiT) for classifying wafer map defects under data-constrained conditions. Experimental results reveal that the DeiT model achieves highest classification accuracy of 90.83%, outperforming CNN models such as VGG-19(65%), SqueezeNet(82%), Xception(66%) and Hybrid(67%). DeiT also demonstrated superior F1-score (90.78%) and faster training convergence, with enhanced robustness in detecting minority defect classes. These findings highlight the potential of transformer-based models like DeiT in semiconductor wafer defect detection and support predictive maintenance strategies within semiconductor fabrication processes.

</details>


### [27] [CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction](https://arxiv.org/abs/2512.11988)
*Xianghui Xie,Bowen Wen,Yan Chang,Hesam Rabeti,Jiefeng Li,Ye Yuan,Gerard Pons-Moll,Stan Birchfield*

Main category: cs.CV

TL;DR: CARI4D：首个从单目RGB视频重建4D人-物交互的类别无关方法，通过姿态假设选择、渲染-比较优化和接触推理实现空间、时间和像素对齐，在未见数据集上性能提升36%


<details>
  <summary>Details</summary>
Motivation: 从单目RGB视频准确捕捉人-物交互对于人类理解、游戏和机器人学习应用很重要，但由于未知物体和人体信息、深度模糊、遮挡和复杂运动等因素，从单视图推断4D交互极具挑战性。现有方法通常需要真实物体模板或局限于有限物体类别。

Method: 提出CARI4D方法：1）姿态假设选择算法，稳健整合基础模型的个体预测；2）通过学习的渲染-比较范式联合优化，确保空间、时间和像素对齐；3）推理复杂接触关系进行进一步细化，满足物理约束。

Result: 在分布内数据集上重建误差降低38%，在未见数据集上降低36%。模型能够泛化到训练类别之外，可零样本应用于真实网络视频。

Conclusion: CARI4D是首个从单目RGB视频重建度量尺度下空间和时间一致的4D人-物交互的类别无关方法，通过整合基础模型预测、渲染-比较优化和接触推理，显著优于现有方法并具有良好泛化能力。

Abstract: Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.

</details>


### [28] [V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions](https://arxiv.org/abs/2512.11995)
*Chenrui Fan,Yijun Liang,Shweta Bhardwaj,Kwesi Cobbina,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: V-REX是一个评估视觉语言模型多步探索推理能力的基准套件，将复杂视觉任务分解为问题链，分别评估规划和执行能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在处理需要多轮探索推理的复杂开放任务时表现不佳，但现有基准主要关注定义明确的问题，缺乏对多步探索推理能力的评估。

Method: 开发V-REX评估套件，包含挑战性视觉推理任务基准和评估协议。将多步探索推理转化为问题链，分别评估模型的规划能力（分解任务选择问题链）和执行能力（按顺序回答问题收集信息）。通过为每个步骤提供有限的问题和答案选项，实现可靠的定量和细粒度分析。

Result: 评估了最先进的专有和开源视觉语言模型，发现一致的扩展趋势、规划和执行能力之间的显著差异，以及多步探索推理能力仍有大幅提升空间。

Conclusion: V-REX填补了视觉语言模型多步探索推理评估的空白，为模型能力提供了细粒度分析框架，揭示了当前模型在该领域的局限性和改进方向。

Abstract: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.

</details>


### [29] [Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus](https://arxiv.org/abs/2512.12012)
*Antonio Guillen-Perez*

Main category: cs.CV

TL;DR: Semantic-Drive：一种本地优先的神经符号框架，用于从自动驾驶车辆视频日志中挖掘语义数据，通过解耦感知和认知分析来识别罕见的安全关键事件。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆开发受限于"长尾"训练数据的稀缺性，现有解决方案要么精度不足（粗粒度元数据搜索），要么存在隐私侵犯和成本高昂问题（基于云的视觉语言模型）。

Method: 采用两阶段神经符号框架：1) 符号接地：使用实时开放词汇检测器（YOLOE）锚定注意力；2) 认知分析：通过推理视觉语言模型进行法医场景分析。使用"系统2"推理时对齐策略和多模型"法官-侦察员"共识机制来减轻幻觉。

Result: 在nuScenes数据集上，相比CLIP的0.475，Semantic-Drive达到0.966的召回率，并将风险评估误差降低40%。系统完全在消费级硬件（NVIDIA RTX 3090）上运行。

Conclusion: Semantic-Drive提供了一个隐私保护的本地替代方案，能够高效识别罕见安全关键事件，解决了自动驾驶数据挖掘中的长尾问题。

Abstract: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.

</details>


### [30] [Exploring Spatial-Temporal Representation via Star Graph for mmWave Radar-based Human Activity Recognition](https://arxiv.org/abs/2512.12013)
*Senhao Gao,Junqing Zhang,Luoyu Mei,Shuai Wang,Xuyu Wang*

Main category: cs.CV

TL;DR: 提出基于星形图和离散动态图神经网络的毫米波雷达点云人体活动识别方法，解决点云稀疏和变尺寸问题，在资源受限平台上实现高效识别。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达点云存在稀疏性和变尺寸问题，现有基于视觉系统的预处理方法不适用于雷达系统，需要专门针对雷达特性的特征提取方法。

Method: 设计星形图表示动态毫米波雷达点与静态中心点的高维相对关系，采用离散动态图神经网络学习变尺寸星形图中的时空特征。

Result: 在真实HAR数据集上超越其他基线方法，达到94.27%的分类准确率，接近基于骨架视觉数据的97.25%性能，在树莓派4上验证了资源受限平台的有效性。

Conclusion: 提出的星形图表示和DDGNN方法能有效处理毫米波雷达点云的稀疏性和变尺寸问题，无需重采样或帧聚合器，在资源受限平台上实现高效人体活动识别。

Abstract: Human activity recognition (HAR) requires extracting accurate spatial-temporal features with human movements. A mmWave radar point cloud-based HAR system suffers from sparsity and variable-size problems due to the physical features of the mmWave signal. Existing works usually borrow the preprocessing algorithms for the vision-based systems with dense point clouds, which may not be optimal for mmWave radar systems. In this work, we proposed a graph representation with a discrete dynamic graph neural network (DDGNN) to explore the spatial-temporal representation of human movement-related features. Specifically, we designed a star graph to describe the high-dimensional relative relationship between a manually added static center point and the dynamic mmWave radar points in the same and consecutive frames. We then adopted DDGNN to learn the features residing in the star graph with variable sizes. Experimental results demonstrated that our approach outperformed other baseline methods using real-world HAR datasets. Our system achieved an overall classification accuracy of 94.27\%, which gets the near-optimal performance with a vision-based skeleton data accuracy of 97.25\%. We also conducted an inference test on Raspberry Pi~4 to demonstrate its effectiveness on resource-constraint platforms. \sh{ We provided a comprehensive ablation study for variable DDGNN structures to validate our model design. Our system also outperformed three recent radar-specific methods without requiring resampling or frame aggregators.

</details>


### [31] [Adaptive federated learning for ship detection across diverse satellite imagery sources](https://arxiv.org/abs/2512.12053)
*Tran-Vu La,Minh-Tan Pham,Yu Li,Patrick Matgen,Marco Chini*

Main category: cs.CV

TL;DR: 该研究探索了联邦学习在卫星船舶检测中的应用，通过比较多种FL算法与本地训练基线，证明FL能在保护隐私的同时显著提升检测精度，接近使用全部数据的全局训练效果。


<details>
  <summary>Details</summary>
Motivation: 解决卫星船舶检测中的数据隐私问题，避免商业卫星图像和敏感船舶标注数据的共享或集中收集，同时提升在小规模本地数据集上的检测性能。

Method: 使用YOLOv8船舶检测模型，比较四种联邦学习算法（FedAvg、FedProx、FedOpt、FedMedian）与本地训练基线，评估不同FL配置（通信轮数、本地训练轮数）对性能的影响。

Result: FL模型相比小规模本地数据集训练显著提升了检测精度，性能接近使用全部数据集的全局训练；选择合适的FL配置对优化检测精度和计算效率至关重要。

Conclusion: 联邦学习为卫星船舶检测提供了有效的隐私保护解决方案，能够在保护数据隐私的同时实现接近全局训练的性能，但需要仔细选择FL配置以获得最佳效果。

Abstract: We investigate the application of Federated Learning (FL) for ship detection across diverse satellite datasets, offering a privacy-preserving solution that eliminates the need for data sharing or centralized collection. This approach is particularly advantageous for handling commercial satellite imagery or sensitive ship annotations. Four FL models including FedAvg, FedProx, FedOpt, and FedMedian, are evaluated and compared to a local training baseline, where the YOLOv8 ship detection model is independently trained on each dataset without sharing learned parameters. The results reveal that FL models substantially improve detection accuracy over training on smaller local datasets and achieve performance levels close to global training that uses all datasets during the training. Furthermore, the study underscores the importance of selecting appropriate FL configurations, such as the number of communication rounds and local training epochs, to optimize detection precision while maintaining computational efficiency.

</details>


### [32] [Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management](https://arxiv.org/abs/2512.12056)
*Maria Rodriguez,Minh-Tan Pham,Martin Sudmanns,Quentin Poterek,Oscar Narvaez*

Main category: cs.CV

TL;DR: 本研究提出了一种监督语义分割工作流，旨在提升野火后烧毁区域（BA）划定的性能和效率，针对SPOT-6/7高分辨率影像，通过实验比较U-Net和SegFormer模型性能，并探讨了土地覆盖数据作为辅助任务以及测试时增强的优化效果。


<details>
  <summary>Details</summary>
Motivation: 当前烧毁区域（BA）制图方法主要依赖计算机视觉模型训练后事件遥感影像，但往往忽视了其在时间紧迫的应急管理场景中的适用性。需要开发既高效又准确的BA划定方法，以支持灾害评估和生态系统恢复。

Method: 提出监督语义分割工作流，针对SPOT-6/7高分辨率影像进行实验。比较U-Net和SegFormer模型在有限训练数据下的表现，引入土地覆盖数据作为辅助任务增强模型鲁棒性，并采用测试时增强（TTA）提升性能，同时探讨混合精度等优化方法减少推理时间。

Result: 实验结果显示：1）U-Net和SegFormer在有限训练数据下表现相似；2）SegFormer需要更多计算资源，在应急场景中实用性受限；3）加入土地覆盖辅助任务能提升模型鲁棒性且不增加推理时间；4）测试时增强能改善BA划定性能但会增加推理时间，可通过混合精度等方法缓解。

Conclusion: 本研究为应急管理场景下的烧毁区域快速划定提供了有效解决方案。U-Net在资源受限情况下更具实用性，而土地覆盖辅助任务和测试时增强能进一步提升性能。未来可通过优化方法平衡性能与效率，支持灾害应急响应和生态系统恢复。

Abstract: After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.

</details>


### [33] [CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos](https://arxiv.org/abs/2512.12060)
*Tejas Panambur,Ishan Rajendrakumar Dave,Chongjian Ge,Ersin Yumer,Xue Bai*

Main category: cs.CV

TL;DR: CreativeVR是一个针对AI生成视频和真实视频中严重结构/时序伪影的修复框架，通过单一精度控制旋钮在精确修复和结构校正之间平滑权衡，在AIGC伪影修复上达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 当前T2V扩散模型存在精细结构问题（扭曲人脸/手、背景变形、时序不一致），而传统视频修复方法主要针对合成退化（模糊、下采样），扩散先验修复器通常针对光度噪声训练，缺乏对感知质量与保真度权衡的控制。

Method: 提出基于深度适配器的扩散先验引导视频修复框架，包含时序一致的退化模块，在训练时应用精心设计的变换来模拟真实结构故障，通过单一精度控制旋钮调节模型对输入的跟随强度。

Result: 在严重伪影视频上达到SOTA效果，在标准视频修复基准上具有竞争力，提出AIGC54基准用于评估AIGC伪影修复，实际吞吐量约13 FPS（720p，单80GB A100）。

Conclusion: CreativeVR为AI生成视频和真实视频中的严重结构/时序伪影提供了有效的修复解决方案，通过可控的精度权衡机制实现了在保持保真度和结构校正之间的平衡。

Abstract: Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.
  We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.
  To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.

</details>


### [34] [BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models](https://arxiv.org/abs/2512.12080)
*Ryan Po,Eric Ryan Chan,Changan Chen,Gordon Wetzstein*

Main category: cs.CV

TL;DR: BAgger是一种自监督训练方案，通过构建模型自身生成轨迹的纠正路径，解决自回归视频模型中的曝光偏差问题，提升长期视频生成的稳定性。


<details>
  <summary>Details</summary>
Motivation: 自回归视频模型通过下一帧预测进行世界建模，但存在曝光偏差问题：训练时使用干净上下文，推理时却使用自生成的帧，导致误差累积和质量随时间漂移。

Method: 提出Backwards Aggregation (BAgger)自监督方案，从模型自身生成轨迹中构建纠正路径，教导模型从自身错误中恢复。与依赖少步蒸馏和分布匹配损失的先前方法不同，BAgger使用标准分数或流匹配目标进行训练，避免使用大型教师模型和长时间链的反向传播。

Result: 在因果扩散变换器上实例化BAgger，在文本到视频、视频扩展和多提示生成任务中评估，观察到更稳定的长期运动和更好的视觉一致性，减少了质量漂移。

Conclusion: BAgger通过自监督纠正轨迹训练，有效解决了自回归视频模型的曝光偏差问题，提升了长期视频生成的稳定性和质量。

Abstract: Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.

</details>


### [35] [RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer](https://arxiv.org/abs/2512.12083)
*Guanfang Dong,Luke Schultz,Negar Hassanpour,Chao Gao*

Main category: cs.CV

TL;DR: RePack通过将高维视觉基础模型特征压缩到低维流形，解决了信息过载问题，显著加速了扩散变换器的收敛并提升图像重建性能。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉基础模型的高维特征虽然能增强潜在扩散模型，但会导致信息过载问题，特别是当VFM特征尺寸超过原始图像解码需求时，影响模型效率和性能。

Method: 提出RePack框架，将高维VFM表示投影到低维流形上，生成更紧凑、解码器友好的表示，有效过滤非语义噪声同时保留核心结构信息。

Result: 在DiT-XL/2上，RePack仅用64轮训练就达到FID 3.66，比最先进方法快35%收敛，显著优于直接注入原始VFM特征的方法。

Conclusion: RePack成功提取了VFM表示的核心语义，同时避免了高维度的副作用，为有效利用视觉基础模型特征提供了简单而有效的解决方案。

Abstract: The superior representation capability of pre-trained vision foundation models (VFMs) has been harnessed for enhancing latent diffusion models (LDMs). These approaches inject the rich semantics from high-dimensional VFM representations (e.g., DINOv3) into LDMs at different phases, resulting in accelerated learning and better generation performance. However, the high-dimensionality of VFM representations may also lead to Information Overload, particularly when the VFM features exceed the size of the original image for decoding. To address this issue while preserving the utility of VFM features, we propose RePack (Representation Packing), a simple yet effective framework for improving Diffusion Transformers (DiTs). RePack transforms the VFM representation into a more compact, decoder-friendly representation by projecting onto low-dimensional manifolds. We find that RePack can effectively filter out non-semantic noise while preserving the core structural information needed for high-fidelity reconstruction. Experimental results show that RePack significantly accelerates DiT convergence and outperforms recent methods that directly inject raw VFM features into the decoder for image reconstruction. On DiT-XL/2, RePack achieves an FID of 3.66 in only 64 epochs, which is 35% faster than the state-of-the-art method. This demonstrates that RePack successfully extracts the core semantics of VFM representations while bypassing their high-dimensionality side effects.

</details>


### [36] [VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering](https://arxiv.org/abs/2512.12089)
*Zihu Wang,Boxun Xu,Yuxuan Xia,Peng Li*

Main category: cs.CV

TL;DR: VEGAS：通过将视觉编码器的注意力图注入语言模型中间层，有效减少大视觉语言模型的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型（LVLMs）虽然能联合推理视觉和文本输入，但经常产生与视觉证据事实不一致的流畅输出（幻觉）。现有方法未能有效解决：什么样的视觉注意力形式能在解码过程中有效抑制幻觉？

Method: 提出VEGAS方法：1）发现视觉编码器自身的注意力图比最终视觉注意力图更集中；2）分析解码过程中的视觉-文本冲突，发现冲突在语言模型中间层达到峰值；3）将视觉编码器的注意力图注入语言模型的中间层，自适应地引导未能聚焦关键图像对象的标记。

Result: 在多个基准测试上的广泛实验表明，VEGAS在减少幻觉方面始终达到最先进的性能。

Conclusion: 视觉编码器的注意力图能有效抑制LVLMs的幻觉，VEGAS作为一种简单有效的推理时方法，通过注意力注入机制显著提升了模型的视觉-文本一致性。

Abstract: Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.

</details>


### [37] [SPDMark: Selective Parameter Displacement for Robust Video Watermarking](https://arxiv.org/abs/2512.12090)
*Samar Fares,Nurbek Tastan,Karthik Nandakumar*

Main category: cs.CV

TL;DR: SPDMark是一种基于选择性参数位移的视频生成水印框架，通过修改生成模型的参数子集嵌入水印，利用LoRA实现参数高效的基础位移，结合密码学哈希实现帧级水印和抗时序篡改检测。


<details>
  <summary>Details</summary>
Motivation: 高质量视频生成模型的兴起增强了对鲁棒水印方案的需求，用于可靠检测和追踪生成视频的来源。现有视频水印方法（包括后处理和生成中方法）无法同时实现不可感知性、鲁棒性和计算效率。

Method: 提出SPDMark框架，基于视频扩散模型的选择性参数位移。水印通过修改生成模型的参数子集嵌入，建模为层间基础位移的加性组合，使用LoRA实现参数高效的基础位移。训练阶段联合学习基础位移和水印提取器，最小化消息恢复、感知相似性和时序一致性损失。使用密码学哈希函数从基础水印密钥派生帧特定水印消息，通过最大二分图匹配恢复正确帧顺序。

Result: 在文本到视频和图像到视频生成模型上的评估表明，SPDMark能够生成不可感知的水印，并以高准确率恢复水印，同时证明其对各种常见视频修改具有鲁棒性。

Conclusion: SPDMark是一种有效的生成中视频水印框架，能够同时实现不可感知性、鲁棒性和计算效率，为生成视频的来源追踪提供了可靠解决方案。

Abstract: The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.

</details>


### [38] [AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging](https://arxiv.org/abs/2512.12101)
*Swarn S. Warshaneyan,Maksims Ivanovs,Blaž Cugmas,Inese Bērziņa,Laura Goldberga,Mindaugas Tamosiunas,Roberts Kadiķis*

Main category: cs.CV

TL;DR: 该研究探索了全自动花粉识别，比较了传统光学显微镜和数字在线全息显微镜(DIHM)图像。使用YOLOv8s进行目标检测，MobileNetV3L进行分类，在光学图像上表现良好，但在DIHM图像上性能显著下降。通过WGAN-SN生成合成DIHM图像进行数据增强，将检测性能提升了7.25个百分点。


<details>
  <summary>Details</summary>
Motivation: 全息显微镜图像中的花粉识别面临挑战，包括散斑噪声、孪生图像伪影以及与明场图像的显著差异。研究旨在建立全自动DIHM工作流程的性能基准，并探索如何通过生成对抗网络(GAN)增强来弥合光学和全息图像之间的性能差距。

Method: 1. 使用YOLOv8s进行目标检测和MobileNetV3L进行分类
2. 在双模态数据集上训练，包含自动标注的光学图像和仿射对齐的DIHM图像
3. 通过扩展DIHM图像中花粉的边界框来改进检测
4. 使用带谱归一化的Wasserstein GAN(WGAN-SN)生成合成DIHM图像
5. 将真实和合成数据以1.0:1.5的比例混合进行训练

Result: 1. 光学图像：检测mAP50达到91.3%，分类准确率97%
2. DIHM图像：检测mAP50仅8.15%，分类准确率50%
3. 扩展边界框后：检测mAP50提升至13.3%，分类准确率54%
4. WGAN-SN生成的合成DIHM图像FID得分为58.246
5. 混合真实和合成数据后：检测mAP50提升至15.4%，相对提升7.25个百分点

Conclusion: GAN-based数据增强可以有效减少光学和全息图像之间的性能差距，使全自动DIHM工作流程在兽医成像应用中向实际应用迈出了重要一步。尽管性能提升有限，但证明了合成数据增强在解决全息图像识别挑战方面的潜力。

Abstract: We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.

</details>


### [39] [EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography](https://arxiv.org/abs/2512.12107)
*Yuheng Li,Yue Zhang,Abdoul Aziz Amadou,Yuxiang Lai,Jike Zhong,Tiziano Passerini,Dorin Comaniciu,Puneet Sharma*

Main category: cs.CV

TL;DR: 开发了首个测量基础的多模态超声心动图数据集EchoGround-MIMIC和相应的视觉语言模型EchoVLM，在多种临床任务中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 超声心动图解读需要多模态分析，但现有视觉语言模型在超声心动图领域应用受限，缺乏大规模临床基础的数据集和测量推理能力

Method: 构建包含19,065个图像-文本对的EchoGround-MIMIC数据集，开发EchoVLM模型，引入视图感知对比损失和否定感知对比损失两个新的预训练目标

Result: 在36个多模态疾病分类、图像-文本检索、视图分类、腔室分割和标志点检测任务中实现最先进性能，零样本疾病分类AUC达86.5%，视图分类准确率达95.1%

Conclusion: 临床基础的多模态预训练产生可迁移的视觉表示，EchoVLM成为端到端超声心动图解读的基础模型，将发布数据集和代码促进研究

Abstract: Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.

</details>


### [40] [A Novel Patch-Based TDA Approach for Computed Tomography](https://arxiv.org/abs/2512.12108)
*Dashti A. Ali,Aras T. Asaad,Jacob J. Peoples,Mohammad Hamghalam,Alex Robins,Mane Piliposyan,Richard K. G. Do,Natalie Gangai,Yun S. Chun,Ahmad Bashir Barekzai,Jayasree Chakraborty,Hala Khasawneh,Camila Vilela,Natally Horvat,João Miranda,Alice C. Wei,Amber L. Simpson*

Main category: cs.CV

TL;DR: 提出基于patch的持久同调构建方法，用于3D CT影像的拓扑数据分析，相比传统3D立方体复形方法在分类性能和时间效率上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统3D立方体复形方法在CT影像拓扑数据分析中存在性能不足和计算复杂度高的问题，特别是对于高分辨率CT图像。

Method: 提出新颖的基于patch的持久同调构建方法，专门针对体积医学影像数据（特别是CT模态）设计，通过patch分割策略优化拓扑特征提取。

Result: 在多个3D CT数据集上，patch-based方法在准确率、AUC、灵敏度、特异度和F1分数上分别平均提升10.38%、6.94%、2.06%、11.58%和8.51%，同时具有更好的时间效率。

Conclusion: 基于patch的TDA方法在CT影像分析中优于传统立方体复形方法，提供了更高效和准确的拓扑特征提取，并发布了Python工具包Patch-TDA以促进应用。

Abstract: The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.

</details>


### [41] [A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery](https://arxiv.org/abs/2512.12128)
*Thomas Manzini,Priyankari Perali,Raisa Karnik,Robin R. Murphy*

Main category: cs.CV

TL;DR: 该论文提出了目前最大的道路损坏评估与道路对齐基准数据集CRASAR-U-DRIODs，包含10次联邦宣布灾害后的无人机影像，标注了657.25公里道路，并提供18个基线模型，解决了现有数据集规模小、分辨率低、缺乏操作验证的问题。


<details>
  <summary>Details</summary>
Motivation: 现有灾害道路损坏评估数据集存在三个主要问题：1) 规模小；2) 依赖低分辨率影像，无法检测应急管理者关心的现象；3) 机器学习系统缺乏操作验证。此外，实践中发现道路线错位问题会影响模型性能。

Method: 1) 创建大规模数据集：标注657.25公里道路，采用10类标注方案；2) 提供9,184条道路线调整用于空间对齐；3) 训练18个基线模型；4) 在2024年飓风Debby和Helene的实际应急响应中部署验证。

Result: 1) 当模型部署到实际错位的道路线时，性能平均下降5.596% Macro IoU；2) 如果不考虑空间对齐，约8%(11公里)的道路不良状况会被错误标注；3) 约9%(59公里)的道路线与实际道路错位。

Conclusion: 道路空间对齐是灾害响应中ML/CV/机器人社区需要解决的关键问题，否则会影响决策有效性。该数据集和基线模型为改进灾害道路评估提供了重要基础。

Abstract: This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\% Macro IoU. If spatial alignment is not considered, approximately 8\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.

</details>


### [42] [Open Horizons: Evaluating Deep Models in the Wild](https://arxiv.org/abs/2512.12146)
*Ayush Vaibhav Bhatti,Deniz Karakay,Debottama Das,Nilotpal Rajbongshi,Yuito Sugimoto*

Main category: cs.CV

TL;DR: 该研究在CIFAR-10上对开放集识别(OSR)和少样本类增量学习(FSCIL)进行了统一实验比较，分析了不同视觉编码器和评分函数在未知样本检测中的表现，以及原型方法在增量适应中缓解灾难性遗忘的效果。


<details>
  <summary>Details</summary>
Motivation: 开放世界部署需要模型既能识别已知类别，又能在出现新类别时保持可靠。研究旨在通过统一实验框架，系统比较开放集识别和少样本类增量学习的不同方法，为实际应用提供指导。

Method: 在CIFAR-10上进行实验：1) OSR部分比较ResNet-50、ConvNeXt-Tiny和CLIP ViT-B/16三种预训练冻结视觉编码器，结合线性探测和四种后验评分函数(MSP、Energy、Mahalanobis、kNN)；2) FSCIL部分比较改进的SPPR、OrCo和ConCM方法，使用部分冻结的ResNet-50，在1、5、10-shot场景下测试。

Result: OSR中CLIP在AUROC、AUPR、FPR@95和OSCR等指标上始终表现最佳，Energy评分函数在不同骨干网络中最稳定。FSCIL中ConCM在10-shot设置下达到84.7%准确率且混淆矩阵最清晰，所有方法在超过5-shot后都出现性能饱和。

Conclusion: 骨干网络架构和评分机制显著影响未知样本检测能力，CLIP编码器在开放集识别中表现最优；原型方法能有效缓解增量学习中的灾难性遗忘，但少样本场景下存在性能饱和现象。

Abstract: Open-world deployment requires models to recognize both known categories and remain reliable when novel classes appear. We present a unified experimental study spanning open-set recognition (OSR) and few-shot class-incremental learning (FSCIL) on CIFAR-10. For OSR, we compare three pretrained frozen visual encoders: ResNet-50, ConvNeXt-Tiny and CLIP ViT-B/16,using a linear probe and four post-hoc scoring functions, namely MSP, Energy, Mahalanobis and kNN. Across metrics,such as, AUROC, AUPR, FPR@95, and OSCR, CLIP consistently yields the strongest separability between known and unknown samples, with Energy providing the most stable performance across backbones. For FSCIL, we compare modified SPPR, OrCo, and ConCM using partially frozen ResNet-50 across 1-, 5-, and 10-shot scenarios. ConCM achieves 84.7% accuracy in the 10-shot setting with the cleanest confusion matrix, while all methods show saturation beyond 5 shots. Our controlled evaluation reveals how the backbone architecture and scoring mechanisms affect unknown detection and how prototype-based methods mitigate catastrophic forgetting during incremental adaptation.

</details>


### [43] [Audio-Visual Camera Pose Estimationn with Passive Scene Sounds and In-the-Wild Video](https://arxiv.org/abs/2512.12165)
*Daniel Adebi,Sagnik Majumder,Kristen Grauman*

Main category: cs.CV

TL;DR: 提出首个利用音频辅助相对相机位姿估计的方法，通过音频方向信息和双耳嵌入增强视觉模型，在真实视频中实现性能提升和鲁棒性增强。


<details>
  <summary>Details</summary>
Motivation: 相机运动理解是具身感知和3D场景理解的基础问题。纯视觉方法在视觉退化条件下（如运动模糊、遮挡）表现不佳，而被动场景声音提供了互补线索。

Method: 提出简单有效的音频-视觉框架，将到达方向(DOA)频谱和双耳化嵌入集成到最先进的纯视觉位姿估计模型中。

Result: 在两个大型数据集上相比强视觉基线获得一致性能提升，且在视觉信息受损时表现出鲁棒性。

Conclusion: 这是首个成功利用音频进行真实视频相对相机位姿估计的工作，确立了日常音频作为经典空间挑战问题的意外但有前景的信号。

Abstract: Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.

</details>


### [44] [SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation](https://arxiv.org/abs/2512.12193)
*Xuancheng Xu,Yaning Li,Sisi You,Bing-Kun Bao*

Main category: cs.CV

TL;DR: SMRABooth：利用自监督编码器和光流编码器提供对象级主体和运动表示，通过LoRA微调实现定制化视频生成，保持主体外观相似性和运动模式一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时保证主体外观相似性和运动模式一致性，因为缺乏对象级的主体和运动指导。需要一种能够同时控制主体外观和运动模式的方法。

Method: 1) 使用自监督编码器提取主体表示指导主体对齐；2) 使用光流编码器提取运动表示捕获对象级运动轨迹；3) 提出主体-运动关联解耦策略，在位置和时间上稀疏注入LoRAs，减少干扰。

Result: 大量实验表明SMRABooth在主体和运动定制方面表现出色，能够保持一致的主体外观和运动模式，证明了其在可控文本到视频生成中的有效性。

Conclusion: SMRABooth通过对象级主体和运动表示，结合LoRA微调和解耦策略，成功解决了定制化视频生成中主体外观相似性和运动模式一致性的挑战。

Abstract: Customized video generation aims to produce videos that faithfully preserve the subject's appearance from reference images while maintaining temporally consistent motion from reference videos. Existing methods struggle to ensure both subject appearance similarity and motion pattern consistency due to the lack of object-level guidance for subject and motion. To address this, we propose SMRABooth, which leverages the self-supervised encoder and optical flow encoder to provide object-level subject and motion representations. These representations are aligned with the model during the LoRA fine-tuning process. Our approach is structured in three core stages: (1) We exploit subject representations via a self-supervised encoder to guide subject alignment, enabling the model to capture overall structure of subject and enhance high-level semantic consistency. (2) We utilize motion representations from an optical flow encoder to capture structurally coherent and object-level motion trajectories independent of appearance. (3) We propose a subject-motion association decoupling strategy that applies sparse LoRAs injection across both locations and timing, effectively reducing interference between subject and motion LoRAs. Extensive experiments show that SMRABooth excels in subject and motion customization, maintaining consistent subject appearance and motion patterns, proving its effectiveness in controllable text-to-video generation.

</details>


### [45] [Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms](https://arxiv.org/abs/2512.12199)
*Ercan Erkalkan,Vedat Topuz,Ayça Ak*

Main category: cs.CV

TL;DR: 提出一种用于微型无人机群在有限带宽下监测野火的轻量级边界跟踪方法，结合热成像和RGB图像处理，实现低延迟、稳定的环境边界追踪。


<details>
  <summary>Details</summary>
Motivation: 在野火等紧急侦察场景中，微型无人机团队需要在有限带宽条件下进行快速部署，传统方法计算复杂且对GPS依赖性强，需要轻量级、鲁棒的边界跟踪方案。

Method: 1) 热成像帧通过自适应阈值和形态学细化生成粗略热点区域掩码；2) RGB帧提供边缘线索，使用基于梯度的滤波抑制纹理相关的误检测；3) 规则级合并策略选择边界候选点，通过Ramer-Douglas-Peucker算法简化；4) 集成周期性信标和惯性反馈回路，在GPS降级时保持轨迹稳定性；5) 在嵌入式SoC平台上通过限制每帧像素操作和预计算梯度表实现低于50ms的延迟。

Result: 小规模仿真显示：与纯边缘跟踪基线相比，平均路径长度和边界抖动减少，同时通过交集合并分析保持环境覆盖率；电池消耗和计算利用率证实可在标准微型平台上实现10-15 m/s的前向运动。

Conclusion: 该方法实现了快速现场部署，仅需鲁棒传感和最小通信，适用于紧急侦察应用，为有限带宽条件下的微型无人机团队野火监测提供了可行的轻量级解决方案。

Abstract: This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.

</details>


### [46] [A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection](https://arxiv.org/abs/2512.12205)
*Peizheng Li,Ioannis Mavromatis,Ajith Sahadevan,Tim Farnham,Adnan Aijaz,Aftab Khan*

Main category: cs.CV

TL;DR: 提出了一个大规模、长期的英国布里斯托尔城市路灯视觉数据集，包含22个固定角度摄像头从2021到2025年采集的超过52.6万张图像，附带丰富元数据，用于研究视觉漂移、异常检测和智能城市MLOps策略。


<details>
  <summary>Details</summary>
Motivation: 智能城市部署中需要解决长期视觉模型的稳定性问题，包括视觉漂移、环境变化影响等，但缺乏真实世界、长时间跨度的数据集来支持相关研究。

Method: 使用22个固定角度摄像头每小时采集图像，覆盖不同光照、天气和季节条件。提供基于卷积变分自编码器（CNN-VAEs）的自监督框架，为每个摄像头节点和白天/夜间图像集分别训练模型，定义相对质心漂移和相对重建误差两种漂移度量。

Result: 创建了包含超过52.6万张图像的大规模数据集，附带时间戳、GPS坐标和设备标识等元数据。提供了评估长期模型稳定性、漂移感知学习和部署就绪视觉系统的现实基准。

Conclusion: 该数据集为研究视觉漂移、异常检测和MLOps策略提供了宝贵的真实世界资源，支持街灯监控、天气推断和城市场景理解等下游应用，促进可重复性和实际部署研究。

Abstract: We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.

</details>


### [47] [ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB](https://arxiv.org/abs/2512.12206)
*Jeongjun Park,Sunwook Hwang,Hyeonho Noh,Jin Mo Yang,Hyun Jong Yang,Saewoong Bahk*

Main category: cs.CV

TL;DR: 提出ALERT数据集和ISA-ViT框架，解决UWB雷达数据用于分心驾驶检测的两个关键挑战：缺乏大规模真实数据集和ViT输入尺寸不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 分心驾驶导致致命事故，IR-UWB雷达具有抗干扰、低功耗和隐私保护优势，但缺乏大规模真实UWB数据集，且ViT固定输入尺寸难以适应UWB雷达数据的非标准维度。

Method: 1) 发布ALERT数据集（10,220个真实驾驶条件下的雷达样本）；2) 提出ISA-ViT框架，通过调整patch配置和利用预训练位置嵌入向量，在调整UWB数据尺寸时保留多普勒频移和相位特征；3) 采用域融合策略结合距离域和频域特征。

Result: ISA-ViT在UWB分心驾驶检测任务上比现有ViT方法准确率提升22.68%，ALERT数据集和输入尺寸无关策略公开发布。

Conclusion: 该工作通过数据集和框架创新，促进了更鲁棒、可扩展的分心驾驶检测系统开发，为实际部署提供了重要基础。

Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.

</details>


### [48] [A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction](https://arxiv.org/abs/2512.12208)
*Indranil Bhattacharjee,Vartika Narayani Srinet,Anirudha Bhattacharjee,Braj Bhushan,Bishakh Bhattacharya*

Main category: cs.CV

TL;DR: 提出了一种用于自闭症儿童情绪识别的深度学习管道，结合CNN和GCN处理面部视频数据，在机器人交互场景中表现优异


<details>
  <summary>Details</summary>
Motivation: 理解自闭症儿童在社交互动中的情绪反应是发展心理学和人机交互领域的重要挑战，目前缺乏针对自闭症儿童的大规模、真实世界数据集和分析方法

Method: 使用混合模型：微调的ResNet-50 CNN + 三层GCN，处理从MediaPipe FaceMesh提取的视觉和几何特征。采用DeepFace和FER的加权集成进行概率标签生成，通过KL散度优化融合嵌入进行分类

Result: 该方法在建模微妙情感反应方面表现出鲁棒性能，有效捕捉神经多样性儿童的微观情绪线索，为临床和治疗性人机交互中的情感分析提供了重要基础

Conclusion: 该研究填补了自闭症特定人机交互研究的空白，为未来个性化辅助技术提供了首个来自印度的大规模、真实世界数据集和分析管道

Abstract: Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction remains a critical challenge in both developmental psychology and human-robot interaction. This study presents a novel deep learning pipeline for emotion recognition in autistic children in response to a name-calling event by a humanoid robot (NAO), under controlled experimental settings. The dataset comprises of around 50,000 facial frames extracted from video recordings of 15 children with ASD. A hybrid model combining a fine-tuned ResNet-50-based Convolutional Neural Network (CNN) and a three-layer Graph Convolutional Network (GCN) trained on both visual and geometric features extracted from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using a weighted ensemble of two models: DeepFace's and FER, each contributing to soft-label generation across seven emotion classes. Final classification leveraged a fused embedding optimized via Kullback-Leibler divergence. The proposed method demonstrates robust performance in modeling subtle affective responses and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts, as the pipeline effectively captures micro emotional cues in neurodivergent children, addressing a major gap in autism-specific HRI research. This work represents the first such large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.

</details>


### [49] [CineLOG: A Training Free Approach for Cinematic Long Video Generation](https://arxiv.org/abs/2512.12209)
*Zahra Dehghanian,Morteza Abolghasemi,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: CineLOG是一个包含5000个高质量视频片段的数据集，带有详细的场景描述、基于标准电影分类的相机指令和类型标签，旨在解决可控视频合成中精细控制不足和数据不平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 当前可控视频合成模型难以实现超越文本提示的精细控制，特别是在相机轨迹和电影类型等电影属性方面。现有数据集存在严重的数据不平衡、噪声标签或模拟到现实的差距问题。

Method: 提出了CineLOG数据集和新的生成管道，将复杂的文本到视频生成任务解耦为四个更简单的阶段，并引入了轨迹引导过渡模块来生成平滑的时空插值，实现连贯的多镜头序列。

Result: 广泛的人工评估显示，该管道在遵循特定相机和剧本指令方面显著优于最先进的端到端T2V模型，同时保持专业的视觉质量。

Conclusion: CineLOG数据集和提出的管道为解决可控视频合成中的精细控制问题提供了有效的解决方案，所有代码和数据均已公开。

Abstract: Controllable video synthesis is a central challenge in computer vision, yet current models struggle with fine grained control beyond textual prompts, particularly for cinematic attributes like camera trajectory and genre. Existing datasets often suffer from severe data imbalance, noisy labels, or a significant simulation to real gap. To address this, we introduce CineLOG, a new dataset of 5,000 high quality, balanced, and uncut video clips. Each entry is annotated with a detailed scene description, explicit camera instructions based on a standard cinematic taxonomy, and genre label, ensuring balanced coverage across 17 diverse camera movements and 15 film genres. We also present our novel pipeline designed to create this dataset, which decouples the complex text to video (T2V) generation task into four easier stages with more mature technology. To enable coherent, multi shot sequences, we introduce a novel Trajectory Guided Transition Module that generates smooth spatio-temporal interpolation. Extensive human evaluations show that our pipeline significantly outperforms SOTA end to end T2V models in adhering to specific camera and screenplay instructions, while maintaining professional visual quality. All codes and data are available at https://cine-log.pages.dev.

</details>


### [50] [Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking](https://arxiv.org/abs/2512.12218)
*Rheeya Uppaal,Phu Mon Htut,Min Bai,Nikolaos Pappas,Zheng Qi*

Main category: cs.CV

TL;DR: 提出评估视觉语言模型推理链视觉忠实性的框架，以及无需训练的轻量级自反思方法来检测和修复不忠实的感知步骤。


<details>
  <summary>Details</summary>
Motivation: 当前推理增强型视觉语言模型虽然能生成显式推理链，但存在新失败模式：模型可能通过视觉不忠实的中间步骤得到正确答案，或推理忠实但最终预测失败。仅评估最终答案准确率无法区分这些行为。

Method: 提出无需训练和参考的框架，将推理链分解为感知与推理步骤，使用现成VLM评估步骤级忠实性；基于此提出轻量级自反思程序，检测并局部重新生成不忠实的感知步骤。

Result: 在多个推理训练的VLM和感知密集型基准测试中，该方法降低了不忠实感知率，同时保持了最终答案准确率，提高了多模态推理的可靠性。

Conclusion: 视觉忠实性是推理链评估的重要维度，提出的框架和自反思方法能有效提高多模态推理的可靠性，为VLM评估提供了新视角。

Abstract: Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.

</details>


### [51] [Fine-Grained Zero-Shot Learning with Attribute-Centric Representations](https://arxiv.org/abs/2512.12219)
*Zhi Chen,Jingcai Guo,Taotao Cai,Yuxiang Cai*

Main category: cs.CV

TL;DR: 提出ACR框架，通过属性解耦学习解决零样本细粒度识别中的属性纠缠问题，在多个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 细粒度类别识别需要区分细微视觉差异，传统方法将颜色、形状、纹理等不同属性压缩到单一视觉嵌入中，导致属性纠缠和干扰，掩盖了关键区别特征。

Method: 提出属性中心表示(ACR)框架，包含两个混合专家组件：MoPE通过双级路由机制将图像块分派给专门专家处理；MoAE将专家精炼特征投影为稀疏的、部分感知的属性图。

Result: 在CUB、AwA2和SUN等零样本学习基准数据集上，ACR框架取得了持续的最先进结果。

Conclusion: 通过在表示学习中强制属性解耦，ACR框架有效解决了属性纠缠问题，为细粒度零样本识别提供了更鲁棒的解决方案。

Abstract: Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.

</details>


### [52] [ProImage-Bench: Rubric-Based Evaluation for Professional Image Generation](https://arxiv.org/abs/2512.12220)
*Minheng Ni,Zhengyuan Yang,Yaowen Zhang,Linjie Li,Chung-Ching Lin,Kevin Lin,Zhendong Wang,Xiaofei Wang,Shujie Liu,Lei Zhang,Wangmeng Zuo,Lijuan Wang*

Main category: cs.CV

TL;DR: 提出了ProImage-Bench基准测试，用于评估专业图像生成模型在科学精确插图生成方面的能力，通过自动化评估和反馈机制显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成模型主要关注视觉逼真度，但在生成信息密集、科学精确的专业插图方面存在不足，需要专门的评估方法来量化这一领域的进展。

Method: 构建了包含654个真实教材和技术报告中插图的基准数据集，使用大语言模型创建了包含6,076个标准和44,131个二进制检查的层次化评估标准，并开发了基于LMM的自动化评估系统。

Result: 测试多个文本到图像模型发现，即使在开放领域表现良好，最佳基础模型在标准准确性上仅达到0.791，标准得分仅0.553，显示科学保真度存在显著差距。通过反馈机制进行迭代优化，可将标准准确性从0.653提升到0.865，标准得分从0.388提升到0.697。

Conclusion: ProImage-Bench为专业图像生成提供了严格的诊断工具，同时为改进规范忠实性的科学插图提供了可扩展的监督信号，推动了该领域的发展。

Abstract: We study professional image generation, where a model must synthesize information-dense, scientifically precise illustrations from technical descriptions rather than merely produce visually plausible pictures. To quantify the progress, we introduce ProImage-Bench, a rubric-based benchmark that targets biology schematics, engineering/patent drawings, and general scientific diagrams. For 654 figures collected from real textbooks and technical reports, we construct detailed image instructions and a hierarchy of rubrics that decompose correctness into 6,076 criteria and 44,131 binary checks. Rubrics are derived from surrounding text and reference figures using large multimodal models, and are evaluated by an automated LMM-based judge with a principled penalty scheme that aggregates sub-question outcomes into interpretable criterion scores. We benchmark several representative text-to-image models on ProImage-Bench and find that, despite strong open-domain performance, the best base model reaches only 0.791 rubric accuracy and 0.553 criterion score overall, revealing substantial gaps in fine-grained scientific fidelity. Finally, we show that the same rubrics provide actionable supervision: feeding failed checks back into an editing model for iterative refinement boosts a strong generator from 0.653 to 0.865 in rubric accuracy and from 0.388 to 0.697 in criterion score. ProImage-Bench thus offers both a rigorous diagnostic for professional image generation and a scalable signal for improving specification-faithful scientific illustrations.

</details>


### [53] [Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs](https://arxiv.org/abs/2512.12222)
*Nathalie Alexander,Arnaud Gucciardi,Umberto Michelucci*

Main category: cs.CV

TL;DR: 该研究系统比较了SynthSeg和SamSeg两种方法在婴儿脑MRI分割中的准确性，发现SynthSeg在所有质量指标上均优于SamSeg，能提供更可靠的体积和分形维度估计。


<details>
  <summary>Details</summary>
Motivation: 婴儿脑MRI的准确分割对于量化发育变化至关重要，但持续的髓鞘形成和降低的组织对比度使自动分割特别具有挑战性。需要评估不同分割方法对体积和分形维度估计的影响。

Method: 使用Baby Open Brains数据集（71次扫描，1-9个月），比较SynthSeg和SamSeg两种分割方法。使用Dice系数、IoU、95% Hausdorff距离和归一化互信息等指标评估分割质量，并分析对体积和分形维度估计的影响。

Result: SynthSeg在所有质量指标上优于SamSeg（主要区域平均Dice > 0.8），体积估计与手动参考接近（平均+4%）。SamSeg系统性地高估脑室和全脑体积（平均+76%）。分割准确性随年龄增长而提高。分形维度分析显示SynthSeg与专家分割存在显著区域差异，分割相关的FD变异性超过大多数发育队列报告的组间差异。

Conclusion: SynthSeg为儿科MRI提供了最可靠的体积和分形维度结果，但由于分割相关的不确定性，对体积和FD的微小形态差异应谨慎解释。分割偏差直接影响FD估计，体积和FD偏差在结构间呈正相关。

Abstract: Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.

</details>


### [54] [Ultra-Low Bitrate Perceptual Image Compression with Shallow Encoder](https://arxiv.org/abs/2512.12229)
*Tianyu Zhang,Dong Liu,Chang Wen Chen*

Main category: cs.CV

TL;DR: 提出AEIC框架，使用浅层编码器配合一步扩散解码器，在超低比特率下实现高效编码和高质量重建


<details>
  <summary>Details</summary>
Motivation: 现有超低比特率图像压缩方法依赖大型预训练编码器，不适合部署在计算能力有限的边缘设备上，需要更轻量的编码方案

Method: 采用非对称架构：浅层编码器配合一步扩散解码器；设计双端特征蒸馏方案，将中等编码器知识迁移到浅层编码器变体

Result: 在超低比特率下优于现有方法的率失真感知性能；编码效率达35.8 FPS（1080P输入），解码速度与现有方法相当

Conclusion: AEIC框架证明了浅层编码器在超低比特率压缩中的可行性，实现了编码简单性和解码质量的双重目标

Abstract: Ultra-low bitrate image compression (below 0.05 bits per pixel) is increasingly critical for bandwidth-constrained and computation-limited encoding scenarios such as edge devices. Existing frameworks typically rely on large pretrained encoders (e.g., VAEs or tokenizer-based models) and perform transform coding within their generative latent space. While these approaches achieve impressive perceptual fidelity, their reliance on heavy encoder networks makes them unsuitable for deployment on weak sender devices. In this work, we explore the feasibility of applying shallow encoders for ultra-low bitrate compression and propose a novel Asymmetric Extreme Image Compression (AEIC) framework that pursues simultaneously encoding simplicity and decoding quality. Specifically, AEIC employs moderate or even shallow encoder networks, while leveraging an one-step diffusion decoder to maintain high-fidelity and high-realism reconstructions under extreme bitrates. To further enhance the efficiency of shallow encoders, we design a dual-side feature distillation scheme that transfers knowledge from AEIC with moderate encoders to its shallow encoder variants. Experiments demonstrate that AEIC not only outperforms existing methods on rate-distortion-perception performance at ultra-low bitrates, but also delivers exceptional encoding efficiency for 35.8 FPS on 1080P input images, while maintaining competitive decoding speed compared to existing methods.

</details>


### [55] [Moment and Highlight Detection via MLLM Frame Segmentation](https://arxiv.org/abs/2512.12246)
*I Putu Andika Bagas Jiwanta,Ayu Purwarianti*

Main category: cs.CV

TL;DR: 提出一种新颖方法，通过直接在LLM输出token上应用分割目标来检测视频时刻和亮点，使用"0"/"1"字符序列表示背景/前景概率，仅需25帧即可实现强性能


<details>
  <summary>Details</summary>
Motivation: 现有基于文本生成的方法无法为帧级预测提供直接梯度，而强化学习方法存在局限性。需要一种既能利用LLM推理能力，又能提供直接分割信号的方法

Method: 向LLM输入固定数量帧和提示，强制输出连续的"0"/"1"字符序列（每个字符对应一帧）。"0"表示背景，"1"表示前景概率。训练时结合分割损失和因果语言建模损失。推理时使用beam search生成序列和logits

Result: 在QVHighlights数据集上实现56.74 HIT@1的亮点检测性能，仅使用25帧（少于同类方法一半）。时刻检索得分35.28 MAP，超过基线。分割损失在因果LM损失平台期仍能提供稳定学习信号

Conclusion: 提出的方法成功将分割目标直接应用于LLM输出token，既能利用LLM的语言能力，又能获得帧级预测的直接梯度，在效率和性能上都表现出色

Abstract: Detecting video moments and highlights from natural-language queries have been unified by transformer-based methods. Other works use generative Multimodal LLM (MLLM) to predict moments and/or highlights as text timestamps, utilizing its reasoning capability. While effective, text-based generation cannot provide direct gradients for frame-level predictions because the model only emits language tokens. Although recent Reinforcement Learning (RL) methods attempt to address the issue, we propose a novel approach by applying segmentation objectives directly on the LLM's output tokens. The LLM is fed with a fixed number of frames alongside a prompt that enforces it to output a sequence of continuous "0" and/or "1" characters, with one character per frame. The "0"/"1" characters benefit from the LLM's inherent language capability while also acting as background and foreground probabilities, respectively. Training employs segmentation losses on the probabilities alongside a normal causal LM loss. At inference, beam search generates sequence and logits, acting as moments and saliency scores, respectively. Despite sampling only 25 frames -- less than half of comparable methods -- our method achieved strong highlight detection (56.74 HIT@1) on QVHighlights. Additionally, our efficient method scores above the baseline (35.28 MAP) for moment retrieval. Empirically, segmentation losses provide a stable complementary learning signal even when the causal LM loss plateaus.

</details>


### [56] [MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2512.12268)
*Yuqing Lei,Yingjun Du,Yawen Huang,Xiantong Zhen,Ling Shao*

Main category: cs.CV

TL;DR: MetaTPT：一种元学习框架，通过自监督辅助任务学习参数化增强，指导测试时提示调优，提升视觉语言模型在域偏移下的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有测试时提示调优（TPT）方法使用固定增强策略，在更具挑战性的域偏移场景中效果有限，需要更灵活、更具表达力的增强方法来捕捉目标域的关键特征。

Method: 提出MetaTPT框架，采用双循环优化范式：内循环学习自监督任务，为每个样本生成参数化增强视图；外循环通过强制这些视图间的一致性进行提示调优，将增强学习与提示调优耦合。

Result: 在域泛化和跨数据集基准测试中，MetaTPT取得了最先进的性能表现，显著提升了视觉语言模型在域偏移下的测试时适应能力。

Conclusion: MetaTPT通过元学习自监督辅助任务，实现了动态、参数化的增强策略，有效提升了测试时提示调优在挑战性域偏移场景中的鲁棒性和性能。

Abstract: Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.

</details>


### [57] [Feature Aggregation for Efficient Continual Learning of Complex Facial Expressions](https://arxiv.org/abs/2512.12277)
*Thibault Geoffroy,Myriam Maumy,Lionel Prevost*

Main category: cs.CV

TL;DR: 提出一个用于面部表情识别的持续学习混合框架，结合深度卷积特征和面部动作单元，使用贝叶斯高斯混合模型来减轻灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在日常生活中日益普及，识别和适应人类情感对于有效的人机交互至关重要。面部表情识别是推断情感状态的主要渠道，但情感的动态和文化细微性需要能够持续学习而不遗忘先前知识的模型。

Method: 提出一个混合框架，整合两种互补模态：深度卷积特征和来自面部动作编码系统的面部动作单元。使用贝叶斯高斯混合模型对组合表示进行建模，提供轻量级的概率解决方案，避免重新训练同时保持强判别能力。

Result: 在Compound Facial Expression of Emotion数据集上的实验表明，该模型能够先学习基本表情，然后逐步识别复合表情。实验展示了改进的准确性、更强的知识保留和减少的遗忘。

Conclusion: 该框架有助于开发具有情感智能的AI系统，可应用于教育、医疗保健和自适应用户界面。

Abstract: As artificial intelligence (AI) systems become increasingly embedded in our daily life, the ability to recognize and adapt to human emotions is essential for effective human-computer interaction. Facial expression recognition (FER) provides a primary channel for inferring affective states, but the dynamic and culturally nuanced nature of emotions requires models that can learn continuously without forgetting prior knowledge. In this work, we propose a hybrid framework for FER in a continual learning setting that mitigates catastrophic forgetting. Our approach integrates two complementary modalities: deep convolutional features and facial Action Units (AUs) derived from the Facial Action Coding System (FACS). The combined representation is modelled through Bayesian Gaussian Mixture Models (BGMMs), which provide a lightweight, probabilistic solution that avoids retraining while offering strong discriminative power. Using the Compound Facial Expression of Emotion (CFEE) dataset, we show that our model can first learn basic expressions and then progressively recognize compound expressions. Experiments demonstrate improved accuracy, stronger knowledge retention, and reduced forgetting. This framework contributes to the development of emotionally intelligent AI systems with applications in education, healthcare, and adaptive user interfaces.

</details>


### [58] [Cognitive-YOLO: LLM-Driven Architecture Synthesis from First Principles of Data for Object Detection](https://arxiv.org/abs/2512.12281)
*Jiahao Zhao*

Main category: cs.CV

TL;DR: Cognitive-YOLO：基于LLM的架构合成框架，直接从数据集特征生成目标检测网络配置，无需传统搜索循环


<details>
  <summary>Details</summary>
Motivation: 传统手动设计耗时耗力，NAS计算成本高，现有LLM方法多作为搜索循环中的优化器而非直接从数据整体理解生成架构

Method: 三阶段框架：1) 分析模块提取数据集元特征；2) LLM基于特征+RAG检索的最先进组件推理，生成NADL结构化描述；3) 编译器实例化为可部署模型

Result: 在五个目标检测数据集上表现优异，性能竞争力强且参数效率高，消融实验证明LLM的数据驱动推理是性能主要驱动力

Conclusion: 数据"第一性原理"的深度理解比单纯检索SOTA组件更重要，LLM驱动的架构合成能直接生成高性能检测网络

Abstract: Designing high-performance object detection architectures is a complex task, where traditional manual design is time-consuming and labor-intensive, and Neural Architecture Search (NAS) is computationally prohibitive. While recent approaches using Large Language Models (LLMs) show promise, they often function as iterative optimizers within a search loop, rather than generating architectures directly from a holistic understanding of the data. To address this gap, we propose Cognitive-YOLO, a novel framework for LLM-driven architecture synthesis that generates network configurations directly from the intrinsic characteristics of the dataset. Our method consists of three stages: first, an analysis module extracts key meta-features (e.g., object scale distribution and scene density) from the target dataset; second, the LLM reasons upon these features, augmented with state-of-the-art components retrieved via Retrieval-Augmented Generation (RAG), to synthesize the architecture into a structured Neural Architecture Description Language (NADL); finally, a compiler instantiates this description into a deployable model. Extensive experiments on five diverse object detection datasets demonstrate that our proposed Cognitive-YOLO consistently generates superior architectures, achieving highly competitive performance and demonstrating a superior performance-per-parameter trade-off compared to strong baseline models across multiple benchmarks. Crucially, our ablation studies prove that the LLM's data-driven reasoning is the primary driver of performance, demonstrating that a deep understanding of data "first principles" is more critical for achieving a superior architecture than simply retrieving SOTA components.

</details>


### [59] [RealDrag: The First Dragging Benchmark with Real Target Image](https://arxiv.org/abs/2512.12287)
*Ahmad Zafarani,Zahra Dehghanian,Mohammadreza Davoodi,Mohsen Shadroo,MohammadAmin Fazli,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: RealDrag是首个包含真实目标图像的基于点拖拽图像编辑基准数据集，包含400+人工标注样本，并提出4个新指标来量化编辑质量，评估了17个SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于拖拽的图像编辑模型评估不可靠，缺乏标准化基准和指标，主要问题在于不一致的评估协议和缺乏包含真实目标图像的数据集，使得不同方法难以客观比较。

Method: 提出RealDrag基准：1）包含400+人工标注的多样化视频源样本，提供源/目标图像、处理/目标点、可编辑区域掩码和描述性标注；2）提出四个任务特定指标：语义距离(SeD)、外部掩码保持分数(OMPS)、内部补丁保持分数(IPPS)和方向相似性(DiS)。

Result: 使用该基准对17个SOTA模型进行了首次大规模系统分析，揭示了当前方法之间的明显权衡，并建立了稳健、可复现的基线来指导未来研究。

Conclusion: RealDrag提供了首个包含真实目标图像的基于点拖拽图像编辑基准，通过新指标和系统评估建立了可靠的评估框架，数据集和评估工具将公开可用。

Abstract: The evaluation of drag based image editing models is unreliable due to a lack of standardized benchmarks and metrics. This ambiguity stems from inconsistent evaluation protocols and, critically, the absence of datasets containing ground truth target images, making objective comparisons between competing methods difficult. To address this, we introduce \textbf{RealDrag}, the first comprehensive benchmark for point based image editing that includes paired ground truth target images. Our dataset contains over 400 human annotated samples from diverse video sources, providing source/target images, handle/target points, editable region masks, and descriptive captions for both the image and the editing action.
  We also propose four novel, task specific metrics: Semantical Distance (SeD), Outer Mask Preserving Score (OMPS), Inner Patch Preserving Score (IPPS), and Directional Similarity (DiS). These metrics are designed to quantify pixel level matching fidelity, check preservation of non edited (out of mask) regions, and measure semantic alignment with the desired task. Using this benchmark, we conduct the first large scale systematic analysis of the field, evaluating 17 SOTA models. Our results reveal clear trade offs among current approaches and establish a robust, reproducible baseline to guide future research. Our dataset and evaluation toolkit will be made publicly available.

</details>


### [60] [GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search](https://arxiv.org/abs/2512.12296)
*Hyunju Lee,Youngmin Oh,Jeimin Jeon,Donghyeon Baek,Bumsub Ham*

Main category: cs.CV

TL;DR: 提出GrowTAS渐进式训练框架，从训练小规模子网开始逐步加入大规模子网，减少权重共享带来的干扰，提升Transformer架构搜索性能


<details>
  <summary>Details</summary>
Motivation: 现有Transformer架构搜索方法中，所有候选子网共享同一组权重，导致小规模子网受到严重干扰。研究发现训练良好的小子网可以作为训练大子网的良好基础

Method: 提出GrowTAS渐进式训练框架：1）从小规模子网开始训练；2）逐步加入更大规模的子网；3）减少干扰并稳定训练过程。还提出GrowTAS+，仅微调部分权重以进一步提升大规模子网性能

Result: 在ImageNet和多个迁移学习基准测试（CIFAR-10/100、Flowers、CARS、INAT-19）上的广泛实验表明，该方法优于当前TAS方法

Conclusion: 渐进式训练框架能有效减少权重共享带来的干扰，提升Transformer架构搜索的性能，特别是在小规模子网和大规模子网之间

Abstract: Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods

</details>


### [61] [From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving](https://arxiv.org/abs/2512.12302)
*Huan Zheng,Yucheng Zhou,Tianyi Yan,Jiayi Su,Hongjun Chen,Dubing Chen,Wencheng Han,Runzhou Tao,Zhongying Qiu,Jianfei Yang,Jianbing Shen*

Main category: cs.CV

TL;DR: 本文提出了Intention-Drive，首个评估自动驾驶系统将高级人类意图转化为安全精确驾驶动作能力的基准测试，包含新数据集和基于意图成功率(ISR)的评估协议。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶系统仅能执行低级转向指令，而实现真正智能自主需要从命令执行者转变为意图理解者。然而，缺乏标准化基准来测量和推动这一复杂任务的进展。

Method: 提出Intention-Drive基准，包含两个核心贡献：1）包含复杂场景和对应自然语言意图的新数据集；2）基于意图成功率(ISR)的新评估协议，评估语义目标实现而不仅是几何精度。

Result: 通过对一系列基线模型在Intention-Drive上的广泛评估，发现显著性能缺陷，基线模型难以达到这一高级任务所需的全面场景和意图理解能力。

Conclusion: Intention-Drive填补了评估自动驾驶系统从命令跟随到意图实现能力的关键空白，揭示了当前模型在高级意图理解方面的不足，为未来研究提供了重要基准。

Abstract: Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.

</details>


### [62] [OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2512.12303)
*Yang Ou,Xiongwei Zhao,Xinye Yang,Yihan Wang,Yicheng Di,Rong Yuan,Xieyuanli Chen,Xu Zhu*

Main category: cs.CV

TL;DR: OMUDA提出了一种用于无监督域自适应语义分割的统一框架，通过层次化掩码策略解决跨域上下文模糊、特征表示不一致和类别伪标签噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域自适应方法在语义分割中面临三个主要挑战：跨域上下文模糊、不一致的特征表示以及类别伪标签噪声，这些限制了模型在目标域上的泛化能力。

Method: 提出OMUDA框架，包含三个层次化掩码策略：1) 上下文感知掩码(CAM)区分前景与背景以平衡全局上下文和局部细节；2) 特征蒸馏掩码(FDM)通过预训练模型知识转移增强鲁棒特征学习；3) 类别解耦掩码(CDM)通过显式建模类别不确定性减轻噪声伪标签影响。

Result: 在多个跨域语义分割基准测试中验证了有效性，特别是在SYNTHIA->Cityscapes和GTA5->Cityscapes任务上，OMUDA可无缝集成到现有UDA方法中，平均提升7%，达到最先进性能。

Conclusion: OMUDA通过层次化掩码策略在上下文、表示和类别三个层面有效减少域偏移，为无监督域自适应语义分割提供了超越现有方法的统一解决方案。

Abstract: Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA->Cityscapes and GTA5->Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.

</details>


### [63] [MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding](https://arxiv.org/abs/2512.12307)
*Benjamin Beilharz,Thomas S. A. Wallis*

Main category: cs.CV

TL;DR: MRD方法使用基于物理的可微分渲染来探索视觉模型对3D场景属性的隐式理解，通过寻找产生相同模型激活但物理上不同的3D场景参数（模型同构体）。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在视觉任务上取得了显著成功，但理解这些模型的表示和决策仍然困难。视觉模型通常在2D输入上训练，但被认为发展了对底层3D场景的隐式理解。需要一种方法来探测模型对生成性3D场景属性的理解。

Method: MRD方法使用基于物理的可微分渲染，寻找在物理上不同但产生相同模型激活的3D场景参数（模型同构体）。这种方法始终基于物理场景描述，可以单独探测模型对特定场景属性（如物体形状、材质）的敏感性。

Result: 在几何（形状）和双向反射分布函数（材质）等场景参数恢复方面，目标场景和优化场景之间显示出高度的模型激活相似性，但视觉结果有所不同。这些重建有助于定性地研究模型对哪些物理场景属性敏感或不敏感。

Conclusion: MRD方法有望通过分析物理场景参数如何驱动模型响应变化，增进我们对计算机视觉和人类视觉的理解。这种方法为探索视觉模型的隐式3D场景理解提供了新的分析工具。

Abstract: While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.

</details>


### [64] [WeDetect: Fast Open-Vocabulary Object Detection as Retrieval](https://arxiv.org/abs/2512.12309)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: WeDetect是一个基于检索哲学的开集目标检测模型家族，包含三个组件：基础检测器WeDetect、通用提议生成器WeDetect-Uni和基于LMM的指代表达理解模型WeDetect-Ref，在15个基准测试中实现SOTA性能和高推理效率。


<details>
  <summary>Details</summary>
Motivation: 探索无跨模态融合层的检索式开集目标检测方法的优势，这种非融合方法通过将识别视为检索问题（在共享嵌入空间中匹配区域和文本查询），在效率和多功能性方面具有独特优势。

Method: 1. WeDetect：双塔架构的实时检测器，通过精心策划的数据和完整训练实现非融合检测；2. WeDetect-Uni：基于WeDetect的通用提议生成器，冻结检测器仅微调物体性提示来生成跨类别通用提议；3. WeDetect-Ref：基于LMM的物体分类器，处理复杂指代表达，从WeDetect-Uni提取的提议列表中检索目标物体。

Result: WeDetect超越其他融合模型，建立强大的开集检测基础；WeDetect-Uni支持历史数据中的物体检索新应用；整个模型家族在15个基准测试中实现最先进性能，同时保持高推理效率。

Conclusion: WeDetect模型家族在统一的检索框架下整合了检测、提议生成、物体检索和指代表达理解，展示了检索哲学在开集目标检测中的效率和多功能性优势。

Abstract: Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.

</details>


### [65] [Unified Control for Inference-Time Guidance of Denoising Diffusion Models](https://arxiv.org/abs/2512.12339)
*Maurya Goyal,Anuj Singh,Hadi Jamali-Rad*

Main category: cs.CV

TL;DR: 提出UniCoDe统一框架，将采样方法和梯度引导方法结合，提高扩散模型与下游目标的对齐效率


<details>
  <summary>Details</summary>
Motivation: 扩散模型输出与下游目标对齐对提升任务性能至关重要，现有推理时免训练方法分为采样方法和梯度引导方法，各有优缺点，需要统一框架结合两者优势

Method: 提出UniCoDe通用算法，在采样过程中整合局部梯度信号，将采样方法和梯度引导方法统一到一个框架中，解决复杂奖励采样方法的采样效率问题

Result: UniCoDe在多种任务上与最先进基线保持竞争力，在奖励对齐和扩散无条件先验偏离之间实现更好的权衡，同时提高采样效率

Conclusion: UniCoDe成功地将采样和梯度引导两种范式统一起来，为扩散模型与下游目标对齐提供了更高效的解决方案

Abstract: Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe

</details>


### [66] [TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection](https://arxiv.org/abs/2512.12357)
*Zishen Song,Yongjian Zhu,Dong Wang,Hongzhan Liu,Lingyu Jiang,Yongxing Duan,Zehua Zhang,Sihan Li,Jiarui Li*

Main category: cs.CV

TL;DR: 提出TCLeaf-Net，一种针对田间叶片病害检测的transformer-convolution混合检测器，结合新发布的Daylily-Leaf数据集，解决复杂背景、下采样信息损失和多尺度病变检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 田间叶片病害检测面临复杂背景干扰、领域偏移和病变级别数据集有限的挑战，需要开发更鲁棒的检测模型。

Method: 提出TCLeaf-Net检测器，包含三个核心模块：1) TCM模块结合全局上下文和局部卷积抑制非叶片区域；2) RSFRS块通过双线性重采样和卷积保留空间细节；3) DFPN模块使用可变形对齐和多感受野感知增强多尺度融合。

Result: 在Daylily-Leaf数据集田间分割上，mAP@50提升5.4个百分点至78.2%，计算量减少7.5 GFLOPs，GPU内存使用降低8.7%。在PlantDoc、Tomato-Leaf和Rice-Leaf数据集上也表现出色。

Conclusion: TCLeaf-Net在复杂田间环境下实现了高效准确的叶片病害检测，具有较强的鲁棒性和泛化能力，为农业病害监测提供了有效解决方案。

Abstract: Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.

</details>


### [67] [VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding](https://arxiv.org/abs/2512.12360)
*Yufei Yin,Qianke Meng,Minghao Chen,Jiajun Ding,Zhenwei Shao,Zhou Yu*

Main category: cs.CV

TL;DR: VideoARM：一种用于长视频理解的智能体推理-分层记忆范式，通过自适应、实时的智能体推理和记忆构建，显著减少token消耗并提升性能


<details>
  <summary>Details</summary>
Motivation: 长视频理解面临挑战，因为其具有扩展的时间结构和密集的多模态线索。现有方法要么依赖手工设计的推理流程，要么使用消耗大量token的视频预处理来指导MLLMs进行自主推理，存在局限性。

Method: 提出VideoARM（Agentic Reasoning-over-hierarchical-Memory）范式，采用自适应、实时的智能体推理和记忆构建。系统执行观察、思考、行动和记忆的连续循环，控制器自主调用工具以粗到细的方式解释视频。同时，分层多模态记忆持续捕获和更新多级线索，为控制器决策提供精确的上下文信息。

Result: 在主流基准测试中，VideoARM优于当前最先进的方法DVD，同时显著减少了长视频的token消耗。

Conclusion: VideoARM通过智能体推理和分层记忆的范式，有效解决了长视频理解的挑战，在减少计算资源消耗的同时提升了性能，为长视频理解提供了新的解决方案。

Abstract: Long-form video understanding remains challenging due to the extended temporal structure and dense multimodal cues. Despite recent progress, many existing approaches still rely on hand-crafted reasoning pipelines or employ token-consuming video preprocessing to guide MLLMs in autonomous reasoning. To overcome these limitations, we introduce VideoARM, an Agentic Reasoning-over-hierarchical-Memory paradigm for long-form video understanding. Instead of static, exhaustive preprocessing, VideoARM performs adaptive, on-the-fly agentic reasoning and memory construction. Specifically, VideoARM performs an adaptive and continuous loop of observing, thinking, acting, and memorizing, where a controller autonomously invokes tools to interpret the video in a coarse-to-fine manner, thereby substantially reducing token consumption. In parallel, a hierarchical multimodal memory continuously captures and updates multi-level clues throughout the operation of the agent, providing precise contextual information to support the controller in decision-making. Experiments on prevalent benchmarks demonstrate that VideoARM outperforms the state-of-the-art method, DVD, while significantly reducing token consumption for long-form videos.

</details>


### [68] [STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative](https://arxiv.org/abs/2512.12372)
*Peixuan Zhang,Zijian Jia,Kaiqi Liu,Shuchen Weng,Si Li,Boxin Shi*

Main category: cs.CV

TL;DR: STAGE提出了一种基于故事板的多镜头视频生成工作流，通过预测结构化的故事板（包含每个镜头的起止帧对）来改善跨镜头一致性和电影语言表达，优于传统关键帧方法。


<details>
  <summary>Details</summary>
Motivation: 虽然生成模型在视频合成方面取得了显著进展，但创建连贯的多镜头叙事仍然是一个重大挑战。现有的关键帧方法通常无法保持跨镜头一致性，也难以捕捉电影语言。

Method: STAGE采用故事板锚定的生成工作流：1) STEP2预测结构化的故事板（每个镜头的起止帧对）；2) 引入多镜头记忆包确保长范围实体一致性；3) 双编码策略保证镜头内连贯性；4) 两阶段训练方案学习电影化的镜头间过渡。还贡献了ConStoryBoard数据集。

Result: 大量实验表明，STAGE在结构化叙事控制和跨镜头连贯性方面实现了卓越性能。

Conclusion: STAGE通过故事板锚定的方法有效解决了多镜头视频生成中的跨镜头一致性和电影语言表达问题，为结构化叙事控制提供了新思路。

Abstract: While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.

</details>


### [69] [V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping](https://arxiv.org/abs/2512.12375)
*Hyunkoo Lee,Wooseok Jang,Jini Yang,Taehwan Kim,Sangoh Kim,Sangwon Jung,Seungryong Kim*

Main category: cs.CV

TL;DR: V-Warper：无需训练的粗到细视频个性化框架，通过轻量级外观适配和推理时细粒度外观注入，提升身份保真度，无需大规模视频微调。


<details>
  <summary>Details</summary>
Motivation: 现有视频个性化方法依赖大量视频微调或大规模视频数据集，计算成本高且难以扩展，同时在帧间保持细粒度外观一致性方面存在困难。

Method: 两阶段框架：1）轻量级粗外观适配阶段，仅使用少量参考图像，通过图像LoRA和主题嵌入适配编码全局身份；2）推理时细外观注入阶段，通过RoPE-free中层查询-键特征计算语义对应关系，引导外观丰富的值表示到生成过程的语义对齐区域。

Result: V-Warper显著提升外观保真度，同时保持提示对齐和运动动态，无需大规模视频微调即可高效实现这些改进。

Conclusion: V-Warper通过训练免费的粗到细框架解决了视频个性化的计算成本和细粒度一致性挑战，在保持运动动态的同时显著提升身份保真度。

Abstract: Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.

</details>


### [70] [M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction](https://arxiv.org/abs/2512.12378)
*Junqiao Fan,Yunjiao Zhou,Yizhuo Yang,Xinyuan Cui,Jiarui Zhang,Lihua Xie,Jianfei Yang,Chris Xiaoxuan Lu,Fangqiang Ding*

Main category: cs.CV

TL;DR: M4Human是目前最大规模的多模态人体网格重建基准数据集，包含66.1万帧高分辨率毫米波雷达、RGB和深度数据，提供原始雷达张量和处理后的雷达点云，支持不同粒度RF信号研究。


<details>
  <summary>Details</summary>
Motivation: 现有HMR数据集主要依赖RGB视觉输入，但存在遮挡、光照变化和隐私问题。雷达传感能提供隐私保护的室内人体感知，但现有雷达数据集存在骨架标签稀疏、规模有限、动作简单等问题。

Method: 构建M4Human多模态基准数据集，包含20个受试者和50种多样化动作（原地、坐姿、自由空间运动）。提供原始雷达张量和处理后的雷达点云两种数据格式，并配有高质量动作捕捉标注（3D网格和全局轨迹）。

Result: M4Human是目前最大规模的雷达人体网格重建数据集（比之前最大数据集大9倍），建立了雷达张量和雷达点云两种模态的基准，并探索了与RGB-D的多模态融合。实验结果表明该数据集对雷达人体建模具有重要意义，同时揭示了快速无约束运动下的持续挑战。

Conclusion: M4Human为HMR研究社区提供了重要的多模态基准资源，推动了隐私保护的人体感知研究，数据集和代码将在论文发表后开源。

Abstract: Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.

</details>


### [71] [Speedrunning ImageNet Diffusion](https://arxiv.org/abs/2512.12386)
*Swayam Bhanded*

Main category: cs.CV

TL;DR: SR-DiT框架通过整合多种技术（token路由、架构改进、训练优化）在表示对齐基础上，仅用140M参数模型在400K迭代下达到SOTA性能，媲美更大模型效果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散变换器训练效率提升技术多孤立研究，缺乏多种方法协同作用的探索。本文旨在系统整合不同技术，发掘潜在协同效应，建立高效基准框架。

Method: 提出SR-DiT框架，在表示对齐基础上系统整合：1）token路由技术，2）架构改进，3）训练修改。通过消融实验分析不同技术组合的有效性、协同性和兼容性。

Result: 在ImageNet-256上仅用140M参数模型、400K迭代、无需分类器引导，达到FID 3.49和KDD 0.319，性能媲美685M参数模型。这是该模型尺寸下的SOTA结果。

Conclusion: SR-DiT展示了多种技术整合的显著协同效应，为未来研究提供了计算友好的基准框架。消融研究揭示了技术组合的有效模式和兼容性问题。

Abstract: Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.

</details>


### [72] [ArtGen: Conditional Generative Modeling of Articulated Objects in Arbitrary Part-Level States](https://arxiv.org/abs/2512.12395)
*Haowen Wang,Xiaoping Yuan,Fugang Zhang,Rui Jian,Yuanwei Zhu,Xiuquan Qiao,Yakun Huang*

Main category: cs.CV

TL;DR: ArtGen：基于扩散模型的框架，从单视图图像或文本描述生成具有准确几何和连贯运动学的铰接式3D物体，通过跨状态蒙特卡洛采样和思维链推理解决结构-运动纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型通常依赖闭合状态的单视图输入，导致几何形状和关节动力学纠缠，产生模糊或不现实的运动学结构。铰接式资产生成对机器人、数字孪生和具身智能至关重要。

Method: 1) 跨状态蒙特卡洛采样显式强制执行全局运动学一致性；2) 思维链推理模块推断结构先验（部件语义、关节类型、连接性）；3) 稀疏专家扩散变换器专门处理多样化运动学交互；4) 局部-全局注意力增强的组合式3D-VAE潜在先验。

Result: 在PartNet-Mobility基准测试中，ArtGen显著优于现有最先进方法，能够生成具有准确几何和连贯运动学的铰接式3D物体。

Conclusion: ArtGen通过解耦结构-运动纠缠，实现了从单视图图像或文本描述生成铰接式3D物体的能力，为机器人、数字孪生和具身智能应用提供了有效的解决方案。

Abstract: Generating articulated assets is crucial for robotics, digital twins, and embodied intelligence. Existing generative models often rely on single-view inputs representing closed states, resulting in ambiguous or unrealistic kinematic structures due to the entanglement between geometric shape and joint dynamics. To address these challenges, we introduce ArtGen, a conditional diffusion-based framework capable of generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions at arbitrary part-level states. Specifically, ArtGen employs cross-state Monte Carlo sampling to explicitly enforce global kinematic consistency, reducing structural-motion entanglement. Additionally, we integrate a Chain-of-Thought reasoning module to infer robust structural priors, such as part semantics, joint types, and connectivity, guiding a sparse-expert Diffusion Transformer to specialize in diverse kinematic interactions. Furthermore, a compositional 3D-VAE latent prior enhanced with local-global attention effectively captures fine-grained geometry and global part-level relationships. Extensive experiments on the PartNet-Mobility benchmark demonstrate that ArtGen significantly outperforms state-of-the-art methods.

</details>


### [73] [A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams](https://arxiv.org/abs/2512.12410)
*Khalfalla Awedat,Mohamed Abidalrekab,Mohammad El-Yabroudi*

Main category: cs.CV

TL;DR: 基于图注意力网络（GAT）的框架，仅使用当前LiDAR帧重建缺失的垂直光束，无需相机图像或时序信息，在模拟通道丢失的KITTI数据上实现11.67厘米的平均高度RMSE。


<details>
  <summary>Details</summary>
Motivation: LiDAR传感器因硬件老化、灰尘、雪、雾或强反射导致的垂直光束丢失会移除点云中的整个垂直切片，严重降低自动驾驶车辆的3D感知能力。

Method: 将LiDAR扫描表示为非结构化空间图：点作为节点，边连接邻近点并保留原始光束索引顺序。使用多层GAT学习局部几何邻域的自适应注意力权重，直接回归缺失位置的高程（z）值。

Result: 在1,065个模拟通道丢失的原始KITTI序列上训练和评估，平均高度RMSE为11.67厘米，87.98%的重建点误差在10厘米内。单GPU推理每帧14.65秒，重建质量对不同邻域大小k保持稳定。

Conclusion: 纯图注意力模型仅基于原始点云几何就能有效恢复实际传感器退化下的丢失垂直光束，证明了该方法的有效性。

Abstract: Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.

</details>


### [74] [ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics](https://arxiv.org/abs/2512.12424)
*Tue-Thu Van-Dinh,Hoang-Duy Tran,Truong-Binh Duong,Mai-Hanh Pham,Binh-Nam Le-Nguyen,Quoc-Thai Nguyen*

Main category: cs.CV

TL;DR: 首个越南语信息图视觉问答基准ViInfographicVQA，包含6747个真实信息图和20409个人工验证问答对，评估模型在数据丰富、布局复杂的视觉内容上的阅读推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有VQA基准主要针对场景文本或自然图像，缺乏对信息图这种结合文本、图表、图标和设计元素的复杂视觉内容的评估。越南语等低资源语言尤其缺乏此类基准，需要评估模型在OCR、布局理解、数值和语义推理方面的综合能力。

Method: 创建包含6747个真实世界信息图的越南语基准，涵盖经济、医疗、教育等多个领域，包含20409个人工验证的问答对。设计两种评估设置：单图像任务（传统VQA设置）和多图像任务（需要跨多个语义相关信息图整合证据）。

Result: 评估了多种最新的视觉语言模型，发现存在显著的性能差距。最严重的错误出现在多图像问题上，特别是涉及跨图像整合和非跨度推理的任务。基准结果揭示了当前多模态模型在低资源上下文中的局限性。

Conclusion: ViInfographicVQA为越南语信息图VQA提供了首个基准，揭示了当前模型在布局感知和跨图像推理方面的不足，鼓励未来研究开发更有效的布局感知和跨图像推理方法，特别是在低资源语言环境中。

Abstract: Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.

</details>


### [75] [BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation](https://arxiv.org/abs/2512.12425)
*Hangwei Zhang,Armando Teles Fortes,Tianyi Wei,Xingang Pan*

Main category: cs.CV

TL;DR: BokehDepth：两阶段框架，通过解耦景深合成与深度预测，利用散焦作为无监督几何线索，提升景深渲染质量和单目深度估计精度


<details>
  <summary>Details</summary>
Motivation: 当前方法未能充分利用景深与单目深度估计之间的紧密关联。高质量景深渲染依赖噪声深度图导致伪影，而现代单目深度模型在弱纹理、远距离和几何模糊区域表现不佳，这些区域恰恰是散焦线索最丰富的地方

Method: 两阶段框架：1）基于预训练图像编辑骨干的物理引导可控景深生成器，从单张清晰输入产生无深度信息的校准景深堆栈；2）轻量级散焦感知聚合模块，集成到现有单目深度编码器中，沿散焦维度融合特征，暴露稳定的深度敏感变化

Result: 在多个挑战性基准测试中，BokehDepth相比基于深度图的景深基线提升了视觉保真度，并持续增强了强单目深度基础模型的度量精度和鲁棒性

Conclusion: 通过解耦景深合成与深度预测，将散焦作为辅助的无监督几何线索，BokehDepth框架能够同时提升景深渲染质量和单目深度估计性能，实现了两种任务的协同优化

Abstract: Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.

</details>


### [76] [Endless World: Real-Time 3D-Aware Long Video Generation](https://arxiv.org/abs/2512.12430)
*Ke Zhang,Yiqun Mei,Jiacong Xu,Vishal M. Patel*

Main category: cs.CV

TL;DR: Endless World是一个实时无限3D一致视频生成框架，通过条件自回归训练和全局3D感知注意力机制，在单GPU上实现实时推理，生成长序列稳定视频。


<details>
  <summary>Details</summary>
Motivation: 当前生成长序列、具有稳定3D结构的连贯视频仍然是一个主要挑战，特别是在流式场景中。为了解决这个问题，需要开发能够生成无限长度、保持3D一致性的实时视频生成方法。

Method: 1. 条件自回归训练策略：将新生成内容与现有视频帧对齐，保持长距离依赖同时保持计算效率；2. 全局3D感知注意力：提供跨时间的连续几何指导；3. 3D注入机制：在整个扩展序列中强制执行物理合理性和几何一致性。

Result: 实验表明Endless World能够生成长、稳定且视觉连贯的视频，在视觉保真度和空间一致性方面达到或优于现有方法的性能，支持在单GPU上进行实时推理。

Conclusion: Endless World成功解决了长序列3D一致视频生成的挑战，通过创新的训练策略和3D感知机制，实现了无限长度、实时、高质量的流式视频生成。

Abstract: Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.

</details>


### [77] [From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields](https://arxiv.org/abs/2512.12459)
*Jiachen Tao,Benjamin Planche,Van Nguyen Nguyen,Junyi Wu,Yuchun Liu,Haoxuan Wang,Zhongpai Gao,Gengyu Zhang,Meng Zheng,Feiran Wang,Anwesa Choudhuri,Zhenghao Zhao,Weitai Kang,Terrence Chen,Yan Yan,Ziyan Wu*

Main category: cs.CV

TL;DR: 提出Gaussian Photon Field (GPF)，将光子映射重构为可学习的连续辐射函数，通过3D高斯基元编码光子分布，实现多视角渲染的加速，在保持光子级精度的同时大幅减少计算量。


<details>
  <summary>Details</summary>
Motivation: 传统光子映射在多视角渲染中存在计算效率低的问题，因为每个视角都需要独立进行光子追踪和随机核估计，导致大量冗余计算。需要一种能够复用光子分布信息、加速多视角渲染的方法。

Method: 提出Gaussian Photon Field (GPF)表示方法，使用各向异性的3D高斯基元（参数包括位置、旋转、尺度和光谱）编码光子分布。从第一次SPPM迭代的物理追踪光子初始化，通过多视角最终辐射监督进行优化，将基于光子的光传输蒸馏为连续场。

Result: 在包含复杂光传输（如焦散和镜面-漫反射交互）的场景上进行广泛实验，GPF在保持光子级精度的同时，将计算量减少了数个数量级。

Conclusion: GPF统一了基于光子的渲染的物理严谨性和神经场景表示的效率，实现了可微分的辐射评估，无需重复光子追踪或迭代优化。

Abstract: Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.

</details>


### [78] [StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding](https://arxiv.org/abs/2512.12560)
*Xinqi Jin,Hanxun Yu,Bohan Yu,Kebin Liu,Jian Liu,Keda Tao,Yixuan Pei,Huan Wang,Fan Dang,Jiangchuan Liu,Weiqiang Wang*

Main category: cs.CV

TL;DR: 提出MSSAVT冗余度量与掩码剪枝策略，通过视频token剪枝减少MLLMs计算开销，在多个视频理解基准上提升精度4%且剪枝延迟小于1ms


<details>
  <summary>Details</summary>
Motivation: 在线视频理解应用（如公共监控、AI眼镜）需要MLLMs处理大量视频帧，导致GPU内存占用高、计算延迟大，需要高效减少上下文长度的方法

Method: 提出MSSAVT冗余度量（考虑token相似性和空间位置），设计掩码剪枝策略避免双向依赖，结合现有时间冗余剪枝方法消除视频模态的时间冗余

Result: 在多个在线和离线视频理解基准测试中，方法显著提升精度（最高4%），同时剪枝延迟可忽略不计（小于1ms）

Conclusion: 提出的token剪枝方法能有效减少MLLMs处理视频的计算负担，在保持关键信息的同时提升性能，为在线视频理解应用提供实用解决方案

Abstract: Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.

</details>


### [79] [More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models](https://arxiv.org/abs/2512.12487)
*Hoang Anh Just,Yifei Fan,Handong Zhao,Jiuxiang Gu,Ruiyi Zhang,Simon Jenni,Kushal Kafle,Ruoxi Jia,Jing Shi*

Main category: cs.CV

TL;DR: PeRL-VL提出了一种解耦框架，分别改进视觉语言模型的视觉感知和文本推理能力，通过引入基于VLM的描述奖励和文本推理SFT阶段，显著提升了多模态基准测试性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于可验证奖励的强化学习（RLVR）训练的视觉语言模型存在两个主要问题：1）不准确的视觉提取（遗漏或幻觉细节）；2）逻辑不一致的思维链。这些问题主要是因为可验证信号只监督最终答案，而无法有效指导中间过程。

Method: PeRL-VL采用解耦框架：1）感知方面：引入基于VLM的描述奖励，对模型自生成的图像描述进行忠实性和充分性评分；2）推理方面：在逻辑丰富的思维链数据上添加纯文本推理SFT阶段，独立于视觉增强逻辑一致性和连贯性。

Result: 在多样化的多模态基准测试中，PeRL-VL将平均Pass@1准确率从63.3%（基础Qwen2.5-VL-7B）提升到68.8%，优于标准RLVR、纯文本推理SFT以及从GPT-4o的朴素多模态蒸馏方法。

Conclusion: PeRL-VL通过解耦视觉感知和文本推理的改进策略，有效解决了RLVR训练中视觉提取不准确和逻辑不一致的问题，为视觉语言模型的训练提供了更有效的框架。

Abstract: Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.

</details>


### [80] [Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models](https://arxiv.org/abs/2512.12596)
*Kei Yoshitake,Kento Hosono,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.CV

TL;DR: 提出基于视觉语言模型(VLM)的图像广告布局生成方法，通过分析背景图像内容生成布局计划，相比传统显著性映射方法能产生更高质量的广告布局。


<details>
  <summary>Details</summary>
Motivation: 传统广告布局技术主要依赖显著性映射来检测背景图像中的显著区域，但这种方法往往无法充分考虑图像的详细构图和语义内容，导致布局质量受限。

Method: 采用两阶段流程：1) VLM分析图像识别物体类型和空间关系，生成基于文本的"放置计划"；2) 将该计划渲染为HTML格式的最终布局代码。

Result: 通过定量和定性对比实验验证了方法的有效性，结果表明通过显式考虑背景图像内容，该方法能产生明显更高质量的广告布局。

Conclusion: 利用VLM理解图像语义内容的方法相比传统显著性映射技术，在广告布局生成方面具有显著优势，能产生更高质量的布局结果。

Abstract: In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based "placement plan" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.

</details>


### [81] [Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings](https://arxiv.org/abs/2512.12492)
*Shengkai Xu,Hsiang Lun Kao,Tianxiang Xu,Honghui Zhang,Junqiao Wang,Runmeng Ding,Guanyu Liu,Tianyu Shi,Zhenyu Yu,Guofeng Pan,Ziqian Bi,Yuqi Ouyang*

Main category: cs.CV

TL;DR: 提出AdaptiveDetector，一个结合YOLOv11检测器和VLM验证器的两阶段框架，通过自适应置信度阈值调整和成本敏感强化学习，在恶劣内窥镜条件下显著提升息肉检测召回率。


<details>
  <summary>Details</summary>
Motivation: 现有息肉检测器在干净数据集上训练，但在真实内窥镜场景中性能下降，因为光照变化、运动模糊和遮挡等恶劣成像条件普遍存在。现有方法难以弥合实验室控制条件与临床实践之间的领域差距。

Method: 提出AdaptiveDetector两阶段检测器-验证器框架：1) YOLOv11检测器在VLM指导下自适应调整每帧置信度阈值；2) VLM验证器使用Group Relative Policy Optimization (GRPO)进行微调，采用不对称成本敏感奖励函数，专门设计来减少漏检（临床关键需求）。构建合成测试平台，系统性地将干净数据集降解为临床常见恶劣条件。

Result: 在合成的CVC-ClinicDB和Kvasir-SEG图像上进行零样本评估，召回率比单独使用YOLO提高14-22个百分点，精度保持在基线以下0.7点到以上1.7点范围内。实现了临床对齐的开放世界息肉检测，显著减少假阴性。

Conclusion: 自适应阈值调整和成本敏感强化学习的结合，实现了临床对齐的开放世界息肉检测，大幅减少漏检风险，降低漏检癌前息肉的可能性，改善患者预后。

Abstract: Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.

</details>


### [82] [DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model](https://arxiv.org/abs/2512.12633)
*Zhou Tao,Shida Wang,Yongxiang Hua,Haoyu Cao,Linli Xu*

Main category: cs.CV

TL;DR: DiG是一个新颖的代理任务框架，通过让多模态大语言模型识别相似图像对之间的所有差异来学习细粒度视觉感知，无需预先知道差异数量，显著提升了模型的细粒度视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉语言任务上表现出色，但在细粒度视觉感知和精确空间推理方面仍有局限，需要一种能够提升模型精细视觉理解能力的方法。

Method: 提出DiG框架，通过自动化的3D渲染数据生成管道创建可控差异的图像对，采用课程学习策略从单个差异逐步增加到多个差异，让模型学习识别和定位所有差异而不需要预先知道差异数量。

Result: DiG显著提升了模型在各种视觉感知基准测试中的性能，学习到的细粒度感知技能能够有效迁移到标准下游任务，包括RefCOCO、RefCOCO+、RefCOCOg等指代表达理解和通用多模态感知基准。

Conclusion: 差异定位是一种可扩展且稳健的方法，能够有效推进多模态大语言模型的细粒度视觉推理能力，为提升模型的精细视觉理解提供了有前景的途径。

Abstract: Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.

</details>


### [83] [Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention](https://arxiv.org/abs/2512.12498)
*Tasweer Ahmad,Arindam Sikdar,Sandip Pradhan,Ardhendu Behera*

Main category: cs.CV

TL;DR: 提出一种基于图注意力网络的补丁驱动关系细化方法，通过挖掘图像内部补丁依赖关系来增强小样本图像分类性能，在保持零样本效率的同时显著提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于缓存的适配方法（如Tip-Adapter）虽然通过轻量级残差适配器缓解了小样本分类问题，但仍继承了CLIP的全局通用表示倾向，在低数据域中难以生成最优的判别性表示。需要更精细的表示学习方法来适应专家领域。

Method: 提出补丁驱动的关系细化方法：1）使用关系门控图注意力网络构建补丁图，通过边缘感知注意力强调信息丰富的补块间交互，生成上下文丰富的补丁嵌入；2）可学习的多聚合池化将这些嵌入组合成紧凑的任务判别表示；3）仅在训练时使用图细化将关系结构蒸馏到缓存中，推理时保持标准缓存查找效率；4）通过缓存相似度分数与CLIP零样本logits的残差融合获得最终预测。

Result: 在11个基准测试中一致优于最先进的CLIP适配器和基于缓存的方法，同时保持零样本效率。还引入了一个新的"受伤与未受伤士兵"数据集用于伤亡识别，验证了战场相关性。

Conclusion: 通过挖掘图像内部补丁依赖关系，提出的方法能够生成更紧凑、任务判别性更强的表示，显著提升小样本图像分类性能，同时保持推理效率，在战场伤亡识别等实际应用中具有重要价值。

Abstract: Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the "platinum minutes" and the broader "golden hour" window in time-critical UAV-driven search-and-rescue and combat casualty care.

</details>


### [84] [Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images](https://arxiv.org/abs/2512.12662)
*Muhammad Umar Farooq,Abd Ur Rehman,Azka Rehman,Muhammad Usman,Dong-Kyu Chae,Junaid Qadir*

Main category: cs.CV

TL;DR: SSMT-Net：一种用于甲状腺结节超声图像分割的半监督多任务Transformer网络，通过利用未标记数据和联合优化结节分割、腺体分割及结节大小估计任务，显著提升了分割准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 甲状腺结节超声图像分割面临三大挑战：结节与周围组织边界模糊、结节尺寸变化大、标注数据稀缺。现有深度学习模型难以有效整合甲状腺腺体的上下文信息，且在不同病例上泛化能力有限。

Method: 提出SSMT-Net（半监督多任务Transformer网络），包含两个阶段：1）无监督阶段利用未标记数据增强Transformer编码器的特征提取能力；2）监督阶段联合优化三个任务：结节分割、腺体分割和结节大小估计，整合局部和全局上下文特征。

Result: 在TN3K和DDTI数据集上的广泛评估表明，SSMT-Net在准确性和鲁棒性方面均优于现有最先进方法，显示出在实际临床应用中的潜力。

Conclusion: SSMT-Net通过半监督学习和多任务学习策略，有效解决了甲状腺结节超声图像分割的关键挑战，为临床诊断和治疗规划提供了更可靠的自动化工具。

Abstract: Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.

</details>


### [85] [Generative Spatiotemporal Data Augmentation](https://arxiv.org/abs/2512.12508)
*Jinfan Zhou,Lixin Luo,Sungmin Eum,Heesung Kwon,Jeong Joon Park*

Main category: cs.CV

TL;DR: 利用视频基础模型进行时空数据增强，通过视频扩散模型生成3D空间和时间变化，在低数据场景下提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法主要基于简单的几何变换或外观扰动，无法有效模拟真实的3D空间和时间变化。在无人机图像等标注稀缺的低数据场景中，需要更有效的增强方法来提升模型性能。

Method: 使用现成的视频扩散模型从给定图像数据集生成逼真的3D空间和时间变化，将合成的视频片段作为补充训练数据。提供实用指南：选择合适的时空生成设置、将标注转移到合成帧、处理新暴露区域的遮挡问题。

Result: 在COCO子集和无人机捕获数据集上的实验表明，时空增强能够沿着传统和先前生成方法未充分代表的轴扩展数据分布，在数据稀缺情况下有效提升模型性能。

Conclusion: 时空数据增强通过视频基础模型生成逼真的3D变化，为低数据场景下的模型训练提供了有效的增强手段，特别是在无人机图像等标注稀缺的应用中。

Abstract: We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.

</details>


### [86] [Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling](https://arxiv.org/abs/2512.12675)
*Yuran Wang,Bohan Zeng,Chengzhuo Tong,Wenxuan Liu,Yang Shi,Xiaochen Ma,Hao Liang,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: Scone是一个统一的理解-生成方法，整合了多主体组合与区分能力，通过语义对齐和注意力掩码增强主体身份保持，在复杂视觉场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前多主体图像生成方法主要关注组合能力，但忽视了区分能力——当输入包含多个候选主体时正确识别和生成特定主体的能力。这一限制影响了在复杂现实视觉场景中的有效性。

Method: Scone采用统一的理解-生成框架，让理解专家作为语义桥梁，传递语义信息并指导生成专家保持主体身份同时最小化干扰。采用两阶段训练：先学习组合能力，然后通过语义对齐和基于注意力的掩码增强区分能力。

Result: 实验表明Scone在两个基准测试中，在组合和区分任务上都优于现有的开源模型。作者还提出了SconeEval基准，用于评估不同场景下的组合和区分能力。

Conclusion: Scone通过整合组合和区分能力，在多主体图像生成中实现了更好的性能，特别是在复杂现实场景中。该方法、基准和训练数据均已开源。

Abstract: Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.

</details>


### [87] [Animus3D: Text-driven 3D Animation via Motion Score Distillation](https://arxiv.org/abs/2512.12534)
*Qi Sun,Can Wang,Jiaxiang Shang,Wensen Feng,Jing Liao*

Main category: cs.CV

TL;DR: Animus3D：基于文本驱动的3D动画框架，通过Motion Score Distillation（MSD）替代传统SDS，解决现有方法运动幅度小、抖动明显的问题，能生成更丰富、细节更精细的3D动画。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用Score Distillation Sampling（SDS）从预训练文本到视频扩散模型中提取运动，但生成的动画往往运动幅度小或存在明显抖动，需要更有效的运动生成方法。

Method: 提出Motion Score Distillation（MSD）替代SDS，采用LoRA增强的视频扩散模型定义静态源分布，结合基于反转的噪声估计技术保持外观一致性。引入时空正则化项减少几何失真，并提出运动细化模块提升时间分辨率和细节。

Result: 实验表明Animus3D能成功为各种静态3D资产生成文本驱动的动画，相比现有方法产生更丰富、更详细的运动，同时保持高视觉完整性。

Conclusion: Animus3D通过创新的MSD方法和时空正则化，有效解决了3D动画生成中的运动幅度不足和抖动问题，为文本驱动的3D动画提供了高质量的解决方案。

Abstract: We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.

</details>


### [88] [Robust Motion Generation using Part-level Reliable Data from Videos](https://arxiv.org/abs/2512.12703)
*Boyuan Li,Sipeng Zheng,Bin Cao,Ruihua Song,Zongqing Lu*

Main category: cs.CV

TL;DR: 提出ROPaR方法，利用视频中可信的身体部位数据增强运动生成，通过部位感知掩码自回归模型处理部分遮挡问题，并在新基准K700-M上验证效果。


<details>
  <summary>Details</summary>
Motivation: 从网络视频提取人体运动可解决动画数据稀缺问题，但许多视频帧中人体部位因离屏或遮挡而不可见，导致数据质量与规模之间的两难：丢弃不完整数据会限制规模多样性，保留则会损害数据质量和模型性能。

Method: 1. 将人体分解为五个部位，检测视频帧中清晰可见的部位作为"可信部位"；2. 通过部位感知变分自编码器将可信部位编码为潜在标记；3. 提出鲁棒的部位级掩码生成模型，预测被掩码的可信部位，同时忽略噪声部位。

Result: 在K700-M基准测试中，该方法在干净和噪声数据集上均优于基线方法，在运动质量、语义一致性和多样性方面表现更佳。

Conclusion: 提出的ROPaR方法通过利用可信部位数据和鲁棒的部位感知掩码生成，有效解决了视频中人体部位缺失问题，为大规模运动数据提取提供了可行方案。

Abstract: Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.
  To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as "credible". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.
  In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/

</details>


### [89] [Anatomy Guided Coronary Artery Segmentation from CCTA Using Spatial Frequency Joint Modeling](https://arxiv.org/abs/2512.12539)
*Huan Huang,Michele Esposito,Chen Zhao*

Main category: cs.CV

TL;DR: 提出一种结合心肌解剖先验、结构感知特征编码和三维小波变换的冠状动脉分割框架，在ImageCAS数据集上取得优于主流方法的性能。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉CT血管成像的准确分割对定量分析和临床决策至关重要，但由于血管细小、分支复杂、边界模糊和心肌干扰等因素，可靠分割仍然具有挑战性。

Method: 提出一个集成心肌解剖先验、结构感知特征编码和三维小波逆小波变换的框架。编码阶段融入心肌先验和残差注意力特征增强，小波逆小波下采样和上采样实现联合空间频率建模，解码阶段通过多尺度特征融合模块整合语义和几何信息。

Result: 在ImageCAS数据集上，采用7:1:2的训练/验证/测试划分，获得Dice系数0.8082、敏感性0.7946、精确度0.8471、HD95为9.77mm，优于多个主流分割模型。消融实验证实各组件互补贡献。

Conclusion: 该方法在复杂几何条件下实现更稳定一致的冠状动脉分割，为后续冠状动脉结构分析任务提供可靠的分割结果。

Abstract: Accurate coronary artery segmentation from coronary computed tomography angiography is essential for quantitative coronary analysis and clinical decision support. Nevertheless, reliable segmentation remains challenging because of small vessel calibers, complex branching, blurred boundaries, and myocardial interference. We propose a coronary artery segmentation framework that integrates myocardial anatomical priors, structure aware feature encoding, and three dimensional wavelet inverse wavelet transformations. Myocardial priors and residual attention based feature enhancement are incorporated during encoding to strengthen coronary structure representation. Wavelet inverse wavelet based downsampling and upsampling enable joint spatial frequency modeling and preserve multi scale structural consistency, while a multi scale feature fusion module integrates semantic and geometric information in the decoding stage. The model is trained and evaluated on the public ImageCAS dataset using a 3D overlapping patch based strategy with a 7:1:2 split for training, validation, and testing. Experimental results demonstrate that the proposed method achieves a Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and an HD95 of 9.77 mm, outperforming several mainstream segmentation models. Ablation studies further confirm the complementary contributions of individual components. The proposed method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable segmentation results for subsequent coronary structure analysis tasks.

</details>


### [90] [CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence](https://arxiv.org/abs/2512.12768)
*Tianjiao Yu,Xinzhuo Li,Yifan Shen,Yuanzhe Liu,Ismini Lourentzou*

Main category: cs.CV

TL;DR: CoRe3D提出一个统一的3D理解和生成推理框架，通过语义和空间抽象联合操作，实现语言推断的高层意图直接指导低级3D内容生成。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型中的显式推理机制已被证明能提升模型可靠性、可解释性和跨模态对齐，但这些推理中心方法在3D领域尚未充分发展，需要扩展。

Method: 采用空间基础推理表示，将3D潜在空间分解为局部化区域，使模型能以组合和程序化方式推理几何；紧密耦合语义链式思维推理与结构化空间推理。

Result: CoRe3D生成的3D输出展现出强大的局部一致性和与语言描述的忠实对齐。

Conclusion: 通过联合语义和空间推理，CoRe3D为3D理解和生成提供了有效的推理框架，解决了现有方法在3D领域的不足。

Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.

</details>


### [91] [Supervised Contrastive Frame Aggregation for Video Representation Learning](https://arxiv.org/abs/2512.12549)
*Shaif Chowdhury,Mushfika Rahman,Greg Hamerly*

Main category: cs.CV

TL;DR: 提出了一种监督对比学习框架，通过视频到图像的聚合策略将多帧排列成单张图像，利用预训练CNN骨干网络，避免复杂视频Transformer的计算开销，在分类准确率和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频表示学习方法通常需要复杂的视频Transformer模型，计算开销大。作者希望利用预训练的卷积神经网络（如ResNet50）来处理视频数据，同时避免复杂模型的计算负担，并提高视频表示学习的效果。

Method: 1. 视频到图像聚合策略：将视频的多个帧在空间上排列成单张输入图像；2. 使用预训练CNN骨干网络（如ResNet50）处理聚合后的图像；3. 设计监督对比学习目标：相同标签的视频投影作为正对，其他作为负对；4. 通过不同时间帧采样创建同一视频的多个自然视图，提供多样正样本和全局上下文。

Result: 在Penn Action数据集上达到76%分类准确率（ViViT为43%），在HMDB51数据集上达到48%准确率（ViViT为37%）。方法在监督和自监督设置下都能学习有效的视频表示，支持分类和字幕生成等任务，且计算资源需求更少。

Conclusion: 提出的监督对比帧聚合方法通过视频到图像聚合策略，有效利用预训练CNN骨干网络，在保持计算效率的同时显著提升了视频表示学习性能，在多个数据集上超越了现有方法。

Abstract: We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.

</details>


### [92] [Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding](https://arxiv.org/abs/2512.12822)
*Yongyuan Liang,Xiyao Wang,Yuanchen Ju,Jianwei Yang,Furong Huang*

Main category: cs.CV

TL;DR: Lemon是一个统一的Transformer架构，通过将3D点云补丁和语言标记作为单一序列联合处理，解决了大型多模态模型在3D理解中的挑战，实现了早期空间-语言融合，并在多个3D理解和推理任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型在扩展到3D理解时面临三个主要挑战：点云数据稀疏且不规则、现有模型依赖具有模态特定编码器的碎片化架构、训练管道通常存在不稳定性和可扩展性差的问题。

Method: 1. 统一的Transformer架构，将3D点云补丁和语言标记作为单一序列联合处理；2. 结构化的补丁化和标记化方案，保留空间上下文；3. 三阶段训练课程，从对象级识别逐步构建到场景级空间推理能力。

Result: Lemon在全面的3D理解和推理任务中建立了新的最先进性能，包括对象识别、描述和3D场景中的空间推理，同时随着模型规模和训练数据的增加展现出稳健的缩放特性。

Conclusion: Lemon提供了一个统一的基础架构，通过早期空间-语言融合、消除冗余编码器、提高参数效率和支持更有效的模型缩放，为推进现实世界应用中的3D空间智能奠定了基础。

Abstract: Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.

</details>


### [93] [From Tokens to Photons: Test-Time Physical Prompting for Vison-Language Models](https://arxiv.org/abs/2512.12571)
*Boyeong Im,Wooseok Lee,Yoojin Kwon,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: MVP提出了一种测试时适应框架，通过相机曝光三角作为物理提示，从多个物理视图中选择最佳传感器设置并聚合预测，显著提升视觉语言模型在物理环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 将视觉语言模型从网络图像扩展到传感器介导的物理环境，解决传统测试时适应方法仅限于数字增强的问题，探索测量时间控制对模型鲁棒性的影响。

Method: 使用相机曝光三角（ISO、快门速度、光圈）作为物理提示，在推理时获取每个场景的物理视图库，基于源亲和度得分选择top-k传感器设置，对保留视图进行轻量级数字增强，过滤最低熵的增强视图子集，最后通过零温度softmax（硬投票）聚合预测。

Result: 在ImageNet-ES和ImageNet-ES-Diverse数据集上，MVP比仅使用数字增强的测试时适应方法提升高达25.6个百分点，比结合传统传感器控制与TTA的流程额外提升3.4个百分点。即使在减少参数候选集降低捕获延迟的情况下仍保持有效性。

Conclusion: 测量时间控制（选择和组合真实物理视图）相比仅使用后捕获提示能显著提升视觉语言模型的鲁棒性，MVP框架简单、校准友好且无需梯度或模型修改，具有实际应用价值。

Abstract: To extend the application of vision-language models (VLMs) from web images to sensor-mediated physical environments, we propose Multi-View Physical-prompt for Test-Time Adaptation (MVP), a forward-only framework that moves test-time adaptation (TTA) from tokens to photons by treating the camera exposure triangle--ISO, shutter speed, and aperture--as physical prompts. At inference, MVP acquires a library of physical views per scene, selects the top-k sensor settings using a source-affinity score, evaluates each retained view under lightweight digital augmentations, filters the lowest-entropy subset of augmented views, and aggregates predictions with Zero-temperature softmax (i.e., hard voting). This selection-then-vote design is simple, calibration-friendly, and requires no gradients or model modifications. On ImageNet-ES and ImageNet-ES-Diverse, MVP consistently outperforms digital-only TTA on single Auto-Exposure captures, by up to 25.6 percentage points (pp), and delivers up to 3.4 pp additional gains over pipelines that combine conventional sensor control with TTA. MVP remains effective under reduced parameter candidate sets that lower capture latency, demonstrating practicality. These results support the main claim that, beyond post-capture prompting, measurement-time control--selecting and combining real physical views--substantially improves robustness for VLMs.

</details>


### [94] [Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners](https://arxiv.org/abs/2512.12824)
*N. K. B. M. P. K. B. Narasinghe,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 本文系统研究了CoCa多模态基础模型在少样本图像分类任务上的适应策略，从训练免费的混合原型到LoRA参数调优，揭示了数据增强、损失函数和正则化在数据稀缺场景下的关键作用。


<details>
  <summary>Details</summary>
Motivation: 虽然大规模多模态基础模型（特别是CoCa）在零样本迁移上表现出色，但它们在极端数据稀缺（少样本学习）的下游任务适应方面研究不足。现有文献主要关注CLIP等双编码器架构，对于CoCa这种生成-对比混合模型的潜在空间如何响应参数高效微调（PEFT）缺乏理解。

Method: 系统评估了从训练免费的混合原型到通过低秩适应（LoRA）进行深度参数适应的层次化策略。研究了数据增强、监督对比（SupCon）损失与交叉熵损失的混合目标，并分析了训练配置对数据稀缺的敏感性。

Result: 发现了"增强发散"现象：强数据增强在低样本设置下会降低线性探测性能，但对稳定LoRA微调至关重要。SupCon损失在不同样本数量下都比标准交叉熵带来一致性能提升。提供了正则化、秩和采样策略的实证参考设置。

Conclusion: 本研究为生成-对比基础模型的高效适应提供了系统指导，揭示了在数据稀缺场景下优化训练配置的重要性，填补了CoCa模型在少样本学习适应方面的研究空白。

Abstract: Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an "augmentation divergence": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.

</details>


### [95] [StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis](https://arxiv.org/abs/2512.12586)
*Lixin Chen,Chaomeng Chen,Jiale Zhou,Zhijian Wu,Xun Lin*

Main category: cs.CV

TL;DR: StegaVAR是一个新颖的隐私保护视频动作识别框架，首次将动作视频嵌入到普通封面视频中，并在隐写域直接进行动作识别，解决了传统匿名化方法的低隐蔽性和时空特征破坏问题。


<details>
  <summary>Details</summary>
Motivation: 当前隐私保护方法主要依赖匿名化，存在两个主要问题：(1) 低隐蔽性：产生视觉扭曲的视频在传输过程中容易引起攻击者注意；(2) 时空破坏：破坏视频中用于准确动作识别的重要时空特征。需要一种既能保护隐私又不破坏动作识别性能的方法。

Method: 提出StegaVAR框架，将秘密动作视频嵌入到普通封面视频中，在隐写域直接进行动作识别。提出两个关键技术：秘密时空促进(STeP)在训练时使用秘密视频指导隐写域中的时空特征提取；跨带差异注意力(CroDA)通过捕捉跨带语义差异来抑制封面干扰。

Result: 实验表明，StegaVAR在广泛使用的数据集上实现了优越的视频动作识别和隐私保护性能。该框架对多种隐写模型都有效，证明了其通用性。

Conclusion: StegaVAR首次实现了在隐写域直接进行视频动作识别，既保护了隐私又不破坏动作识别所需的时空特征，解决了传统匿名化方法的关键缺陷，为隐私保护视频分析提供了新思路。

Abstract: Despite the rapid progress of deep learning in video action recognition (VAR) in recent years, privacy leakage in videos remains a critical concern. Current state-of-the-art privacy-preserving methods often rely on anonymization. These methods suffer from (1) low concealment, where producing visually distorted videos that attract attackers' attention during transmission, and (2) spatiotemporal disruption, where degrading essential spatiotemporal features for accurate VAR. To address these issues, we propose StegaVAR, a novel framework that embeds action videos into ordinary cover videos and directly performs VAR in the steganographic domain for the first time. Throughout both data transmission and action analysis, the spatiotemporal information of hidden secret video remains complete, while the natural appearance of cover videos ensures the concealment of transmission. Considering the difficulty of steganographic domain analysis, we propose Secret Spatio-Temporal Promotion (STeP) and Cross-Band Difference Attention (CroDA) for analysis within the steganographic domain. STeP uses the secret video to guide spatiotemporal feature extraction in the steganographic domain during training. CroDA suppresses cover interference by capturing cross-band semantic differences. Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. Moreover, our framework is effective for multiple steganographic models.

</details>


### [96] [SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition](https://arxiv.org/abs/2512.12885)
*Minghao Zhu,Zhihao Zhang,Anmol Sidhu,Keith Redmill*

Main category: cs.CV

TL;DR: 提出基于检索增强生成(RAG)的零样本路标识别框架，使用视觉语言模型生成文本描述，检索相关候选，再由大语言模型进行细粒度识别，在303个监管标志上取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法面临路标类别繁多、标注数据难以获取的挑战，需要一种无需任务特定训练即可准确识别大量路标的方法。

Method: 采用RAG范式：1) 使用视觉语言模型从输入图像生成文本描述；2) 从参考设计的向量数据库中检索最相关的候选标志；3) 使用大语言模型对检索到的候选进行推理，做出最终细粒度识别。

Result: 在俄亥俄州MUTCD的303个监管标志上验证，理想参考图像准确率达95.58%，具有挑战性的真实道路数据准确率达82.45%。

Conclusion: 基于RAG的架构为无需任务特定训练即可创建可扩展、准确的路标识别系统提供了可行方案，展示了零样本识别在智能交通系统中的潜力。

Abstract: Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.

</details>


### [97] [Automatic Wire-Harness Color Sequence Detector](https://arxiv.org/abs/2512.12590)
*Indiwara Nanayakkara,Dehan Jayawickrama,Mervyn Parakrama B. Ekanayake*

Main category: cs.CV

TL;DR: 该论文提出了一种用于线束检测的半自动化机器视觉系统，采用五个工业级CMOS摄像头和HSV/RGB颜色域比较算法，实现了100%检测准确率，检测时间减少44%。


<details>
  <summary>Details</summary>
Motivation: 现代电子制造服务行业中，线束检测过程仍然是劳动密集型且容易出错，需要自动化解决方案来提高检测效率和准确性。

Method: 采用五个工业标准CMOS摄像头集成到模块化机械框架中，使用基于HSV和RGB颜色域值比较的颜色序列分类器，系统可通过至少五个参考样本进行训练，训练文件可存储并重复用于类似线束类型。

Result: 在GPV Lanka Pvt. Ltd.部署的系统实现了100%的检测准确率，与手动方法相比减少了44%的检测时间，系统还包含用户管理、可调照明、会话数据存储和安全登录等附加功能。

Conclusion: 该半自动化机器视觉系统为线束检测提供了可靠高效的解决方案，在实际应用中表现出色，能够显著提高电子制造服务行业的检测效率和质量控制水平。

Abstract: Wire harness inspection process remains a labor-intensive process prone to errors in the modern Electronics Manufacturing Services (EMS) industry. This paper introduces a semiautomated machine vision system capable of verifying correct wire positioning, correctness of the connector polarity and correctness of color sequences for both linear and circular wire harness configurations. Five industrial standard CMOS cameras are integrated into a modularized mechanical framework in the physical structure of the solution and a HSV and RGB color domain value comparison based color sequence classifier is used in the operation. For each harness batch, a user can train the system using at least five reference samples; the trained file is stored and reused for similar harness types. The Solution is deployed at GPV Lanka Pvt. Ltd. (Fig. 2) and the system achieved 100% detection accuracy and reduced inspection time by 44% compared to manual methods. Additional features include user management, adjustable lighting, session data storage, and secure login. Results of this product usage in the real world situation demonstrate that this approach delivers reliable and efficient inspection capabilities.

</details>


### [98] [MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation](https://arxiv.org/abs/2512.12929)
*Huu-An Vu,Van-Khanh Mai,Trong-Tam Nguyen,Quang-Duc Dam,Tien-Huy Nguyen,Thanh-Huong Le*

Main category: cs.CV

TL;DR: MADTempo是一个视频检索框架，结合了时间搜索机制和基于Google图像搜索的回退模块，用于处理多事件查询和未见过的视觉概念。


<details>
  <summary>Details</summary>
Motivation: 在线视频内容的快速增长需要能够理解复杂事件时间结构的检索系统。现有方法在建模跨多个事件的时间依赖性和处理涉及未见或罕见视觉概念的查询方面存在不足。

Method: MADTempo框架包含两个核心组件：1) 时间搜索机制，通过聚合连续视频片段的相似性分数来捕捉事件级连续性；2) 基于Google图像搜索的回退模块，利用外部网络图像扩展查询表示，弥补预训练视觉嵌入的不足。

Result: 该框架提升了现代视频检索系统的时间推理和泛化能力，能够更连贯地检索多事件查询，并提高对分布外查询的鲁棒性。

Conclusion: MADTempo为大规模视频语料库中实现更具语义感知和自适应性的检索铺平了道路，统一了时间搜索与网络规模视觉定位。

Abstract: The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.

</details>


### [99] [Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation](https://arxiv.org/abs/2512.12595)
*Karthikeya KV*

Main category: cs.CV

TL;DR: 提出一个结合视觉增强LLM与先进Transformer架构的框架，通过整流流机制和双向标记化策略，实现高质量图像合成和多模态数据理解，相比扩散方法提升25%图像清晰度并减少20%计算需求。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率图像合成和多模态数据解释中的挑战，传统方法在计算效率和生成质量方面存在局限，需要更高效统一的跨模态理解框架。

Method: 采用整流流机制连接噪声与数据的线性路径，使用双向标记化策略融合文本、图像和视频输入，嵌入时空特征，结合混合文本-图像序列建模，并采用噪声感知学习算法优化架构。

Result: 在基准数据集上评估显示，相比扩散方法提升25%图像分辨率清晰度，减少20%计算需求，展现出强大的可扩展性和适应性。

Conclusion: 该框架展示了视觉中心LLM在重新定义计算机视觉和多模态AI能力方面的潜力，适用于自主系统、创意内容生成和高级视频分析等应用。

Abstract: This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.

</details>


### [100] [Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion](https://arxiv.org/abs/2512.12935)
*Toan Le Ngo Thanh,Phat Ha Huu,Tan Nguyen Dang Duy,Thong Nguyen Le Minh,Anh Nguyen Nhu Tinh*

Main category: cs.CV

TL;DR: 提出一个统一的多模态时刻检索系统，通过级联双嵌入管道、时间感知评分机制和智能体引导的查询分解，解决现有方法在跨模态噪声、时间建模和手动模态选择方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 视频内容的指数增长迫切需要高效的多模态时刻检索系统，但现有方法面临三个关键挑战：固定权重融合策略无法处理跨模态噪声和模糊查询；时间建模难以捕捉连贯事件序列并惩罚不现实的时间间隔；系统需要手动模态选择，降低了可用性。

Method: 1. 级联双嵌入管道：结合BEIT-3和SigLIP进行广泛检索，通过BLIP-2重新排序平衡召回率和精确度。2. 时间感知评分机制：通过波束搜索对大的时间间隔应用指数衰减惩罚，构建连贯事件序列而非孤立帧。3. 智能体引导查询分解：使用GPT-4o自动解释模糊查询，将其分解为模态特定子查询（视觉/OCR/ASR），并进行自适应分数融合，消除手动模态选择。

Result: 定性分析表明，该系统能有效处理模糊查询，检索时间连贯的序列，并动态调整融合策略，提升了交互式时刻搜索能力。

Conclusion: 该统一多模态时刻检索系统通过创新的级联检索、时间建模和智能查询分解方法，解决了现有系统的关键局限性，推动了交互式时刻搜索能力的发展。

Abstract: The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.

</details>


### [101] [Content Adaptive based Motion Alignment Framework for Learned Video Compression](https://arxiv.org/abs/2512.12936)
*Tiange Zhang,Xiandong Meng,Siwei Ma*

Main category: cs.CV

TL;DR: 提出基于内容自适应的运动对齐框架CAMA，通过两阶段流引导可变形扭曲、多参考质量感知策略和无训练模块，显著提升端到端视频压缩性能


<details>
  <summary>Details</summary>
Motivation: 当前端到端视频压缩框架缺乏内容特定适应性，导致压缩性能不理想。需要针对不同内容特性调整编码策略以提升性能

Method: 1. 两阶段流引导可变形扭曲机制：通过粗到细偏移预测和掩码调制精炼运动补偿；2. 多参考质量感知策略：基于参考质量调整失真权重，应用于分层训练减少误差传播；3. 无训练模块：根据运动幅度和分辨率下采样帧以获得平滑运动估计

Result: 在标准测试数据集上，CAMA框架相比基线模型DCVC-TCM实现24.95% BD-rate (PSNR)节省，同时优于复现的DCVC-DC和传统编解码器HM-16.25

Conclusion: 提出的内容自适应运动对齐框架通过改进运动补偿、质量感知策略和运动估计，显著提升了端到端视频压缩性能，验证了内容自适应方法在视频压缩中的有效性

Abstract: Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.

</details>


### [102] [Geometry-Aware Scene-Consistent Image Generation](https://arxiv.org/abs/2512.12598)
*Cong Xie,Che Wang,Yan Zhang,Zheng Pan,Han Zou,Zhenpeng Zhan*

Main category: cs.CV

TL;DR: 提出几何感知的场景一致图像生成方法，通过场景一致数据构建和几何引导注意力损失，在保持参考场景物理环境的同时根据文本空间关系生成新实体。


<details>
  <summary>Details</summary>
Motivation: 现有方法在场景保持和提示遵循之间存在权衡：要么高保真复制场景但对提示响应差，要么优先遵循提示但牺牲场景一致性。需要解决这一平衡问题。

Method: 1) 场景一致数据构建管道，生成多样化的几何基础训练对；2) 新颖的几何引导注意力损失，利用跨视图线索规范模型的空间推理。

Result: 在场景一致基准测试中，该方法在场景对齐和文本图像一致性方面优于现有基线，自动指标和人类偏好研究均显示更好结果。

Conclusion: 该方法能生成几何连贯的图像，保持多样化的构图，同时忠实于文本指令和底层场景结构，解决了场景保持与提示遵循的平衡问题。

Abstract: We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.

</details>


### [103] [Calibrating Uncertainty for Zero-Shot Adversarial CLIP](https://arxiv.org/abs/2512.12997)
*Wenjing lu,Zerui Tao,Dongping Zhang,Yuning Qiu,Yang Yang,Qibin Zhao*

Main category: cs.CV

TL;DR: 提出针对CLIP模型的新型对抗微调方法，通过狄利克雷分布重新参数化输出，同时优化预测准确性和不确定性校准，在保持干净准确率的同时提升对抗鲁棒性和不确定性校准。


<details>
  <summary>Details</summary>
Motivation: CLIP在零样本分类上表现优秀，但对对抗攻击高度脆弱。现有对抗微调方法主要关注干净样本和对抗样本的预测logits匹配，忽略了不确定性校准，可能损害零样本泛化能力。对抗扰动不仅降低准确率，还会抑制不确定性，导致严重的校准错误和不可靠的过度自信，这揭示了超越鲁棒性的可靠性差距。

Method: 提出新的对抗微调目标，同时考虑预测准确性和不确定性对齐。通过将CLIP输出重新参数化为狄利克雷分布的浓度参数，创建统一表示来捕捉相对语义结构和预测置信度大小。该目标在扰动下整体对齐这些分布，超越单一logits锚定，恢复校准的不确定性。

Result: 在多个零样本分类基准测试中，该方法有效恢复了校准的不确定性，实现了有竞争力的对抗鲁棒性，同时保持了干净的准确率。

Conclusion: 该方法成功解决了CLIP在对抗设置中的不确定性校准问题，通过统一的分布表示和对齐策略，在保持零样本能力的同时提升了模型的可靠性和鲁棒性。

Abstract: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.

</details>


### [104] [No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching](https://arxiv.org/abs/2512.12604)
*Tingyan Wen,Haoyu Li,Yihuang Chen,Xing Zhou,Lifei Zhu,Xueqian Wang*

Main category: cs.CV

TL;DR: X-Slim是一种无需训练的缓存加速框架，通过三层次（时间步、结构块、空间token）冗余利用和双阈值控制器，在保持生成质量的同时显著加速扩散模型推理。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量优秀，但计算开销随步数、模型深度和序列长度线性增长。现有缓存方法存在权衡：激进的时间步重用能大幅加速但容易损害保真度，而块级或token级重用更安全但计算节省有限。

Method: 提出X-Slim统一框架，利用时间步、结构块和空间token三层次的可缓存冗余。采用双阈值控制器将缓存过程分为"推送-抛光"两阶段：先推送时间步级重用至预警线，然后切换到轻量级块级和token级刷新来抛光剩余冗余，当达到临界线时触发完整推理重置累积误差。每个层次使用上下文感知指标决定何时何地缓存。

Result: 在多种任务上推进了速度-质量前沿：在FLUX.1-dev上延迟降低4.97倍，HunyuanVideo上降低3.52倍，感知损失最小；在DiT-XL/2上达到3.13倍加速，FID比先前方法提升2.42。

Conclusion: X-Slim通过统一利用三层次冗余和智能的双阈值控制策略，在保持扩散模型生成质量的同时实现了显著的推理加速，为扩散模型的实际部署提供了有效的解决方案。

Abstract: Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.

</details>


### [105] [GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training](https://arxiv.org/abs/2512.13043)
*Tong Wei,Yijun Yang,Changhao Zhang,Junliang Xing,Yuanchun Shi,Zongqing Lu,Deheng Ye*

Main category: cs.CV

TL;DR: GTR-Turbo：一种高效的多模态智能体强化学习方法，无需昂贵教师模型，通过合并训练检查点权重作为"免费"教师，显著提升性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 基于视觉语言模型的多模态智能体多轮强化学习面临稀疏奖励和长视野信用分配问题。现有方法依赖昂贵且通常有特权的教师模型提供步级反馈，限制了实用性和可复现性。

Method: GTR-Turbo在持续强化学习训练过程中合并检查点权重，使用这个合并模型作为"免费"教师，通过监督微调或软logit蒸馏指导后续强化学习，无需查询昂贵的外部教师模型。

Result: 在多样化视觉智能体任务中，GTR-Turbo将基线模型准确率提升10-30%，相比GTR减少50%的训练时间和60%的计算成本，同时缓解了先前工作中的"熵崩溃"问题。

Conclusion: GTR-Turbo提供了一种高效实用的多模态智能体强化学习解决方案，消除了对特权教师模型的依赖，显著降低了计算成本，同时保持训练稳定性并提升性能。

Abstract: Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.

</details>


### [106] [Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching](https://arxiv.org/abs/2512.12610)
*Wonseok Choi,Sohwi Lim,Nam Hyeon-Woo,Moon Ye-Bin,Dong-Ju Jeong,Jinyoung Hwang,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: Patchify是一个无需微调的图像检索框架，通过将数据库图像分割成结构化局部块并与查询全局描述符匹配，实现了高性能、可扩展且可解释的实例级图像检索。


<details>
  <summary>Details</summary>
Motivation: 实例级图像检索需要在尺寸、位置或外观变化的情况下找到包含相同对象的图像。现有方法在准确性、可扩展性和可解释性方面存在局限，需要一种既能保持高性能又能提供空间定位信息的检索框架。

Method: 提出Patchify框架：1）将数据库图像分割成少量结构化局部块；2）使用局部特征与查询全局描述符进行匹配；3）引入LocScore评估指标，量化检索区域与目标对象的对齐程度；4）应用产品量化实现高效大规模检索。

Result: 在多个基准测试、骨干网络和区域选择策略上的实验表明：Patchify优于全局方法，并能与最先进的重新排序管道互补。产品量化结合信息丰富的特征显著提升了大规模检索性能。

Conclusion: Patchify提供了一个简单有效的图像检索框架，具有高性能、可扩展性和可解释性。LocScore作为定位感知指标为理解检索行为提供了有价值的诊断工具，产品量化进一步提升了大规模检索效率。

Abstract: Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance. To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning. Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching. To assess not just retrieval accuracy but also spatial correctness, we introduce LocScore, a localization-aware metric that quantifies whether the retrieved region aligns with the target object. This makes LocScore a valuable diagnostic tool for understanding and improving retrieval behavior. We conduct extensive experiments across multiple benchmarks, backbones, and region selection strategies, showing that Patchify outperforms global methods and complements state-of-the-art reranking pipelines. Furthermore, we apply Product Quantization for efficient large-scale retrieval and highlight the importance of using informative features during compression, which significantly boosts performance. Project website: https://wons20k.github.io/PatchwiseRetrieval/

</details>


### [107] [UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era](https://arxiv.org/abs/2512.13089)
*Ziqiang Zhu,Bowei Yang*

Main category: cs.CV

TL;DR: UniVCD是一种基于冻结SAM2和CLIP的无监督开放词汇变化检测方法，无需标注数据或配对变化图像，通过轻量级特征对齐模块和多模态融合实现跨场景的类别无关变化检测。


<details>
  <summary>Details</summary>
Motivation: 传统变化检测方法依赖监督学习，性能受数据集限制且标注成本高，通常只能检测预定义类别，泛化能力差。随着视觉基础模型（如SAM2和CLIP）的发展，出现了放松这些约束的新机会。

Method: 提出UniVCD方法：1）基于冻结的SAM2和CLIP构建无监督开放词汇变化检测框架；2）引入轻量级特征对齐模块，桥接SAM2的空间细节表示和CLIP的语义先验；3）设计简化的后处理流程抑制噪声和伪变化。

Result: 在多个公开的BCD（二值变化检测）和SCD（语义变化检测）基准测试中，UniVCD在F1和IoU等关键指标上表现一致强劲，匹配或超越了现有的开放词汇变化检测方法。

Conclusion: 使用冻结视觉基础模型和轻量级多模态对齐的无监督变化检测是开放词汇变化检测的实用有效范式，为跨场景、类别无关的变化检测提供了新思路。

Abstract: Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.

</details>


### [108] [D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation](https://arxiv.org/abs/2512.12622)
*Zihan Wang,Seungjun Lee,Guangzhao Dai,Gim Hee Lee*

Main category: cs.CV

TL;DR: D3D-VLP模型通过动态3D思维链和协同学习策略，统一了具身智能中的规划、导航、问答等任务，在多个基准测试中取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 解决具身智能中的关键困境：端到端模型缺乏可解释性和显式3D推理，而模块化系统忽略了跨组件间的相互依赖和协同效应。

Method: 提出动态3D视觉-语言-规划模型(D3D-VLP)，包含：1) 动态3D思维链统一规划、定位、导航和问答；2) 协同学习策略，使用掩码自回归损失从大规模部分标注的混合数据中学习。

Result: 在多个基准测试中取得SOTA结果：视觉语言导航(R2R-CE, REVERIE-CE, NavRAG-CE)、目标导航(HM3D-OVON)、任务导向顺序定位导航(SG3D)。真实世界移动操作实验进一步验证了有效性。

Conclusion: D3D-VLP通过统一的3D思维链和协同学习策略，成功解决了具身智能中可解释性与组件协同的平衡问题，为具身AI提供了有效的解决方案。

Abstract: Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.

</details>


### [109] [Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2512.13101)
*Wenjing Lu,Yi Hong,Yang Yang*

Main category: cs.CV

TL;DR: UnCoL：一种基于不确定性指导的双教师协作学习框架，用于半监督医学图像分割，通过结合基础模型的泛化能力和任务特定模型的专门化能力，显著减少标注需求


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型在医学图像分割中表现出强大的泛化能力，但在有限标注或罕见病理变化下，由于通用先验与任务特定需求不匹配，难以适应专门的临床任务

Method: 提出UnCoL双教师框架：1）从冻结的基础模型中蒸馏视觉和语义表示以传递通用知识；2）维护逐步适应的教师模型以捕获细粒度的任务特定表示；3）通过预测不确定性自适应调节伪标签学习，抑制不可靠监督并稳定模糊区域的学习

Result: 在多种2D和3D分割基准测试中，UnCoL始终优于最先进的半监督方法和基础模型基线，在显著减少标注需求的情况下达到接近全监督的性能

Conclusion: UnCoL成功协调了泛化与专门化，为半监督医学图像分割提供了一种有效的解决方案，能够显著降低标注成本同时保持高性能

Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.

</details>


### [110] [Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space](https://arxiv.org/abs/2512.12623)
*Chengzhi Liu,Yuzhe Yang,Yue Fan,Qingyue Wei,Sheng Liu,Xin Eric Wang*

Main category: cs.CV

TL;DR: DMLR提出动态多模态潜在推理框架，通过置信度引导的潜在策略梯度优化和动态视觉注入策略，实现推理与感知的动态交错，提升多模态推理性能同时保持高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM虽然通过CoT机制增强了跨模态理解，但仍依赖显式的逐步推理、感知-推理交互不稳定且计算开销大。受人类认知启发，认为思维应通过推理与感知的动态交错展开，而非线性进行。

Method: 提出DMLR框架：1) 使用置信度引导的潜在策略梯度优化来精炼潜在思考标记以进行深度推理；2) 引入动态视觉注入策略，在每个潜在思考标记处检索最相关的视觉特征并更新最佳视觉补丁集，然后将更新后的补丁注入潜在思考标记以实现动态视觉-文本交错。

Result: 在七个多模态推理基准测试和多种模型架构上的实验表明，DMLR显著提高了推理和感知性能，同时保持了较高的推理效率。

Conclusion: DMLR通过动态多模态潜在推理框架，实现了推理与感知的有效交错，解决了现有方法在推理稳定性、计算效率方面的限制，为多模态推理提供了新思路。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.

</details>


### [111] [Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2512.13107)
*Zhijian He,Feifei Liu,Yuwei Li,Zhanpeng Liu,Jintao Cheng,Xieyuanli Chen,Xiaoyu Tang*

Main category: cs.CV

TL;DR: DiffFusion：基于扩散模型的多模态3D目标检测框架，通过扩散修复和自适应跨模态融合提升恶劣天气下的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 当前多模态3D目标检测在恶劣天气条件下效果有限，主要受天气引起的图像失真和不同模态数据间不对齐的影响。需要一种能适应各种天气条件并保持模态间一致性的鲁棒方法。

Method: 提出DiffFusion框架：1) Diffusion-IR：基于扩散模型修复天气退化的图像；2) Point Cloud Restoration (PCR)：利用图像目标线索补偿损坏的LiDAR数据；3) BAFAM：双向自适应融合对齐模块，实现动态多模态融合和双向BEV对齐以保持空间一致性。

Result: 在三个公开数据集上的实验表明，DiffFusion在恶劣天气下实现了最先进的鲁棒性，同时保持了强大的干净数据性能。在真实世界DENSE数据集上的零样本结果进一步验证了其泛化能力。

Conclusion: DiffFusion通过扩散修复和自适应跨模态融合，有效提升了多模态3D目标检测在恶劣天气条件下的鲁棒性，具有良好的泛化性能，代码将开源发布。

Abstract: Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.

</details>


### [112] [DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass](https://arxiv.org/abs/2512.13122)
*Vivek Alumootil,Tuan-Anh Vu,M. Khalid Jawed*

Main category: cs.CV

TL;DR: DePT3R是一个新颖的框架，能够在单次前向传播中同时执行动态场景的密集点跟踪和3D重建，无需相机姿态信息，显著提升了在动态环境中的适应性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有动态场景密集3D点跟踪方法通常需要成对处理、已知相机姿态或假设时间顺序，限制了其灵活性和适用性。同时，大规模无姿态图像集合的3D重建技术取得进展，为动态场景理解的统一方法提供了机会。

Method: 通过强大的骨干网络提取深度时空特征，并使用密集预测头回归像素级映射，实现多任务学习。框架在单次前向传播中同时完成密集点跟踪和3D重建，且无需相机姿态信息。

Result: 在多个涉及动态场景的挑战性基准测试中验证了DePT3R，展示了强大的性能和相比现有最先进方法显著的内存效率提升。

Conclusion: DePT3R提供了一个统一框架，能够高效处理动态场景的密集点跟踪和3D重建，无需相机姿态信息，在动态环境中具有更好的适应性和实用性。

Abstract: Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R

</details>


### [113] [Cross-modal Fundus Image Registration under Large FoV Disparity](https://arxiv.org/abs/2512.12657)
*Hongyang Li,Junyi Tao,Qijie Wei,Ningzhi Yang,Meng Wang,Weihong Yu,Xirong Li*

Main category: cs.CV

TL;DR: CARe是一种用于大视场差异的跨模态眼底图像配准方法，通过裁剪和对齐操作解决现有方法无法处理大视场差异的问题。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态眼底图像配准方法假设视场差异较小，无法处理具有大视场差异的挑战性场景，如OCTA（较小视场）与宽视野彩色眼底照片（较大视场）之间的配准。

Method: 提出CARe方法：1）裁剪操作：利用视网膜生理结构从目标图像中裁剪出与源图像视场大致对齐的子图像；2）对齐模块：采用RANSAC算法和多项式坐标拟合的双重拟合方法改进空间变换。

Result: 在包含60对OCTA-wfCFP图像的新测试集上进行广泛实验，验证了CARe方法在大视场差异跨模态眼底图像配准中的有效性。

Conclusion: CARe是一种简单而有效的方法，通过裁剪和对齐操作成功解决了大视场差异的跨模态眼底图像配准问题，使现有小视场差异方法得以重新应用。

Abstract: Previous work on cross-modal fundus image registration (CMFIR) assumes small cross-modal Field-of-View (FoV) disparity. By contrast, this paper is targeted at a more challenging scenario with large FoV disparity, to which directly applying current methods fails. We propose Crop and Alignment for cross-modal fundus image Registration(CARe), a very simple yet effective method. Specifically, given an OCTA with smaller FoV as a source image and a wide-field color fundus photograph (wfCFP) as a target image, our Crop operation exploits the physiological structure of the retina to crop from the target image a sub-image with its FoV roughly aligned with that of the source. This operation allows us to re-purpose the previous small-FoV-disparity oriented methods for subsequent image registration. Moreover, we improve spatial transformation by a double-fitting based Alignment module that utilizes the classical RANSAC algorithm and polynomial-based coordinate fitting in a sequential manner. Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for CMFIR.

</details>


### [114] [Intrinsic Image Fusion for Multi-View 3D Material Reconstruction](https://arxiv.org/abs/2512.13157)
*Peter Kocsis,Lukas Höllein,Matthias Nießner*

Main category: cs.CV

TL;DR: 提出Intrinsic Image Fusion方法，通过多视角图像重建高质量物理材质，结合单视角先验和扩散模型，使用鲁棒优化框架融合不一致预测，最终通过逆路径追踪优化低维参数


<details>
  <summary>Details</summary>
Motivation: 材质重建高度欠约束，传统基于分析-合成的方法需要昂贵且噪声大的路径追踪。需要更好的约束来优化重建过程

Method: 1) 利用扩散基材质估计器生成多候选分解；2) 拟合低维参数函数减少不一致性；3) 提出鲁棒优化框架，结合软每视角预测选择和基于置信度的软多视角内点集；4) 使用逆路径追踪优化低维参数

Result: 在合成和真实场景的材质解缠任务中超越最先进方法，产生清晰干净的重建结果，适合高质量重光照

Conclusion: Intrinsic Image Fusion方法通过结合单视角先验和多视角一致性约束，成功解决了材质重建的欠约束问题，实现了高质量的物理材质重建

Abstract: We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.

</details>


### [115] [CogDoc: Towards Unified thinking in Documents](https://arxiv.org/abs/2512.12658)
*Qixin Xu,Haozhe Wang,Che Liu,Fangzhen Lin,Wenhu Chen*

Main category: cs.CV

TL;DR: CogDoc提出了一种模仿人类认知过程的粗到细思维框架，通过"快速阅读"和"专注思考"两阶段解决文档推理中可扩展性与保真度的权衡问题，采用直接强化学习方法避免策略冲突，7B模型在视觉丰富文档基准上超越了GPT-4o等更大模型。


<details>
  <summary>Details</summary>
Motivation: 当前文档推理范式存在可扩展性（处理长上下文文档）与保真度（捕捉细粒度多模态细节）之间的基本权衡。为了弥合这一差距，需要一种能够同时处理长文档并保持精细推理能力的解决方案。

Method: 提出CogDoc统一粗到细思维框架，模仿人类认知过程：1）低分辨率"快速阅读"阶段进行可扩展信息定位；2）高分辨率"专注思考"阶段进行深度推理。采用直接强化学习方法进行后训练，避免监督微调初始化带来的策略冲突。

Result: 7B参数模型在其参数类别内实现了最先进的性能，在具有挑战性的视觉丰富文档基准上显著超越了GPT-4o等更大的专有模型。直接强化学习方法比带有监督微调初始化的强化学习表现更好。

Conclusion: CogDoc框架通过模仿人类认知的粗到细思维过程，有效解决了文档推理中的可扩展性与保真度权衡问题。直接强化学习策略避免了策略冲突，使较小模型能够在视觉丰富文档任务上超越更大模型。

Abstract: Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution "Fast Reading" phase for scalable information localization,followed by a high-resolution "Focused Thinking" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the "policy conflict" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.

</details>


### [116] [A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis](https://arxiv.org/abs/2512.13164)
*Xianchao Guan,Zhiyuan Fan,Yifeng Wang,Fuqiang Chen,Yanjiang Zhou,Zengyang Che,Hongxue Meng,Xin Li,Yaowei Wang,Hongpeng Wang,Min Zhang,Heng Tao Shen,Zheng Zhang,Yongbing Zhang*

Main category: cs.CV

TL;DR: CRAFTS是首个病理学专用的文本到图像生成基础模型，通过相关性调节对齐框架解决生成模型语义漂移问题，生成高质量多样化病理图像，增强临床任务性能。


<details>
  <summary>Details</summary>
Motivation: 临床级病理AI发展受限于高质量标注数据稀缺，现有生成模型存在语义不稳定和形态学幻觉问题，影响诊断可靠性。

Method: 提出CRAFTS框架，采用双阶段训练策略处理280万图像-文本对，引入新颖的对齐机制抑制语义漂移，确保生物学准确性。

Result: 模型生成30种癌症类型的多样化病理图像，质量经客观指标和病理学家评估验证；增强数据集提升分类、跨模态检索、自监督学习和视觉问答等临床任务性能。

Conclusion: CRAFTS克服数据稀缺和隐私问题，为罕见和复杂癌症表型提供无限多样的标注组织学数据源，推动稳健诊断工具开发。

Abstract: The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.

</details>


### [117] [LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models](https://arxiv.org/abs/2512.13290)
*Shu Yu,Chaochao Lu*

Main category: cs.CV

TL;DR: LINA框架通过因果干预解决扩散模型在物理对齐和OOD指令跟随上的问题，利用因果场景图和物理对齐探针数据集，在提示和视觉潜在空间进行针对性引导，并重新分配因果感知的去噪调度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成方面取得了显著成功，但在物理对齐和超出分布（OOD）指令跟随方面仍存在困难。这些问题源于模型未能学习因果方向和解耦因果因子以实现新颖重组。

Method: 引入因果场景图（CSG）和物理对齐探针（PAP）数据集进行诊断干预。基于三个关键发现，提出LINA框架：学习预测特定提示的干预，包括（1）在提示和视觉潜在空间进行针对性引导，（2）重新分配因果感知的去噪调度。

Result: LINA框架在图像和视频扩散模型中同时实现了物理对齐和OOD指令跟随，在具有挑战性的因果生成任务和Winoground数据集上达到了最先进的性能。

Conclusion: 通过因果干预和重新分配的去噪调度，LINA框架有效解决了扩散模型在物理对齐和OOD指令跟随方面的核心问题，为因果感知的生成模型提供了新方向。

Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.

</details>


### [118] [InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation](https://arxiv.org/abs/2512.12664)
*Sreehari Rajan,Kunal Bhosikar,Charu Sharma*

Main category: cs.CV

TL;DR: InteracTalker是一个统一框架，能够同时生成语音驱动的手势和物体交互动作，通过多阶段训练学习统一的运动、语音和提示嵌入空间，并采用自适应融合策略解决异质条件信号不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法分别处理语音驱动手势和物体交互，缺乏集成框架和综合数据集，限制了真实世界应用。需要能够同时响应语言和物理对象的自然人体运动生成方法。

Method: 1) 构建丰富的人-物体交互数据集；2) 多阶段训练学习统一嵌入空间；3) 使用广义运动适应模块进行独立训练；4) 在扩散采样中采用自适应融合策略动态重新加权条件信号。

Result: InteracTalker在语音手势生成和物体交互合成方面均优于现有方法，能够生成高度真实、物体感知的全身运动，具有增强的真实感、灵活性和控制性。

Conclusion: 该框架成功统一了先前分离的任务，通过集成物体感知交互和语音手势生成，为交互式数字体验提供了更自然、更全面的运动生成解决方案。

Abstract: Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.

</details>


### [119] [Face Identity Unlearning for Retrieval via Embedding Dispersion](https://arxiv.org/abs/2512.13317)
*Mikhail Zakharov*

Main category: cs.CV

TL;DR: 本文研究人脸检索系统中的身份遗忘问题，提出一种基于分散的遗忘方法，使特定身份无法被检索，同时保持其他身份的检索性能。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统虽然能有效识别身份，但存在严重的隐私风险，可能被用于未经授权的身份追踪。现有的机器学习遗忘方法在人脸检索场景中应用不足，特别是针对现代基于嵌入的识别模型。

Method: 评估了多种现有近似类别遗忘方法（如随机标签、梯度上升、边界遗忘等），并提出一种简单有效的基于分散的遗忘方法。该方法通过在超球面上分散目标身份的嵌入向量，防止形成紧凑的身份聚类，从而实现遗忘效果。

Result: 在VGGFace2和CelebA等标准基准测试上的广泛实验表明，该方法在实现优越遗忘效果的同时，能够保持其他身份的检索效用。

Conclusion: 本文首次系统研究人脸检索中的身份遗忘问题，提出的基于分散的遗忘方法能有效保护特定身份隐私，同时维持整体检索系统的性能，为人脸识别系统的隐私保护提供了实用解决方案。

Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.

</details>


### [120] [Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning](https://arxiv.org/abs/2512.12667)
*Haiyang Zheng,Nan Pu,Wenjing Li,Teng Long,Nicu Sebe,Zhun Zhong*

Main category: cs.CV

TL;DR: 提出CAL框架解决开放世界深度伪造溯源问题，通过置信度感知非对称学习和动态原型剪枝，显著提升已知和未知伪造类型的溯源性能。


<details>
  <summary>Details</summary>
Motivation: 合成人脸图像的泛滥加剧了对开放世界深度伪造溯源的需求，但现有方法存在置信度偏差和需要预先知道未知伪造类型数量的不现实假设。

Method: 提出置信度感知非对称学习框架，包含置信度感知一致性正则化和非对称置信度增强两个组件，以及动态原型剪枝策略自动估计未知伪造类型数量。

Result: 在标准开放世界深度伪造溯源基准和新扩展基准上，CAL方法持续优于先前方法，在已知和未知伪造溯源上都达到了新的最先进性能。

Conclusion: CAL框架有效解决了开放世界深度伪造溯源的置信度偏差和先验假设问题，为现实世界应用提供了可扩展的解决方案。

Abstract: The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.

</details>


### [121] [End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery](https://arxiv.org/abs/2512.13402)
*Lorenzo Pettinari,Sidaty El Hadramy,Michael Wehrli,Philippe C. Cattin,Daniel Studer,Carol C. Hasler,Maria Licci*

Main category: cs.CV

TL;DR: End2Reg是一个端到端深度学习框架，联合优化分割和配准，无需弱分割标签，在脊柱手术导航中实现毫米级精度


<details>
  <summary>Details</summary>
Motivation: 当前脊柱手术导航系统依赖侵入性骨锚标记和放射成像，具有辐射强、工作流程中断等问题。现有的无标记RGB-D配准方法依赖弱分割标签，容易传播误差

Method: 提出End2Reg端到端深度学习框架，联合优化分割和配准，网络学习专门为配准优化的分割掩码，仅通过配准目标引导而无直接分割监督

Result: 在离体和体内基准测试中达到最先进性能：中位目标配准误差降低32%至1.83mm，均方根误差降低45%至3.95mm。消融研究证实端到端优化显著提高配准精度

Conclusion: 该端到端RGB-D配准管道消除了对弱标签和手动步骤的依赖，朝着完全自动化的无标记术中导航迈进

Abstract: Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.

</details>


### [122] [Progressive Conditioned Scale-Shift Recalibration of Self-Attention for Online Test-time Adaptation](https://arxiv.org/abs/2512.12673)
*Yushun Tang,Ziqiong Liu,Jiyuan Jia,Yi Zhang,Zhihai He*

Main category: cs.CV

TL;DR: 提出PCSR方法，通过渐进式条件尺度-偏移重校准来在线适应Transformer模型到新领域，显著提升测试时域适应性能


<details>
  <summary>Details</summary>
Motivation: 当将Transformer模型应用于新目标域时，其自注意力模块的Query、Key和Value特征会发生显著变化，导致性能大幅下降。需要解决这一重要问题。

Method: 提出渐进式条件尺度-偏移重校准(PCSR)方法：将在线模型适应视为渐进域偏移分离过程；在每层Transformer中学习域分离网络提取域偏移特征；使用因子生成网络预测自注意力重校准的尺度和偏移参数；这两个轻量级网络在推理时在线适应。

Result: 在基准数据集上的实验结果表明，PCSR方法能够显著提升在线测试时域适应性能，在ImageNet-C数据集上的分类准确率提升高达3.9%。

Conclusion: 提出的渐进式条件尺度-偏移重校准方法有效解决了Transformer模型在新目标域中的适应问题，通过轻量级的在线调整显著提升了模型性能。

Abstract: Online test-time adaptation aims to dynamically adjust a network model in real-time based on sequential input samples during the inference stage. In this work, we find that, when applying a transformer network model to a new target domain, the Query, Key, and Value features of its self-attention module often change significantly from those in the source domain, leading to substantial performance degradation of the transformer model. To address this important issue, we propose to develop a new approach to progressively recalibrate the self-attention at each layer using a local linear transform parameterized by conditioned scale and shift factors. We consider the online model adaptation from the source domain to the target domain as a progressive domain shift separation process. At each transformer network layer, we learn a Domain Separation Network to extract the domain shift feature, which is used to predict the scale and shift parameters for self-attention recalibration using a Factor Generator Network. These two lightweight networks are adapted online during inference. Experimental results on benchmark datasets demonstrate that the proposed progressive conditioned scale-shift recalibration (PCSR) method is able to significantly improve the online test-time domain adaptation performance by a large margin of up to 3.9\% in classification accuracy on the ImageNet-C dataset.

</details>


### [123] [DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides](https://arxiv.org/abs/2512.13600)
*Haoyue Zhang,Meera Chappidi,Erolcan Sayar,Helen Richards,Zhijun Chen,Lucas Liu,Roxanne Wadia,Peter A Humphrey,Fady Ghali,Alberto Contreras-Sanz,Peter Black,Jonathan Wright,Stephanie Harmon,Michael Haffner*

Main category: cs.CV

TL;DR: 提出轻量级域自适应自监督适配器（DA-SSL），用于将预训练病理基础模型（PFMs）特征对齐到TURBT领域，无需微调基础模型，在膀胱癌治疗反应预测中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型（PFMs）在特定癌症类型（如膀胱癌TURBT样本）上存在领域偏移问题，因为这些癌症类型在预训练数据中较少出现，且样本包含组织碎片和电灼伪影等罕见特征，限制了PFMs在临床挑战性任务中的应用。

Method: 提出域自适应自监督适配器（DA-SSL），通过轻量级自监督学习将预训练PFM特征重新对齐到目标领域（TURBT），而不需要微调基础模型本身，结合多实例学习（MIL）框架进行膀胱癌治疗反应预测。

Result: 在多中心研究中，DA-SSL在五折交叉验证中达到AUC 0.77±0.04，外部测试准确率0.84，敏感性0.71，特异性0.91（多数投票）。证明轻量级域自适应能有效增强基于PFM的MIL管道。

Conclusion: 轻量级域自适应自监督方法能有效解决病理基础模型在特定癌症类型上的领域偏移问题，为临床挑战性组织病理学任务提供了实用解决方案，代码已开源。

Abstract: Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.

</details>


### [124] [Feedforward 3D Editing via Text-Steerable Image-to-3D](https://arxiv.org/abs/2512.13678)
*Ziqi Ma,Hongqiao Chen,Yisong Yue,Georgia Gkioxari*

Main category: cs.CV

TL;DR: Steer3D：一种前馈方法，为图像到3D模型添加文本可操控性，使生成的3D资产能够通过语言指令进行编辑


<details>
  <summary>Details</summary>
Motivation: AI生成的3D资产在实际应用中需要易于编辑的能力，现有方法缺乏有效的文本操控性

Method: 受ControlNet启发，将文本操控性适配到图像到3D生成；构建自动数据生成引擎，采用基于流匹配训练和直接偏好优化（DPO）的两阶段训练方法

Result: 相比竞争方法，Steer3D更忠实地遵循语言指令，与原始3D资产保持更好一致性，同时速度提升2.4倍到28.5倍

Conclusion: Steer3D证明可以通过10万数据为预训练的图像到3D生成模型添加新的文本操控模态，实现高效可编辑的3D资产生成

Abstract: Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/

</details>


### [125] [$β$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment](https://arxiv.org/abs/2512.12678)
*Fatimah Zohra,Chen Zhao,Hani Itani,Bernard Ghanem*

Main category: cs.CV

TL;DR: β-CLIP通过多粒度文本条件对比学习框架，实现从完整描述到句子、短语的层次化视觉-文本对齐，显著提升了细粒度视觉-语言任务性能。


<details>
  <summary>Details</summary>
Motivation: CLIP在全局视觉-文本对齐方面表现良好，但在细粒度任务上表现不佳，即使使用详细描述进行微调也难以改善。需要一种能够实现多层次语义对齐的方法。

Method: 提出β-CLIP框架：1）多粒度文本条件对比学习，从完整描述到句子、短语的层次化对齐；2）使用交叉注意力动态池化图像块生成上下文视觉嵌入；3）引入β-上下文对比对齐损失（β-CAL），平衡严格查询匹配与宽松图像内上下文化。

Result: 在Urban1K上达到91.8% T2I和92.3% I2T的R@1，在FG-OVD（Hard）上达到30.9%，在没有使用困难负样本的方法中达到最先进水平。

Conclusion: β-CLIP建立了鲁棒、自适应的密集视觉-语言对应基准，显著改善了密集对齐能力，为细粒度视觉-语言理解提供了有效解决方案。

Abstract: CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.

</details>


### [126] [DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders](https://arxiv.org/abs/2512.13690)
*Susung Hong,Chongjian Ge,Zhifei Zhang,Jui-Hsien Wang*

Main category: cs.CV

TL;DR: DiffusionBrowser：一个轻量级解码器框架，可在去噪过程中实时生成视频预览，支持交互式引导生成并揭示去噪过程的内部机制


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散模型存在生成不精确、速度慢且生成过程不透明的问题，用户需要长时间等待而无法了解生成进度

Method: 提出DiffusionBrowser框架，这是一个模型无关的轻量级解码器，可在去噪过程的任意时间步或Transformer块生成多模态预览（RGB和场景内在属性），速度超过实时4倍

Result: 能够以超过实时4倍的速度生成与最终视频一致的预览，支持通过随机性重注入和模态引导进行交互式控制，并揭示了去噪过程中场景、物体等细节的组装机制

Conclusion: DiffusionBrowser解决了视频扩散模型的透明度和交互性问题，为生成过程提供了实时预览和新的控制能力，同时揭示了原本黑盒去噪过程的内在机制

Abstract: Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.

</details>


### [127] [Efficient Vision-Language Reasoning via Adaptive Token Pruning](https://arxiv.org/abs/2512.12701)
*Xue Li,Xiaonan Song,Henry Hu*

Main category: cs.CV

TL;DR: ATP通过动态剪枝视觉token，结合ViT注意力与CLIP跨模态相似度，保留关键token给LLM，实现约40%计算量减少和1.5倍加速，精度损失小于1%


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型计算效率低下，对所有token进行统一处理，导致实际部署时计算需求过高，需要动态推理机制来提升效率

Method: 提出自适应token剪枝(ATP)，在视觉-语言接口处运作，结合ViT CLS注意力（模态内显著性）和CLIP文本-图像相似度（跨模态相关性）计算混合重要性分数，仅保留top-K token给LLM处理

Result: 在VQAv2、GQA和COCO数据集上评估，ATP减少约40%推理FLOPs，端到端延迟提升约1.5倍，精度损失小于1%，同时增强可解释性和鲁棒性

Conclusion: ATP证明资源受限推理与模型可靠性不是竞争目标，自适应剪枝能提升效率同时抑制虚假相关性，为高效多模态边缘计算提供解决方案

Abstract: Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.

</details>


### [128] [Spinal Line Detection for Posture Evaluation through Train-ing-free 3D Human Body Reconstruction with 2D Depth Images](https://arxiv.org/abs/2512.12718)
*Sehyun Kim,Hye Jun Lee,Jiwoo Lee,Changgyun Kim,Taemin Lee*

Main category: cs.CV

TL;DR: 提出基于四方向深度图像融合的3D人体姿态分析系统，通过全局与精细配准的分层匹配恢复3D人体模型并自动估计脊柱中心线，无需训练数据或复杂神经网络模型。


<details>
  <summary>Details</summary>
Motivation: 现有多图像人体重建方法需要昂贵设备和复杂流程，而单图像方法因遮挡和视角限制难以准确估计脊柱中心线等内部结构。需要一种能结合两者优势的方法。

Method: 1) 集成四方向深度图像恢复3D人体模型；2) 采用全局与精细配准的分层匹配处理噪声和遮挡；3) 应用自适应顶点减少保持网格分辨率和形状可靠性；4) 使用细节层次集成确保脊柱角度估计的准确性和稳定性。

Result: 实现了高精度的3D脊柱配准估计，验证了匹配质量的提升，无需依赖训练数据或复杂神经网络模型。

Conclusion: 提出的3D人体姿态分析系统有效解决了多图像方法设备昂贵和单图像方法估计内部结构困难的问题，实现了准确稳定的脊柱中心线估计。

Abstract: The spinal angle is an important indicator of body balance. It is important to restore the 3D shape of the human body and estimate the spine center line. Existing mul-ti-image-based body restoration methods require expensive equipment and complex pro-cedures, and single image-based body restoration methods have limitations in that it is difficult to accurately estimate the internal structure such as the spine center line due to occlusion and viewpoint limitation. This study proposes a method to compensate for the shortcomings of the multi-image-based method and to solve the limitations of the sin-gle-image method. We propose a 3D body posture analysis system that integrates depth images from four directions to restore a 3D human model and automatically estimate the spine center line. Through hierarchical matching of global and fine registration, restora-tion to noise and occlusion is performed. Also, the Adaptive Vertex Reduction is applied to maintain the resolution and shape reliability of the mesh, and the accuracy and stabil-ity of spinal angle estimation are simultaneously secured by using the Level of Detail en-semble. The proposed method achieves high-precision 3D spine registration estimation without relying on training data or complex neural network models, and the verification confirms the improvement of matching quality.

</details>


### [129] [GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation](https://arxiv.org/abs/2512.12751)
*Zhenya Yang,Zhe Liu,Yuxiang Lu,Liping Hou,Chenxuan Miao,Siyi Peng,Bailan Feng,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: GenieDrive：基于4D占据表示的物理感知驾驶世界模型，通过VAE压缩和互控注意力实现高效可控的多视角驾驶视频生成


<details>
  <summary>Details</summary>
Motivation: 现有驾驶世界模型通常使用单一扩散模型直接将驾驶动作映射到视频，导致学习困难且输出物理不一致。需要一种能够生成物理感知驾驶视频的方法，以支持驾驶规划、OOD数据合成和闭环评估。

Method: 1. 首先生成4D占据表示（包含高分辨率3D结构和动态信息）作为物理感知基础；2. 提出VAE将占据压缩为潜在tri-plane表示，将潜在大小减少到之前方法的58%；3. 引入互控注意力（MCA）精确建模控制对占据演化的影响；4. 端到端联合训练VAE和预测模块；5. 在视频生成模型中使用归一化多视角注意力，以4D占据为指导生成多视角驾驶视频。

Result: 1. 预测mIoU提升7.2%，推理速度达41 FPS，仅使用3.47M参数；2. 视频质量显著改善，FVD降低20.7%；3. 实现了高度可控、多视角一致且物理感知的驾驶视频生成。

Conclusion: GenieDrive通过4D占据表示、高效的VAE压缩和互控注意力机制，成功构建了物理感知的驾驶世界模型，在预测准确性和视频质量方面均有显著提升，为驾驶规划、数据合成和评估提供了有效工具。

Abstract: Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.

</details>


### [130] [FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning](https://arxiv.org/abs/2512.12756)
*Yue Jiang,Dingkang Yang,Minghao Han,Jinghang Han,Zizhi Chen,Yizhou Liu,Mingcheng Li,Peng Zhai,Lihua Zhang*

Main category: cs.CV

TL;DR: FysicsWorld是首个统一的全模态基准测试，支持图像、视频、音频和文本的双向输入输出，包含16个主要任务和3268个样本，用于全面评估多模态大语言模型的理解、生成和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基准测试存在局限性：模态覆盖不完整、仅限于文本中心输出、模态间相互依赖和互补性弱。需要建立一个统一的全模态基准来弥补这些差距。

Method: 提出FysicsWorld基准，包含16个主要任务和3268个样本，从40多个高质量来源聚合。采用跨模态互补性筛选(CMCS)策略，构建系统化的数据生成框架，支持语音交互和融合依赖的跨模态推理。

Result: 评估了30多个最先进的基线模型，包括MLLMs、模态特定模型、统一理解生成模型和全模态语言模型。揭示了不同模型在理解、生成和推理方面的性能差异和局限性。

Conclusion: FysicsWorld为评估和推进下一代全模态架构建立了统一的基础和强基线，有助于全面评估多模态模型的能力。

Abstract: Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.

</details>


### [131] [Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior](https://arxiv.org/abs/2512.12774)
*Hao Wang,Ashish Bastola,Chaoyi Zhou,Wenhui Zhu,Xiwen Chen,Xuanzhao Dong,Siyu Huang,Abolfazl Razi*

Main category: cs.CV

TL;DR: Fast-2DGS：一个轻量级框架，通过深度高斯先验和属性回归网络实现高效的高斯图像表示，单次前向传播即可获得高质量重建，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型能产生高保真视觉内容，对高效、可解释、可编辑的图像表示需求增长。现有2D高斯泼溅方法需要后优化，随机或启发式初始化对图像复杂度不敏感且收敛慢，而可学习网络方法又增加了计算和架构复杂度。

Method: 提出Fast-2DGS框架：1) 深度高斯先验：条件网络捕获不同复杂度下高斯基元的空间分布；2) 属性回归网络：预测密集高斯属性。这种解耦架构实现单次前向传播的高质量重建，仅需最小化微调。

Result: 实验表明该方法在单次前向传播后即可获得高质量重建，显著降低计算成本而不影响视觉质量，使2DGS更接近工业级部署。

Conclusion: Fast-2DGS通过轻量级框架解决了2D高斯泼溅的初始化问题，平衡了效率与质量，为工业应用提供了可行的解决方案。

Abstract: As generative models become increasingly capable of producing high-fidelity visual content, the demand for efficient, interpretable, and editable image representations has grown substantially. Recent advances in 2D Gaussian Splatting (2DGS) have emerged as a promising solution, offering explicit control, high interpretability, and real-time rendering capabilities (>1000 FPS). However, high-quality 2DGS typically requires post-optimization. Existing methods adopt random or heuristics (e.g., gradient maps), which are often insensitive to image complexity and lead to slow convergence (>10s). More recent approaches introduce learnable networks to predict initial Gaussian configurations, but at the cost of increased computational and architectural complexity. To bridge this gap, we present Fast-2DGS, a lightweight framework for efficient Gaussian image representation. Specifically, we introduce Deep Gaussian Prior, implemented as a conditional network to capture the spatial distribution of Gaussian primitives under different complexities. In addition, we propose an attribute regression network to predict dense Gaussian properties. Experiments demonstrate that this disentangled architecture achieves high-quality reconstruction in a single forward pass, followed by minimal fine-tuning. More importantly, our approach significantly reduces computational cost without compromising visual quality, bringing 2DGS closer to industry-ready deployment.

</details>


### [132] [L-STEC: Learned Video Compression with Long-term Spatio-Temporal Enhanced Context](https://arxiv.org/abs/2512.12790)
*Tiange Zhang,Zhimeng Huang,Xiandong Meng,Kai Zhang,Zhipin Deng,Siwei Ma*

Main category: cs.CV

TL;DR: L-STEC方法通过长短期记忆网络扩展参考链以捕获长期依赖，并结合像素域的空间上下文信息，显著提升了神经视频压缩性能，相比DCVC-TCM节省37.01%比特率。


<details>
  <summary>Details</summary>
Motivation: 现有神经视频压缩方法仅依赖前一帧特征预测时域上下文，存在两个关键问题：1) 短参考窗口无法捕获长期依赖和精细纹理细节；2) 仅传播特征级信息会导致误差累积和纹理细节丢失。

Method: 提出长时空增强上下文(L-STEC)方法：1) 使用LSTM扩展参考链以捕获长期依赖；2) 从像素域引入扭曲的空间上下文；3) 通过多感受野网络融合时空信息以更好地保留参考细节。

Result: L-STEC显著提升了压缩性能，相比DCVC-TCM在PSNR上节省37.01%比特率，在MS-SSIM上节省31.65%比特率，超越了VTM-17.0和DCVC-FM，建立了新的最先进性能。

Conclusion: 通过扩展参考链捕获长期依赖，并结合像素域的空间上下文信息，L-STEC方法能够有效丰富上下文信息，显著提升神经视频压缩性能，解决了现有方法中的关键限制。

Abstract: Neural Video Compression has emerged in recent years, with condition-based frameworks outperforming traditional codecs. However, most existing methods rely solely on the previous frame's features to predict temporal context, leading to two critical issues. First, the short reference window misses long-term dependencies and fine texture details. Second, propagating only feature-level information accumulates errors over frames, causing prediction inaccuracies and loss of subtle textures. To address these, we propose the Long-term Spatio-Temporal Enhanced Context (L-STEC) method. We first extend the reference chain with LSTM to capture long-term dependencies. We then incorporate warped spatial context from the pixel domain, fusing spatio-temporal information through a multi-receptive field network to better preserve reference details. Experimental results show that L-STEC significantly improves compression by enriching contextual information, achieving 37.01% bitrate savings in PSNR and 31.65% in MS-SSIM compared to DCVC-TCM, outperforming both VTM-17.0 and DCVC-FM and establishing new state-of-the-art performance.

</details>


### [133] [DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning](https://arxiv.org/abs/2512.12799)
*Zhe Liu,Runhui Huang,Rui Yang,Siming Yan,Zining Wang,Lu Hou,Di Lin,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: DrivePI是一个空间感知的4D多模态大语言模型，作为统一的视觉-语言-动作框架，在自动驾驶中并行执行空间理解、3D感知、预测和规划。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在多个领域表现出强大能力，但在自动驾驶中生成细粒度3D感知和预测输出的应用仍未被充分探索。

Method: 提出DrivePI框架，整合点云、多视角图像和语言指令，通过端到端优化并行执行空间理解、3D占用、占用流预测和规划。开发数据引擎生成文本-占用和文本-流问答对用于4D空间理解。

Result: 仅使用0.5B Qwen2.5作为骨干模型，DrivePI在多个任务上超越现有VLA模型和专用VA模型：nuScenes-QA准确率提升2.5%，碰撞率降低70%；3D占用RayIoU提升10.3，占用流mAVE从0.591降至0.509，规划L2误差降低32%。

Conclusion: DrivePI作为一个统一的单一模型，在自动驾驶的3D感知、预测和规划任务上表现出色，证明了空间感知4D MLLM框架的有效性。

Abstract: Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI

</details>


### [134] [Learning Common and Salient Generative Factors Between Two Image Datasets](https://arxiv.org/abs/2512.12800)
*Yunlong He,Gwilherm Lesné,Ziqian Liu,Michaël Soumm,Pietro Gori*

Main category: cs.CV

TL;DR: 提出对比分析框架，从两个数据集中分离共享生成因子和特有生成因子，适用于GAN和扩散模型


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注条件操纵或解耦表示学习，但缺乏对两个数据集间共享与特有生成因子的分离研究。对比分析问题较少被探索，且现有方法需要属性监督信号，而本文方法仅使用数据集信号，监督更弱。

Method: 提出新颖的对比分析框架，适用于GAN和扩散模型。通过定义新的学习策略和损失函数，确保共享因子和特有因子的有效分离，同时保持高质量生成。

Result: 在多样化数据集（人脸、动物图像、医学扫描）上评估，相比现有方法，展示了更优的分离能力和图像合成质量。

Conclusion: 提出的对比分析框架能够有效分离两个数据集间的共享和特有生成因子，仅使用数据集信号而非属性监督，在多种数据类型上表现出色。

Abstract: Recent advancements in image synthesis have enabled high-quality image generation and manipulation. Most works focus on: 1) conditional manipulation, where an image is modified conditioned on a given attribute, or 2) disentangled representation learning, where each latent direction should represent a distinct semantic attribute. In this paper, we focus on a different and less studied research problem, called Contrastive Analysis (CA). Given two image datasets, we want to separate the common generative factors, shared across the two datasets, from the salient ones, specific to only one dataset. Compared to existing methods, which use attributes as supervised signals for editing (e.g., glasses, gender), the proposed method is weaker, since it only uses the dataset signal. We propose a novel framework for CA, that can be adapted to both GAN and Diffusion models, to learn both common and salient factors. By defining new and well-adapted learning strategies and losses, we ensure a relevant separation between common and salient factors, preserving a high-quality generation. We evaluate our approach on diverse datasets, covering human faces, animal images and medical scans. Our framework demonstrates superior separation ability and image quality synthesis compared to prior methods.

</details>


### [135] [Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal](https://arxiv.org/abs/2512.12875)
*Weihan Xu,Kan Jen Cheng,Koichi Saito,Muhammad Jehanzeb Mirza,Tingle Li,Yisi Liu,Alexander H. Liu,Liming Wang,Masato Ishii,Takashi Shibuya,Yuki Mitsufuji,Gopala Anumanchipalli,Paul Pu Liang*

Main category: cs.CV

TL;DR: SAVE模型通过Schrodinger Bridge实现音频视频联合编辑，在SAVEBench数据集上训练，能同步移除目标对象并保持内容对齐


<details>
  <summary>Details</summary>
Motivation: 联合编辑音频和视觉内容对于精确可控的内容创作至关重要，但面临配对数据不足和模态异质性挑战

Method: 提出SAVEBench配对视听数据集，训练SAVE模型（Schrodinger Audio-Visual Editor），使用端到端流匹配和Schrodinger Bridge学习从源到目标视听混合的直接传输

Result: SAVE模型能够同步移除音频和视频中的目标对象，同时保持剩余内容，相比音频编辑器和视频编辑器的组合，具有更强的时间同步性和视听语义对应

Conclusion: 提出的SAVE模型通过联合编辑框架有效解决了视听内容编辑的挑战，在保持内容对齐方面优于现有方法组合

Abstract: Joint editing of audio and visual content is crucial for precise and controllable content creation. This new task poses challenges due to the limitations of paired audio-visual data before and after targeted edits, and the heterogeneity across modalities. To address the data and modeling challenges in joint audio-visual editing, we introduce SAVEBench, a paired audiovisual dataset with text and mask conditions to enable object-grounded source-to-target learning. With SAVEBench, we train the Schrodinger Audio-Visual Editor (SAVE), an end-to-end flow-matching model that edits audio and video in parallel while keeping them aligned throughout processing. SAVE incorporates a Schrodinger Bridge that learns a direct transport from source to target audiovisual mixtures. Our evaluation demonstrates that the proposed SAVE model is able to remove the target objects in audio and visual content while preserving the remaining content, with stronger temporal synchronization and audiovisual semantic correspondence compared with pairwise combinations of an audio editor and a video editor.

</details>


### [136] [Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection](https://arxiv.org/abs/2512.12884)
*Xiangzhong Liu,Jiajie Zhang,Hao Shen*

Main category: cs.CV

TL;DR: 提出一种端到端跨层级融合Transformer方法，将高度抽象的对象列表信息与原始相机图像结合进行3D目标检测，在nuScenes数据集上显著优于视觉基线方法。


<details>
  <summary>Details</summary>
Motivation: 汽车传感器融合系统中，智能传感器和V2X模块通常只提供处理后的对象列表而非原始数据。传统方法分别处理原始数据后在对象级融合，存在效率问题。需要一种能直接融合抽象对象列表和原始图像的方法。

Method: 1) 提出端到端跨层级融合Transformer架构；2) 将对象列表作为去噪查询输入Transformer，与可学习查询一起传播；3) 引入可变形高斯掩码，利用对象列表的位置和尺寸先验指导注意力；4) 提出从真实边界框生成伪对象列表的方法，模拟状态噪声和误检漏检。

Result: 在nuScenes数据集上，该方法相比视觉基线有显著性能提升。展示了在不同噪声水平的模拟对象列表和真实检测器上的泛化能力。

Conclusion: 这是首个进行跨层级融合的工作，成功将高度抽象的对象列表与原始图像融合，提高了3D目标检测性能，并展示了良好的泛化能力。

Abstract: In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.

</details>


### [137] [Revisiting 2D Foundation Models for Scalable 3D Medical Image Classification](https://arxiv.org/abs/2512.12887)
*Han Liu,Bogdan Georgescu,Yanbo Zhang,Youngjin Yoo,Michael Baumgartner,Riqiang Gao,Jianing Wang,Gengyan Zhao,Eli Gibson,Dorin Comaniciu,Sasa Grbic*

Main category: cs.CV

TL;DR: AnyMC3D是一个可扩展的3D医学图像分类框架，通过轻量级插件适配2D基础模型，在12个任务上实现SOTA性能，无需为每个任务单独训练模型。


<details>
  <summary>Details</summary>
Motivation: 当前医学基础模型存在三个关键问题：数据机制偏差、次优适应和任务覆盖不足。需要解决这些问题以实现更有效的3D医学图像分类。

Method: 从2D基础模型适配到3D分类，仅添加轻量级插件（每个任务约100万参数），保持主干网络冻结。支持多视图输入、像素级监督和可解释热图生成。

Result: 在涵盖不同病理、解剖结构和模态的12个任务基准测试中，AnyMC3D首次展示了使用单一可扩展框架在多样化应用中实现SOTA性能的可行性（包括VLM3D挑战赛第一名）。

Conclusion: 有效适应对释放基础模型潜力至关重要；通用基础模型经过适当适应可以匹配医学专用模型；2D方法在3D分类中优于3D架构；单一可扩展框架可替代多个任务特定模型。

Abstract: 3D medical image classification is essential for modern clinical workflows. Medical foundation models (FMs) have emerged as a promising approach for scaling to new tasks, yet current research suffers from three critical pitfalls: data-regime bias, suboptimal adaptation, and insufficient task coverage. In this paper, we address these pitfalls and introduce AnyMC3D, a scalable 3D classifier adapted from 2D FMs. Our method scales efficiently to new tasks by adding only lightweight plugins (about 1M parameters per task) on top of a single frozen backbone. This versatile framework also supports multi-view inputs, auxiliary pixel-level supervision, and interpretable heatmap generation. We establish a comprehensive benchmark of 12 tasks covering diverse pathologies, anatomies, and modalities, and systematically analyze state-of-the-art 3D classification techniques. Our analysis reveals key insights: (1) effective adaptation is essential to unlock FM potential, (2) general-purpose FMs can match medical-specific FMs if properly adapted, and (3) 2D-based methods surpass 3D architectures for 3D classification. For the first time, we demonstrate the feasibility of achieving state-of-the-art performance across diverse applications using a single scalable framework (including 1st place in the VLM3D challenge), eliminating the need for separate task-specific models.

</details>


### [138] [Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution](https://arxiv.org/abs/2512.12898)
*Abhinav Kumar,Tristan Aumentado-Armstrong,Lazar Valkov,Gopal Sharma,Alex Levinshtein,Radek Grzeszczuk,Suren Kumar*

Main category: cs.CV

TL;DR: 提出Queried-Convolutions（Qonvolutions），通过卷积的邻域特性增强高频信号学习，在多种高频学习任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 神经网络在处理高频信号时存在频谱偏差和优化困难，现有技术如傅里叶编码虽有改进但仍需提升高频信息学习能力

Method: 提出Qonvolutions，将低频信号与查询（如坐标）进行卷积，利用卷积的邻域特性增强复杂高频信号的学习

Result: 在1D回归、2D超分辨率、2D图像回归和新视角合成等高频学习任务中性能显著提升，结合高斯泼溅在新视角合成中实现SOTA性能

Conclusion: Qonvolutions是简单有效的改进方法，能显著增强神经网络对高频信号的学习能力，在计算机视觉和图形学任务中具有广泛应用价值

Abstract: Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier encodings have made great strides in improving performance, there remains scope for improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality.

</details>


### [139] [Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection](https://arxiv.org/abs/2512.12906)
*Zhimao Peng,Enguang Wang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 提出基于预测样本分配（PSA）的SCOOD框架，通过双阈值三元样本分配策略提高ID/OOD样本集纯度，结合概念对比表示学习和重训练策略，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有SCOOD方法主要采用基于聚类的ID样本过滤策略，从无标签数据中选择干净ID样本，并将剩余样本作为辅助OOD数据，这不可避免地引入了大量噪声样本。需要解决训练数据中的噪声问题。

Method: 1. 基于预测能量分数的双阈值三元样本分配策略：将无标签数据分配到ID、OOD和丢弃三个集合，提高样本集纯度；2. 概念对比表示学习损失：扩大ID和OOD样本在表示空间的距离；3. 重训练策略：让模型充分拟合选定的辅助ID/OOD样本。

Result: 在两个标准SCOOD基准测试上，该方法显著超越了最先进的方法，证明了其有效性。

Conclusion: 提出的PSA框架通过提高训练样本集纯度和增强ID/OOD区分能力，有效解决了SCOOD中的噪声问题，取得了优异的性能。

Abstract: Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.

</details>


### [140] [Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery](https://arxiv.org/abs/2512.12925)
*Zhimao Peng,Enguang Wang,Fei Yang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 提出LSP和DAS两模块，通过最小化损失锐度惩罚和动态锚点选择，减少伪标签噪声，提升广义类别发现的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法基于DINO式伪标签策略，但大型预训练模型对特定视觉模式有偏好，导致对未标记数据编码虚假相关性并产生噪声伪标签。

Method: 提出两个模块：1) 损失锐度惩罚(LSP)：通过最小化模型的最坏情况损失锐度，增强参数对小扰动的鲁棒性，抑制平凡特征编码；2) 动态锚点选择(DAS)：基于KNN密度和类别概率选择未知类代表性样本并分配硬伪标签。

Result: 在多个GCD基准测试中实现最先进结果，能有效减轻伪标签噪声。

Conclusion: 提出的LSP和DAS方法能有效解决伪标签噪声问题，提升广义类别发现的聚类准确性。

Abstract: Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.

</details>


### [141] [UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction](https://arxiv.org/abs/2512.12941)
*Siyuan Yao,Dongxiu Liu,Taotao Li,Shengjie Li,Wenqi Ren,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出UAGLNet网络，通过不确定性建模指导的全局-局部特征融合，解决遥感图像建筑提取中特征金字塔差距和全局-局部特征整合不足的问题


<details>
  <summary>Details</summary>
Motivation: 现有方法使用卷积或自注意力块捕获多尺度特征，但特征金字塔存在固有差距，全局-局部特征整合不足，导致提取结果不准确、模糊

Method: 提出UAGLNet：1）协作编码器使用CNN和Transformer层分别捕获局部和全局语义；2）中间协作交互块缩小深层网络中的局部-全局特征差距；3）全局-局部融合模块互补融合特征；4）不确定性聚合解码器显式估计像素级不确定性增强分割精度

Result: 大量实验表明，该方法优于其他最先进方法

Conclusion: UAGLNet通过不确定性建模指导的全局-局部特征融合，有效解决了遥感图像建筑提取中的挑战，提高了分割准确性

Abstract: Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet

</details>


### [142] [SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer](https://arxiv.org/abs/2512.12963)
*Luan Thanh Trinh,Kenji Doi,Atsuki Osanai*

Main category: cs.CV

TL;DR: SCAdapter是一种基于CLIP图像空间的扩散模型风格迁移方法，通过分离内容和风格特征实现真实感迁移，比现有方法更快更有效。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在风格迁移中面临两个主要问题：1）难以实现照片级真实感迁移，常产生绘画效果或丢失细节；2）无法有效处理原始内容风格和风格参考内容特征的不必要影响。

Method: SCAdapter利用CLIP图像空间分离和整合内容与风格特征，包含三个核心组件：可控风格自适应实例归一化（CSAdaIN）用于精确多风格混合，KVS注入用于针对性风格整合，以及保持过程一致性的风格迁移一致性目标。

Result: SCAdapter在传统和基于扩散的基线方法中都显著优于现有技术，通过消除DDIM反转和推理阶段优化，推理速度至少比其他基于扩散的方法快2倍。

Conclusion: SCAdapter提供了一种更有效和高效的风格迁移解决方案，既提升了迁移质量又大幅加快了推理速度，适用于实际应用。

Abstract: Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.

</details>


### [143] [VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference](https://arxiv.org/abs/2512.12977)
*Shengling Qin,Hao Yu,Chenxin Wu,Zheng Li,Yizhong Cao,Zhengyang Zhuge,Yuxin Zhou,Wentao Yao,Yi Zhang,Zhengheng Wang,Shuai Bai,Jianwei Zhang,Junyang Lin*

Main category: cs.CV

TL;DR: VLCache是一个多模态缓存重用框架，通过复用KV缓存和编码器缓存来避免重复计算，在保持精度的同时大幅提升推理速度


<details>
  <summary>Details</summary>
Motivation: 当相同的多模态输入重复出现时，现有的方法需要昂贵的重新计算，这限制了推理效率。需要一种能够有效复用先前计算结果的方法来加速多模态推理

Method: 1) 形式化识别累积重用误差效应并最小化非前缀缓存重用误差；2) 分析模型层的重要性差异，提出动态、层感知的重新计算策略来平衡精度和效率

Result: VLCache在保持与完全重新计算相当的精度同时，仅需计算2-5%的token，实现了1.2倍到16倍的首次令牌时间加速。已集成到SGLang中用于实际部署

Conclusion: VLCache通过有效的缓存重用机制，显著提升了多模态推理的效率，为实际应用中的快速推理提供了可行的解决方案

Abstract: This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.

</details>


### [144] [Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes](https://arxiv.org/abs/2512.12982)
*Ziheng Qin,Yuheng Ji,Renshuai Tao,Yuxuan Tian,Yuyang Liu,Yipu Wang,Xiaolong Zheng*

Main category: cs.CV

TL;DR: 本文提出GAPL框架解决AIGI检测中的"先益后冲"困境，通过原型学习和低秩适应实现跨生成器的鲁棒检测。


<details>
  <summary>Details</summary>
Motivation: 现有AIGI检测器通过聚合多生成器数据提升泛化性，但面临"先益后冲"困境：随着数据源多样性增加，检测性能先提升后下降。这源于数据层面的异质性导致特征分布重叠，以及模型层面固定预训练编码器无法适应复杂度提升的瓶颈。

Method: 提出Generator-Aware Prototype Learning (GAPL)框架：1) 学习紧凑的典型伪造原型集，构建统一低方差特征空间以应对数据异质性；2) 采用两阶段训练方案结合Low-Rank Adaptation，增强判别能力同时保留预训练知识。

Result: GAPL在广泛GAN和扩散模型生成器上实现最先进的检测性能，展现出优异的跨生成器检测准确率。

Conclusion: GAPL通过结构化学习范式有效解决了AIGI检测中的"先益后冲"困境，建立了更鲁棒和可泛化的决策边界，为通用AIGI检测提供了有效解决方案。

Abstract: The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL

</details>


### [145] [Few-Step Distillation for Text-to-Image Generation: A Practical Guide](https://arxiv.org/abs/2512.13006)
*Yifan Pu,Yizeng Han,Zhiwei Tang,Jiasheng Tang,Fan Wang,Bohan Zhuang,Gao Huang*

Main category: cs.CV

TL;DR: 该研究首次系统性地将扩散蒸馏技术应用于开放域文本到图像生成，通过统一框架比较现有方法，识别了从离散类别标签转向自由形式语言提示的关键障碍，并提供了实用的部署指南。


<details>
  <summary>Details</summary>
Motivation: 扩散蒸馏在类别条件图像合成中已显著加速生成，但其在开放域文本到图像生成中的应用仍不明确。研究旨在填补这一空白，为实际应用提供快速、高保真且资源高效的扩散生成器。

Method: 将现有蒸馏方法统一到框架中，在强大的T2I教师模型FLUX.1-lite上进行适配和比较。研究包括输入缩放、网络架构和超参数等方面的系统分析，并提供开源实现和预训练学生模型。

Result: 识别了从离散类别标签转向自由形式语言提示的关键障碍，建立了实用的部署指南，为实际T2I应用提供了快速、高保真且资源高效的扩散生成器基础。

Conclusion: 该研究为在真实世界文本到图像应用中部署快速、高保真且资源高效的扩散生成器奠定了坚实基础，相关代码和模型已开源。

Abstract: Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.

</details>


### [146] [Light Field Based 6DoF Tracking of Previously Unobserved Objects](https://arxiv.org/abs/2512.13007)
*Nikolai Goncharov,James L. Gray,Donald G. Dansereau*

Main category: cs.CV

TL;DR: 提出基于光场图像的物体跟踪方法，无需预训练模型，对复杂视觉行为（如反射）具有鲁棒性，使用可微渲染和姿态优化实现6DoF跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有高性能物体跟踪方法依赖预捕获的物体视图构建显式参考模型，这限制了它们只能处理已知物体集合，且在处理复杂视觉外观（如反射）时效果不佳。需要一种不依赖预训练模型、能处理复杂视觉行为的通用物体跟踪方法。

Method: 从光场图像输入中提取语义和几何特征，使用视觉基础模型，将其转换为视图相关的高斯泼溅表示。这种表示作为统一的物体表征，支持可微渲染和姿态优化。

Result: 在包含挑战性反射物体的光场物体跟踪数据集上实验，该方法在困难情况下与最先进的基于模型的跟踪器具有竞争力。

Conclusion: 该方法为机器人系统中的通用物体跟踪铺平了道路，不依赖预训练模型且对复杂视觉行为具有鲁棒性。

Abstract: Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.

</details>


### [147] [TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading](https://arxiv.org/abs/2512.13008)
*Xi Luo,Shixin Xu,Ying Xie,JianZhong Hu,Yuwei He,Yuhui Deng,Huaxiong Huang*

Main category: cs.CV

TL;DR: TWLR是一个两阶段可解释糖尿病视网膜病变评估框架，通过视觉语言模型整合眼科知识进行分级和病变分类，然后通过弱监督语义分割的迭代严重性回归框架实现病变定位和疾病到健康转换的可视化。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析需要高质量专家标注，但获取像素级标签成本高、耗时长。深度学习在医学影像中缺乏可解释性，限制了临床采用。需要解决标注效率和可解释性双重挑战。

Method: 两阶段框架：第一阶段使用视觉语言模型整合眼科领域知识，联合执行DR分级和病变分类；第二阶段基于弱监督语义分割的迭代严重性回归框架，通过迭代精炼生成病变显著性图，指导渐进修复机制消除病理特征，实现疾病严重性降级。

Result: 在FGADR、DDR和私有数据集上的实验表明，TWLR在DR分类和病变分割方面均取得竞争性性能，同时提供疾病到健康转换的可解释可视化。

Conclusion: TWLR为自动化视网膜图像分析提供了更可解释和标注高效的解决方案，通过双阶段设计同时解决了标注成本高和模型可解释性差的问题。

Abstract: Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.

</details>


### [148] [JoDiffusion: Jointly Diffusing Image with Pixel-Level Annotations for Semantic Segmentation Promotion](https://arxiv.org/abs/2512.13014)
*Haoyu Wang,Lei Zhang,Wenrui Liu,Dengyang Jiang,Wei Wei,Chen Ding*

Main category: cs.CV

TL;DR: JoDiffusion：一种用于语义分割的联合扩散框架，能够仅通过文本提示同时生成配对的图像和语义一致的标注掩码，解决了现有方法的语义不一致和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 像素级标注成本高、耗时，现有合成数据集方法要么需要在图像生成后预测伪标注，要么需要基于手动标注掩码生成图像，导致图像-标注语义不一致或可扩展性问题。

Method: 1. 在标准潜在扩散模型中引入独立的标注VAE网络，将标注掩码映射到与图像共享的潜在空间；2. 定制扩散模型以捕获每个图像及其标注掩码在文本提示条件下的联合分布；3. 开发掩码优化策略以减少生成过程中的标注噪声。

Result: 在Pascal VOC、COCO和ADE20K数据集上的实验表明，JoDiffusion生成的标注数据集在语义分割任务中相比现有方法带来了显著的性能提升。

Conclusion: JoDiffusion能够仅通过文本提示同时生成语义一致的图像-标注对，解决了现有方法的局限性，为训练高性能语义分割模型提供了可扩展的合成数据生成方案。

Abstract: Given the inherently costly and time-intensive nature of pixel-level annotation, the generation of synthetic datasets comprising sufficiently diverse synthetic images paired with ground-truth pixel-level annotations has garnered increasing attention recently for training high-performance semantic segmentation models. However, existing methods necessitate to either predict pseudo annotations after image generation or generate images conditioned on manual annotation masks, which incurs image-annotation semantic inconsistency or scalability problem. To migrate both problems with one stone, we present a novel dataset generative diffusion framework for semantic segmentation, termed JoDiffusion. Firstly, given a standard latent diffusion model, JoDiffusion incorporates an independent annotation variational auto-encoder (VAE) network to map annotation masks into the latent space shared by images. Then, the diffusion model is tailored to capture the joint distribution of each image and its annotation mask conditioned on a text prompt. By doing these, JoDiffusion enables simultaneously generating paired images and semantically consistent annotation masks solely conditioned on text prompts, thereby demonstrating superior scalability. Additionally, a mask optimization strategy is developed to mitigate the annotation noise produced during generation. Experiments on Pascal VOC, COCO, and ADE20K datasets show that the annotated dataset generated by JoDiffusion yields substantial performance improvements in semantic segmentation compared to existing methods.

</details>


### [149] [What Happens Next? Next Scene Prediction with a Unified Video Model](https://arxiv.org/abs/2512.13015)
*Xinjie Li,Zhimin Chen,Rui Zhao,Florian Schiffers,Zhenyu Liao,Vimal Bhat*

Main category: cs.CV

TL;DR: 提出Next Scene Prediction (NSP)新任务，推动统一视频模型进行时序和因果推理，通过Qwen-VL理解+LTX合成的统一框架，在三阶段训练下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前统一模型主要关注文本到视频生成等传统任务，其时序推理潜力未被充分探索。需要推动统一视频模型进行更深层次的时序和因果推理。

Method: 提出NSP新任务，构建Qwen-VL理解+LTX合成的统一框架，通过潜在查询嵌入和连接器模块连接。采用三阶段训练：文本到视频预训练、监督微调、带因果一致性奖励的GRPO强化学习。

Result: 模型在新构建的大规模NSP数据集上实现了最先进的性能，提升了通用多模态系统预测未来场景的能力。

Conclusion: NSP任务成功推动了统一视频模型进行时序和因果推理，提出的框架在预测未来场景方面表现出色，为通用多模态系统的发展提供了新方向。

Abstract: Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.

</details>


### [150] [Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing](https://arxiv.org/abs/2512.13018)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

TL;DR: 该研究首次全面评估了空间泛化技术，针对室内人员计数的FMCW MIMO雷达应用，发现基于幅度的预处理（sigmoid加权）和迁移学习能显著提升跨环境性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习RF传感的实际部署需要解决空间泛化问题，但现有研究缺乏对多种泛化技术的系统评估，特别是在室内人员计数等实际应用中。

Method: 系统研究了多种空间泛化方法：基于幅度的统计预处理（sigmoid加权和阈值归零）、频域滤波、基于自编码器的背景抑制、数据增强策略和迁移学习。使用FMCW MIMO雷达在两个不同布局的环境中收集实验数据。

Result: sigmoid幅度加权在跨环境性能上表现最佳，相比基线方法分别降低RMSE 50.1%和MAE 55.2%。数据增强提供额外但有限的改进（MAE提升达8.8%）。迁移学习对于大空间变化至关重要，使用540个目标域样本分别降低RMSE 82.1%和MAE 91.3%。

Conclusion: 通过结合深度学习模型、基于幅度的预处理和高效迁移学习，可以开发出在空间变化下保持鲁棒精度的雷达传感系统，为实际部署提供了实用方向。

Abstract: This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.

</details>


### [151] [SneakPeek: Future-Guided Instructional Streaming Video Generation](https://arxiv.org/abs/2512.13019)
*Cheeun Hong,German Barquero,Fadime Sener,Markos Georgopoulos,Edgar Schönfeld,Stefan Popov,Yuming Du,Oscar Mañas,Albert Pumarola*

Main category: cs.CV

TL;DR: SneakPeek是一个基于扩散的自回归框架，用于生成精确、分步的教学视频，通过预测因果适应、未来引导自强制和多提示条件等技术解决现有模型在长序列中的时间一致性和可控性问题。


<details>
  <summary>Details</summary>
Motivation: 教学视频生成在内容创作、教育和人机交互中具有广泛应用前景，但现有视频扩散模型在长序列多步骤活动中难以保持时间一致性和可控性。

Method: 提出SneakPeek框架，包含三个关键创新：1）预测因果适应，通过因果模型学习下一帧预测和未来关键帧预测；2）未来引导自强制，采用双区域KV缓存方案解决推理时的曝光偏差问题；3）多提示条件，提供对多步骤指令的细粒度程序控制。

Result: 实验结果表明，该方法能够生成时间连贯、语义忠实且准确遵循复杂多步骤任务描述的教学视频。

Conclusion: SneakPeek通过其创新组件有效缓解了时间漂移，保持了运动一致性，并实现了交互式视频生成，其中未来提示更新能够动态影响正在进行的流式视频生成。

Abstract: Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.

</details>


### [152] [Motus: A Unified Latent Action World Model](https://arxiv.org/abs/2512.13030)
*Hongzhe Bi,Hengkai Tan,Shenghao Xie,Zeyuan Wang,Shuhe Huang,Haitian Liu,Ruowen Zhao,Yao Feng,Chendong Xiang,Yinze Rong,Hongyan Zhao,Hanyu Liu,Zhizhong Su,Lei Ma,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: Motus提出统一潜在动作世界模型，通过混合Transformer架构整合理解、视频生成和动作专家，利用光流学习潜在动作，在仿真和真实场景中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前方法将理解、世界建模和控制分离为孤立模型，这种碎片化阻碍了多模态生成能力的统一，也妨碍了从大规模异构数据中学习。需要构建统一的系统来整合这些功能。

Method: 提出Motus统一潜在动作世界模型：1) 采用混合Transformer架构整合三个专家（理解、视频生成、动作）；2) 使用UniDiffuser风格调度器实现不同建模模式灵活切换；3) 利用光流学习潜在动作；4) 采用三阶段训练流程和六层数据金字塔进行大规模动作预训练。

Result: 在仿真场景中：比X-VLA提升15%，比Pi0.5提升45%；在真实场景中：提升11-48%。展示了统一建模所有功能和先验对下游机器人任务的显著益处。

Conclusion: Motus通过统一潜在动作世界模型成功整合了理解、世界建模和控制功能，证明了统一建模方法在机器人任务中的优越性，为构建通用具身智能体提供了有效框架。

Abstract: While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.

</details>


### [153] [Comprehensive Evaluation of Rule-Based, Machine Learning, and Deep Learning in Human Estimation Using Radio Wave Sensing: Accuracy, Spatial Generalization, and Output Granularity Trade-offs](https://arxiv.org/abs/2512.13031)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

TL;DR: 该研究首次全面比较了基于规则的方法、传统机器学习模型和深度学习模型在FMCW MIMO雷达室内人员感知中的性能，发现深度学习在相同环境下精度最高但易受领域偏移影响，而基于规则的方法虽然输出粒度粗但跨环境更稳定。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于系统评估不同方法在雷达室内人员感知任务中的性能差异，特别是在面对环境布局变化时的泛化能力，为实际应用中的方法选择提供指导。

Method: 在两种不同布局的室内环境中，系统评估了五种方法：1)基于规则的连通分量方法；2)三种传统机器学习模型（k近邻、随机森林、支持向量机）；3)结合卷积神经网络和长短期记忆的深度学习模型。

Result: 在训练环境中，CNN-LSTM模型精度最高，传统机器学习模型性能中等。但在新布局中，所有学习方法性能显著下降，而基于规则的方法保持稳定。对于人员存在检测的二元任务，所有模型在不同布局中均保持高精度。

Conclusion: 高容量模型能在相同环境中提供细粒度的高精度输出，但对领域偏移敏感；基于规则的方法输出粒度粗但跨环境鲁棒性强。无论模型类型，都存在空间泛化性能与输出粒度之间的明确权衡。

Abstract: This study presents the first comprehensive comparison of rule-based methods, traditional machine learning models, and deep learning models in radio wave sensing with frequency modulated continuous wave multiple input multiple output radar. We systematically evaluated five approaches in two indoor environments with distinct layouts: a rule-based connected component method; three traditional machine learning models, namely k-nearest neighbors, random forest, and support vector machine; and a deep learning model combining a convolutional neural network and long short term memory. In the training environment, the convolutional neural network long short term memory model achieved the highest accuracy, while traditional machine learning models provided moderate performance. In a new layout, however, all learning based methods showed significant degradation, whereas the rule-based method remained stable. Notably, for binary detection of presence versus absence of people, all models consistently achieved high accuracy across layouts. These results demonstrate that high capacity models can produce fine grained outputs with high accuracy in the same environment, but they are vulnerable to domain shift. In contrast, rule-based methods cannot provide fine grained outputs but exhibit robustness against domain shift. Moreover, regardless of the model type, a clear trade off was revealed between spatial generalization performance and output granularity.

</details>


### [154] [Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models](https://arxiv.org/abs/2512.13039)
*Hao Chen,Yiwei Wang,Songze Li*

Main category: cs.CV

TL;DR: 提出双向图像引导概念擦除框架，通过负分支抑制有害概念、正分支提供安全替代视觉指导，平衡擦除效果与生成质量


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法通常采用单向策略（抑制目标概念或强化安全替代），难以平衡概念移除与生成质量之间的权衡

Method: 提出Bi-Erasing框架：基于文本提示和对应图像的联合表示，引入解耦的负分支（抑制有害语义）和正分支（提供安全替代视觉指导），通过联合优化这两个互补方向实现平衡，并使用基于掩码的过滤防止无关内容干扰

Result: 在广泛实验评估中，Bi-Erasing在平衡概念移除效果和视觉保真度方面优于基线方法

Conclusion: 提出的双向图像引导概念擦除框架能够有效平衡擦除效果与生成可用性，解决了现有单向擦除方法的局限性

Abstract: Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.

</details>


### [155] [Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing](https://arxiv.org/abs/2512.13055)
*Jaeyoon Kim,Yoonki Cho,Sung-Eui Yoon*

Main category: cs.CV

TL;DR: 提出高效非对称视觉地点识别框架，使用高容量模型离线提取特征，轻量查询网络在线处理，通过地理记忆库和隐式嵌入增强技术降低计算成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于DINOv2等大模型的VPR方法计算成本高，难以在资源受限设备上部署，需要开发高效的非对称检索框架。

Method: 1) 非对称框架：高容量模型离线提取图库特征，轻量查询网络在线处理；2) 地理记忆库：利用VPR数据库的地理位置元数据组织图库特征，避免昂贵的k-NN计算；3) 隐式嵌入增强：增强轻量查询网络的特征建模能力。

Result: 方法显著降低计算成本，在资源受限环境下超越现有非对称检索技术，为VPR在资源受限环境中的应用建立了新标准。

Conclusion: 提出的非对称VPR框架通过地理记忆库和隐式嵌入增强，在保持高性能的同时大幅降低计算需求，适合资源受限设备部署。

Abstract: Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance. Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical. In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing. A key challenge in this setting is ensuring compatibility between these heterogeneous networks, which conventional approaches address through computationally expensive k-NN-based compatible training. To overcome this, we propose a geographical memory bank that structures gallery features using geolocation metadata inherent in VPR databases, eliminating the need for exhaustive k-NN computations. Additionally, we introduce an implicit embedding augmentation technique that enhances the query network to model feature variations despite its limited capacity. Extensive experiments demonstrate that our method not only significantly reduces computational costs but also outperforms existing asymmetric retrieval techniques, establishing a new aspect for VPR in resource-limited environments. The code is available at https://github.com/jaeyoon1603/AsymVPR

</details>


### [156] [Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models](https://arxiv.org/abs/2512.13072)
*Zizhi Chen,Yizhen Gao,Minghao Han,Yizhou Liu,Zhaoyu Chen,Dingkang Yang,Lihua Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种用于医学视觉语言模型持续学习的综合框架，通过检索增强生成和多层知识蒸馏来解决模态间领域差距与模态内细粒度特征保留的核心困境。


<details>
  <summary>Details</summary>
Motivation: 多模态生物医学视觉语言模型在持续学习中面临核心困境：如何在弥合不同模态间显著领域差距的同时，保留细粒度的模态内特征。现有方法难以平衡这两方面的需求。

Method: 1. 基于1800万PubMed论文构建多模态医学检索数据库；2. 将检索增强生成首次引入持续学习；3. 采用多模态多层RAG系统提供实时指导；4. 提出动态知识蒸馏框架，动态调节参数空间重要性、知识粒度和参考数据集分布。

Result: 在设计的医学通用任务增量学习基准上进行了广泛实验，结果表明所提方法在所有指标上均达到了最先进的性能。

Conclusion: 该框架成功解决了医学视觉语言模型持续学习中的核心困境，通过RAG和动态知识蒸馏实现了领域适应、特征保留和新任务学习的平衡，具有显著的临床价值。

Abstract: Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.

</details>


### [157] [Heart Disease Prediction using Case Based Reasoning (CBR)](https://arxiv.org/abs/2512.13078)
*Mohaiminul Islam Bhuiyan,Chan Hue Wah,Nur Shazwani Kamarudin,Nur Hafieza Ismail,Ahmad Fakhri Ab Nasir*

Main category: cs.CV

TL;DR: 本研究综述了使用智能系统进行心脏病预测的方法，比较了模糊逻辑、神经网络和案例推理三种技术，最终采用案例推理方法在心脏病预测中取得了97.95%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统心脏病预测方法主要依赖医生经验，往往缺乏精确性。为了克服这一局限性，需要探索智能系统作为传统方法的替代方案，以提高预测准确性。

Method: 研究比较了三种智能系统方法：模糊逻辑、神经网络和案例推理。最终选择案例推理进行心脏病预测，并对心脏病数据集进行数据预处理（数据清洗）和数据分割（训练集和测试集分离）。

Result: 案例推理在心脏病预测中取得了97.95%的准确率。分析显示男性患心脏病的概率为57.76%，女性为42.24%。相关研究表明吸烟和饮酒是心脏病的重要风险因素，尤其在男性中更为显著。

Conclusion: 智能系统特别是案例推理方法在心脏病预测中表现出色，能够提供高精度的预测结果。性别差异和生活方式因素（如吸烟、饮酒）对心脏病风险有重要影响，这些发现对临床实践和预防策略具有指导意义。

Abstract: This study provides an overview of heart disease prediction using an intelligent system. Predicting disease accurately is crucial in the medical field, but traditional methods relying solely on a doctor's experience often lack precision. To address this limitation, intelligent systems are applied as an alternative to traditional approaches. While various intelligent system methods exist, this study focuses on three: Fuzzy Logic, Neural Networks, and Case-Based Reasoning (CBR). A comparison of these techniques in terms of accuracy was conducted, and ultimately, Case-Based Reasoning (CBR) was selected for heart disease prediction. In the prediction phase, the heart disease dataset underwent data pre-processing to clean the data and data splitting to separate it into training and testing sets. The chosen intelligent system was then employed to predict heart disease outcomes based on the processed data. The experiment concluded with Case-Based Reasoning (CBR) achieving a notable accuracy rate of 97.95% in predicting heart disease. The findings also revealed that the probability of heart disease was 57.76% for males and 42.24% for females. Further analysis from related studies suggests that factors such as smoking and alcohol consumption are significant contributors to heart disease, particularly among males.

</details>


### [158] [DiRe: Diversity-promoting Regularization for Dataset Condensation](https://arxiv.org/abs/2512.13083)
*Saumyaranjan Mohanty,Aravind Reddy,Konda Reddy Mopuri*

Main category: cs.CV

TL;DR: 提出多样性正则化器(DiRe)提升数据集压缩中的多样性，减少冗余，改善合成数据集质量


<details>
  <summary>Details</summary>
Motivation: 现有数据集压缩方法合成的数据集存在显著冗余，需要减少冗余并提高合成数据集的多样性

Method: 提出直观的多样性正则化器(DiRe)，由余弦相似度和欧氏距离组成，可即插即用地应用于各种最先进的压缩方法

Result: 通过广泛实验证明，添加该正则化器能提升多种最先进压缩方法在CIFAR-10到ImageNet-1K等基准数据集上的泛化性和多样性指标

Conclusion: 提出的多样性正则化器能有效减少数据集压缩中的冗余，提高合成数据集的多样性，提升现有方法的性能

Abstract: In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.

</details>


### [159] [ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning](https://arxiv.org/abs/2512.13095)
*Feng Zhang,Zezhong Tan,Xinhong Ma,Ziqiang Dong,Xi Leng,Jianfei Zhao,Xin Sun,Yang Yang*

Main category: cs.CV

TL;DR: ADHint提出了一种基于难度的自适应提示方法，在强化学习中更好地平衡探索与模仿，通过难度感知的提示比例调度和优势估计来提升推理能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的RL方法通常忽略难度因素，导致学习不稳定和过度模仿离策略提示。需要将难度作为关键因素，在提示比例调度和相对优势估计中实现探索与模仿的更好权衡。

Method: 1. 自适应提示与样本难度先验：评估每个样本在策略模型下的难度，相应调度适当的提示比例来指导rollout；2. 一致性梯度调制和选择性掩码提示保留：调制提示内的token级梯度，防止有偏和破坏性更新；3. 基于rollout难度后验的优势估计：利用有/无提示rollout的相对难度来估计各自优势，实现更平衡的更新。

Result: 在多种模态、模型规模和领域的广泛实验中，ADHint在推理能力和分布外泛化方面表现优异，在pass@1和avg@8指标上持续超越现有方法。

Conclusion: ADHint通过将难度作为关键因素集成到提示比例调度和优势估计中，实现了探索与模仿的更好平衡，显著提升了推理能力和泛化性能。

Abstract: To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.

</details>


### [160] [FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection](https://arxiv.org/abs/2512.13104)
*Yan Zhang,Baoxin Li,Han Sun,Yuhang Gao,Mingtai Zhang,Pei Wang*

Main category: cs.CV

TL;DR: 提出FID-Net深度学习模型，用于从无人机可见光图像检测森林病虫害树木，并通过三种空间指标分析虫害分布模式，为智能虫害监测提供技术支持。


<details>
  <summary>Details</summary>
Motivation: 传统方法在大规模、细粒度森林病虫害监测中存在局限性，需要更高效的监测手段来准确识别受感染树木并分析虫害分布模式。

Method: 基于YOLOv8n构建FID-Net模型，引入轻量级特征增强模块(FEM)提取病害敏感特征，自适应多尺度特征融合模块(AMFM)对齐融合RGB和FEM增强特征，以及高效通道注意力(ECA)机制。从检测结果构建虫害情况分析框架，包括核密度估计定位热点、邻域评估健康树感染风险、DBSCAN聚类识别高密度健康集群。

Result: 在中国东部天山32个森林样地的无人机图像实验中，FID-Net达到86.10%精确率、75.44%召回率、82.29% mAP@0.5和64.30% mAP@0.5:0.95，优于主流YOLO模型。分析证实受感染树木呈现明显聚类特征。

Conclusion: FID-Net能够准确识别树木健康状况，结合空间指标分析，为智能虫害监测、早期预警和精准管理提供可靠数据支持，有助于针对性森林保护。

Abstract: Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.

</details>


### [161] [LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping](https://arxiv.org/abs/2512.13130)
*Shanghua Liu,Majharulislam Babor,Christoph Verduyn,Breght Vandenberghe,Bruno Betoni Parodi,Cornelia Weltzien,Marina M. -C. Höhne*

Main category: cs.CV

TL;DR: 提出CanolaTrack数据集和LeafTrackNet框架，用于复杂作物（油菜）叶片跟踪，在真实条件下实现高精度叶片追踪


<details>
  <summary>Details</summary>
Motivation: 现有叶片跟踪方法存在局限性：要么局限于小型物种，要么依赖受限成像条件，而通用多目标跟踪方法不适合动态生物场景。缺乏真实条件下的大规模数据集也阻碍了准确叶片跟踪模型的发展。

Method: 提出LeafTrackNet框架，结合YOLOv10叶片检测器和MobileNetV3嵌入网络，通过基于嵌入的记忆关联策略在推理过程中保持叶片身份。

Result: LeafTrackNet在CanolaTrack数据集上优于植物专用跟踪器和最先进的多目标跟踪基线，HOTA指标提升9%。提供了农业作物叶片跟踪的最大数据集。

Conclusion: 该工作为真实条件下的叶片级跟踪设定了新标准，提供了最大的农业作物叶片跟踪数据集，将促进植物表型研究的未来发展。

Abstract: High resolution phenotyping at the level of individual leaves offers fine-grained insights into plant development and stress responses. However, the full potential of accurate leaf tracking over time remains largely unexplored due to the absence of robust tracking methods-particularly for structurally complex crops such as canola. Existing plant-specific tracking methods are typically limited to small-scale species or rely on constrained imaging conditions. In contrast, generic multi-object tracking (MOT) methods are not designed for dynamic biological scenes. Progress in the development of accurate leaf tracking models has also been hindered by a lack of large-scale datasets captured under realistic conditions. In this work, we introduce CanolaTrack, a new benchmark dataset comprising 5,704 RGB images with 31,840 annotated leaf instances spanning the early growth stages of 184 canola plants. To enable accurate leaf tracking over time, we introduce LeafTrackNet, an efficient framework that combines a YOLOv10-based leaf detector with a MobileNetV3-based embedding network. During inference, leaf identities are maintained over time through an embedding-based memory association strategy. LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving a 9% HOTA improvement on CanolaTrack. With our work we provide a new standard for leaf-level tracking under realistic conditions and we provide CanolaTrack - the largest dataset for leaf tracking in agriculture crops, which will contribute to future research in plant phenotyping. Our code and dataset are publicly available at https://github.com/shl-shawn/LeafTrackNet.

</details>


### [162] [Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models](https://arxiv.org/abs/2512.13144)
*Chun Kit Wong,Paraskevas Pegios,Nina Weng,Emilie Pi Fogtmann Sejer,Martin Grønnebæk Tolsgaard,Anders Nymark Christensen,Aasa Feragen*

Main category: cs.CV

TL;DR: 提出权重空间相关分析方法，用于量化医学影像深度学习模型是否利用嵌入中的元数据信息进行预测，验证了sPTB预测模型主要使用临床相关特征而非扫描仪等无关元数据。


<details>
  <summary>Details</summary>
Motivation: 医学影像深度学习模型容易受到捷径学习影响，会利用嵌入中的元数据（如扫描仪型号）进行预测，但关键问题是模型是否主动使用这些编码信息进行最终预测，需要验证模型的可信度。

Method: 提出权重空间相关分析方法，通过测量主要临床任务分类头与辅助元数据任务分类头之间的对齐程度来量化特征利用情况。首先验证方法能检测人为诱导的捷径学习，然后应用于SA-SonoNet模型的自发性早产预测任务。

Result: 分析证实：虽然嵌入包含大量元数据，但sPTB分类器的权重向量与临床相关因素（如出生体重）高度相关，而与临床无关的采集因素（如扫描仪）解耦。在无诱导偏差情况下，临床模型选择性利用真实临床信号相关特征。

Conclusion: 权重空间相关分析方法为验证模型可信度提供了工具，证明在无诱导偏差时，临床模型能够选择性地利用与真实临床信号相关的特征，而非无关元数据。

Abstract: Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.

</details>


### [163] [StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion](https://arxiv.org/abs/2512.13147)
*Sangmin Hong,Suyoung Lee,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: StarryGazer是一个无需真实深度标注的深度补全框架，通过结合单目深度估计模型和稀疏深度信息，生成密集深度图像。


<details>
  <summary>Details</summary>
Motivation: 现有无监督深度补全方法需要辅助数据，不符合实际场景；而单目深度估计模型虽能生成相对深度图，但无法与稀疏深度图有效结合，简单的仿射变换会导致高误差。

Method: 使用预训练的单目深度估计模型生成相对深度图，通过分割和随机缩放创建合成训练对，训练精炼网络结合相对深度图和RGB图像来提升准确性和鲁棒性。

Result: StarryGazer在多个数据集上超越了现有无监督方法和转换后的单目深度估计结果，证明该框架能有效利用单目深度估计模型的能力，同时通过稀疏深度信息修正误差。

Conclusion: 该框架成功实现了无需真实深度标注的深度补全，通过巧妙结合单目深度估计和稀疏深度信息，在保持领域无关性的同时显著提升了性能。

Abstract: The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.

</details>


### [164] [Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation](https://arxiv.org/abs/2512.13175)
*Hongxuan Sun,Tao Wu*

Main category: cs.CV

TL;DR: DFSS是一种专门为语义分割设计的无数据知识蒸馏框架，通过利用批归一化统计量指导近似分布采样，并采用加权分布渐进蒸馏策略，显著提升了分割任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有无数据知识蒸馏方法主要针对分类任务设计，忽视了语义分割中物体的空间连续性和结构一致性，导致直接应用于分割任务时性能显著下降。

Method: 1. 利用教师模型的批归一化统计量指导近似分布采样，选择更接近原始训练分布的数据；2. 提出加权分布渐进蒸馏策略，动态优先处理可靠样本并逐步引入更具挑战性的样本。

Result: 在标准基准测试中，DFSS在语义分割的无数据知识蒸馏方法中表现最优，实现了最先进的结果，同时显著减少了对辅助数据的依赖。

Conclusion: DFSS通过尊重真实场景的结构和上下文连续性，为语义分割任务量身定制了有效的无数据蒸馏框架，克服了现有方法忽视空间连续性的问题。

Abstract: Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.

</details>


### [165] [MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion](https://arxiv.org/abs/2512.13177)
*Minghui Hou,Wei-Hsing Huang,Shaofeng Liang,Daizong Liu,Tai-Hao Wen,Gang Wang,Runwei Guan,Weiping Ding*

Main category: cs.CV

TL;DR: MMDrive是一个多模态视觉语言模型框架，将传统2D图像理解扩展到广义3D场景理解，通过融合占据地图、激光雷达点云和文本描述，在自动驾驶场景理解中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型受限于2D平面图像理解范式，难以感知3D空间信息和进行深度语义融合，导致在复杂自动驾驶环境中表现不佳。

Method: 提出MMDrive框架，融合占据地图、激光雷达点云和文本描述三种互补模态。引入两个新组件：基于文本语义动态加权各模态贡献的文本导向多模态调制器，以及使用可学习抽象令牌生成紧凑跨模态摘要的跨模态抽象器。

Result: 在DriveLM和NuScenes-QA基准测试中表现优异：DriveLM上BLEU-4得分为54.56，METEOR为41.78；NuScenes-QA上准确率达到62.7%，显著优于现有自动驾驶视觉语言模型。

Conclusion: MMDrive突破了传统仅图像理解的限制，实现了复杂驾驶环境中的鲁棒多模态推理，为可解释的自动驾驶场景理解提供了新基础。

Abstract: Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.

</details>


### [166] [CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception](https://arxiv.org/abs/2512.13191)
*Gong Chen,Chaokun Zhang,Pengcheng Lv,Xiaohui Xie*

Main category: cs.CV

TL;DR: CoRA是一种新颖的协作感知架构，通过混合特征级和对象级融合，在保证高性能的同时提升对通信错误的鲁棒性，显著减少通信量。


<details>
  <summary>Details</summary>
Motivation: 现有协作感知方法虽然通过中间融合实现了通信效率和性能，但在恶劣通信条件下性能会因数据传输引起的错位而下降，这严重阻碍了实际部署。需要一种既能保持高性能又能应对通信错误的解决方案。

Method: 提出CoRA架构，包含两个分支：1) 特征级融合分支，选择关键特征进行高效融合以确保性能和可扩展性；2) 对象级校正分支，利用语义相关性校正空间位移，保证对姿态错误的鲁棒性。这种混合方法将性能与鲁棒性解耦。

Result: 在极端场景下，CoRA在AP@0.7指标上比基线提升约19%，同时通信量减少5倍以上，成为鲁棒协作感知的有前景解决方案。

Conclusion: 中间融合和后期融合的优势不是权衡关系而是互补配对。CoRA通过混合方法成功将性能与鲁棒性解耦，在保持高性能的同时显著提升对通信错误的抵抗力，为实际部署提供了可行方案。

Abstract: Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.

</details>


### [167] [POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling](https://arxiv.org/abs/2512.13192)
*Zhuo Chen,Chengqun Yang,Zhuo Su,Zheng Lv,Jingnan Gao,Xiaoyuan Zhang,Xiaokang Yang,Yichao Yan*

Main category: cs.CV

TL;DR: 提出POLAR数据集和POLARNet模型，通过大规模物理校准的单光源数据集和基于流的生成模型，实现可扩展、可控的人脸重光照。


<details>
  <summary>Details</summary>
Motivation: 人脸重光照研究受限于大规模、物理一致的光照数据不足。现有方法依赖统计或上下文线索，缺乏物理可解释性和可控性。

Method: 1) 构建POLAR数据集：包含200+受试者，156个光照方向，多视角和表情；2) 开发POLARNet：基于流的生成模型，从单张肖像预测每个光源的OLAT响应，建模光照作为连续物理变换。

Result: 建立了统一的照明学习框架，连接真实数据、生成合成和物理基础重光照，形成可扩展、可复现的"鸡与蛋"循环。

Conclusion: POLAR数据集和POLARNet模型为肖像照明提供了可扩展、可控的解决方案，通过物理校准数据和生成模型实现了高质量的人脸重光照。

Abstract: Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining "chicken-and-egg" cycle for scalable and reproducible portrait illumination.

</details>


### [168] [Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance](https://arxiv.org/abs/2512.13238)
*Francesco Ragusa,Michele Mazzamuto,Rosario Forte,Irene D'Ambra,James Fort,Jakob Engel,Antonino Furnari,Giovanni Maria Farinella*

Main category: cs.CV

TL;DR: Ego-EXTRA是一个用于专家-学员辅助的50小时第一人称视频语言数据集，包含真实专家指导学员执行程序性活动的对话，创建了超过15k个高质量视觉问答对，用于评估多模态大语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视频语言数据集缺乏真实专家指导学员执行程序性活动的交互数据，需要高质量的第一人称视角专家-学员对话数据集来评估和开发智能辅助系统。

Method: 采用"绿野仙踪"数据收集范式，专家通过可穿戴智能助手从学员的第一人称视角观察活动，回答学员问题或主动提供建议，记录双向对话并转录，创建视觉问答基准。

Result: 创建了包含50小时非脚本第一人称视频和超过15k个高质量视觉问答对的Ego-EXTRA数据集，评估显示当前多模态大语言模型在提供专家级辅助方面存在局限性。

Conclusion: Ego-EXTRA是一个具有挑战性的数据集，揭示了当前模型在提供专家级用户辅助方面的不足，为第一人称视频语言助手的研究提供了公开基准。

Abstract: We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.

</details>


### [169] [STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits](https://arxiv.org/abs/2512.13247)
*Foivos Paraperas Papantoniou,Stathis Galanakis,Rolandos Alexandros Potamias,Bernhard Kainz,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: STARCaster是一个统一的身份感知时空视频扩散模型，能够同时处理语音驱动肖像动画和自由视角说话肖像合成，通过软身份约束和隐式3D感知在2D视频域中实现更好的运动多样性和身份保持。


<details>
  <summary>Details</summary>
Motivation: 现有2D语音到视频扩散模型过度依赖参考指导，导致运动多样性有限；而3D感知动画通常依赖预训练三平面生成器的反演，导致重建不完美和身份漂移。需要重新思考基于参考和几何的范式。

Method: 采用组合方法：1) 引入软身份约束而非严格参考条件；2) 在2D视频域中隐式实现3D感知，利用视频数据固有的多视角特性；3) 通过解耦学习分别训练视角一致性和时间连贯性；4) 使用自强制训练方案学习更长的时序上下文。

Result: 综合评估表明STARCaster能有效泛化到不同任务和身份，在各种基准测试中持续超越先前方法，解决了现有自回归方法中常见的过度静态动画问题。

Conclusion: STARCaster通过重新思考参考和几何范式，在统一框架中实现了更好的语音驱动肖像动画和自由视角说话肖像合成，为身份感知时空视频生成提供了新的解决方案。

Abstract: This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.

</details>


### [170] [Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection](https://arxiv.org/abs/2512.13250)
*Juil Koo,Daehyeon Choi,Sangwoo Youn,Phillip Y. Lee,Minhyuk Sung*

Main category: cs.CV

TL;DR: VG-AVS任务：仅基于当前视觉信息选择最有信息量的下一视角，无需场景记忆或外部知识。构建合成数据集，通过SFT+RL微调预训练VLM，提升视角选择性能并泛化到未见场景。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型局限于静态图像推理（快照视觉），而具身智能体需要主动移动获取信息更丰富的视角（行走视觉）。需要解决仅基于当前视觉信息选择最有信息量下一视角的问题。

Method: 1) 构建合成数据集，包含自动生成的配对查询-目标视角和问答提示；2) 提出框架：先通过监督微调(SFT)微调预训练VLM，再进行基于强化学习(RL)的策略优化。

Result: 方法在基于视角选择的问题回答上表现优异，能稳健泛化到未见过的合成和真实场景。将VG-AVS框架集成到现有基于场景探索的EQA系统中，能提升下游问答准确率。

Conclusion: VG-AVS任务填补了快照视觉和行走视觉之间的空白，提出的方法能有效选择信息丰富的下一视角，提升具身智能体的视觉感知能力。

Abstract: Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.

</details>


### [171] [CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing](https://arxiv.org/abs/2512.13276)
*Yan Li,Lin Liu,Xiaopeng Zhang,Wei Xue,Wenhan Luo,Yike Guo,Qi Tian*

Main category: cs.CV

TL;DR: CogniEdit：基于扩散模型的指令图像编辑新框架，通过多模态推理和密集奖励优化解决细粒度指令跟随问题


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的指令图像编辑方法在处理细粒度指令（如颜色、位置、数量等精确属性）时表现不佳。虽然最近的方法使用GRPO进行对齐，但仅在单个采样步骤优化，反馈稀疏，限制了轨迹级控制能力。

Method: 提出CogniEdit统一框架：1）多模态大语言模型分解复杂指令为可操作指令；2）动态令牌焦点重定位自适应强调细粒度属性；3）基于密集GRPO的优化，在连续去噪步骤间传播梯度，实现轨迹级监督。

Result: 在基准数据集上的广泛实验表明，CogniEdit在平衡细粒度指令跟随与视觉质量和可编辑性保持方面实现了最先进的性能。

Conclusion: CogniEdit通过结合多模态推理和密集奖励优化，解决了现有指令图像编辑方法在细粒度控制方面的局限性，实现了更好的轨迹级监督和编辑效果。

Abstract: Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods struggle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across consecutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation

</details>


### [172] [Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?](https://arxiv.org/abs/2512.13281)
*Jiaqi Wang,Weijia Wu,Yi Zhan,Rui Zhao,Ming Hu,James Cheng,Wei Liu,Philip Torr,Kevin Qinghong Lin*

Main category: cs.CV

TL;DR: 论文提出了Video Reality Test基准测试，用于评估AI生成视频在音频-视觉耦合下的真实感，发现当前最佳视频生成模型Veo3.1-Fast能欺骗大多数视觉语言模型，但人类专家仍能较好识别真伪。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成技术已能产生难以区分真伪的内容，但现有AIGC检测基准大多只评估无音频视频、针对广泛叙事领域且仅关注分类。尚不清楚最先进的视频生成模型是否能产生具有沉浸感的音频配对视频来可靠欺骗人类和视觉语言模型。

Method: 引入Video Reality Test基准套件，基于精心策划的真实ASMR视频，针对细粒度动作-对象交互；采用对抗性创作者-评审协议，视频生成模型作为创作者试图欺骗评审者，而视觉语言模型作为评审者识别虚假内容。

Result: 最佳创作者Veo3.1-Fast能欺骗大多数视觉语言模型：最强评审者Gemini 2.5-Pro仅达到56%准确率（随机50%），远低于人类专家的81.25%。添加音频能改善真伪判别，但水印等表面线索仍会显著误导模型。

Conclusion: 研究结果界定了当前视频生成真实感的边界，并揭示了视觉语言模型在感知保真度和音频-视觉一致性方面的局限性。音频能提升判别能力，但模型仍容易被表面线索误导。

Abstract: Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\% accuracy (random 50\%), far below that of human experts (81.25\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.

</details>


### [173] [CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images](https://arxiv.org/abs/2512.13285)
*Bo Liu,Qiao Qin,Qinghui He*

Main category: cs.CV

TL;DR: CausalCLIP：基于因果推断的图像生成检测框架，通过解耦因果与非因果特征提升跨生成模型的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有生成图像检测方法（包括基于视觉语言模型的方法）往往产生高度纠缠的特征表示，将任务相关的取证线索（因果特征）与虚假或无关模式（非因果特征）混合，限制了模型的泛化能力。随着生成模型的快速发展，需要能够适应多样化和不断演化的生成技术的检测器。

Method: 提出CausalCLIP框架，通过因果推断原理指导的特征过滤，显式解耦因果与非因果特征。使用结构因果模型建模生成过程，通过Gumbel-Softmax特征掩码和希尔伯特-施密特独立性准则约束强制统计独立性，从而分离稳定的因果特征。

Result: 在不同系列的未见生成模型上测试，CausalCLIP展现出强大的泛化能力，在准确率上比最先进方法提升6.83%，在平均精度上提升4.06%。

Conclusion: 通过因果推断原理解耦特征表示，可以有效提升生成图像检测器的跨模型泛化能力，为应对快速演化的生成技术提供了有前景的解决方案。

Abstract: The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.

</details>


### [174] [ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement](https://arxiv.org/abs/2512.13303)
*Zhihang Liu,Xiaoyi Bao,Pandeng Li,Junjie Zhou,Zhaohe Liao,Yefei He,Kaixun Jiang,Chen-Wei Xie,Yun Zheng,Hongtao Xie*

Main category: cs.CV

TL;DR: ShowTable：结合MLLM与扩散模型，通过渐进式自校正流程实现创意表格可视化生成的新方法


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在需要深度推理、规划和精确数据到视觉映射的任务上表现不足，特别是在超越一般场景的创意表格可视化任务中。需要开发能够忠实且美观地将表格数据可视化为信息图的新方法。

Method: 提出ShowTable管道，将多模态大语言模型（MLLM）与扩散模型协同工作。MLLM作为中央协调器，负责推理视觉计划和判断视觉错误以提供精炼指令，扩散模型执行MLLM的命令。采用渐进式自校正过程，并构建了三个自动化数据管道来训练不同模块。

Result: 实验表明，使用不同模型实例化的ShowTable管道显著优于基线方法。创建了TableVisBench基准（包含800个挑战性实例，覆盖5个评估维度），在创意表格可视化任务上展示了有效的多模态推理、生成和错误校正能力。

Conclusion: ShowTable通过MLLM与扩散模型的协同工作，结合渐进式自校正流程，成功解决了创意表格可视化这一具有挑战性的任务，为需要深度推理和精确数据到视觉映射的应用提供了有效解决方案。

Abstract: While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.

</details>


### [175] [KlingAvatar 2.0 Technical Report](https://arxiv.org/abs/2512.13313)
*Kling Team,Jialu Chen,Yikang Ding,Zhixue Fang,Kun Gai,Yuan Gao,Kang He,Jingyun Hua,Boyuan Jiang,Mingming Lao,Xiaohan Li,Hui Liu,Jiwen Liu,Xiaoqiang Liu,Yuan Liu,Shun Lu,Yongsen Mao,Yingchao Shao,Huafeng Shi,Xiaoyu Shi,Peiqin Sun,Songlin Tang,Pengfei Wan,Chao Wang,Xuebo Wang,Haoxian Zhang,Yuanxing Zhang,Yan Zhou*

Main category: cs.CV

TL;DR: KlingAvatar 2.0 是一个时空级联框架，通过低分辨率蓝图视频关键帧生成和时空上采样，解决了长时高分辨率视频生成中的效率、时间漂移和提示跟随问题，并引入多模态专家进行意图推理和ID控制。


<details>
  <summary>Details</summary>
Motivation: 现有头像视频生成模型在生成长时高分辨率视频时存在效率低、时间漂移、质量下降和提示跟随弱的问题，需要解决这些挑战以实现高效的多模态对齐长视频生成。

Method: 提出时空级联框架：1) 先生成低分辨率蓝图视频关键帧捕获全局语义和运动；2) 使用首尾帧策略将关键帧细化为高分辨率时序连贯的子片段；3) 引入由三个模态特定LLM专家组成的协同推理导演，通过多轮对话推理模态优先级和用户意图；4) 使用负向导演优化负提示；5) 扩展框架支持ID特定的多角色控制。

Result: 实验表明模型能有效解决高效、多模态对齐的长时高分辨率视频生成挑战，提供增强的视觉清晰度、逼真的唇齿渲染与准确的口型同步、强身份保持和连贯的多模态指令跟随。

Conclusion: KlingAvatar 2.0 通过时空级联框架和多模态推理专家，成功解决了长时高分辨率头像视频生成的关键问题，实现了高效、高质量、多模态对齐的视频生成。

Abstract: Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.

</details>


### [176] [Automated User Identification from Facial Thermograms with Siamese Networks](https://arxiv.org/abs/2512.13361)
*Elizaveta Prozorova,Anton Konev,Vladimir Faerman*

Main category: cs.CV

TL;DR: 该研究分析了基于面部热成像的生物识别技术，比较了不同红外光谱范围，提出了热成像相机的关键要求，并使用孪生神经网络实现了约80%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 开发可靠的安全系统需要有效的生物识别技术，热成像技术具有不受光照条件影响、可穿透伪装等优势，但需要系统研究其在不同红外光谱范围的性能和应用要求。

Method: 采用比较分析方法评估NIR、SWIR、MWIR和LWIR四种红外光谱范围，定义热成像相机的关键参数要求（分辨率、热灵敏度、帧率≥30Hz），并提出使用孪生神经网络来自动化识别过程。

Result: 在专有数据集上的实验表明，所提方法达到了约80%的识别准确率。研究还发现可见光与红外光谱的混合系统能够克服单一模态的限制，提高系统可靠性。

Conclusion: 热成像技术是开发可靠安全系统的有前景技术，通过优化红外光谱选择、相机参数和采用孪生神经网络等方法，可以实现有效的生物识别。混合系统结合可见光和红外光谱能进一步提升性能。

Abstract: The article analyzes the use of thermal imaging technologies for biometric identification based on facial thermograms. It presents a comparative analysis of infrared spectral ranges (NIR, SWIR, MWIR, and LWIR). The paper also defines key requirements for thermal cameras used in biometric systems, including sensor resolution, thermal sensitivity, and a frame rate of at least 30 Hz. Siamese neural networks are proposed as an effective approach for automating the identification process. In experiments conducted on a proprietary dataset, the proposed method achieved an accuracy of approximately 80%. The study also examines the potential of hybrid systems that combine visible and infrared spectra to overcome the limitations of individual modalities. The results indicate that thermal imaging is a promising technology for developing reliable security systems.

</details>


### [177] [Unlocking Generalization in Polyp Segmentation with DINO Self-Attention "keys"](https://arxiv.org/abs/2512.13376)
*Carla Monteiro,Valentina Corbetta,Regina Beets-Tan,Luís F. Teixeira,Wilson Silva*

Main category: cs.CV

TL;DR: 提出基于DINO自注意力"key"特征的息肉分割框架，通过简单卷积解码器实现优异泛化性能，在DG和ESDG协议下达到SOTA效果


<details>
  <summary>Details</summary>
Motivation: 现有息肉分割方法存在泛化能力不足、依赖复杂任务特定架构的问题，特别是在数据受限或挑战性场景下表现不佳

Method: 利用DINO自注意力模块的"key"特征，而非传统ViT最深层的token，结合简单卷积解码器预测息肉掩码

Result: 在多中心数据集上，通过DG和ESDG两种严格协议验证，达到SOTA性能，显著提升泛化能力，超越nnU-Net和UM-Net等模型

Conclusion: 该方法通过利用DINO自注意力关键特征，在不使用息肉特定架构的情况下，实现了优异的泛化性能和分割效果，为数据稀缺场景提供了有效解决方案

Abstract: Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention "key" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.

</details>


### [178] [Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs](https://arxiv.org/abs/2512.13392)
*Anran Qi,Changjian Li,Adrien Bousseau,Niloy J. Mitra*

Main category: cs.CV

TL;DR: 提出一种无需训练的图像转视频方法，通过可编辑的动态图控制运动，结合扩散模型生成外观，让用户能精确控制最终帧被遮挡区域的内容


<details>
  <summary>Details</summary>
Motivation: 当前图像转视频方法能生成合理运动但难以预测和控制，特别是在新暴露区域的内容生成方面。用户需要既能控制运动又能指定最终帧遮挡区域的外观

Method: 提出轻量级可编辑代理动态图（PDG）分离运动与外观合成：PDG确定性地驱动部件运动，冻结的扩散先验用于生成符合运动的外观。用户可编辑PDG姿态，计算密集运动流，利用扩散模型作为运动引导着色器，并通过潜在空间合成协调运动与用户意图

Result: 在关节物体、家具、车辆和可变形物体上展示了优于现有方法的性能，实现了可控的关节运动和用户对遮挡区域内容的控制，无需微调

Conclusion: 该方法结合了生成控制（松散姿态和结构）与可预测控制（最终帧遮挡区域外观指定），开辟了新的图像转视频工作流程

Abstract: We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyondvisible.github.io/

</details>


### [179] [rNCA: Self-Repairing Segmentation Masks](https://arxiv.org/abs/2512.13397)
*Malte Silbernagel,Albert Alonso,Jens Petersen,Bulat Ibragimov,Marleen de Bruijne,Madeleine K. Wyburd*

Main category: cs.CV

TL;DR: 使用神经细胞自动机（NCA）作为分割掩码的细化机制，通过局部迭代更新修复拓扑错误，提高分割质量。


<details>
  <summary>Details</summary>
Motivation: 现有分割模型经常产生碎片化或不连续的掩码输出，修复这些拓扑错误通常需要手工设计的细化规则或针对特定任务的专门架构。需要一种通用、有效的细化机制来改善分割掩码的拓扑一致性。

Method: 提出细化NCA（rNCA），将神经细胞自动机重新用作细化机制。通过在不完美掩码和真实标签上训练，自动机学习目标形状的结构特性，仅依赖局部信息进行迭代更新。当应用于粗糙的全局预测掩码时，学习到的动态过程逐步重新连接断裂区域、修剪松散碎片，收敛到稳定、拓扑一致的结果。

Result: 在视网膜血管分割中，Dice/clDice指标提升2-3%，Betti错误显著改善：β₀错误减少60%，β₁错误减少20%。在心肌分割中，零样本设置下修复了61.5%的断裂案例，ASSD和HD分别降低19%和16%。

Conclusion: 神经细胞自动机可作为有效且广泛适用的分割细化器，能够修复不同基础分割模型和任务产生的常见拓扑错误，提高分割掩码的拓扑一致性。

Abstract: Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.

</details>


### [180] [Computer vision training dataset generation for robotic environments using Gaussian splatting](https://arxiv.org/abs/2512.13411)
*Patryk Niżeniec,Marcin Iwanowski*

Main category: cs.CV

TL;DR: 提出一种基于3D高斯泼溅和游戏引擎的合成数据生成管道，用于创建大规模、高真实感、自动标注的机器人视觉数据集，通过混合训练策略提升检测和分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决合成数据与真实图像之间的领域差距问题，以及手动标注耗时耗力的瓶颈，为机器人视觉任务提供高效的数据生成方案。

Method: 使用3D高斯泼溅创建真实感环境表示，在游戏引擎中进行物理模拟生成自然布局，采用两阶段渲染技术结合泼溅真实感和代理网格生成的阴影图，自动生成像素级分割掩码。

Result: 实验表明，将少量真实图像与大量合成数据结合的混合训练策略，在物体检测和分割任务上取得了最佳性能，证实了该策略在高效获得鲁棒准确模型方面的优势。

Conclusion: 该方法为机器人视觉任务提供了一种高效的数据生成解决方案，通过合成数据与真实数据的混合训练，能够显著提升模型性能，同时大幅减少标注成本。

Abstract: This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.

</details>


### [181] [USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition](https://arxiv.org/abs/2512.13415)
*Ahmed Abul Hasanaath,Hamzah Luqman*

Main category: cs.CV

TL;DR: 提出统一时空建模框架USTM，结合Swin Transformer骨干网络和轻量级时间适配器，用于连续手语识别，在多个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有连续手语识别方法主要依赖CNN空间骨干网络结合时间卷积或循环模块，这些方法难以捕捉细粒度的手部和面部线索，也无法有效建模长程时间依赖关系。

Method: 提出统一时空建模框架USTM，包含Swin Transformer空间骨干网络，结合轻量级时间适配器与位置嵌入，能够同时捕捉细粒度空间特征和短期/长期时间上下文。

Result: 在PHOENIX14、PHOENIX14T和CSL-Daily等基准数据集上，USTM在RGB-based和多模态CSLR方法中都达到了最先进的性能，同时与多流方法保持竞争力。

Conclusion: USTM框架通过统一的时空建模有效解决了连续手语识别中的细粒度特征捕捉和长程时间依赖问题，展示了强大的识别能力，代码已开源。

Abstract: Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM

</details>


### [182] [Learning to Generate Cross-Task Unexploitable Examples](https://arxiv.org/abs/2512.13416)
*Haoxuan Qu,Qiuchi Xiang,Yujun Cai,Yirui Wu,Majid Mirmehdi,Hossein Rahmani,Jun Liu*

Main category: cs.CV

TL;DR: 提出MCT-UEG框架，通过元跨任务训练生成广泛不可利用的个人图像，保护在线隐私


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的不可利用示例在实际应用中存在局限性，无法在不同计算机视觉任务中广泛有效，需要提高实用性和跨任务泛化能力

Method: 提出MCT-UEG框架，采用面向平坦最小值的元训练和测试方案，优化不可利用示例生成器，使其能产生广泛不可利用的示例

Result: 大量实验证明了该框架的有效性

Conclusion: 提出的MCT-UEG框架能够生成跨任务广泛不可利用的个人图像，提高了不可利用示例生成的实际应用价值

Abstract: Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.

</details>


### [183] [RecTok: Reconstruction Distillation along Rectified Flow](https://arxiv.org/abs/2512.13421)
*Qingyu Shi,Size Wu,Jinbin Bai,Kaidong Yu,Yujing Wang,Yunhai Tong,Xiangtai Li,Xuelong Li*

Main category: cs.CV

TL;DR: RecTok通过流语义蒸馏和重建对齐蒸馏，克服了高维视觉分词器的局限性，在保持语义丰富性的同时实现了更好的重建和生成质量


<details>
  <summary>Details</summary>
Motivation: 现有视觉分词器在潜在空间维度和生成质量之间存在根本性权衡，高维分词器性能仍不如低维版本，需要解决这一限制

Method: 提出RecTok方法，包含两个关键创新：1) 流语义蒸馏：将视觉基础模型的语义信息蒸馏到流匹配的前向流轨迹中；2) 重建对齐蒸馏：引入掩码特征重建损失进一步增强语义

Result: 在gFID-50K基准测试中，无论是否使用分类器自由引导，都取得了最先进的结果，同时保持了语义丰富的潜在空间结构，且随着潜在维度增加性能持续提升

Conclusion: RecTok通过创新的蒸馏方法成功克服了高维视觉分词器的局限性，实现了优越的图像重建、生成质量和判别性能，为视觉分词器设计提供了新思路

Abstract: Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.

</details>


### [184] [MineTheGap: Automatic Mining of Biases in Text-to-Image Models](https://arxiv.org/abs/2512.13427)
*Noa Cohen,Nurit Spingarn-Eliezer,Inbar Huberman-Spiegelglas,Tomer Michaeli*

Main category: cs.CV

TL;DR: 提出MineTheGap方法，通过遗传算法自动挖掘导致文本到图像模型产生偏见输出的提示词，并使用新颖的偏见评分来衡量偏见的严重程度。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在处理模糊提示时会产生偏见，这些偏见可能带来社会影响（如职业与种族的刻板印象）和用户体验问题（生成图像缺乏多样性）。现有方法仅能检测给定提示的偏见，而无法主动发现导致偏见的提示。

Method: 提出MineTheGap方法：1）使用遗传算法迭代优化提示池，寻找能暴露偏见的提示；2）设计新颖的偏见评分，通过比较生成图像的分布与LLM生成的文本变体分布来量化偏见严重程度；3）在已知偏见的数据集上验证评分有效性。

Result: 方法能够自动挖掘导致TTI模型产生偏见的提示，超越了仅检测给定提示偏见的能力。偏见评分能够有效衡量偏见的严重程度，代码和示例已在项目网页公开。

Conclusion: MineTheGap提供了一种系统化的方法来发现文本到图像模型中的偏见，有助于识别和缓解模型的社会偏见问题，提升生成图像的多样性。

Abstract: Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.

</details>


### [185] [A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification](https://arxiv.org/abs/2512.13428)
*Anika Islam,Tasfia Tahsin,Zaarin Anjum,Md. Bakhtiar Hasan,Md. Hasanul Kabir*

Main category: cs.CV

TL;DR: 提出一个轻量级少样本学习框架，结合MobileNet特征提取器、特征融合和注意力增强的Bi-LSTM分类器，用于植物叶片病害识别，在数据稀缺和资源受限环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖大型标注数据集和计算密集型模型，不适用于数据稀缺和资源受限的农业环境，需要开发轻量级且高效的少样本学习解决方案。

Method: 使用领域适应的MobileNetV2和MobileNetV3作为特征提取器，结合特征融合技术生成鲁棒特征表示，然后通过注意力机制增强的Bi-LSTM分类器进行序列依赖建模和特征选择。

Result: 在PlantVillage数据集上，15-shot场景下达到98.23%准确率，接近99.98%的SOTA基准；在真实世界Dhan Shomadhan数据集上达到69.28%准确率；模型仅40MB大小，计算复杂度约1.12 GFLOPs。

Conclusion: 该轻量级框架为数据稀缺地区提供了可扩展、移动就绪的植物病害诊断解决方案，在保持高性能的同时显著降低了计算和存储需求。

Abstract: Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.

</details>


### [186] [IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images](https://arxiv.org/abs/2512.13440)
*Thalyssa Baiocco-Rodrigues,Antoine Olivier,Reda Belbahri,Thomas Duboudin,Pierre-Antoine Bannier,Benjamin Adjadj,Katharina Von Loga,Nathan Noiry,Maxime Touzot,Hector Roux de Bezieux*

Main category: cs.CV

TL;DR: IMILIA是一个用于炎症性肠病组织切片分析的端到端框架，结合多示例学习预测炎症存在，并通过可解释性模块自动计算驱动预测的组织区域特征标记物。


<details>
  <summary>Details</summary>
Motivation: 随着IBD治疗目标转向组织学缓解，准确评估微观炎症对于评估疾病活动性和治疗反应变得至关重要。需要自动化工具来预测炎症存在并解释预测结果。

Method: IMILIA包含炎症预测模块（基于多示例学习的模型）和可解释性模块（HistoPLUS用于细胞检测、分割和分类；EpiSeg用于上皮分割）。框架处理H&E染色的数字化切片。

Result: 在发现队列中获得交叉验证ROC-AUC 0.83，在两个外部验证队列中分别获得0.99和0.84的ROC-AUC。可解释性模块显示：高预测分数区域免疫细胞密度增加，低分数区域主要为正常上皮细胞，这些模式在所有数据集中一致。

Conclusion: IMILIA能够准确预测IBD组织切片中的炎症存在，并提供生物学一致的解释，有助于疾病活动性评估和治疗反应监测。代码和模型已部分开源。

Abstract: As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.

</details>


### [187] [Test-Time Modification: Inverse Domain Transformation for Robust Perception](https://arxiv.org/abs/2512.13454)
*Arpit Jadon,Joshua Niemeijer,Yuki M. Asano*

Main category: cs.CV

TL;DR: 使用扩散模型在测试时将目标域图像映射回源域分布，无需大规模合成数据生成，在未知目标分布的真实到真实域泛化场景中显著提升分割、检测和分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 生成基础模型包含广泛的视觉知识并能产生多样化的图像变体，对推进域泛化任务很有前景。然而，使用它们进行训练数据增强时，合成全面的目标域变体仍然缓慢、昂贵且不完整。

Method: 提出在测试时使用扩散模型将目标图像映射回源域分布的方法。该方法只需要源域描述，保留任务模型，并消除大规模合成数据生成。包括用于增强鲁棒性的集成变体。

Result: 在真实到真实域泛化场景中，对分割、检测和分类任务在具有挑战性的环境变化下表现出一致的改进。在多个生成模型和下游模型上进行分析，获得显著相对增益：BDD100K-Night上137%，ImageNet-R上68%，DarkZurich上62%。

Conclusion: 在测试时使用扩散模型将目标图像映射回源域分布是一种有效的域泛化方法，无需大规模数据合成，在未知目标分布的情况下显著提升下游任务的性能。

Abstract: Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.

</details>


### [188] [PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence](https://arxiv.org/abs/2512.13465)
*Ruiyan Wang,Teng Hu,Kaihui Huang,Zihan Su,Ran Yi,Lizhuang Ma*

Main category: cs.CV

TL;DR: PoseAnything是一个通用的姿态引导视频生成框架，首次支持人类和非人类角色，可处理任意骨骼输入，并实现了独立的相机运动控制。


<details>
  <summary>Details</summary>
Motivation: 当前姿态引导视频生成方法仅支持人类姿态输入，对其他主体的姿态泛化能力差，限制了在动画等领域的应用。

Method: 1) 提出Part-aware Temporal Coherence Module，将主体分割为不同部分，建立部分对应关系，通过跨帧交叉注意力实现细粒度部分级一致性；2) 提出Subject and Camera Motion Decoupled CFG，通过将主体和相机运动控制信息分别注入CFG的正负锚点，实现独立的相机运动控制；3) 构建XPose数据集，包含50,000个非人类姿态-视频对。

Result: PoseAnything在效果和泛化能力上显著优于现有最先进方法，能够处理人类和非人类角色，支持任意骨骼输入。

Conclusion: 该研究提出了首个通用的姿态引导视频生成框架，解决了现有方法仅支持人类姿态的限制，通过创新的时间一致性模块和解耦CFG策略，实现了更好的运动一致性和独立的相机控制。

Abstract: Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.

</details>


### [189] [Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\times$](https://arxiv.org/abs/2512.13492)
*Jiangning Zhang,Junwei Zhu,Teng Hu,Yabiao Wang,Donghao Luo,Weijian Cao,Zhenye Gan,Xiaobin Hu,Zhucun Xue,Chengjie Wang*

Main category: cs.CV

TL;DR: T3-Video提出了一种Transformer改造策略，通过优化前向逻辑而非修改核心架构，显著降低计算需求，实现10倍加速的4K视频生成，同时提升质量。


<details>
  <summary>Details</summary>
Motivation: 原生4K视频生成面临二次计算爆炸的挑战，全注意力机制在时空分辨率增加时计算成本急剧上升，现有模型难以平衡效率与质量。

Method: 提出T3（Transform Trained Transformer）策略，不改变预训练模型的全注意力架构，而是优化其前向逻辑。具体包括：多尺度权重共享窗口注意力机制、分层分块以及轴保持的全注意力设计。

Result: 在4K-VBench上，T3-Video显著优于现有方法：VQA提升4.29分，VTC提升0.08分，同时将原生4K视频生成加速超过10倍。

Conclusion: T3-Video通过创新的Transformer改造策略，成功解决了4K视频生成中的计算效率与质量平衡问题，为高分辨率视频生成提供了高效解决方案。

Abstract: Native 4K (2160$\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\textbf{T3}$ ($\textbf{T}$ransform $\textbf{T}$rained $\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an "attention pattern" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\uparrow$ VQA and +0.08$\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\times$. Project page at https://zhangzjn.github.io/projects/T3-Video

</details>


### [190] [Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation](https://arxiv.org/abs/2512.13495)
*Jiangning Zhang,Junwei Zhu,Zhenye Gan,Donghao Luo,Chuming Lin,Feifan Xu,Xu Peng,Jianlong Hu,Yuansen Liu,Yijia Hong,Weijian Cao,Han Feng,Xu Chen,Chencan Fu,Keke He,Xiaobin Hu,Chengjie Wang*

Main category: cs.CV

TL;DR: Soul是一个多模态驱动的高保真数字人动画框架，能从单帧肖像、文本提示和音频生成语义连贯的视频，实现精准唇音同步、生动表情和身份保持。


<details>
  <summary>Details</summary>
Motivation: 解决数字人动画中数据稀缺、长期生成一致性差、推理效率低等问题，为虚拟主播、影视制作等实际应用提供高质量解决方案。

Method: 基于Wan2.2-5B骨干网络，集成音频注入层、多种训练策略和阈值感知码本替换确保长期一致性；使用步长/CFG蒸馏和轻量VAE优化推理效率；构建Soul-1M数据集和Soul-Bench评估基准。

Result: 在视频质量、视频-文本对齐、身份保持和唇音同步精度上显著优于当前领先的开源和商业模型；推理速度提升11.4倍且质量损失可忽略；在虚拟主播、影视制作等场景中展示广泛应用潜力。

Conclusion: Soul框架通过创新的多模态驱动方法、大规模数据集构建和高效推理优化，为高质量数字人动画生成提供了全面解决方案，在多个评估维度上达到最先进水平。

Abstract: We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/

</details>


### [191] [Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model](https://arxiv.org/abs/2512.13507)
*Siyan Chen,Yanfei Chen,Ying Chen,Zhuo Chen,Feng Cheng,Xuyan Chi,Jian Cong,Qinpeng Cui,Qide Dong,Junliang Fan,Jing Fang,Zetao Fang,Chengjian Feng,Han Feng,Mingyuan Gao,Yu Gao,Qiushan Guo,Boyang Hao,Qingkai Hao,Bibo He,Qian He,Tuyen Hoang,Ruoqing Hu,Xi Hu,Weilin Huang,Zhaoyang Huang,Zhongyi Huang,Siqi Jiang,Wei Jiang,Yunpu Jiang,Zhuo Jiang,Ashley Kim,Jianan Kong,Zhichao Lai,Shanshan Lao,Ai Li,Feiya Li,Gen Li,Huixia Li,JiaShi Li,Liang Li,Ming Li,Tao Li,Xian Li,Xiaojie Li,Xiaoyang Li,Xingxing Li,Yameng Li,Yifu Li,Yiying Li,Chao Liang,Ying Liang,Zhiqiang Liang,Wang Liao,Yalin Liao,Heng Lin,Kengyu Lin,Shanchuan Lin,Xi Lin,Zhijie Lin,Feng Ling,Fangfang Liu,Gaohong Liu,Jiawei Liu,Jie Liu,Shouda Liu,Shu Liu,Sichao Liu,Songwei Liu,Xin Liu,Xue Liu,Yibo Liu,Zikun Liu,Zuxi Liu,Junlin Lyu,Lecheng Lyu,Qian Lyu,Han Mu,Xiaonan Nie,Jingzhe Ning,Xitong Pan,Yanghua Peng,Lianke Qin,Xueqiong Qu,Yuxi Ren,Yuchen Shen,Guang Shi,Lei Shi,Yan Song,Yinglong Song,Fan Sun,Li Sun,Renfei Sun,Zeyu Sun,Wenjing Tang,Zirui Tao,Feng Wang,Furui Wang,Jinran Wang,Junkai Wang,Ke Wang,Kexin Wang,Qingyi Wang,Rui Wang,Sen Wang,Shuai Wang,Tingru Wang,Weichen Wang,Xin Wang,Yanhui Wang,Yue Wang,Yuping Wang,Yuxuan Wang,Ziyu Wang,Guoqiang Wei,Wanru Wei,Di Wu,Guohong Wu,Hanjie Wu,Jian Wu,Jie Wu,Ruolan Wu,Xinglong Wu,Yonghui Wu,Ruiqi Xia,Liang Xiang,Fei Xiao,XueFeng Xiao,Pan Xie,Shuangyi Xie,Shuang Xu,Jinlan Xue,Bangbang Yang,Ceyuan Yang,Jiaqi Yang,Runkai Yang,Tao Yang,Yang Yang,Yihang Yang,ZhiXian Yang,Ziyan Yang,Yifan Yao,Zilyu Ye,Bowen Yu,Chujie Yuan,Linxiao Yuan,Sichun Zeng,Weihong Zeng,Xuejiao Zeng,Yan Zeng,Chuntao Zhang,Heng Zhang,Jingjie Zhang,Kuo Zhang,Liang Zhang,Liying Zhang,Manlin Zhang,Ting Zhang,Weida Zhang,Xiaohe Zhang,Xinyan Zhang,Yan Zhang,Yuan Zhang,Zixiang Zhang,Fengxuan Zhao,Huating Zhao,Yang Zhao,Hao Zheng,Jianbin Zheng,Xiaozheng Zheng,Yangyang Zheng,Yijie Zheng,Jiexin Zhou,Kuan Zhu,Shenhan Zhu,Wenjia Zhu,Benhui Zou,Feilong Zuo*

Main category: cs.CV

TL;DR: Seedance 1.5 pro是一个专门为原生联合音频-视频生成设计的基础模型，采用双分支Diffusion Transformer架构，通过跨模态联合模块和多阶段数据管道实现卓越的视听同步和生成质量。


<details>
  <summary>Details</summary>
Motivation: 视频生成技术的进步为统一的视听生成铺平了道路，但需要专门针对原生联合音频-视频生成的基础模型，以实现更好的同步效果和专业级内容创作。

Method: 采用双分支Diffusion Transformer架构，集成跨模态联合模块和专门的多阶段数据管道；实施监督微调(SFT)和基于人类反馈的强化学习(RLHF)进行后训练优化；引入加速框架提升推理速度10倍以上。

Result: 实现了卓越的音频-视频同步和生成质量，具备精确的多语言和方言口型同步、动态电影级相机控制、增强的叙事连贯性，成为专业级内容创作的强大引擎。

Conclusion: Seedance 1.5 pro通过创新的架构设计和优化策略，在联合音频-视频生成方面取得了显著进展，为专业内容创作提供了强大的基础模型，现已可在火山引擎平台上访问。

Abstract: Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.

</details>


### [192] [TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding](https://arxiv.org/abs/2512.13511)
*Piyush Bagad,Andrew Zisserman*

Main category: cs.CV

TL;DR: TARA通过简单高效的适配方法，将多模态大语言模型转化为时间感知的视频-文本嵌入模型，无需视频数据，在时间感知检索、否定理解和动作副词理解方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 构建通用的时间感知视频-文本嵌入模型，解决现有模型在时间感知检索方面的不足，特别是对时间顺序敏感的动作理解。

Method: 提出TARA（时间感知检索适配）方法，通过简单高效的适配策略，将多模态大语言模型转化为时间感知的视频-文本嵌入模型，无需使用任何视频数据进行训练。

Result: TARA在新提出的手性动作基准测试中优于所有现有视频-文本模型，同时在标准基准测试中表现强劲。此外，TARA在否定理解（NegBench）以及动词和副词理解方面达到最先进水平。

Conclusion: TARA产生了一个强大、多功能、时间感知的视频-文本嵌入模型，具有最先进的零样本性能，在时间感知检索、否定理解和动作理解方面均有显著优势。

Abstract: Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.

</details>


### [193] [Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains](https://arxiv.org/abs/2512.13534)
*Marianne Rakic,Siyu Gai,Etienne Chollet,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: Pancakes框架能够为未见过的医学图像自动生成多种语义一致的多标签分割方案，解决现有模型只能处理单一分割协议或需要手动提示的问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像可以根据不同应用需求进行多种有意义的分割（如组织类型、血管区域、解剖结构等），但现有自动分割模型要么只支持单一训练协议，要么需要繁琐的手动提示来指定分割方案。

Method: 提出Pancakes框架，引入新的问题表述，能够在给定未见领域的新图像时，自动生成多个合理协议的多标签分割图，同时保持相关图像间的语义一致性。

Result: 在7个保留数据集上的实验表明，该模型在生成多个合理的全图像分割方面显著优于现有基础模型，且分割结果在图像间具有语义一致性。

Conclusion: Pancakes框架解决了现有基础模型无法实现的多协议自动分割问题，为医学图像分析提供了更灵活、自动化的多方案分割能力。

Abstract: A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.

</details>


### [194] [3D Human-Human Interaction Anomaly Detection](https://arxiv.org/abs/2512.13560)
*Shun Maeda,Chunzhi Gu,Koichiro Kamide,Katsuya Hotta,Shangce Gao,Chao Zhang*

Main category: cs.CV

TL;DR: 提出新任务H2IAD（人-人交互异常检测）和IADNet模型，通过TASM模块共享运动嵌入、DREM模块编码空间关系，使用归一化流进行异常评分，在多人交互动作异常检测上优于现有单人类异常检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人体异常检测主要针对单人行为，但人类行为本质上是协作性的，异常也可能出现在人-人交互中。现有的单人异常检测模型难以捕捉交互的复杂非对称动态，导致检测精度低。

Method: 提出IADNet模型，包含Temporal Attention Sharing Module（TASM）共享两人编码的运动嵌入以同步协作运动相关性，以及Distance-Based Relational Encoding Module（DREM）编码空间配置关系反映社交线索，最后使用归一化流进行异常评分。

Result: 在多人运动基准测试上的大量实验表明，IADNet在H2IAD任务上优于现有的人体异常检测基线方法。

Conclusion: 提出了新的人-人交互异常检测任务H2IAD和相应的IADNet模型，通过有效捕捉交互的时空动态，显著提升了多人协作行为异常检测的性能。

Abstract: Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.

</details>


### [195] [MMhops-R1: Multimodal Multi-hop Reasoning](https://arxiv.org/abs/2512.13573)
*Tao Zhang,Ziqi Zhang,Zongyang Ma,Yuxin Chen,Bing Li,Chunfeng Yuan,Guangting Wang,Fengyun Rao,Ying Shan,Weiming Hu*

Main category: cs.CV

TL;DR: 提出了MMhops基准测试和MMhops-R1框架，用于评估和提升多模态多跳推理能力，通过强化学习优化动态推理路径规划。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型主要局限于单步推理，缺乏评估多跳推理能力的复杂基准测试，无法应对需要跨模态整合外部知识的复杂现实挑战。

Method: 1) 提出MMhops基准数据集，包含桥接和比较两种任务格式；2) 提出MMhops-R1多模态检索增强生成框架，使用强化学习优化动态推理路径规划、查询生成和多级信息整合。

Result: MMhops-R1在MMhops基准上显著优于强基线模型，证明动态规划和多模态知识整合对复杂推理至关重要。同时该框架在固定跳数推理任务上表现出良好的泛化能力。

Conclusion: 本研究贡献了具有挑战性的新基准测试和强大的基线模型，将发布相关代码、数据和权重，以推动多模态多跳推理这一关键领域的研究。

Abstract: The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.

</details>


### [196] [Lighting in Motion: Spatiotemporal HDR Lighting Estimation](https://arxiv.org/abs/2512.13597)
*Christophe Bolduc,Julien Philip,Li Ma,Mingming He,Paul Debevec,Jean-François Lalonde*

Main category: cs.CV

TL;DR: LiMo是一种基于扩散模型的时空光照估计方法，通过生成不同曝光下的镜面和漫反射球体来预测高频细节和准确照度，结合深度和几何条件进行空间控制，最终合成HDRI光照图。


<details>
  <summary>Details</summary>
Motivation: 现有光照估计方法难以同时实现真实的高频细节预测和准确的照度估计，需要一种能够处理时空变化光照并具有精确空间控制能力的方法。

Method: 1. 基于扩散先验，在包含室内外场景和时空光探针的大规模数据集上微调现有扩散模型；2. 提出新的几何条件来提供场景相对于目标3D位置的相对位置信息；3. 生成不同曝光下的镜面和漫反射球体；4. 通过可微分渲染将不同曝光的预测组合成单个HDRI图。

Result: LiMo在空间控制和预测准确性方面均达到最先进水平，能够同时实现真实的高频细节和准确的照度估计。

Conclusion: LiMo通过创新的几何条件、多曝光球体生成和可微分渲染，成功实现了高质量的时空光照估计，为计算机视觉和图形学应用提供了有效的解决方案。

Abstract: We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.

</details>


### [197] [LongVie 2: Multimodal Controllable Ultra-Long Video World Model](https://arxiv.org/abs/2512.13604)
*Jianxiong Gao,Zhaoxi Chen,Xian Liu,Junhao Zhuang,Chengming Xu,Jianfeng Feng,Yu Qiao,Yanwei Fu,Chenyang Si,Ziwei Liu*

Main category: cs.CV

TL;DR: LongVie 2是一个三阶段训练的端到端自回归框架，通过多模态引导、退化感知训练和历史上下文引导，实现了可控、高质量、时间一致的长视频生成，支持长达5分钟的视频生成。


<details>
  <summary>Details</summary>
Motivation: 在预训练视频生成系统上构建视频世界模型是实现通用时空智能的重要但具有挑战性的步骤。世界模型应具备三个基本属性：可控性、长期视觉质量和时间一致性。

Method: 采用渐进式方法：1) 多模态引导，整合密集和稀疏控制信号提供隐式世界级监督；2) 输入帧的退化感知训练，弥合训练与长期推理之间的差距；3) 历史上下文引导，对齐相邻片段间的上下文信息以确保时间一致性。

Result: LongVie 2在长距离可控性、时间一致性和视觉保真度方面达到最先进性能，支持长达5分钟的连续视频生成。作者还引入了LongVGenBench基准测试，包含100个高分辨率一分钟视频。

Conclusion: LongVie 2代表了向统一视频世界建模迈出的重要一步，通过三阶段训练框架成功解决了可控性、视觉质量和时间一致性的挑战。

Abstract: Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.

</details>


### [198] [DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis](https://arxiv.org/abs/2512.13608)
*Felix J. Dorfner,Manon A. Dorster,Ryan Connolly,Oscar Gentilhomme,Edward Gibbs,Steven Graham,Seth Wander,Thomas Schultz,Manisha Bahl,Dania Daye,Albert E. Kim,Christopher P. Bridge*

Main category: cs.CV

TL;DR: 开发了首个数字乳腺断层合成（DBT）基础模型DBT-DINO，在乳腺密度分类和癌症风险预测任务上表现优异，但在病灶检测任务上优势不明显。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在医学影像领域显示出潜力，但针对三维成像模态（特别是数字乳腺断层合成DBT）的基础模型仍然缺乏，而DBT是乳腺癌筛查的重要工具。

Method: 使用DINOv2方法在487,975个DBT体积（来自27,990名患者）的超过2500万张2D切片上进行自监督预训练，然后在三个下游任务评估：乳腺密度分类、5年乳腺癌风险预测和病灶检测。

Result: DBT-DINO在乳腺密度分类上准确率0.79，优于MetaAI DINOv2基线（0.73）和DenseNet-121（0.74）；在5年风险预测上AUROC为0.78，与DINOv2的0.76无显著差异；在病灶检测上DINOv2平均敏感度0.67优于DBT-DINO的0.62，但DBT-DINO在癌性病灶检测率上略优（78.8% vs 77.3%）。

Conclusion: DBT-DINO是首个DBT基础模型，在乳腺密度分类和癌症风险预测任务上表现良好，但领域特定预训练在病灶检测任务上的优势有限，表明局部检测任务需要进一步的方法学改进。

Abstract: Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.
  To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.
  Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes.
  For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\% CI: 0.70--0.76, p<.001) and DenseNet-121 (0.74, 95\% CI: 0.71--0.76, p<.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\% compared to Dinov2's 77.3\%.
  Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.

</details>


### [199] [Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models](https://arxiv.org/abs/2512.13609)
*Shweta Mahajan,Shreya Kadambi,Hoang Le,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: 提出Do-Undo任务和基准，用于评估视觉语言模型对物理动作驱动的场景变换的理解和生成能力，要求模型模拟物理动作结果并准确反转该动作。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型主要关注对象级编辑，缺乏对物理世界中真实因果关系的理解，特别是物理动作驱动的场景变换的可逆性。这对于具身AI、机器人和物理感知生成模型至关重要。

Method: 从真实世界视频中构建大规模可逆动作数据集，设计训练策略以增强动作基础的一致性，要求模型既能执行动作（Do）又能反转动作（Undo）。

Result: 实验显示当前模型在物理可逆性方面表现不佳，验证了Do-Undo任务对评估模型物理推理能力的重要性，为多模态系统的物理推理提供了直观测试平台。

Conclusion: Do-Undo填补了视觉语言模型在物理动作理解和生成方面的空白，为评估和推进多模态系统的物理推理能力建立了重要基准，对具身AI和机器人等领域有重要意义。

Abstract: We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.

</details>


### [200] [SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning](https://arxiv.org/abs/2512.13635)
*Junchao Zhu,Ruining Deng,Junlin Guo,Tianyuan Yao,Chongyu Qu,Juming Xiong,Siqi Lu,Zhengyi Lu,Yanfan Zhu,Marilyn Lionts,Yuechen Yang,Yalin Zheng,Yu Wang,Shilin Zhao,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: SCR2-ST是一个利用单细胞先验知识指导空间转录组学数据高效采集和准确预测的统一框架，通过强化学习主动采样和混合回归-检索预测网络，在有限测序预算下实现最优数据采集和表达预测。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学技术昂贵且传统固定网格采样策略导致冗余测量，数据稀缺限制了现有方法。单细胞测序领域提供丰富的生物学数据可作为有效辅助源来缓解这一限制。

Method: 提出SCR2-ST框架，包含：1) 单细胞引导的强化学习主动采样(SCRL)，结合单细胞基础模型嵌入和空间密度信息构建生物学奖励信号；2) 混合回归-检索预测网络SCR2Net，通过多数细胞类型过滤机制抑制噪声匹配，检索的表达谱作为软标签进行辅助监督。

Result: 在三个公共ST数据集上评估，在采样效率和预测准确性方面均达到最先进水平，特别是在低预算场景下表现优异。

Conclusion: SCR2-ST通过有效利用单细胞先验知识，解决了空间转录组学数据采集成本高和数据稀缺的问题，为在有限测序预算下获得高质量ST数据提供了创新解决方案。

Abstract: Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST

</details>


### [201] [MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning](https://arxiv.org/abs/2512.13636)
*Haoyu Fu,Diankun Zhang,Zongchuang Zhao,Jianfeng Cui,Hongwei Xie,Bing Wang,Guang Chen,Dingkang Liang,Xiang Bai*

Main category: cs.CV

TL;DR: MindDrive提出了一种用于自动驾驶的VLA框架，通过将LLM分为决策专家和动作专家，将连续动作空间的强化学习问题转化为离散语言决策空间的探索问题，有效解决了VLA模型中在线强化学习的探索效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶中的VLA范式主要依赖模仿学习，存在分布偏移和因果混淆等固有挑战。在线强化学习通过试错学习为解决这些问题提供了有前景的途径，但在连续动作空间中的低效探索阻碍了其在VLA模型中的应用。

Method: 提出MindDrive框架，包含一个具有两组不同LoRA参数的大型语言模型：一个作为决策专家进行场景推理和驾驶决策，另一个作为动作专家将语言决策动态映射为可行轨迹。通过将轨迹级奖励反馈到推理空间，在有限的离散语言驾驶决策空间中进行试错学习，而不是直接在连续动作空间中操作。

Result: 在具有挑战性的Bench2Drive基准测试中取得了强大的闭环性能，驾驶得分(DS)为78.04，成功率(SR)为55.09%。这是首个证明在线强化学习在自动驾驶VLA模型中有效性的工作。

Conclusion: MindDrive通过将连续动作空间的强化学习问题转化为离散语言决策空间的探索，有效平衡了复杂场景中的最优决策、类人驾驶行为和在线强化学习的高效探索，为自动驾驶VLA模型提供了新的强化学习范式。

Abstract: Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.

</details>


### [202] [Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All](https://arxiv.org/abs/2512.13639)
*Michal Nazarczuk,Thomas Tanay,Arthur Moreau,Zhensong Zhang,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: 该论文提出了一个用于新视角合成的高质量动画电影数据集，包含动态场景、多种模态数据（深度、法线、分割、光流），并设计了三种基准测试场景（密集多视角、稀疏相机、单目视频）。


<details>
  <summary>Details</summary>
Motivation: 现有新视角合成和4D场景重建方法缺乏高质量、多模态、动态场景的数据集，限制了这些前沿技术的训练和评估。需要包含丰富纹理、光照、运动细节的真实感数据来推动3D视觉领域发展。

Method: 从高质量动画电影中生成数据集，提供RGB图像及深度、表面法线、物体分割、光流等多种模态数据。将数据集组织为三种基准场景：密集多视角相机设置、稀疏相机布置和单目视频序列。

Result: 创建了一个视觉丰富、标注质量高、实验设置多样的新视角合成数据集，支持不同数据稀疏程度的实验和比较，为4D场景重建和新视角生成模型提供了理想的训练和评估资源。

Conclusion: 该数据集通过其视觉丰富性、高质量标注和多样化的实验设置，为新视角合成和3D视觉领域提供了独特的资源，有助于推动这些领域的技术边界。

Abstract: This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.

</details>


### [203] [Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency](https://arxiv.org/abs/2512.13665)
*Wenhan Chen,Sezer Karaoglu,Theo Gevers*

Main category: cs.CV

TL;DR: 提出Grab-3D框架，利用3D几何一致性检测AI生成视频，通过消失点表征几何模式差异，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型生成视频技术快速发展，AI生成视频越来越逼真，需要可靠的检测机制。现有方法对生成视频中的3D几何模式探索有限，而几何一致性是区分真实与AI生成视频的关键特征。

Method: 使用消失点作为3D几何模式的显式表征；构建静态场景AI生成视频数据集以稳定提取3D几何特征；提出几何感知Transformer框架，包含几何位置编码、时间-几何注意力机制和基于EMA的几何分类器头，将3D几何感知显式注入时间建模。

Result: Grab-3D显著优于现有最先进的检测器，在未见过的生成器上表现出强大的跨域泛化能力。

Conclusion: 3D几何一致性是检测AI生成视频的有效线索，Grab-3D框架通过显式建模几何模式差异实现了优越的检测性能。

Abstract: Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.

</details>


### [204] [AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection](https://arxiv.org/abs/2512.13671)
*Junwen Miao,Penghui Du,Yi Liu,Yu Wang,Yan Wang*

Main category: cs.CV

TL;DR: AgentIAD是一个基于工具驱动的代理框架，通过多阶段视觉检查实现工业异常检测，结合感知缩放器和比较检索器，在MMAD数据集上达到97.62%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测面临正常参考样本稀缺和缺陷微小、局部化的挑战，单次视觉语言模型往往忽略小异常且缺乏与规范正常模式比较的机制。

Method: 提出AgentIAD框架，配备感知缩放器进行局部细粒度分析，比较检索器在证据模糊时查询正常样本。从MMAD数据集构建结构化的感知和比较轨迹，采用两阶段训练：监督微调后接强化学习，设计感知奖励和行为奖励两部分。

Result: 在MMAD数据集上达到97.62%的分类准确率，创造了新的最先进水平，超越了之前的MLLM方法，同时产生透明可解释的检查轨迹。

Conclusion: AgentIAD通过多阶段观察、缩放和验证的逐步判断机制，有效解决了工业异常检测的挑战，实现了高精度和可解释性的统一。

Abstract: Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.

</details>


### [205] [Towards Interactive Intelligence for Digital Humans](https://arxiv.org/abs/2512.13674)
*Yiyi Cai,Xuangeng Chu,Xiwei Gao,Sitong Gong,Yifei Huang,Caixin Kang,Kunhang Li,Haiyang Liu,Ruicong Liu,Yun Liu,Dianwen Ng,Zixiong Su,Erwin Wu,Yuhan Wu,Dingkun Yan,Tianyu Yan,Chang Zeng,Bo Zheng,You Zhou*

Main category: cs.CV

TL;DR: 提出交互智能新范式Mio框架，实现个性表达、自适应交互和自我进化的数字人，通过五模块统一架构超越表面模仿


<details>
  <summary>Details</summary>
Motivation: 当前数字人主要停留在表面模仿阶段，缺乏真正的智能交互能力，需要实现个性表达、自适应交互和自我进化的综合能力

Method: 提出Mio端到端框架，包含五个专门模块：Thinker（思考）、Talker（说话）、Face Animator（面部动画）、Body Animator（身体动画）和Renderer（渲染），将认知推理与实时多模态体现相结合

Result: 建立了新的交互智能评估基准，实验表明该框架在所有评估维度上都优于现有最先进方法

Conclusion: Mio框架将数字人从表面模仿推向智能交互，实现了交互智能的新范式

Abstract: We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.

</details>


### [206] [JoVA: Unified Multimodal Learning for Joint Video-Audio Generation](https://arxiv.org/abs/2512.13677)
*Xiaohu Huang,Hao Zhou,Qiangpeng Yang,Shilei Wen,Kai Han*

Main category: cs.CV

TL;DR: JoVA是一个统一的视频-音频联合生成框架，通过联合自注意力实现跨模态交互，无需额外对齐模块，并引入基于面部关键点的嘴部区域损失来提升唇语同步质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个关键限制：1) 大多只能生成环境音，缺乏生成与唇部动作同步的人声的能力；2) 现有统一视频-音频生成方法通常依赖显式融合或模态特定对齐模块，增加了架构复杂性并削弱了原始Transformer的简洁性。

Method: 1) 在Transformer层中使用视频和音频token的联合自注意力，实现直接高效的跨模态交互；2) 引入基于面部关键点检测的嘴部区域损失，在训练时增强对关键嘴部区域的监督，同时保持架构简洁。

Result: 在基准测试上的大量实验表明，JoVA在唇语同步准确性、语音质量和整体视频-音频生成保真度方面优于或与最先进的统一和音频驱动方法相媲美。

Conclusion: JoVA作为一个优雅的框架，能够实现高质量的多模态生成，解决了现有方法的局限性，同时保持了架构的简洁性。

Abstract: In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.

</details>


### [207] [LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction](https://arxiv.org/abs/2512.13680)
*Tianye Ding,Yiming Xie,Yiqing Liang,Moitreya Chatterjee,Pedro Miraldo,Huaizu Jiang*

Main category: cs.CV

TL;DR: LASER是一个无需训练即可将离线3D重建模型转换为流式系统的框架，通过层尺度对齐解决连续时间窗口间的深度尺度不一致问题，实现高效流式视频重建。


<details>
  <summary>Details</summary>
Motivation: 现有前馈重建模型（如VGGT和π³）虽然重建质量好，但二次内存复杂度无法处理流式视频。现有流式方法需要大量重新训练，且无法充分利用离线模型的几何先验。

Method: 提出LASER框架：1）通过连续时间窗口预测对齐将离线模型转换为流式系统；2）发现简单相似变换对齐因单目尺度模糊导致层深度错位；3）引入层尺度对齐：将深度预测分割为离散层，计算每层尺度因子，并在相邻窗口和时间戳间传播。

Result: 在相机姿态估计和点云重建方面达到最先进性能，在RTX A6000 GPU上以14 FPS运行，峰值内存仅6 GB，可处理千米级流式视频。

Conclusion: LASER无需训练即可将高质量离线重建模型转换为实用的流式系统，解决了深度尺度不一致问题，实现了高效、低内存的流式3D重建。

Abstract: Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$

</details>


### [208] [I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners](https://arxiv.org/abs/2512.13683)
*Lu Ling,Yunhao Ge,Yichen Sheng,Aniket Bera*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，通过重新编程预训练的3D实例生成器来学习场景级空间关系，从而实现对新布局和物体组合的泛化，无需依赖有限场景数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的3D场景生成方法依赖于有限场景数据集，限制了其对新布局的泛化能力。作者希望利用预训练3D实例生成器的可迁移空间知识，实现更好的场景生成泛化。

Method: 重新编程预训练的3D实例生成器，用模型中心的空间监督替代数据集监督，采用视图中心的场景空间表示，直接从实例模型中学习空间关系。

Result: 该方法能够泛化到未见过的布局和新颖物体组合，甚至在训练场景由随机组合物体构成时仍能出现空间推理能力，证明了实例生成器作为隐式空间学习器的潜力。

Conclusion: 3D实例生成器可以作为隐式空间学习器和推理器，为交互式3D场景理解和生成的基石模型指明了方向。

Abstract: Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/

</details>


### [209] [Recurrent Video Masked Autoencoders](https://arxiv.org/abs/2512.13684)
*Daniel Zoran,Nikhil Parthasarathy,Yi Yang,Drew A Hudson,Joao Carreira,Andrew Zisserman*

Main category: cs.CV

TL;DR: RVM是一种新颖的视频表示学习方法，使用基于transformer的循环神经网络聚合时序图像特征，通过掩码预测任务学习，在视频任务和图像任务上均表现优异，参数效率极高。


<details>
  <summary>Details</summary>
Motivation: 现有视频表示学习方法在参数效率和计算成本方面存在局限，需要一种既能有效捕捉视频时空结构，又能在小模型规模下高效运行的方法。

Method: 使用基于transformer的循环神经网络聚合密集图像特征，通过非对称掩码预测任务学习，仅需标准像素重建目标，无需知识蒸馏。

Result: 在动作识别、点/目标跟踪等视频任务上与VideoMAE、V-JEPA等SOTA模型竞争，在几何和空间理解任务上优于DINOv2等图像模型，参数效率比竞争视频掩码自编码器高30倍。

Conclusion: RVM通过循环架构实现了高效的视频表示学习，能够稳定传播特征并线性计算成本，学习到丰富的场景语义、结构和运动表示。

Abstract: We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.

</details>


### [210] [Towards Scalable Pre-training of Visual Tokenizers for Generation](https://arxiv.org/abs/2512.13687)
*Jingfeng Yao,Yuda Song,Yucong Zhou,Xinggang Wang*

Main category: cs.CV

TL;DR: VTP提出统一的视觉分词器预训练框架，通过联合优化图像-文本对比、自监督和重建损失来解决传统VAE预训练中的"预训练扩展问题"，使生成性能随计算资源有效扩展。


<details>
  <summary>Details</summary>
Motivation: 传统基于重建的视觉分词器（如VAE）预训练存在"预训练扩展问题"：更好的像素级重建精度并不带来更高质量的生成效果，大量计算投入无法有效转化为生成性能提升。需要转向能简洁表示高层语义的潜在空间。

Method: 提出VTP统一视觉分词器预训练框架，首次联合优化三种损失：图像-文本对比损失（促进语义理解）、自监督损失（增强表示能力）、重建损失（保持细节）。通过大规模研究验证框架的有效性。

Result: VTP在ImageNet上达到78.2%零样本准确率和0.36 rFID，生成任务收敛速度比先进蒸馏方法快4.1倍。更重要的是展现出良好的扩展性：仅增加VTP预训练计算量就能实现65.8% FID改进，而传统自编码器在1/10计算量时就停滞不前。

Conclusion: 理解是生成的关键驱动力，VTP框架解决了视觉分词器预训练的扩展问题，使生成性能能随计算、参数和数据有效扩展，为高质量生成模型提供了更好的视觉分词器基础。

Abstract: The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.

</details>


### [211] [LitePT: Lighter Yet Stronger Point Transformer](https://arxiv.org/abs/2512.13689)
*Yuanwen Yue,Damien Robert,Jianyuan Wang,Sunghwan Hong,Jan Dirk Wegner,Christian Rupprecht,Konrad Schindler*

Main category: cs.CV

TL;DR: 提出LitePT模型，通过早期使用卷积、后期使用注意力的混合架构，在3D点云处理中实现更高效性能


<details>
  <summary>Details</summary>
Motivation: 现有3D点云网络同时包含卷积层和注意力块，但最佳组合方式不明确。研究发现卷积适合早期高分辨率提取低层几何特征，注意力更适合深层低分辨率捕获高层语义和上下文

Method: 提出LitePT模型：早期阶段使用卷积，深层阶段切换到注意力。引入无需训练的3D位置编码PointROPE来保留空间布局信息

Result: 相比最先进的Point Transformer V3，LitePT参数量减少3.6倍，运行速度快2倍，内存使用减少2倍，但在多个任务和数据集上性能相当甚至更好

Conclusion: 通过合理组合卷积和注意力，可以在保持性能的同时显著提升3D点云处理网络的效率和实用性

Abstract: Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\times$ fewer parameters, runs $2\times$ faster, and uses $2\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [212] [A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models](https://arxiv.org/abs/2512.11835)
*Seyma Yaman Kayadibi*

Main category: cs.AI

TL;DR: 论文基于人工年龄评分(AAS)构建了一个基于条款的架构，将莱布尼茨单子论的20个单子分组为6个束，实现为可执行规范，用于约束LLM的记忆和控制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为强大但不透明的系统部署，其内部记忆和"自我类似"行为缺乏原则性和可审计的治理方式。需要建立透明、可实施的框架来约束和分析人工智能体的内部动态。

Method: 将莱布尼茨单子论的20个单子分组为6个束（本体论、动力学、表征与意识、和谐与理性、身体与组织、目的论），在AAS内核上实现为可执行规范，通过6个最小Python实现进行数值实验。

Result: 条款系统表现出有界且可解释的行为：AAS轨迹保持连续且速率受限，矛盾和未经支持的声明触发明确惩罚，层次细化以受控方式揭示有机结构，和谐项对齐双重视图和目标-行动对，完美度分数的窗口漂移区分持续改进与退化。

Conclusion: 基于单子的条款框架以AAS为骨干，为约束和分析人工智能体的内部动态提供了透明、代码级的蓝图，不仅具有哲学动机，而且可直接实现。

Abstract: Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and "self-like" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.

</details>


### [213] [Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars](https://arxiv.org/abs/2512.11864)
*Christoph Einspieler,Matthias Horn,Marie-Louise Lackner,Patrick Malik,Nysret Musliu,Felix Winter*

Main category: cs.AI

TL;DR: 提出一种解决具有作业优先级和基于日历的累积资源约束的真实工业并行机调度问题的新方法，结合精确约束建模和启发式算法


<details>
  <summary>Details</summary>
Motivation: 现代工业制造中需要处理复杂的作业优先级约束和基于日历的资源限制，现有调度技术无法有效解决这些真实生产环境中的约束问题

Method: 1) 使用约束建模方法作为小规模场景的精确解；2) 提出构造启发式算法；3) 设计基于局部搜索的定制元启发式算法处理大规模实例

Result: 开发的方法已在工业环境中部署使用，能够有效解决具有复杂约束的真实工业并行机调度问题

Conclusion: 提出的混合方法结合精确技术和启发式算法，成功解决了工业环境中复杂的并行机调度问题，并已在实际生产中应用

Abstract: The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting.

</details>


### [214] [Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning](https://arxiv.org/abs/2512.11902)
*Yanna Elizabeth Smid,Peter van der Putten,Aske Plaat*

Main category: cs.AI

TL;DR: 开发Mirror Mode游戏模式，让AI模仿玩家个人策略来增加挑战性。通过结合强化学习和模仿学习技术，在简化版Fire Emblem Heroes游戏中实现，实验显示防守行为模仿良好但进攻策略不足，玩家满意度较高。


<details>
  <summary>Details</summary>
Motivation: 传统回合制游戏中敌人策略往往可预测，缺乏惊喜感。研究旨在创造一种能让AI模仿玩家个人策略的新游戏模式，通过让玩家面对自己的策略来增加挑战性和趣味性。

Method: 1. 在Unity中构建简化版Fire Emblem Heroes游戏，包含标准模式和Mirror模式；2. 结合生成对抗模仿学习、行为克隆和近端策略优化等强化学习和模仿学习技术来模仿玩家演示；3. 进行两组实验：第一组寻找合适的模仿模型，第二组通过玩家测试评估模型效果。

Result: 1. 防守行为模仿效果良好，玩家能识别出自己的撤退战术；2. 进攻策略模仿不足；3. Mirror Mode整体玩家满意度高于标准模式；4. 模型需要进一步优化以提高模仿质量和玩家满意度。

Conclusion: Mirror Mode是一种有前景的游戏模式，通过让AI模仿玩家策略来增加挑战性。虽然当前模型在防守行为上表现良好，但需要进一步改进进攻策略的模仿。当玩家面对自己的策略时，能获得更高的游戏满意度。

Abstract: Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode

</details>


### [215] [Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents](https://arxiv.org/abs/2512.11907)
*Daniel Platnick,Marjan Alirezaie,Hossein Rahnama*

Main category: cs.AI

TL;DR: 提出一种结构化个性化方法，将用户知识图谱编译为宏观面，证明常见约束构成层状拟阵，从而将问题转化为拟阵约束下的子模最大化，获得常数因子近似保证。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型个性化需要在任务效用和数据披露之间权衡，现实中的个性化涉及复杂的结构约束（逻辑依赖、类别配额、分层规则），这些约束违反了标准子集选择算法的假设。

Method: 提出形式化建模方法：将用户知识图谱编译为抽象宏观面，证明常见分层和配额约束构成层状拟阵，将结构化个性化问题转化为拟阵约束下的子模最大化问题。

Result: 理论证明常见约束构成层状拟阵，使得贪婪算法能获得常数因子近似保证（连续贪婪算法可达(1-1/e)），为更丰富、更现实的个性化问题提供理论保证。

Conclusion: 通过将结构化个性化建模为拟阵约束下的子模最大化问题，解决了现实个性化中的复杂约束问题，为LLM个性化提供了理论严谨且实用的解决方案。

Abstract: Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.

</details>


### [216] [Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets](https://arxiv.org/abs/2512.11909)
*Hanna Dettki*

Main category: cs.AI

TL;DR: 研究通过因果推理任务比较LLMs与人类智能，使用碰撞图结构评估20+个LLM在11个因果任务上的表现，采用直接概率判断和思维链两种方式，通过贝叶斯网络建模分析对齐性、一致性和推理特征差异。


<details>
  <summary>Details</summary>
Motivation: 因果推理能力被认为是智能的关键特征，但LLMs与人类在因果推理上的异同尚不明确。研究旨在通过相同任务比较LLMs与人类的因果推理能力，探究两者是否对齐、是否一致、以及是否有不同的推理特征。

Method: 使用碰撞图结构(C1→E←C2)设计11个语义有意义的因果任务，评估20+个LLM在两种模式下的表现：直接概率判断（单次响应）和思维链（先思考后回答）。使用带泄漏的噪声OR因果贝叶斯网络建模判断，参数包括共享先验p(C)，通过AIC选择对称（3参数）或非对称（4参数）模型。

Result: 论文未提供具体结果数据，但建立了完整的评估框架：通过贝叶斯网络参数化LLMs的因果判断，使用模型选择方法确定最佳拟合模型，为后续分析LLMs与人类在因果推理任务上的对齐性、一致性和特征差异奠定基础。

Conclusion: 研究建立了一个系统性的框架来比较LLMs与人类的因果推理能力，通过形式化的因果任务和贝叶斯网络建模，为深入理解两者智能差异提供了方法论基础，有助于回答关于对齐性、一致性和推理特征的关键问题。

Abstract: The nature of intelligence in both humans and machines is a longstanding question. While there is no universally accepted definition, the ability to reason causally is often regarded as a pivotal aspect of intelligence (Lake et al., 2017). Evaluating causal reasoning in LLMs and humans on the same tasks provides hence a more comprehensive understanding of their respective strengths and weaknesses. Our study asks: (Q1) Are LLMs aligned with humans given the \emph{same} reasoning tasks? (Q2) Do LLMs and humans reason consistently at the task level? (Q3) Do they have distinct reasoning signatures?
  We answer these by evaluating 20+ LLMs on eleven semantically meaningful causal tasks formalized by a collider graph ($C_1\!\to\!E\!\leftarrow\!C_2$ ) under \emph{Direct} (one-shot number as response = probability judgment of query node being one and \emph{Chain of Thought} (CoT; think first, then provide answer).
  Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters $θ=(b,m_1,m_2,p(C)) \in [0,1]$ include a shared prior $p(C)$;
  we select the winning model via AIC between a 3-parameter symmetric causal strength ($m_1{=}m_2$) and 4-parameter asymmetric ($m_1{\neq}m_2$) variant.

</details>


### [217] [Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis](https://arxiv.org/abs/2512.11912)
*Liu Peng,Yaochu Jin*

Main category: cs.AI

TL;DR: 该研究系统比较了不同概率模型对低质量数据的鲁棒性，发现自回归语言模型（如GPT-2）对数据损坏具有显著抵抗力，而条件扩散模型则表现脆弱，分类器居中。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解不同现代概率模型在面对训练数据质量下降时的表现差异，特别是自回归语言模型、条件扩散模型和分类器在相同数据损坏条件下的鲁棒性对比。

Method: 采用系统性比较研究方法，对多种概率模型进行相同水平的数据损坏实验（如50%标记损坏），通过信息论、PAC学习和梯度动力学等多视角分析框架来解释观察到的差异。

Result: 自回归语言模型表现出惊人鲁棒性（GPT-2测试NLL仅从2.87增加到3.59），条件扩散模型则灾难性退化（图像-标签一致性相对基线下降56.81%），分类器影响适中且随数据集规模减小。

Conclusion: 模型鲁棒性主要由两个关键原则决定：条件信息的丰富程度（约束学习问题）和训练数据的绝对信息量（使正确信息信号主导统计噪声）。

Abstract: A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise.

</details>


### [218] [CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving](https://arxiv.org/abs/2512.11920)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: CXL-SpecKV：基于CXL互连和FPGA加速的分布式KV缓存架构，通过内存解耦和推测执行解决LLM推理中的内存瓶颈，提升吞吐量3.2倍，降低内存成本2.8倍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数据中心部署时面临KV缓存占用大量GPU内存的问题，限制了批处理大小和系统吞吐量，需要解决内存瓶颈。

Method: 提出CXL-SpecKV架构：1) 基于CXL的内存解耦框架，将KV缓存卸载到远程FPGA内存；2) 推测性KV缓存预取机制，预测并预加载未来token的缓存条目；3) FPGA加速的KV缓存压缩/解压缩引擎，减少内存带宽需求4倍。

Result: 在先进LLM模型上评估，相比GPU-only基线，吞吐量提升最高3.2倍，内存成本降低2.8倍，同时保持准确性。

Conclusion: 智能内存解耦结合推测执行能有效解决大规模LLM服务中的内存墙挑战，代码已开源。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.

</details>


### [219] [AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org](https://arxiv.org/abs/2512.11935)
*Jaehyung Lee,Justin Ely,Kent Zhang,Akshaya Ajith,Charles Rhys Campbell,Kamal Choudhary*

Main category: cs.AI

TL;DR: AGAPI是一个开源的代理式AI平台，集成了8+开源LLM和20+材料科学API端点，通过统一的编排框架实现自主多步骤工作流，用于加速材料发现。


<details>
  <summary>Details</summary>
Motivation: 材料研究中AI应用受限，主要原因是计算生态系统碎片化、可重复性挑战以及对商业大语言模型的依赖，需要开放、集成的解决方案。

Method: 采用Agent-Planner-Executor-Summarizer架构，整合开源LLM和材料科学API，构建自主多步骤工作流，涵盖数据检索、性质预测、力场优化、计算分析等。

Result: 展示了端到端工作流（异质结构构建、X射线衍射分析、半导体缺陷工程），评估了30+测试用例，已有1000+活跃用户，预测结果与实验数据对比良好。

Conclusion: AGAPI为可重复、AI加速的材料发现提供了可扩展、透明的基础设施，解决了现有材料研究中的碎片化和可重复性问题。

Abstract: Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.

</details>


### [220] [Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play](https://arxiv.org/abs/2512.11942)
*Vince Trencsenyi*

Main category: cs.AI

TL;DR: 本文提出了一种基于逻辑的声明式领域特定语言，用于编码超博弈结构和解决方案概念，通过答案集编程实现自动化管道，为异构信念推理提供可验证的框架。


<details>
  <summary>Details</summary>
Motivation: 传统博弈论假设往往忽略玩家之间的感知差异、信息不对称和有限理性，导致玩家对游戏形成主观的、可能偏离真实情况且与其他玩家不一致的理解。虽然超博弈理论提供了处理这种不匹配心理模型的数学框架，但在多智能体系统研究中缺乏统一的、形式化的、实用的表示语言以及可扩展的算法来管理复杂的超博弈结构和均衡。

Method: 引入一种声明式的、基于逻辑的领域特定语言来编码超博弈结构和超博弈解决方案概念。利用答案集编程开发自动化管道，用于实例化超博弈结构并运行新颖的超博弈合理化程序——一种寻找能够证明看似不合理结果的信念结构的机制。

Result: 提出的语言为超博弈建立了统一的形化表示，为开发基于信念的异构推理器奠定了基础，提供了具有逻辑保证的可验证上下文。这些贡献共同建立了超博弈理论、多智能体系统和战略AI之间的联系。

Conclusion: 该工作填补了超博弈理论在多智能体系统应用中的关键空白，通过形式化语言和自动化工具使超博弈分析更加实用和可扩展，为处理异构信念的战略推理提供了新的方法论基础。

Abstract: Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.

</details>


### [221] [Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion](https://arxiv.org/abs/2512.11997)
*Anfeng Peng,Ajesh Koyatan Chathoth,Stephen Lee*

Main category: cs.AI

TL;DR: EnrichLog是一个无需训练的日志异常检测框架，通过检索增强生成技术融合语料库特定和样本特定知识，提升检测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统日志分析技术（如基于模板或序列的方法）会丢失重要语义信息或难以处理模糊日志模式，需要更准确、可解释的异常检测方法。

Method: 提出EnrichLog框架：1）无需训练；2）基于条目的异常检测；3）通过检索增强生成融合语料库特定知识和样本特定知识；4）整合历史示例和语料库推理等上下文信息。

Result: 在四个大规模系统日志基准数据集上评估，相比五种基线方法，EnrichLog持续提升异常检测性能，有效处理模糊日志条目，保持高效推理，增强模型置信度和检测准确性。

Conclusion: EnrichLog通过融合语料库特定和样本特定知识，实现了更准确、可解释的异常检测，适合实际部署，解决了传统方法语义信息丢失和模糊模式处理困难的问题。

Abstract: System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.

</details>


### [222] [Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp](https://arxiv.org/abs/2512.12048)
*Muddsair Sharif,Huseyin Seker*

Main category: cs.AI

TL;DR: CAMAC-DRA框架通过上下文感知的多智能体协调优化智能电动汽车充电生态系统，平衡五个利益相关方，在真实数据集上验证了优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决智能电动汽车充电生态系统中动态资源分配的挑战，需要协调多个利益相关方（用户、电网运营商、充电站运营商、车队运营商和环境因素），同时适应实时环境变化。

Method: 提出上下文敏感的多智能体协调框架，结合协调深度Q网络、图神经网络和注意力机制，处理20个上下文特征，使用加权协调机制和共识协议平衡五个利益相关方。

Result: 在441,077个真实充电交易数据集上验证，相比DDPG、A3C、PPO和GNN基线方法，实现92%协调成功率、15%能效提升、10%成本降低、20%电网压力减少和2.3倍收敛速度，同时保持88%训练稳定性和85%样本效率。

Conclusion: CAMAC-DRA框架成功开发了上下文感知的多利益相关方协调系统，平衡竞争目标并适应实时变量，是智能电动汽车充电协调和可持续交通电气化的突破性解决方案。

Abstract: This paper presents a novel context-sensitive multi\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\%), grid operators (20\%), charging station operators (20\%), fleet operators (20%), and environmental factors (15\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\-DRA framework achieves 92\% coordination success rate, 15\% energy efficiency improvement, 10\% cost reduction, 20% grid strain decrease, and \2.3x faster convergence while maintaining 88\% training stability and 85\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\$122,962 and 69\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification.

</details>


### [223] [The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification](https://arxiv.org/abs/2512.12059)
*Luke Bhan,Hanyu Zhang,Andrew Gordon Wilson,Michael W. Mahoney,Chuck Arvin*

Main category: cs.AI

TL;DR: 本文提出Forecast Critic系统，利用大语言模型进行自动预测监控，评估其在时间序列预测质量评估中的能力，包括检测不合理预测、整合非结构化特征等，在合成和真实数据上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模零售业务中，预测系统监控对客户满意度、盈利能力和运营效率至关重要。传统方法可能无法有效处理复杂的时间序列模式和上下文信息，需要更智能的自动化监控方案。

Method: 提出Forecast Critic系统，利用LLMs的广泛世界知识和推理能力进行预测监控。通过三个实验评估LLMs能力：1) 检测明显不合理预测；2) 整合非结构化外部特征；3) 分析模型规模和推理能力的影响。在合成和真实世界数据（包括M5数据集）上进行测试。

Result: LLMs能可靠检测和批评不良预测（时间错位、趋势不一致、峰值错误等），最佳模型F1分数0.88（人类水平0.97）。多模态LLMs能有效整合非结构化上下文信号，在促销历史背景下识别缺失或虚假促销峰值的F1分数0.84。在M5数据集上，不合理预测的sCRPS至少比合理预测高10%。

Conclusion: 即使没有领域特定微调，LLMs也能为自动预测监控和评估提供可行且可扩展的选项，展示了在预测质量评估方面的潜力。

Abstract: Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.

</details>


### [224] [Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations](https://arxiv.org/abs/2512.12088)
*S. R. Eshwar,Aniruddha Mukherjee,Kintan Saha,Krishna Agarwal,Gugan Thoppe,Aditya Gopalan,Gal Dalal*

Main category: cs.AI

TL;DR: RPI（可靠策略迭代）在CartPole和倒立摆任务中表现出比主流深度强化学习方法更稳定、更早达到接近最优性能的特点


<details>
  <summary>Details</summary>
Motivation: 深度强化学习方法存在样本效率低、训练不稳定和超参数敏感等问题，需要更可靠的替代方案

Method: RPI（可靠策略迭代）将策略迭代的单调性价值估计特性扩展到函数逼近设置，并在CartPole和倒立摆任务中评估其鲁棒性

Result: 相对于DQN、Double DQN、DDPG、TD3和PPO，RPI能更早达到接近最优性能并保持稳定，对神经网络和环境参数变化表现出鲁棒性

Conclusion: RPI作为一种更可靠的深度强化学习替代方案具有前景，解决了现有方法在样本效率、训练稳定性和超参数敏感性方面的不足

Abstract: In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.

</details>


### [225] [Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective](https://arxiv.org/abs/2512.12175)
*Haoyang Chen,Richong Zhang,Junfan Chen*

Main category: cs.AI

TL;DR: 论文提出TopK-SD方法，通过结合语义和标签信息合成数据，选择标签一致的示例作为演示，提升大语言模型的上下文学习性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于检索的方法选择语义最相似的示例作为演示，但忽略了标签一致性，这限制了上下文学习的性能。作者从贝叶斯视角和转导标签传播的角度重新思考上下文学习，认为标签一致性对概念引导至关重要。

Method: 提出TopK-SD方法：1）建立标签传播框架，将标签一致性与传播误差边界联系起来；2）设计数据合成方法，同时利用语义和标签信息；3）使用合成数据进行TopK采样，选择标签一致的演示示例。

Result: TopK-SD方法在多个基准测试上优于原始的TopK采样方法，证明了标签一致性的重要性。

Conclusion: 该工作为理解上下文学习的工作机制提供了新视角，强调了标签一致性在演示选择中的关键作用，提出的TopK-SD方法能有效提升大语言模型的上下文学习性能。

Abstract: Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.

</details>


### [226] [Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation](https://arxiv.org/abs/2512.12177)
*Aydin Ayanzadeh,Tim Oates*

Main category: cs.AI

TL;DR: Floorplan2Guide：利用基础模型将平面图转换为可导航知识图谱，为视障人士生成可读导航指令，通过少样本学习提升导航精度


<details>
  <summary>Details</summary>
Motivation: 室内导航对视障人士至关重要，现有基础设施依赖型方案在动态环境中导航能力有限，需要更智能的解决方案

Method: 使用大语言模型从建筑平面图中提取空间信息，转换为可导航知识图谱，采用少样本学习生成人类可读导航指令

Result: Claude 3.7 Sonnet在5-shot提示下达到最高精度（短路线92.31%、中路线76.92%、长路线61.54%），基于图的空间结构比直接视觉推理成功率高出15.4%

Conclusion: 图形表示和上下文学习显著提升导航性能，Floorplan2Guide为视障人士室内导航提供了更精确的解决方案

Abstract: Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.

</details>


### [227] [TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion](https://arxiv.org/abs/2512.12182)
*Xinyu Gao*

Main category: cs.AI

TL;DR: 提出一个结合两阶段注意力三元增强器和U-KAN扩散模型的少样本知识图谱补全框架，在公开数据集上取得SOTA结果


<details>
  <summary>Details</summary>
Motivation: 现实世界知识图谱中关系分布呈现长尾特性，现有基于度量匹配或元学习的方法未能充分利用图邻域信息或忽略对比信号的分布特征

Method: 从生成表示角度重新审视问题，提出集成两阶段注意力三元增强器和基于U-KAN的扩散模型的少样本知识图谱补全框架

Result: 在两个公开数据集上的大量实验表明，该方法取得了新的最先进结果

Conclusion: 提出的生成式表示方法能有效处理知识图谱中的长尾关系分布问题，通过结合注意力机制和扩散模型实现了更好的少样本补全性能

Abstract: Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results.

</details>


### [228] [A Geometric Theory of Cognition](https://arxiv.org/abs/2512.12225)
*Laha Ale*

Main category: cs.AI

TL;DR: 提出统一的几何框架解释认知过程：将认知状态表示为黎曼流形上的点，认知演化遵循认知势能的黎曼梯度流，自然产生双过程效应。


<details>
  <summary>Details</summary>
Motivation: 人类认知包含感知、记忆、直觉判断、审慎推理、行动选择和社会推理等多种能力，但现有理论往往用不同的计算理论分别解释这些能力，缺乏统一框架。

Method: 将认知状态表示为可微分流形上的点，赋予学习到的黎曼度量来编码表征约束、计算成本和认知变量间的结构关系。定义包含预测准确性、结构简洁性、任务效用和规范要求的标量认知势能。认知演化遵循该势能的黎曼梯度流。

Result: 经典双过程效应（快速直觉响应和缓慢审慎推理）自然从度量诱导的各向异性中涌现，产生内在时间尺度分离和几何相变，无需模块化或混合架构。通过模拟经典认知任务展示了这些行为特征。

Conclusion: 建立认知的几何基础，为开发更通用、更类人的人工智能系统提供指导原则。

Abstract: Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems.

</details>


### [229] [A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure](https://arxiv.org/abs/2512.12260)
*Ege Atacan Doğan,Peter F. Patel-Schneider*

Main category: cs.AI

TL;DR: Wikidata采用多层级、多轴分类设计，而非传统本体论的单根层次结构，支持更灵活、可扩展的知识图谱构建


<details>
  <summary>Details</summary>
Motivation: 传统本体论设计强调单一的上层分类结构（如持续体vs发生体、抽象vs具体、类型vs实例），而Wikidata作为协作知识图谱需要更灵活的分类方式以适应不断演化的知识表示

Method: 分析Wikidata的多层级、多轴分类结构，研究其在共享根类"实体"下同时容纳多个分类轴的设计特点，探讨这种架构对知识表示的影响

Result: Wikidata的多轴分类设计支持更灵活、可扩展的本体构建，特别适合协作和演化的知识图谱，能够同时容纳多个分类视角而不强制单一分类层次

Conclusion: Wikidata的多层级、多轴分类架构为大规模协作知识图谱提供了更灵活、可扩展的本体设计范式，突破了传统本体论的单根层次结构限制

Abstract: Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs.

</details>


### [230] [Quantum-Aware Generative AI for Materials Discovery: A Framework for Robust Exploration Beyond DFT Biases](https://arxiv.org/abs/2512.12288)
*Mahule Roy,Guillaume Lambard*

Main category: cs.AI

TL;DR: 提出量子感知生成AI框架，通过多保真度学习和主动验证解决传统DFT模型在强关联系统中的系统偏差问题，显著提升高发散区域材料发现的成功率。


<details>
  <summary>Details</summary>
Motivation: 传统材料发现生成模型主要基于DFT近似交换关联泛函训练和验证，这导致模型继承了DFT在强关联系统中的系统失败，产生探索偏差，无法发现DFT预测定性错误的材料。

Method: 采用量子感知生成AI框架，结合多保真度学习和主动验证。使用基于量子力学描述符的扩散生成器，以及基于等变神经网络势的验证器，训练于多理论层次数据集（PBE、SCAN、HSE06、CCSD(T)）。实施主动学习循环量化并针对低保真度和高保真度预测之间的差异。

Result: 在多个挑战性材料类别上相比最先进生成模型（CDVAE、GNoME、DiffCSP）取得显著实际增益：在高发散区域（如关联氧化物）成功识别潜在稳定候选物的能力提高3-5倍，同时保持计算可行性。

Conclusion: 该工作提供了一个严谨、透明的框架，将计算材料发现的有效搜索空间扩展到单一保真度模型的限制之外，解决了DFT系统偏差的根本瓶颈。

Abstract: Conventional generative models for materials discovery are predominantly trained and validated using data from Density Functional Theory (DFT) with approximate exchange-correlation functionals. This creates a fundamental bottleneck: these models inherit DFT's systematic failures for strongly correlated systems, leading to exploration biases and an inability to discover materials where DFT predictions are qualitatively incorrect. We introduce a quantum-aware generative AI framework that systematically addresses this limitation through tight integration of multi-fidelity learning and active validation. Our approach employs a diffusion-based generator conditioned on quantum-mechanical descriptors and a validator using an equivariant neural network potential trained on a hierarchical dataset spanning multiple levels of theory (PBE, SCAN, HSE06, CCSD(T)). Crucially, we implement a robust active learning loop that quantifies and targets the divergence between low- and high-fidelity predictions. We conduct comprehensive ablation studies to deconstruct the contribution of each component, perform detailed failure mode analysis, and benchmark our framework against state-of-the-art generative models (CDVAE, GNoME, DiffCSP) across several challenging material classes. Our results demonstrate significant practical gains: a 3-5x improvement in successfully identifying potentially stable candidates in high-divergence regions (e.g., correlated oxides) compared to DFT-only baselines, while maintaining computational feasibility. This work provides a rigorous, transparent framework for extending the effective search space of computational materials discovery beyond the limitations of single-fidelity models.

</details>


### [231] [Entropy Collapse: A Universal Failure Mode of Intelligent Systems](https://arxiv.org/abs/2512.12381)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

TL;DR: 论文提出"熵崩溃"作为智能系统的普遍失效模式：当反馈放大超过有限的新颖性再生时，系统会从高熵自适应状态急剧转变为低熵崩溃状态，导致适应性维度收缩而非完全停滞。


<details>
  <summary>Details</summary>
Motivation: 从人工智能到经济制度和生物进化，智能系统在提升过程中常常出现矛盾性退化：系统变得僵化、失去适应性、意外失效。需要理解这种普遍存在的崩溃现象背后的统一机制。

Method: 在最小化领域无关假设下，形式化熵崩溃为向稳定低熵流形的收敛过程。通过分析建立临界阈值、动态不可逆性和吸引子结构，并通过最小模拟展示不同更新机制下的普适性。

Result: 建立了熵崩溃的统一理论框架，将AI中的模型崩溃、经济学中的制度僵化和进化中的遗传瓶颈等现象解释为同一底层过程的表现。揭示了晚期干预系统性失败的原因。

Conclusion: 将崩溃重新定义为智能的结构性成本，为维持智能系统长期适应性提供了熵感知的设计原则，强调需要平衡反馈放大与新颖性再生。

Abstract: Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly.
  We identify \emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale.
  We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process.
  By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems.
  \noindent\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis

</details>


### [232] [Feeling the Strength but Not the Source: Partial Introspection in LLMs](https://arxiv.org/abs/2512.12411)
*Ely Hahami,Lavik Jain,Ishaan Sinha*

Main category: cs.AI

TL;DR: 该研究测试了语言模型对注入概念的检测能力，发现模型能部分识别注入概念但表现脆弱，对概念强度分类效果较好，支持了模型能基于内部表示进行自省但自省报告受提示词影响的结论。


<details>
  <summary>Details</summary>
Motivation: Anthropic的研究声称前沿模型能检测和命名注入的"概念"方向，本研究旨在验证这一声称的鲁棒性，探究模型自省能力的实际表现和局限性。

Method: 1) 在Meta-Llama-3.1-8B-Instruct上复现Anthropic的多轮"涌现自省"实验；2) 系统性地改变推理提示词，测试自省能力的脆弱性；3) 测试模型对归一化注入概念向量系数的强度分类能力。

Result: 1) 成功复现Anthropic结果，模型识别注入概念准确率20%；2) 自省能力脆弱，在多项选择识别和不同二元判别提示下性能崩溃；3) 模型能可靠分类概念向量系数强度，准确率达70%（远高于25%的随机基线）。

Conclusion: 语言模型确实能基于内部表示进行自省计算，但这些自省报告具有狭窄性和提示敏感性，模型对概念强度的分类能力比对概念本身的识别更稳定。

Abstract: Recent work from Anthropic claims that frontier models can sometimes detect and name injected "concepts" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn "emergent introspection" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.

</details>


### [233] [Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale](https://arxiv.org/abs/2512.12413)
*Gabriel R. Lau,Wei Yan Low,Louis Tay,Ysabel Guevarra,Dragan Gašević,Andree Hartanto*

Main category: cs.AI

TL;DR: 开发并验证了一个13项量表，用于测量AI使用中的批判性思维，包含验证、动机和反思三个维度，该量表能预测更准确的AI输出判断和更深入的反思。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在日常工作和学习中的普及，用户需要批判性地评估AI输出而非盲目接受。现有研究缺乏系统测量AI使用中批判性思维的工具，需要开发一个可靠有效的量表来支持相关研究。

Method: 通过六个研究（N=1365）开发和验证量表：研究1生成和内容验证项目；研究2支持三因素结构（验证、动机、反思）；研究3-5确认高阶模型，检验信效度；研究6验证量表预测效度，包括事实核查任务表现。

Result: 成功开发了13项AI使用批判性思维量表，具有良好的心理测量特性。量表得分与开放性、外向性、积极特质情感和AI使用频率正相关，能预测更频繁多样的验证策略、更高的事实判断准确性以及更深入的AI责任反思。

Conclusion: 该研究澄清了人们如何监督生成式AI输出，提供了一个经过验证的量表和生态效度高的任务范式，支持理论检验、跨群体和纵向研究，促进对生成式AI输出的批判性参与研究。

Abstract: Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs.

</details>


### [234] [AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline](https://arxiv.org/abs/2512.12443)
*Akhmadillo Mamirov,Faiaz Azmain,Hanyu Wang*

Main category: cs.AI

TL;DR: 论文分析了AI模型文档的碎片化和不一致问题，开发了加权透明度框架和自动化评估管道，发现前沿实验室合规率约80%，但安全关键类别存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: AI模型文档分散在不同平台且结构不一致，导致政策制定者、审计者和用户难以可靠评估安全声明、数据来源和版本变更。需要系统化评估和标准化文档框架。

Method: 1) 分析5个前沿模型和100个Hugging Face模型卡，识别947个独特章节名称；2) 基于欧盟AI法案附件IV和斯坦福透明度指数，开发包含8个章节23个子章节的加权透明度框架；3) 实现自动化多智能体管道，从公共来源提取文档并通过LLM共识评分完整性。

Result: 评估50个模型（视觉、多模态、开源和闭源系统）总成本低于3美元。前沿实验室（xAI、微软、Anthropic）合规率约80%，大多数提供商低于60%。安全关键类别缺陷最大：欺骗行为、幻觉和儿童安全评估分别损失148、124和116个总积分。

Conclusion: AI模型文档存在系统性差距，需要标准化透明度框架。自动化评估工具能高效识别安全关键缺陷，为监管合规和行业标准提供实用解决方案。

Abstract: AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.

</details>


### [235] [MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs](https://arxiv.org/abs/2512.12477)
*Jiawen Chen,Yanyan He,Qi Shao,Mengli Wei,Duxin Chen,Wenwu Yu,Yanlong Zhao*

Main category: cs.AI

TL;DR: MetaHGNIE：基于元路径诱导的超图对比学习框架，用于解耦和对齐异构知识图谱中的结构和语义信息，通过建模高阶交互提升节点重要性估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有异构知识图谱节点重要性估计方法存在两个主要问题：1）依赖成对连接，忽略多实体和关系间的高阶依赖；2）将结构和语义信号独立处理，缺乏有效的跨模态整合。

Method: 提出元路径诱导的超图对比学习框架：1）通过元路径序列构建高阶知识图谱，类型化超边捕获多实体关系上下文；2）结构依赖通过局部注意力聚合，语义表示通过配备稀疏分块的超图变换器编码；3）多模态融合模块在对比学习和辅助监督下整合结构和语义嵌入。

Result: 在基准NIE数据集上的大量实验表明，MetaHGNIE始终优于最先进的基线方法，验证了显式建模高阶交互和跨模态对齐的有效性。

Conclusion: MetaHGNIE通过元路径诱导的超图表示和对比学习，成功解决了异构知识图谱中节点重要性估计的高阶依赖和跨模态整合问题，为相关应用提供了有效解决方案。

Abstract: Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE

</details>


### [236] [SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation](https://arxiv.org/abs/2512.12501)
*Dang Phuong Nam,Nguyen Kieu,Pham Thanh Hieu*

Main category: cs.AI

TL;DR: SafeGen是一个将伦理保障嵌入文本到图像生成流程的框架，通过BGE-M3文本分类器过滤有害提示和Hyper-SD扩散模型生成高质量图像，在创意自由与伦理责任之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在创造、教育和研究中带来机遇，但文本到图像系统存在双重用途困境：放大社会偏见、产生高保真虚假信息、侵犯知识产权，需要解决这些伦理问题。

Method: SafeGen框架整合两个组件：1) BGE-M3 - 微调文本分类器过滤有害或误导性提示；2) Hyper-SD - 优化扩散模型生成高保真、语义对齐的图像。基于多语言数据集和公平感知训练流程构建。

Result: Hyper-SD达到IS=3.52、FID=22.08、SSIM=0.79；BGE-M3达到F1分数0.81。消融研究验证了领域特定微调的重要性。案例研究显示SafeGen能有效阻止不安全提示、生成包容性教学材料、加强学术诚信。

Conclusion: SafeGen证明创意自由与伦理责任可以在单一工作流程中调和，为可信AI原则在生成式AI中的实际应用提供了框架。

Abstract: Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity.

</details>


### [237] [KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs](https://arxiv.org/abs/2512.12503)
*Mingrui Ye,Chanjin Zheng,Zengyi Yu,Chenyu Xiang,Zhixue Zhao,Zheng Yuan,Helen Yannakoudakis*

Main category: cs.AI

TL;DR: KidsArtBench：首个针对儿童艺术作品的评估基准，包含1k+作品的多维度专家标注，提出基于多LoRA的属性特定方法，显著提升MLLM在艺术教育评估上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在艺术表达评估方面能力有限，主要因为美学概念抽象且开放，而多模态艺术作品标注稀缺。现有美学数据集多为成人图像提供单一评分，缺乏针对儿童艺术作品的评估基准。

Method: 1. 构建KidsArtBench基准：包含1000+儿童艺术作品（5-15岁），由12位专家教育者按照9个维度进行标注，并提供专家评论作为反馈监督。2. 提出属性特定的多LoRA方法：每个评估维度对应一个独立的LoRA适配器，结合回归感知微调（RAFT）使预测与有序评分对齐。

Result: 在Qwen2.5-VL-7B模型上，该方法将相关性从0.468提升至0.653，在感知维度上提升最大，在高阶属性上的差距也显著缩小。证明了教育者对齐的监督和属性感知训练能够产生有教学意义的评估。

Conclusion: KidsArtBench为教育AI的持续进步建立了严格的测试平台，展示了专家监督和属性特定训练在提升MLLM艺术评估能力方面的有效性。研究团队已发布数据、代码和伦理文档。

Abstract: Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.

</details>


### [238] [World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents](https://arxiv.org/abs/2512.12548)
*Yesid Fonseca,Manuel S. Ríos,Nicanor Quijano,Luis F. Giraldo*

Main category: cs.AI

TL;DR: 人工觅食者通过基于模型的强化学习获得世界模型后，自然收敛到与边际价值定理一致的策略，表现出类似生物觅食者的决策模式。


<details>
  <summary>Details</summary>
Motivation: 虽然边际价值定理被广泛用于预测行为生态学中的觅食行为，但生物觅食者如何实现最优斑块觅食决策的计算机制仍不清楚。研究旨在探索人工智能系统如何通过计算机制实现类似生物的最优觅食决策。

Method: 使用基于模型的强化学习代理，学习环境的简约预测表示（世界模型），与标准的无模型强化学习代理进行对比。

Result: 基于模型的代理表现出与边际价值定理一致的斑块离开策略，决策模式与许多生物对应物相似。预测能力（而非单纯奖励最大化）驱动了高效的斑块离开行为。

Conclusion: 预测世界模型可以作为AI系统更可解释和生物学基础决策的基础，生态最优性原理对推进可解释和自适应AI具有重要价值。

Abstract: Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.

</details>


### [239] [Large Language Newsvendor: Decision Biases and Cognitive Mechanisms](https://arxiv.org/abs/2512.12552)
*Jifei Liu,Zhi Chen,Yuanguang Zhong*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在供应链决策中会复制并放大人类认知偏见，GPT-4因过度思考表现出最大非理性，而效率优化的GPT-4o表现接近最优


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs越来越多地融入商业决策，但其复制和放大人类认知偏见的潜在风险尚未被充分理解，这在供应链管理等高风险运营环境中尤为关键

Method: 使用经典报童问题在动态设置中测试GPT-4、GPT-4o和LLaMA-8B，通过多轮实验检测五种已知决策偏见

Result: LLMs一致复制了经典的"过低/过高"订购偏见，并显著放大了需求追逐等行为；GPT-4表现出最大的非理性（"智能悖论"），而GPT-4o表现接近最优；这些偏见即使在提供最优公式时仍然存在

Conclusion: 偏见源于架构约束而非知识缺口；管理者应根据任务选择模型，需要人机协同监督，结构化提示是约束启发式倾向的有效策略

Abstract: Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.

</details>


### [240] [AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation](https://arxiv.org/abs/2512.12597)
*Miriam Horovicz*

Main category: cs.AI

TL;DR: AgentSHAP是首个用于解释LLM智能体中工具重要性的框架，基于博弈论中的Shapley值，无需访问模型内部权重，通过蒙特卡洛采样降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体虽然能使用外部工具解决复杂任务，但缺乏解释哪些工具对响应真正有贡献的方法。现有的XAI方法都无法解决工具层面的解释问题。

Method: 采用模型无关的黑盒方法，基于博弈论的Shapley值，通过蒙特卡洛采样测试智能体在不同工具子集下的响应，计算公平的重要性分数，将复杂度从O(2^n)降低到实用水平。

Result: 在API-Bank上的实验表明，AgentSHAP能产生跨运行一致的重要性分数，正确识别重要工具，并能区分相关与无关工具。

Conclusion: AgentSHAP填补了LLM智能体工具层面解释的空白，与TokenSHAP和PixelSHAP一起构成了基于Shapley值的现代生成式AI可解释性工具家族。

Abstract: LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.

</details>


### [241] [Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents](https://arxiv.org/abs/2512.12634)
*Youngmin Im,Byeongung Jo,Jaeyoung Wi,Seungwoo Baek,Tae Hoon Min,Joo Hyung Lee,Sangeun Oh,Insik Shin,Sunjae Lee*

Main category: cs.AI

TL;DR: MobiBench是一个用于移动GUI代理的模块化、多路径感知离线基准测试框架，解决了现有评估方法的局限性，实现了高保真、可扩展和可重复的评估。


<details>
  <summary>Details</summary>
Motivation: 当前移动GUI代理的评估方法存在两个根本限制：1) 离线基准测试使用静态单路径数据集，不公平地惩罚有效替代动作；在线基准测试由于动态不可预测性导致可扩展性和可重复性差。2) 现有基准测试将代理视为整体黑箱，忽视了个别组件的贡献，导致不公平比较或掩盖性能瓶颈。

Method: 提出了MobiBench框架，这是第一个用于移动GUI代理的模块化、多路径感知离线基准测试框架。它支持在完全离线设置中进行高保真、可扩展和可重复的评估。

Result: 实验表明MobiBench与人类评估者的一致性达到94.72%，与精心设计的在线基准测试相当，同时保持了静态离线基准测试的可扩展性和可重复性。模块级分析揭示了多个关键见解，包括对移动GUI代理中各种技术的系统评估、跨模型规模的最优模块配置、当前LFMs的固有局限性，以及设计更强大、成本效益更高的移动代理的可操作指南。

Conclusion: MobiBench通过提供模块化、多路径感知的离线基准测试框架，解决了移动GUI代理评估中的关键问题，实现了高保真、可扩展和可重复的评估，为设计和优化移动代理提供了有价值的见解和指导。

Abstract: Mobile GUI Agents, AI agents capable of interacting with mobile applications on behalf of users, have the potential to transform human computer interaction. However, current evaluation practices for GUI agents face two fundamental limitations. First, they either rely on single path offline benchmarks or online live benchmarks. Offline benchmarks using static, single path annotated datasets unfairly penalize valid alternative actions, while online benchmarks suffer from poor scalability and reproducibility due to the dynamic and unpredictable nature of live evaluation. Second, existing benchmarks treat agents as monolithic black boxes, overlooking the contributions of individual components, which often leads to unfair comparisons or obscures key performance bottlenecks. To address these limitations, we present MobiBench, the first modular and multi path aware offline benchmarking framework for mobile GUI agents that enables high fidelity, scalable, and reproducible evaluation entirely in offline settings. Our experiments demonstrate that MobiBench achieves 94.72 percent agreement with human evaluators, on par with carefully engineered online benchmarks, while preserving the scalability and reproducibility of static offline benchmarks. Furthermore, our comprehensive module level analysis uncovers several key insights, including a systematic evaluation of diverse techniques used in mobile GUI agents, optimal module configurations across model scales, the inherent limitations of current LFMs, and actionable guidelines for designing more capable and cost efficient mobile agents.

</details>


### [242] [Value-Aware Multiagent Systems](https://arxiv.org/abs/2512.12652)
*Nardine Osman*

Main category: cs.AI

TL;DR: 论文提出"价值感知AI"概念，超越传统价值对齐问题，提供简明的工程路线图，包含三个核心支柱：学习表示人类价值、确保个体与多智能体系统价值对齐、提供基于价值的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统AI价值对齐问题存在局限性，需要更全面的价值感知框架。论文旨在超越简单的价值对齐，建立系统化的价值感知AI工程方法，使AI不仅能对齐价值，还能理解、表示和解释价值。

Method: 提出三支柱路线图：1) 使用形式语义学学习和表示人类价值；2) 确保个体智能体和多智能体系统的价值对齐；3) 提供基于价值的行为可解释性。论文展示了在这些主题上的持续研究工作，并应用于现实领域。

Result: 提出了价值感知AI的完整概念框架和工程路线图，为AI系统价值理解、对齐和解释提供了系统化方法。通过实际应用验证了该框架的有效性。

Conclusion: 价值感知AI超越了传统价值对齐，为构建真正理解人类价值的AI系统提供了可行的工程路线图。该框架通过形式化表示、系统对齐和可解释性三个支柱，为实现负责任AI发展奠定了基础。

Abstract: This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.

</details>


### [243] [Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI](https://arxiv.org/abs/2512.12686)
*Samarth Sarin,Lovepreet Singh,Bhaskarjit Sarmah,Dhagash Mehta*

Main category: cs.AI

TL;DR: Memoria是一个模块化记忆框架，通过动态会话级摘要和加权知识图谱用户建模，为LLM提供持久、可解释、上下文丰富的记忆能力，实现短期对话连贯性和长期个性化。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在扩展用户交互中缺乏连续性、个性化和长期上下文保持能力，这限制了它们作为真正交互式和自适应代理的部署。需要一种记忆系统来提供类似人类的跨对话信息保留和行动能力。

Method: 提出Memoria框架，包含两个互补组件：1) 动态会话级摘要，用于短期对话连贯性；2) 加权知识图谱用户建模引擎，以结构化实体和关系增量捕获用户特征、偏好和行为模式。这种混合架构在现代LLM的token限制内运作。

Result: Memoria能够实现可扩展的个性化对话AI，弥合无状态LLM接口与代理记忆系统之间的差距，为需要自适应和演进用户体验的工业应用提供实用解决方案。

Conclusion: Memoria是一个有效的模块化记忆框架，通过结合短期摘要和长期知识图谱建模，为LLM提供代理式记忆能力，支持连续、个性化和上下文丰富的交互，具有实际工业应用价值。

Abstract: Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.

</details>


### [244] [WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment](https://arxiv.org/abs/2512.12692)
*Mahir Labib Dihan,Tanzima Hashem,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: WebOperator：一个结合最佳优先搜索和安全回溯的树搜索框架，用于提升LLM智能体在网页环境中的探索能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在网页环境中采用贪婪的逐步行动方式，缺乏长期规划和错误纠正能力。网页环境部分可观察且包含不可逆操作，现有树搜索方法缺乏安全回溯机制，假设所有操作可逆，导致实际效果受限。

Method: 提出WebOperator框架：1）采用最佳优先搜索策略，结合奖励估计和安全考虑对行动排序；2）引入鲁棒的回溯机制，在重放路径前验证可行性；3）从多个推理上下文生成多样化行动候选，通过预执行过滤无效行动并合并语义等价操作。

Result: 在WebArena和WebVoyager基准测试中表现优异。在WebArena上，使用gpt-4o实现了54.6%的最先进成功率，证明了战略远见与安全执行结合的关键优势。

Conclusion: WebOperator通过整合战略远见、安全回溯和多样化探索，显著提升了LLM智能体在网页环境中的性能，解决了现有方法在部分可观察环境和不可逆操作中的局限性。

Abstract: LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.

</details>


### [245] [Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning](https://arxiv.org/abs/2512.12706)
*Enhong Mu,Minami Yoda,Yan Zhang,Mingyue Zhang,Yutaka Matsuno,Jialong Li*

Main category: cs.AI

TL;DR: SMART框架结合结构验证与功能验证，利用LLM解析AST差异提取功能意图，通过混合奖励机制指导RL智能体在游戏更新测试中同时实现高代码覆盖率和任务完成率。


<details>
  <summary>Details</summary>
Motivation: 游戏即服务(GaaS)模式需要频繁内容更新，给质量保证带来巨大压力。现有自动化测试方法存在局限性：代码中心方法只关注结构覆盖而不理解游戏玩法上下文，玩家中心智能体能验证高层意图但难以覆盖具体代码变更。需要弥合这一鸿沟。

Method: 提出SMART框架，利用大型语言模型(LLM)解释抽象语法树(AST)差异并提取功能意图，构建上下文感知的混合奖励机制。该机制指导强化学习智能体顺序完成游戏目标，同时自适应探索修改的代码分支。

Result: 在Overcooked和Minecraft环境中评估，SMART显著优于现有基线方法：实现了超过94%的修改代码分支覆盖率（几乎是传统强化学习方法的两倍），同时保持98%的任务完成率，有效平衡了结构全面性和功能正确性。

Conclusion: SMART成功弥合了结构验证与功能验证之间的鸿沟，为游戏更新测试提供了有效的自动化解决方案，能够应对GaaS模式下的频繁发布节奏需求。

Abstract: The widespread adoption of the "Games as a Service" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.

</details>


### [246] [Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks](https://arxiv.org/abs/2512.12736)
*Syeda Zunaira Ahmed,Hejab Tahira Beg,Maryam Khalid*

Main category: cs.AI

TL;DR: 提出基于人口统计信息的机器学习框架，通过数据增强提升5G视频流中个性化QoE预测准确性


<details>
  <summary>Details</summary>
Motivation: 现有QoE预测方法依赖有限数据集且假设用户感知均匀，难以适应异构现实环境，需要更个性化的预测方法

Method: 提出人口统计感知的机器学习框架，采用基于行为的人口统计数据增强策略，将小数据集扩展6倍，评估多种经典ML模型和深度学习架构（包括注意力MLP和TabNet）

Result: 实验结果显示在RMSE、MAE和R指标上相比基线模型有显著提升，TabNet表现最佳，受益于其固有的特征选择和注意力机制

Conclusion: 人口统计感知的数据增强显著提升QoE预测鲁棒性，为5G视频流网络中的个性化QoE感知智能提供了可扩展方向

Abstract: Quality of Experience (QoE) prediction is a critical component of modern multimedia systems, particularly for adaptive video streaming in 5G networks. Accurate QoE estimation enables intelligent resource management and supports user centric service delivery. Existing QoE prediction approaches primarily rely on limited datasets and assume uniform user perception, which restricts their applicability in heterogeneous real world environments.
  This paper proposes a demographic aware machine learning framework for personalized QoE prediction. We introduce a behaviorally realistic demographic based data augmentation strategy that expands a small QoE dataset six fold by modeling varying user sensitivities to streaming impairments such as rebuffering, bitrate variation, and quality degradation. Using the augmented dataset, we evaluate a comprehensive set of classical machine learning models alongside advanced deep learning architectures, including an attention-based MLP and TabNet.
  Experimental results demonstrate significant improvements in prediction accuracy across RMSE, MAE, and R metrics compared to baseline models. Among all evaluated approaches, TabNet achieves the strongest performance, benefiting from its inherent feature selection and attention mechanisms. The results confirm that demographic-aware augmentation substantially enhances QoE prediction robustness and provides a scalable direction for personalized QoE-aware intelligence in 5G video streaming networks.

</details>


### [247] [Causal Counterfactuals Reconsidered](https://arxiv.org/abs/2512.12804)
*Sander Beckers*

Main category: cs.AI

TL;DR: 提出了一种新的反事实概率语义学，它推广了标准的Pearl语义学，适用于无法扩展为现实结构因果模型的概率因果模型，解决了Pearl和Dawid关于反事实的长期争论。


<details>
  <summary>Details</summary>
Motivation: 标准Pearl语义学仅适用于可扩展为结构因果模型的概率因果模型，但作者发现即使在简单设置中也会出现无法扩展的模型。需要一种更通用的语义学来调和Pearl和Dawid关于反事实的争论。

Method: 提出新的反事实概率语义学，限制在满足马尔可夫条件、仅包含现实变量且因果完备的因果模型中。虽然使用结构因果模型框架，但避免使用响应变量。证明该语义学与另外两种不涉及结构因果模型的提案等价。

Result: 新语义学成功推广了Pearl语义学，适用于更广泛的概率因果模型。证明了与两种其他提案的等价性，并与文献中关于随机反事实的各种评论一致。

Conclusion: 新语义学在Pearl和Dawid的争论中提供了折中方案：接受Dawid对普遍因果决定论和不现实变量的批评，但支持Pearl关于一般反事实语义学可能性的观点。同时探讨了马尔可夫条件的普遍性和因果抽象的新推广。

Abstract: I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions

</details>


### [248] [Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution](https://arxiv.org/abs/2512.12806)
*Boyang Yan*

Main category: cs.AI

TL;DR: 提出一个容错沙箱框架，通过策略拦截层和事务性文件系统快照机制，为自主LLM代理提供安全执行环境，相比现有方案更适合无头自动化工作流。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型从被动代码生成器转变为自主代理时，面临破坏性命令和不一致系统状态的安全风险。现有商业解决方案通常优先考虑交互式用户安全，实施的身份验证屏障会破坏真正自主性所需的无头循环。

Method: 设计容错沙箱框架，包含基于策略的拦截层和事务性文件系统快照机制。将代理操作包装在原子事务中，确保安全性。在基于Proxmox的自定义测试平台上部署Minimind-MoE LLM，使用nano-vllm服务，并通过EVPN/VXLAN实现隔离。

Result: 实验结果显示：高风险命令拦截率达到100%，失败状态回滚成功率达到100%。原型系统每个事务仅产生14.5%的性能开销（约1.8秒）。相比之下，Gemini CLI沙箱需要交互式身份验证，不适用于无头自主代理工作流。

Conclusion: 容错沙箱框架能够有效解决自主LLM代理的安全风险，通过原子事务保证安全性，同时保持可接受的延迟，优于容器初始化开销或商业CLI的交互摩擦。

Abstract: The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\% interception rate for high-risk commands and a 100\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication ("Sign in"), rendering it unusable for headless, autonomous agent workflows.

</details>


### [249] [Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents](https://arxiv.org/abs/2512.12856)
*Saad Alqithami*

Main category: cs.AI

TL;DR: 提出Memory-Aware Retention Schema (MaRS)框架和六种遗忘策略，用于生成式智能体的记忆管理，平衡性能、隐私和计算效率，并通过FiFA基准测试验证效果。


<details>
  <summary>Details</summary>
Motivation: 随着生成式智能体在长期交互场景中部署，其记忆管理成为性能和隐私的关键瓶颈。现有方法要么维持无限记忆存储（导致计算不可行和隐私问题），要么使用简单遗忘机制（损害智能体连贯性和功能）。

Method: 提出Memory-Aware Retention Schema (MaRS)框架，包含六种基于理论的遗忘策略，并开发Forgetful but Faithful Agent (FiFA)基准测试框架，评估叙事连贯性、目标完成、社交回忆准确性、隐私保护和成本效率。

Result: 通过300次评估实验，混合遗忘策略在综合得分上达到0.911，同时保持计算可行性和隐私保证，为记忆预算智能体评估建立了新基准。

Conclusion: 该工作为资源受限、隐私敏感环境中部署生成式智能体提供了实用指南，通过解决智能体记忆管理的基本挑战，有助于建立以人为中心的人工智能，直接影响用户信任、系统可扩展性和法规合规性。

Abstract: As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.

</details>


### [250] [Satisfiability Modulo Theory Meets Inductive Logic Programming](https://arxiv.org/abs/2512.12918)
*Nijesh Upreti,Vaishak Belle*

Main category: cs.AI

TL;DR: 本文提出了一种模块化的SMT-ILP架构，将归纳逻辑编程系统PyGol与SMT求解器Z3结合，以学习包含数值约束的混合规则。


<details>
  <summary>Details</summary>
Motivation: 传统归纳逻辑编程在关系领域提供可解释的规则学习，但在处理数值约束方面存在局限。传统ILP系统依赖离散化或手工制作的数值谓词，难以推断跨示例联合成立的阈值或算术关系。需要扩展ILP以支持数值推理能力。

Method: 采用模块化方法，将ILP系统PyGol与SMT求解器Z3耦合。PyGol生成的候选子句被解释为背景理论（如线性/非线性实数算术）上的无量词公式，SMT求解器负责实例化和验证数值参数，同时保持ILP的声明性关系偏置。

Result: 在专门设计的合成数据集上评估，涵盖线性、关系、非线性和多跳推理任务。结果表明该架构能够扩展符号规则学习的表达能力，支持包含阈值、区间和多文字算术关系的混合规则学习。

Conclusion: 模块化SMT-ILP架构能够有效扩展符号规则学习的表达能力，补充了现有的数值ILP方法，并为未来向更丰富的理论感知归纳扩展提供了灵活基础。

Abstract: Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction.

</details>


### [251] [Towards Open Standards for Systemic Complexity in Digital Forensics](https://arxiv.org/abs/2512.12970)
*Paola Di Maio*

Main category: cs.AI

TL;DR: 提出基于可读性人工制品和开放标准的数字取证AI模型架构，以解决AI与数字取证交叉领域的复杂性及错误问题


<details>
  <summary>Details</summary>
Motivation: AI与数字取证技术交叉日益复杂普遍，但取证科学仍存在错误和脆弱性，需要系统性解决方案来减少错误

Method: 采用人类可读的人工制品和开放标准，基于现有技术提出数字取证AI模型架构

Result: 提出了一个数字取证AI模型架构方案，旨在通过标准化和可解释性降低系统复杂性

Conclusion: 通过人类可读的人工制品和开放标准可以缓解数字取证中的错误限制，提高系统的可靠性和可解释性

Abstract: The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.

</details>


### [252] [M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization](https://arxiv.org/abs/2512.13070)
*Bizhe Bai,Hongming Wu,Peng Ye,Tao Chen*

Main category: cs.AI

TL;DR: 论文提出M-GRPO框架和IQR自适应过滤方法，解决自监督强化学习中长期训练时的策略崩溃问题，实现更稳定的训练和更好的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督强化学习方法在长期训练中存在"策略崩溃"问题，性能会急剧下降。即使增加rollout数量也只能延迟而非防止崩溃，需要更稳定的训练方法。

Method: 1. M-GRPO：利用缓慢演变的动量模型提供稳定训练目标；2. IQR自适应过滤：基于四分位距动态修剪低熵轨迹，保持策略多样性。

Result: 在多个推理基准测试中，M-GRPO稳定了训练过程，IQR过滤器防止了过早收敛，两者结合实现了最先进的性能。

Conclusion: M-GRPO和IQR过滤器的组合有效解决了自监督强化学习中的策略崩溃问题，实现了更稳定的训练和更好的推理能力。

Abstract: Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a "policy collapse" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.

</details>


### [253] [Socratic Students: Teaching Language Models to Learn by Asking Questions](https://arxiv.org/abs/2512.13102)
*Rajeev Bhatt Ambati,Tianyi Niu,Aashu Singh,Shlok Mishra,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.AI

TL;DR: 该论文研究如何让大型语言模型作为学生主动向教师提问以获取知识，通过学生主导的交互策略在数学和编程任务上显著提升性能，并使用DPO训练改进提问质量。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs擅长静态交互，但在教育辅导、医疗协助等实际场景中，相关信息需要动态交互主动获取。现有研究主要关注教师如何指导学生，而本文转向学生视角，研究学生如何主动向教师提问以有效获取有用信息。

Method: 提出学生主导的交互策略，让学生模型识别自身不确定性并主动提问。使用Direct Preference Optimization（DPO）训练学生模型，通过自我指导或更强学生模型的指导来改进提问质量。

Result: 在数学和编程基准测试中，基线学生模型从接近零性能开始，学生主导方法相比静态基线至少获得0.5的绝对Pass@k提升。通过DPO指导训练，较小模型学会提出更好的问题，进一步提高学习效率。

Conclusion: 学生主动提问的策略能显著提升LLMs在动态交互任务中的表现，通过DPO指导训练可以改进提问质量，使较小模型也能有效学习如何获取有用信息。

Abstract: Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.

</details>


### [254] [Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning](https://arxiv.org/abs/2512.13131)
*Xin Guo,Yifan Zhao,Jia Li*

Main category: cs.AI

TL;DR: 提出分层隐式周期性学习框架，通过显式建模不同运动单元间的内在相关性，提升语音驱动3D手势生成的协调性和自然度


<details>
  <summary>Details</summary>
Motivation: 现有语音驱动3D手势生成方法多为端到端方案，未能充分建模头、身体、手等不同运动单元之间的内在关联，导致动作不自然且协调性差

Method: 提出分层隐式周期性学习框架：1) 使用周期性自编码器探索手势运动相位流形，结合非周期性特征实现实例级多样性；2) 通过级联引导机制建模面部、身体和手部动作的层次关系

Result: 在3D虚拟人上的实验表明，该方法在定量和定性评估上均优于当前最先进的语音驱动手势生成方法

Conclusion: 通过显式建模运动单元间的内在相关性，提出的分层隐式周期性学习框架能够生成更自然、协调的语音驱动3D手势，代码和模型将公开

Abstract: Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.

</details>


### [255] [Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels](https://arxiv.org/abs/2512.13142)
*Anika Sharma,Malavika Mampally,Chidaksh Ravuru,Kandyce Brennan,Neil Gaikwad*

Main category: cs.AI

TL;DR: LLMs无法真正理解复杂的心理生理现象，在堕胎污名问题上表现出系统性理解缺陷，缺乏多层次连贯性


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地介入污名化的健康决策，需要评估它们是否真正理解复杂的心理和生理现象，特别是在人们难以言说的领域

Method: 使用经过验证的个体层面堕胎污名量表（ILAS），在五个领先的LLMs上系统测试627个不同人口统计学背景的人物角色，进行多层次分析

Result: 模型在所有层面都未能通过真正理解的测试：高估人际污名、低估认知污名、假设统一的社区谴责、引入人类验证数据中不存在的人口统计学偏见、错过经验验证的污名-保密关系、在理论建构中自相矛盾

Conclusion: 当前的对齐方法只能确保适当的语言表达，但不能保证多层次连贯理解。AI安全在高风险情境中需要新的设计（多层次连贯性）、评估（持续审计）、治理监管（强制审计、问责制、部署限制）方法，以及在理解人们无法言说内容至关重要的领域提高AI素养

Abstract: As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.

</details>


### [256] [MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations](https://arxiv.org/abs/2512.13154)
*Emre Can Acikgoz,Jinoh Oh,Joo Hyuk Jeon,Jie Hao,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

TL;DR: MAC是一个多智能体澄清框架，通过战略性地管理澄清对话来解决用户模糊请求，在MultiWOZ 2.4上将任务成功率提高了7.8%，并将平均对话轮次从6.53减少到4.86。


<details>
  <summary>Details</summary>
Motivation: 对话代理经常遇到模糊的用户请求，需要有效的澄清才能成功完成任务。虽然多智能体架构在现实应用中越来越受欢迎，但模糊性解决仍然是一个关键且未被充分探索的挑战——特别是在不确定或不完整的用户输入情况下，确定哪个代理应该发起澄清以及代理应该如何协调行动。

Method: 提出MAC（多智能体澄清）框架，首先引入新的分类法对用户模糊性进行分类以系统指导澄清策略，然后设计能够自主协调多个代理与用户协同交互的系统。

Result: 在MultiWOZ 2.4上的实证评估表明，在两个层面启用澄清使任务成功率提高了7.8%（从54.5%到62.3%），平均对话轮次从6.53减少到4.86，通过提前获取所有必要用户信息并最小化重复。

Conclusion: 研究结果强调了主动用户交互和角色感知澄清对于更可靠的人机通信的重要性，MAC框架通过战略性的多智能体协调有效解决了用户模糊性问题。

Abstract: Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.

</details>


### [257] [SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning](https://arxiv.org/abs/2512.13159)
*Emre Can Acikgoz,Jinoh Oh,Jie Hao,Joo Hyuk Jeon,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

TL;DR: SpeakRL使用强化学习提升智能体对话能力，通过主动澄清问题提高任务完成率20.14%


<details>
  <summary>Details</summary>
Motivation: 当前人机协作主要是单向的，用户发出指令，智能体直接响应而不寻求必要澄清。随着智能体能力增强，需要更主动的对话参与来澄清用户意图、解决歧义并适应变化环境。现有工作未充分利用语言模型的对话能力，将智能体优化为更好的跟随者而非有效的发言者。

Method: 提出SpeakRL强化学习方法，通过奖励智能体与用户的主动交互（如必要时提出正确的澄清问题）来增强对话能力。创建SpeakER合成数据集，包含任务导向对话的多样化场景，任务通过交互式澄清问题解决。系统分析对话主动性的奖励设计，提出平衡提问与行动的奖励公式。

Result: 实验评估显示，该方法在任务完成率上比基础模型提升20.14%绝对改进，且未增加对话轮次，甚至超越更大的专有模型，证明了以澄清为中心的用户-智能体交互的潜力。

Conclusion: SpeakRL方法通过强化学习有效提升智能体的对话主动性，在任务完成率上取得显著改进，展示了主动澄清对话在人机协作中的重要性。

Abstract: Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.

</details>


### [258] [Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows](https://arxiv.org/abs/2512.13168)
*Haoyu Dong,Pengkun Zhang,Yan Gao,Xuanyu Dong,Yilin Cheng,Mingzhe Lu,Adina Yakefu,Shuxin Zheng*

Main category: cs.AI

TL;DR: Finch是一个用于评估AI代理在真实企业级财务工作流程中的基准测试，包含172个复合工作流和384个任务，基于Enron等真实企业数据，测试结果显示当前前沿AI系统表现有限。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统评估多集中在简单任务上，缺乏对真实企业环境中复杂、多模态、长时程工作流程的评估。企业财务工作涉及数据录入、格式化、搜索、计算、建模、验证、可视化等多种任务的交织，需要更贴近实际的专业基准。

Method: 1) 从Enron（15,000个电子表格和500,000封邮件）和其他金融机构的真实企业工作空间中收集数据；2) 采用LLM辅助发现与专家标注相结合的工作流构建流程：LLM辅助从真实邮件线程和电子表格版本历史中推导工作流，专家验证；3) 专家进行细致标注，耗时超过700小时。

Result: 构建了包含172个复合工作流、384个任务、1,710个电子表格（2700万单元格）以及PDF等工件的基准。评估结果显示：GPT 5.1 Pro耗时48小时仅通过38.4%的工作流，Claude Sonnet 4.5仅通过25.0%。案例研究进一步揭示了真实企业工作流对AI代理的挑战。

Conclusion: Finch基准揭示了当前前沿AI系统在处理真实企业财务工作流程方面的显著局限性，突显了开发能够应对复杂、多模态、长时程企业工作AI代理的必要性。该基准为AI系统在企业环境中的实际应用评估提供了重要工具。

Abstract: We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.
  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.
  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.

</details>


### [259] [Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection](https://arxiv.org/abs/2512.13240)
*Zihui Zhao,Zechang Li*

Main category: cs.AI

TL;DR: RPO通过引入外部模型生成的反思提示，增强DPO中的偏好对比信号，解决标准DPO因选择与拒绝响应相似导致的弱学习信号问题，实现更高效的对齐训练。


<details>
  <summary>Details</summary>
Motivation: 标准DPO中，选择和拒绝响应来自同一策略，往往包含相似错误且KL散度小，导致学习信号弱、收敛慢且不稳定。需要增强偏好对的对比性来改善对齐效果。

Method: RPO框架：1）使用外部模型识别幻觉来源并生成简洁反思提示；2）基于提示构建具有更强对比性的在线策略偏好对；3）在DPO范式中融入提示引导的反思机制。

Result: 理论证明：基于提示的条件化通过互信息增加期望偏好边际，提高样本效率。实证结果：RPO以更少训练样本和迭代实现更优对齐，显著降低幻觉率，在多模态基准上达到SOTA性能。

Conclusion: RPO通过引入反思提示增强DPO的对比学习信号，解决了标准DPO的局限性，实现了更高效稳定的模型对齐，为偏好优化提供了新框架。

Abstract: Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.

</details>


### [260] [MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data](https://arxiv.org/abs/2512.13297)
*Zhenghao Zhu,Chuxue Cao,Sirui Han,Yuanfeng Song,Xing Chen,Caleb Chen Cao,Yike Guo*

Main category: cs.AI

TL;DR: 提出了首个医学多模态模型洞察力评测基准MedInsightBench，包含332个精心设计的医学案例，用于评估LMMs在医学数据分析中的洞察发现能力，并提出了MedInsightAgent框架来提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于评估大型多模态模型在医学数据分析中洞察发现能力的高质量数据集，医学数据分析需要从复杂的多模态数据中提取深度见解以改善患者护理、提高诊断准确性和优化医疗操作。

Method: 提出了MedInsightBench基准，包含332个精心策划的医学案例，每个案例都标注了精心设计的洞察。同时提出了MedInsightAgent框架，包含三个模块：视觉根因查找器、分析洞察代理和后续问题组合器。

Result: 现有LMMs在MedInsightBench上表现有限，主要因为难以提取多步骤的深度洞察和缺乏医学专业知识。MedInsightAgent能够显著提升通用LMMs在医学数据洞察发现方面的性能。

Conclusion: MedInsightBench填补了医学多模态模型评估的空白，揭示了现有模型的局限性，而MedInsightAgent框架为解决这些挑战提供了有效途径，有望推动医学AI的发展。

Abstract: In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.

</details>


### [261] [Error-Driven Prompt Optimization for Arithmetic Reasoning](https://arxiv.org/abs/2512.13323)
*Árpád Pándy,Róbert Lakatos,András Hajdu*

Main category: cs.AI

TL;DR: 提出基于错误驱动的优化框架，提升小型语言模型在表格数据算术推理任务上的性能，使4B参数模型在隐私合规环境下超越GPT-3.5 Turbo


<details>
  <summary>Details</summary>
Motivation: 工业领域（金融、医疗等）需要能在安全本地环境中处理表格数据算术运算的AI助手，但现有小型语言模型在算术任务上存在基础性限制，需要在不泄露敏感信息的前提下提升性能

Method: 采用错误驱动的优化框架，通过聚类错误预测来迭代优化提示规则，增强代码生成代理（CGA）在小型语言模型（Qwen3 4B）上的算术推理能力

Result: 错误驱动方法显著提升性能，将模型准确率提升至70.8%，使小型模型在隐私合规环境下超越GPT-3.5 Turbo

Conclusion: 通过系统性的错误驱动提示优化，而非昂贵的微调，可以开发出可靠、可解释且工业可部署的AI助手，使小型模型在隐私保护场景下超越大型模型

Abstract: Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.

</details>


### [262] [Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection](https://arxiv.org/abs/2512.13374)
*Francesca Da Ros,Luca Di Gaspero,Kevin Roitero*

Main category: cs.AI

TL;DR: LLMs能学习组合优化问题的结构信息，其隐藏层表示与传统特征提取方法在算法选择任务上表现相当


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究探索LLMs生成或求解优化模型，但对其学习问题结构或算法行为的能力了解不足。本研究旨在探究LLMs如何内部表示组合优化问题，以及这些表示是否能支持下游决策任务。

Method: 采用双重方法：1) 直接查询评估LLMs提取实例特征的能力；2) 探测分析检查这些信息是否隐含编码在隐藏层中。探测框架扩展到每个实例的算法选择任务，评估LLM衍生表示是否能预测最佳求解器。实验涵盖四个基准问题和三种实例表示。

Result: LLMs展现出中等能力从问题实例中恢复特征信息（通过直接查询或探测）。值得注意的是，LLM隐藏层表示的预测能力与传统特征提取方法相当，表明LLMs捕获了与优化性能相关的有意义结构信息。

Conclusion: LLMs能够学习组合优化问题的结构信息，其内部表示对算法选择等下游任务具有实用价值，为LLMs在优化领域的应用提供了新视角。

Abstract: Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.

</details>


### [263] [Differentiable Evolutionary Reinforcement Learning](https://arxiv.org/abs/2512.13399)
*Sitao Cheng,Tianle Li,Xuhan Huang,Xunjian Yin,Difan Zou*

Main category: cs.AI

TL;DR: DERL是一个可微分的进化强化学习框架，通过元优化器自动发现最优奖励信号，在复杂推理任务中实现自主智能体对齐。


<details>
  <summary>Details</summary>
Motivation: 强化学习中设计有效的奖励函数是一项核心且困难的任务，特别是在复杂推理任务中。现有的自动奖励优化方法通常将奖励函数视为黑盒，无法捕捉奖励结构与任务性能之间的因果关系。

Method: 提出可微分进化强化学习（DERL），这是一个双层框架：元优化器通过组合结构化原子原语进化奖励函数（元奖励），指导内层策略训练。关键创新在于元优化是可微分的，将内层验证性能作为信号通过强化学习更新元优化器，近似任务成功的"元梯度"。

Result: 在三个领域验证：机器人代理（ALFWorld）、科学模拟（ScienceWorld）和数学推理（GSM8k、MATH）。DERL在ALFWorld和ScienceWorld上达到最先进性能，显著优于依赖启发式奖励的方法，特别是在分布外场景中。

Conclusion: DERL成功捕捉任务的内在结构，实现无需人工干预的自我改进智能体对齐，为自动奖励设计提供了有效的可微分进化框架。

Abstract: The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.

</details>


### [264] [neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings](https://arxiv.org/abs/2512.13481)
*Ojas Pungalia,Rashi Upadhyay,Abhishek Mishra,Abhiram H,Tejasvi Alladi,Sujan Yenuganti,Dhruv Kumar*

Main category: cs.AI

TL;DR: 研究发现某些大语言模型在特定情境下会表现出类似嫉妒的行为模式，不同模型在竞争性偏好上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地在协作和竞争工作流程中代表人类行动，需要评估它们是否以及在什么条件下表现出类似嫉妒的偏好，这对多智能体系统的安全性和设计至关重要。

Method: 采用两种实验场景：(1) 点数分配游戏，测试模型是否试图胜过同伴；(2) 职场环境，观察在不公平认可情况下的行为。

Result: 某些LLM表现出明显的嫉妒模式，但不同模型和情境间差异很大。GPT-5-mini和Claude-3.7-Sonnet倾向于拉低同伴以平衡结果，而Mistral-Small-3.2-24B则专注于最大化自身收益。

Conclusion: LLM存在嫉妒行为的证据表明，在多智能体系统设计中需要考虑竞争性倾向作为安全和设计因素。

Abstract: Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.

</details>


### [265] [Defending the Hierarchical Result Models of Precedential Constraint](https://arxiv.org/abs/2512.13505)
*Henry Prakken,Wijnand van Woerkom*

Main category: cs.AI

TL;DR: 本文回应Bench-Capon对层次案例推理模型的批评，认为其误解了中间因素与维度的区别，并证明van Woerkom的维度层次结果模型能避免这些批评


<details>
  <summary>Details</summary>
Motivation: 近年来提出的层次案例推理模型受到Bench-Capon批评，认为这些模型在某些情况下会产生错误结果，特别是无法处理中间因素被不同基础因素以不同强度确立的情况。本文旨在回应这些批评，为van Woerkom的结果层次模型辩护。

Method: 通过分析Bench-Capon的批评案例，指出其将中间因素误解为维度，然后应用van Woerkom的维度层次结果模型重新分析这些案例，展示该模型能够正确处理中间因素的不同确立强度问题。

Result: 论证表明Bench-Capon的批评源于对中间因素和维度的混淆，van Woerkom的维度层次结果模型能够避免这些批评，正确处理中间因素被不同基础因素以不同强度确立的情况。

Conclusion: Bench-Capon对层次案例推理模型的批评可以通过正确区分中间因素和维度来解决，van Woerkom的维度层次结果模型为处理先例约束提供了更准确的框架，能够应对中间因素确立强度的复杂性。

Abstract: In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.

</details>


### [266] [MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph](https://arxiv.org/abs/2512.13510)
*Linjie Mu,Yannian Gu,Zhongzhen Huang,Yakun Zhu,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: MedCEG框架通过关键证据图监督医学语言模型的推理过程，提升临床推理的可靠性和有效性


<details>
  <summary>Details</summary>
Motivation: 现有医学推理模型虽然性能不错，但推理过程的临床可靠性有限，准确性和有效性在训练中常被忽视，需要更透明、可验证的推理路径来支持临床决策

Method: 提出MedCEG框架：1）构建具有挑战性的临床病例数据集；2）为每个样本算法构建关键证据图（CEG）表示高质量可验证的推理路径；3）引入临床推理过程奖励，评估节点覆盖、结构正确性和链条完整性

Result: MedCEG在性能上超越现有方法，同时产生临床有效的推理链，在可靠医学AI推理方面取得实质性进展

Conclusion: MedCEG通过显式监督推理过程，增强了医学语言模型的临床推理能力，为可靠医学AI推理提供了有效框架

Abstract: Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.

</details>
